alpha	0.5
theta	2.0
total number	3.946757530608836
occurrence frequencies	2.692551012815212
method significantly	2.0853665562579624
processing community	2.427896658773757
precision value	2.1508323275880765
time frame	2.136143821660751
one need	2.2776740307447065
function mapping	2.1771387933279884
following aspects	2.34443972705697
deterministic finite	2.495713266711637
previous stage	2.4586300081102097
event sequence	2.019557844009582
various heuristics	2.272505380879168
evaluation results	3.3583599762335776
discourse cues	2.0042986332519286
single template	2.0763533583416773
different news	2.230524125991308
input symbol	2.244292585232636
horizontal line	2.342099165994403
ap newswire	2.1160084991347907
coreference annotation	2.067433171659904
complete structure	2.12319209177683
data file	2.0805478791345156
formal devices	2.037402853550243
task completion	2.1197601206946324
last rule	2.3448347819209543
pruning algorithm	2.052695869512826
techniques could	2.0727410895435407
occur often	2.2169936022425736
using wordnet	2.956535967176514
investigate methods	2.166102255087602
occur simultaneously	2.166102255087602
different clauses	2.0376102740271858
2we use	2.109437912434099
test document	2.3121986548750417
whose output	2.487920588919004
people understand	2.1290482690107417
corpora used	2.81592773686255
unseen test	2.769305852044371
using bayes	2.5735622558404367
tagging process	2.5906863678210494
possible fillers	2.2654812855760516
specific translation	2.0919612108326557
among discourse	2.0264528217305253
text given	2.216702135240303
departure time	2.220701536382727
examples taken	2.3879279742060486
handle complex	2.2314006437834086
good idea	2.260402086792694
interpretation must	2.099458264360342
also discusses	2.067747107964575
prevent us	2.3763414719464064
chunk parser	2.068014043715559
related concepts	2.6397330970806463
classification task	3.271426322675465
main modules	2.435632598873795
different contexts	3.159503868502606
features indicate	2.067081229293324
third case	2.5234052553525608
different occurrences	2.142909974697701
examples discussed	2.3109116208105
complex relations	2.1743914143962
one context	2.5146247686420895
graphical models	2.3387507053816545
word phrase	2.1972748577926797
examples per	2.023617009900904
graphical model	2.337466861226445
information content	3.0236609356412436
verb semantics	2.293917114524819
different resources	2.0497018247949423
detailed evaluation	2.307382090261814
several attempts	2.5270204646552323
pick one	2.159746288077376
resolution task	2.124172121389049
place predicate	2.39788611458522
two arcs	2.0563784920465475
two parts	3.5338327690630464
measure similarity	2.1479184330021646
system automatically	2.5705404235780493
meaning components	2.2270336584012744
high speed	2.103579684462196
relationships among	3.199702582267926
various values	2.289161918175575
relations involved	2.055118260693857
intervening material	2.1112053387580056
candidate feature	2.051769134547804
document using	2.3922788066689504
sufficient training	2.5276537243274797
existing speech	2.0312646501273113
closer examination	2.5002982171432886
connected speech	2.1022820411872436
character words	2.2761908901350303
briefly mentioned	2.014014536653706
evidence provided	2.1810793546234883
efficient approach	2.061212299166531
experimental procedure	2.0429058304291114
later stages	2.729577235349204
possible ways	2.9234930269651795
different dictionaries	2.130117553808244
syntactic object	2.0843611433825657
term extraction	2.561127559489011
discourse analysis	3.0312390301072902
rightmost column	2.11638795219105
next version	2.160738859182355
section 23	2.849608907693104
state transition	2.8005128810605204
possible pairs	2.6367822611852167
evaluation data	2.8192818720965476
text comprehension	2.192729087350867
contain many	2.974569984526262
among features	2.114299018453181
provide insight	2.3879279742060486
representation called	2.3231354352745397
complete coverage	2.271753499640152
indexing process	2.0682699365126793
various measures	2.0330146025299998
e sentence	2.075729255121515
sophisticated models	2.285913086717917
implementation uses	2.487920588919004
lines show	2.230524125991308
rule would	2.6301273808610395
highest number	2.1797375715874177
information encoded	2.800887357997906
describes two	2.50352395005998
semantic elements	2.3060965980727834
qa systems	2.701022854359016
discourse structures	2.782077299751296
vocabulary item	2.014014536653706
highest correlation	2.1270872663745455
single phrase	2.481877068750121
various forms	2.716064628499455
trees associated	2.2735102839729096
database query	2.9197380916394087
linguistic objects	2.6742686670877664
full grammar	2.261744351334622
german language	2.412877092827686
syntactic considerations	2.083497359221134
expressions using	2.2434846698300097
information derived	2.803617976206151
100 word	2.0490697481769016
slot fills	2.1275183696479623
type constraints	2.3165445772463
based semantics	2.3312450102132827
threshold used	2.0577375124403856
research purposes	2.599560511697465
perceptron algorithm	2.101411198199213
like x	2.045521226679158
sensitive rules	2.161981932055161
four stages	2.096756183704444
driven algorithm	2.035258167298517
retrieving documents	2.0853665562579624
likes mary	2.125474724430905
bilingual data	2.1588753720085525
head information	2.3209162848731717
written texts	2.8491039522542776
differ substantially	2.135844661161516
answering task	2.164563612984502
cognitive process	2.114707400235429
first input	2.190982236626934
finite verb	3.0115103427923238
every tree	2.2820453393980955
statistical approaches	3.155876266803932
posterior probabilities	2.176357883153759
case b	2.0215688023747362
input must	2.222406898745732
weighting scheme	2.784260539954849
segmentation performance	2.1731292406569978
f measure	2.4213575161480003
final output	2.821736904091149
verb argument	2.357408699570028
pos features	2.250169515022302
word given	2.747810767174429
parser described	2.7087511698067495
grammar defined	2.146224553829869
computer model	2.1518980086114476
many languages	3.1719070155548588
grammar developed	2.3922788066689504
extraction algorithms	2.1813446704867467
system achieves	2.828676933629126
tree pairs	2.091378946668589
small class	2.102902855044873
graduate fellowship	2.2062448606993037
grouping words	2.0918364857418936
wang et	2.3334813960747285
corpus contained	2.475593129161638
unordered set	2.413045927629673
transition network	2.862848845698216
unique path	2.015795287891615
character encoding	2.156307938487117
crucial aspect	2.067747107964575
one system	2.856168148858
single classifier	2.2243148225188545
system parameters	2.2936614707952163
time proportional	2.1432314806483994
final parse	2.363764773639148
knowledge could	2.3097823680289724
rich language	2.045229961808099
second phase	2.7953454920964123
modeling toolkit	2.45078770424615
basic premise	2.0376102740271858
driven parser	2.0186929054100804
evaluation may	2.0817120252117016
several meanings	2.2328679513998626
test sentences	3.2824908508153903
jurafsky et	2.0840523766681054
process cannot	2.0376102740271858
air force	2.242335071601555
training documents	2.5494112403561062
results indicate	3.42040141030505
free backbone	2.293110136013148
spontaneous spoken	2.3106335854714524
new evaluation	2.4006907243796993
technique based	2.443147787685106
nominal head	2.191114775755527
acoustic model	2.6386445207168636
general task	2.2492578335740094
current paper	2.6294821515607194
pairwise comparison	2.1295010830873577
primary source	2.4197547133270083
following terms	2.067747107964575
second kind	2.5474579294863355
nlp group	2.2891619181755756
parses per	2.137129822393408
new approach	3.0655026177644205
speech features	2.135557076105693
sample input	2.026260250766906
carlson et	2.07973054600461
initial list	2.116591742190463
based qa	2.0281677197784704
500 words	2.644235456675108
appendix b	2.670938095697846
system accuracy	2.3207245000618566
top ranking	2.060640253035934
sense 2	2.3044799136613
tagset used	2.0532484523230123
single concept	2.412081342249917
following simple	2.7458258715558244
indefinite nps	2.1852845559069003
present context	2.394949830547247
two theories	2.1187241243259916
prolog terms	2.159713448891175
system starts	2.33582780054359
lead one	2.1290482690107417
000 tokens	2.4389033006295895
word clusters	2.238628713744975
feature used	2.2112071020073643
computer assisted	2.329938689142474
increase precision	2.1774937848124125
central element	2.0965741974051317
textual corpora	2.297303831999146
also applies	2.683832253072309
entity information	2.1048578709586345
verbal predicates	2.0380249892713826
complex systems	2.3866967846580778
limited time	2.3107061751596207
sections 1	2.272505380879168
several nlp	2.200598690831077
performance drop	2.039007283873737
languages use	2.0312646501273113
di eugenio	2.404654186617533
patterns described	2.159746288077376
lower one	2.166102255087602
interesting insights	2.022402441477414
processing efficiency	2.1340952951218273
system need	2.0376102740271858
computational theories	2.0312646501273113
global optimum	2.2325858671848176
two events	2.762810024706934
transitive relation	2.252283339430644
text editor	2.3207834939032357
briefly mention	2.067747107964575
user might	2.7764097235537366
single elementary	2.012261138619913
definition 5	2.3300721612997837
reasonable approximation	2.2571860900326115
segment boundaries	2.6996195250192274
cannot explain	2.2380656204269957
tag information	2.4134550463226545
e h	2.0985593775545333
node represents	2.764081294752176
way using	2.1072480253375243
significant difference	3.1972813388627013
different groups	2.554932902968686
several applications	2.5275580183983113
arda aquaint	2.075729255121515
data preparation	2.251824255316957
third set	2.358255776160617
general form	3.0822039944199946
made use	3.0609780538064735
items within	2.159746288077376
example 2	3.1166463624985927
test cases	2.4893076345052223
noun followed	2.060145782650642
quintus prolog	2.509365157065348
word number	2.081402555168837
two machine	2.2368067747493123
number 1	2.8512264643729073
entire space	2.072290098419245
linguistic performance	2.2588879405091196
question marks	2.2934572585866873
current performance	2.1852404006823356
single syntactic	2.3332911335775464
mechanism described	2.3981332997989453
resulting tree	2.639874996724513
verb classes	2.7039155230147744
summaries produced	2.1777377461712746
based representations	2.3682011991121854
verbs taking	2.2169980947988455
possible sets	2.014014536653706
certain sequences	2.067747107964575
machine learning	3.8533051491150085
likelihood estimation	2.982473253698977
coherence relation	2.128242377613641
share certain	2.1414753566437956
recognition algorithms	2.5028035508844177
main areas	2.1946340157728974
whose probability	2.2916287452490876
parser could	2.578841910708236
various modules	2.341285221188147
complex structures	2.801135520618405
current training	2.099458264360342
also report	2.876370696643767
primary means	2.1023887620661283
subject position	3.0443150858752794
text format	2.278461809175595
particular word	3.276252778311528
qa task	2.1776912319867474
system description	2.133413667980742
annotated sentence	2.233997204181393
sole purpose	2.089026915173973
tagging error	2.1485465654330245
features relevant	2.2965916473768724
please refer	2.4583328736209746
overall translation	2.282398402847935
utterances containing	2.155898506665032
features corresponding	2.2380656204269957
formalism based	2.2020202169645566
acoustic information	2.4404222167296337
idf weights	2.043597524575342
feature descriptions	2.1042770249682574
techniques based	2.7290337691271698
usual sense	2.227249320716272
algorithm applied	2.1353034784701492
improve recognition	2.412417536370418
spread across	2.230590646944645
new corpus	2.772998879686392
journal text	2.4047215428666187
exact string	2.140860171621032
single number	2.2384438317931403
little effort	2.4829696016728517
information found	2.72037572348466
concept hierarchy	2.5584001094686024
psycholinguistic literature	2.1813446704867467
complete analyses	2.022410241232288
np object	2.365219785515711
state verbs	2.120473489839857
algorithm gives	2.2959583628072817
often correspond	2.1479184330021646
methods using	2.6693221622082706
following text	2.6494837486301566
present system	2.284664116662711
computational framework	2.4875508749451836
moore et	2.044877827032594
earlier versions	2.9010119388112083
software tools	2.669623770991574
basic method	2.204741649140805
relative salience	2.1331533133181058
approach outlined	2.242793580526907
requested information	2.2280813991508994
following pair	2.5213477264853457
given domain	2.961796537570582
linguistic approaches	2.2339270481759206
individual features	2.562015908199497
word lattice	2.6615135295694237
two information	2.0490697481769016
direct application	2.228686201933606
material used	2.067747107964575
tree node	2.600694817128814
taking place	2.3359535348879894
x v	2.241067356454239
discuss related	2.406719372930616
sensitive languages	2.3446177174899105
agreement rate	2.2201941452648146
network representation	2.3069171037267924
something new	2.10637101627493
various conditions	2.14622455382987
term may	2.3146254738630754
selection procedure	2.568565997800362
system needs	2.747384888202724
improved accuracy	2.453255888575513
opposite direction	2.4465713188722917
across speakers	2.122563170379007
150 words	2.022410241232288
natural sciences	2.55061890816116
languages might	2.1479184330021646
directly compare	2.3823135849845682
distributions p	2.360227436801601
search spaces	2.0619692547190884
design principles	2.357947172648072
many problems	2.939135236276392
problems described	2.3605881269336804
grammar checker	2.0317664222867107
vocabulary speech	2.5890896120266826
relation types	2.4439996035433964
candidate sentences	2.3146900534708363
probable alignment	2.034620821190689
appropriate form	2.286725248739412
patterns could	2.1563079384871173
constituent analysis	2.1177973539400865
program uses	2.399252281275852
000 characters	2.1131099776164364
000 instances	2.04852975583991
tool used	2.1222463152036486
different orders	2.506283241256022
second occurrence	2.2894442892901377
query system	2.557674227671344
longer ones	2.481655964382881
example word	2.016336526582215
analysis tree	2.246563939689154
information seeking	2.2821676628210232
subtle distinctions	2.130459427106893
within text	2.045229961808099
grammatical rules	2.816890144358437
lexical insertion	2.089312456705218
extract useful	2.159746288077376
explicit mention	2.065429826730168
semantic links	2.312027026625475
know something	2.086884416965048
system outperforms	2.461724203020167
4 hours	2.012347776929124
two heuristics	2.1119430342456553
lisp function	2.1275452238699737
first sentence	3.48364762352313
model achieves	2.6666719422735268
sag 1994	2.0840523766681054
several senses	2.242793580526907
category np	2.312928660106194
constituent structures	2.703697140113397
another clause	2.1499148428335464
terms may	2.204785856351918
project aims	2.172274429303222
related words	3.0301451940236785
system combines	2.3229739757638654
small example	2.200598690831077
phrases could	2.122246315203648
model parameters	3.203800917845485
grammar described	2.0376102740271858
central topic	2.060145782650642
lexical data	2.8113132590777483
algorithms like	2.116591742190463
system believes	2.124830893639583
categorial grammar	3.0720228233266496
complex task	2.5504966689714355
input consists	2.3567860333521553
like language	2.135844661161516
computational grammars	2.0965741974051317
following figure	2.49265343807072
global structure	2.352535185621204
parameter estimates	2.3360720980752725
word classes	3.2981286674805474
two lexicons	2.022410241232288
selected documents	2.2074757805024428
following process	2.183647914993236
elements like	2.1943253913209606
order logic	2.9257473767522684
stemming algorithm	2.174190173079227
accurate word	2.0844862373503052
highest accuracy	2.7886286414876316
numerical values	2.412372144625752
word like	2.7684389300727967
conditional random	2.8686575468703803
three noun	2.022410241232288
prague school	2.096909117910051
new phrase	2.2787695143109565
higher probability	2.5541098734079872
test results	2.847566613758458
software development	2.093309546163501
two case	2.046031088376705
three times	2.806194465768446
implemented systems	2.211790418408607
provides access	2.2421869127123264
computational lexicons	2.2722047992398653
dynamic nature	2.0232353778385734
utterances whose	2.1485183747988197
words except	2.264070934029513
first discuss	2.6927179621980586
possible alternative	2.4511389563643227
two values	2.9188534530291284
syntactic disambiguation	2.4128313190853135
questions could	2.067081229293324
local ambiguities	2.16746374282026
classifier uses	2.1816335351833818
document frequencies	2.0278097453671355
marginal probability	2.198250699280892
lexicon entries	2.8994158919903374
large subset	2.2992838842823984
rule n	2.1312839727750825
direct objects	2.745339820448986
corpora contain	2.2169936022425736
text retrieval	3.0103864203724426
c given	2.0490697481769016
framework allows	2.3725247035269392
similarity among	2.306748976471099
features 1	2.0235371114279648
information theory	2.7769436393432194
parsing strategies	2.7428708510207396
user says	2.3627526679697404
bilingual dictionary	3.0597946262574833
dictionary contains	2.750267341285829
lexical material	2.5649385922905026
identification module	2.0621571607676303
measures used	2.529105055367556
particular grammar	2.6322563338953717
candidate antecedents	2.0572652873397135
slots filled	2.19110807291794
similar system	2.2631802623080817
behaves like	2.384227704698257
remains unchanged	2.260365882857534
preliminary results	3.0957980647984065
bilingual text	2.2048944901810263
key feature	2.2575385024862005
additional constraint	2.6496278664583253
required information	2.562836111735062
two measures	2.9101661252444044
template matching	2.344914899325536
nominal compounds	2.452414937336399
new variables	2.063693154062279
define two	2.717344743337547
online dictionaries	2.0499337467777923
two propositions	2.3939296731070545
small size	2.8097565769452406
tile language	2.101222560614464
definition 3	2.8042434821350284
smallest set	2.439868148716185
arrows indicate	2.314625473863075
next phase	2.6263731694518686
chinese sentences	2.669492040168392
implementation using	2.014014536653706
dialogue manager	2.7448530668440783
local decisions	2.0514353566256425
4 summarizes	2.5302215052732078
key ideas	2.244715125892863
frequency within	2.1275552563638707
cardinal number	2.058162491400105
information state	2.1330544038272317
structural information	3.121464003551309
r b	2.191341477013969
2004 senseval	2.3644803559732956
null tions	2.552387399287706
expressions within	2.164235940665368
following shows	2.2225315747955348
euclidean distance	2.6436328018222905
lexical patterns	2.4585832556822673
gain ratio	2.1987647617200046
predict whether	2.0532436584758034
calculate p	2.045229961808099
words belong	2.3271110475429713
stanford university	2.526642750486463
related problem	2.462292105818408
system consisting	2.3922788066689504
lexical heads	2.709569692688778
classification performance	2.5224268175190048
discourse coherence	2.4791604517394656
major types	2.60867513122038
bene ts	2.1042368022268714
words belonging	2.7663283036069837
three evaluation	2.067081229293324
large corpus	3.6129258081074735
test data	3.8634655842365673
data consortium	2.954684703667953
different expressions	2.345768269478816
preceding clause	2.0336370743002643
essential part	2.654437714338689
distance measure	2.7140836710806546
based rules	2.414770859487892
many utterances	2.2380656204269957
best parses	2.1036980112027397
atomic categories	2.28511301013238
specific entities	2.0381169104851677
problems arising	2.3483606540998085
components within	2.105195669894195
artificial intelligence	3.4084726255276716
work needed	2.022261218861711
larger training	2.7698438373529752
e w	2.0676459460079197
broad semantic	2.12319209177683
grammatical knowledge	2.6951593971259316
translation performance	2.4819776608959256
effective way	2.687449485678765
native english	2.599206756355478
work involves	2.414320698244546
improved system	2.2169936022425736
new concept	2.6875448315549737
evaluation process	2.668023009863143
main body	2.353676249882287
different target	2.419651923103911
structure represents	2.267991541156711
length greater	2.28182140187005
tuple g	2.1222463152036486
grammatical functions	2.913493883752193
crucial role	2.86266303933057
two states	2.627262844145707
grammar could	2.6112532300437925
full feature	2.096447361767761
structured representation	2.465813979753836
estimation process	2.3176677051503645
poor precision	2.00238351760398
information must	2.991593423426214
tag set	3.038429343517764
9 sentences	2.064890501159733
step involves	2.2823116493074065
independent speech	2.049571290936349
parser runs	2.00238351760398
therefore propose	2.2112071020073643
based formalisms	2.64083950082266
example grammar	2.361992219887629
human annotations	2.1784840626689173
correct links	2.012347776929124
rewriting rules	2.6431107761297863
e v	2.560074448450069
maximum depth	2.3162726447741377
different runs	2.2380656204269957
control strategy	2.4587328515014493
acoustic features	2.4392333242691273
human scores	2.1525656420447294
use words	2.5865641709707763
used ill	2.301436088628126
word types	2.918047082219819
rule formalism	2.4763582574960896
ef cient	2.2076650154480255
output format	2.2865069737092583
randomly select	2.5260231106536652
illustrative examples	2.380830363798011
often results	2.3644803559732956
skewed distribution	2.1543888787937475
repeat step	2.1739387095397857
alternative models	2.0074734400897967
specific instance	2.1380488015835484
class based	2.223199804725935
neural networks	2.926448027039736
organization names	2.6241704945676543
test material	2.5594220780060395
parser based	2.8469938391259317
experiment results	2.7434977601993227
new event	2.056595698920818
tags given	2.061212299166531
domain question	2.3352046159244884
component produces	2.0824927688889554
length 3	2.1975706215825945
remaining elements	2.257869825585851
compositional way	2.1380488015835484
complement clauses	2.185988736652183
particular instantiation	2.0262602507669065
events may	2.3263148914896092
word accuracy	2.7375153670487324
algorithm creates	2.0817120252117016
classes corresponding	2.0376102740271858
error occurred	2.140860171621032
applying rules	2.346889123261037
translation approaches	2.038352515476918
disambiguate among	2.0312646501273113
contains many	2.923823713785366
existing components	2.164612330119926
reliable results	2.145668076397672
possible input	2.2458302187516197
also model	2.014014536653706
evaluation shows	2.6928054474151533
future evaluations	2.0853665562579624
maximum length	2.7509867437705933
total function	2.0139679414602396
system usually	2.053986137989304
hundred words	2.3271110475429713
baseline result	2.060145782650642
case roles	2.483092366018436
inflectional forms	2.2402822419343122
example output	2.086884416965048
phone models	2.2809864007672025
produce high	2.3633976741456193
significant portion	2.011603895774721
represent concepts	2.3229739757638654
yields better	2.4995406926829338
relative ordering	2.435632598873795
models contain	2.0376102740271858
attachment sites	2.2935474961173803
processing phase	2.5357266975203556
many thanks	2.833023680127851
los angeles	2.0535389105900927
phoneme conversion	2.2090181839550684
subordinate conjunction	2.000145006229941
analysis program	2.2701277776615774
surface sentences	2.180232943997303
semantic interpretation	3.532002775648297
chinese segmentation	2.0797193795405384
terminal string	2.4328723887846535
plans include	2.089026915173973
testing corpora	2.131569621150367
frame structure	2.140342763033963
assigned tags	2.007315006862137
highest ranking	2.3982267941138034
semantic relatedness	2.361960779810106
sentences include	2.089026915173973
test suites	2.104596295321506
model combines	2.022261218861711
simple method	2.9838965594196574
bayesian model	2.038352515476918
example parse	2.060145782650642
high complexity	2.075729255121515
see footnote	2.3956433384568747
1 proc	2.2328679513998626
statistical mt	2.4733870246573257
polysemous word	2.419927967212277
smoothing parameters	2.1156428254927526
final classification	2.070902079606615
computational effort	2.314625473863075
following step	2.0965741974051317
order n	2.5331578681069202
dialogue interaction	2.1242879517427893
existing rules	2.30606794696972
grammars would	2.089026915173973
linear classifiers	2.0223531762070586
statistical significance	2.9551637817044525
patterns based	2.3236155386271937
assess whether	2.286725248739412
many word	2.727691786155192
set membership	2.319063195109515
word categories	2.4737488048632414
based knowledge	2.7294404798936984
relevant word	2.21826141526814
absolute value	2.547312986373394
terms appearing	2.256863208816413
two modules	2.7767503421721753
2 indicates	2.4459101490553152
term vector	2.0782768626411174
component uses	2.3483606540998085
correct translation	2.9165800628531273
representations used	2.391315425665341
two rows	2.466954315728382
aha et	2.064310474990315
problems inherent	2.01761133185089
correct dependency	2.2601220552236354
canadian hansard	2.1816335351833818
terms related	2.099458264360342
use instead	2.0376102740271858
improves upon	2.2722558893814297
construction algorithm	2.0642284923463103
vertical axis	2.5261406633992483
different noun	2.0709020796066153
reliable data	2.0376102740271858
rules acquired	2.1596559835135904
related events	2.27639976832766
structure rules	3.143531758805716
structures whose	2.0312646501273113
overall structure	2.8917685000313216
current node	2.721635386926817
set l	2.6131631661286496
structure assigned	2.2062448606993037
explicit use	2.3448347819209543
phrase within	2.045521226679158
illocutionary force	2.3614289287719137
natural way	3.3307520652351585
features provide	2.2722558893814297
existing tools	2.207234477629015
pairwise comparisons	2.1019363531211965
incorrect word	2.2812105504549427
phrases based	2.2320627568691824
relative pronoun	2.7310439284524555
based system	3.446154827605865
second category	2.368834809141683
shorter ones	2.33582780054359
recent papers	2.332240952226321
mechanism based	2.2640709340295126
table indicates	2.3540470303697685
null another	2.4816559643828815
3 lists	2.6719027109268394
new goal	2.177338384924136
model parameter	2.4188636886992168
examples may	2.318793079863193
linguistic interest	2.1119430342456558
section show	2.022261218861711
current prototype	2.1548405799063595
sentence 5	2.0714405652862133
evaluation test	2.6442661992821703
manual evaluation	2.395702817629168
constraint violation	2.1772066539107287
frequent sense	2.67175989621925
retrieval conference	2.708517933201097
experiment also	2.1479184330021646
complex objects	2.1042904425091606
response generation	2.104788495967902
score threshold	2.007315006862137
trees used	2.242793580526907
darpa tides	2.1499148428335464
process starts	2.5768708070487776
combined system	2.4610400165809914
others use	2.067747107964575
relative prominence	2.0947271027985517
annotation errors	2.1872712019595335
third class	2.33582780054359
word tokens	3.0650162756491097
corpus sizes	2.2742878796306982
transformation based	2.2682746776391536
2 types	2.0880414073297953
likelihood scores	2.0163489964818804
perform poorly	2.521311760953824
question word	2.1901044885909866
current frame	2.0643620962409943
verb construction	2.1986952273495683
recent research	3.070290591269873
main focus	2.5248937301889613
interested reader	2.654437714338689
known example	2.352331304842637
person names	2.855638854988439
model consisting	2.0539861379893036
structural features	2.60894390794158
style parsing	2.0918364857418936
thousand words	2.56735003485172
thematic roles	2.905380027004078
questions based	2.096756183704444
underlying assumption	2.551177240893016
syntactic word	2.083679193991461
null consider	2.0817120252117016
semantic concepts	2.582961148404659
focus space	2.095489495732694
serious problems	2.3088655084602197
approach used	2.7244010250838464
conditions may	2.2047194975469235
specific parts	2.2112071020073643
features according	2.2328679513998626
training iterations	2.245962205821318
average similarity	2.1525123781596456
speech signal	2.9362993706236042
beam width	2.3720227336675395
method also	2.616322459949979
verb lexicon	2.2828915413914315
test phase	2.146842878786015
values must	2.2224068987457315
jensen et	2.1432314806484
perplexity 60	2.1572134025120344
particular kind	2.7651498820268006
first attempt	2.8683001747706225
dependency links	2.283203057623078
nlp tools	2.5635311769220257
following table	2.965675850598903
synthesis systems	2.1762259150979375
structure description	2.123414369294872
single character	2.633825359067133
table look	2.025073621873738
complex semantic	2.550825375198232
000 documents	2.4690531136601277
molecular biology	2.181881084374428
better model	2.612534244730822
verb plus	2.1739387095397857
successive sentences	2.0221951140579186
method may	2.6963607597673884
valuable discussions	2.331780823064821
conceptual information	2.5247033911344228
single tokens	2.0619463749107956
building natural	2.014014536653706
simple keyword	2.2093487881958334
translation procedure	2.1569808507858435
appropriate training	2.116591742190463
related information	2.682409035483107
formal models	2.242793580526907
null b	2.483548673600854
second type	3.1465941698709208
target texts	2.2850235669627876
additional techniques	2.060145782650642
correctly identifies	2.3162726447741377
formal model	2.5720120227075234
whose antecedents	2.008083705228559
classifiers used	2.189546252614866
words 2	2.1774937848124125
many studies	2.6864535948379773
raw data	2.7774955129183563
pronoun reference	2.126134786675915
minimum error	2.1392176903056628
real data	2.780787819180067
new classes	2.1113149637969375
contain different	2.194634015772898
principles governing	2.053986137989304
data given	2.1063881437503693
existing knowledge	2.7052822355494777
three variables	2.116591742190463
sentence set	2.134498600939419
three sections	2.5834522496061427
novel algorithm	2.0994361843150067
phrase boundary	2.4588051811176967
dictionary containing	2.546714071267395
simply means	2.314007818435079
coverage parsers	2.067081229293324
natural class	2.125470520333839
objective evaluation	2.4039418597775994
iff x	2.2107864417857894
pour une	2.0665126236063323
words co	2.3368956713930786
new parsing	2.2251427952863683
recent work	3.6428521623197687
cannot deal	2.41747566988659
actual use	2.4160706821139106
communication among	2.172274429303222
chinese data	2.1060130327826547
like structures	2.4403483572687596
alignment task	2.333563118597696
aotrr 1992	2.1023887620661283
ill figure	2.257869825585851
perform quite	2.0390072838737376
cooperative response	2.0388110413873806
worth investigating	2.2062448606993037
monolingual english	2.1222270206314313
high word	2.009868063569498
comparison among	2.1739387095397857
source node	2.095689671088669
based lexicon	2.336861779932038
information consists	2.2112071020073643
gazdar et	2.664242537840034
approach gives	2.3839874389711913
runs using	2.055633163459631
similar rule	2.012347776929124
evaluation would	2.233997204181393
special interest	2.2468139954255237
phrase translation	2.4706887959560135
figures 5	2.4403483572687596
lexical meanings	2.2842187306527597
grammar based	2.7440724079248304
large number	3.9613009807514077
semantic labels	2.517978662856289
texts taken	2.0023835176039797
current mt	2.0133763638918083
incomplete sentences	2.2949699611535226
one paragraph	2.0390072838737376
chomsky normal	2.450196781022586
position feature	2.007038640285683
generator must	2.465741416547694
state sequences	2.0863469125705048
uniform representation	2.1222463152036486
morphological features	2.829092898553656
dependency syntax	2.038288693676368
set theory	2.2311428065237457
single item	2.3388887802872658
certain property	2.0817120252117016
let q	2.3061507157754177
test text	2.4054627905800006
possible directions	2.022261218861711
linguistic significance	2.00238351760398
similar distribution	2.267991541156711
2000 words	2.3310204571264768
figure 9	3.3544966342942857
relevant data	2.542396886895901
verbal complements	2.1156428254927526
possible sentence	2.350402863076348
weight associated	2.3010208320130205
system capabilities	2.2024639441989313
word phrases	2.4165997388460214
probabilistic parsers	2.0566156493467185
upper bound	3.5293276299203624
native language	2.6416032759065255
editing operations	2.152314398954019
nlp tasks	3.269277622557766
modular fashion	2.014014536653706
use machine	2.633298839019382
tests using	2.3417181621687195
research efforts	2.902550105002544
feature combination	2.149305561135341
training sample	2.5942939203624484
knowledge obtained	2.014014536653706
human judges	3.006301378488096
analysis method	2.376591284886259
world model	2.4727599925852224
substitution operation	2.0584061175169692
whose use	2.022410241232288
literal translations	2.124245509180353
lexical ambiguity	3.1206254460477805
term research	2.1298730493372275
concept node	2.045323154888165
measuring similarity	2.079019887609717
correct interpretations	2.116591742190463
semantic structure	3.2251684851395446
machine interface	2.0774165797489923
surprising result	2.4251394527998116
dialogue turns	2.065429826730168
irrelevant documents	2.2924303915182893
different corpus	2.530028728978662
1 states	2.1353034784701492
results according	2.1774937848124125
optimization techniques	2.127375476291898
character string	2.3284032005403175
van rijsbergen	2.634253178541059
second technique	2.075729255121515
argument structure	3.4359537611989026
extensive set	2.194634015772898
three steps	3.255571228175383
certain level	2.359312030798309
many alternatives	2.0539861379893036
possessive pronoun	2.4331178930281068
three classifiers	2.106282164933443
selectional constraints	2.4759872189848853
huang et	2.396955881935088
maximum matching	2.2381022909237505
given language	3.0600747357881493
meaningful information	2.016336526582215
particular feature	2.3635492667854168
tree according	2.1479184330021646
two areas	2.2325700413801073
use human	2.0312646501273113
semantic characteristics	2.387219491506807
use either	2.515272207357606
error recovery	2.302123489603865
senses using	2.096756183704444
entropy markov	2.2110863618165526
classification accuracies	2.1050838385526642
language sentences	3.0679827955376684
candidate terms	2.3083287486224293
experiments presented	2.564564348665246
briefly consider	2.1479184330021646
automatic disambiguation	2.0920241304388747
specific events	2.2062448606993037
syntactic grammar	2.5234319361882225
larger sets	2.272505380879168
issues relating	2.0376102740271858
across corpora	2.0261492610528666
human computer	2.1192403894738625
harabagiu et	2.5850452515140123
collect information	2.0074734400897967
corpus available	2.377838240407285
set must	2.216382284267942
require less	2.2924536885149736
recall levels	2.1903796153292543
conducted experiments	2.725974467705149
generation modules	2.253114600967547
context provided	2.1331653459774356
magnini et	2.109467897337734
computational requirements	2.211790418408607
line 4	2.165115471861699
following pattern	2.1543888787937475
single characters	2.2665701662018987
markov modeling	2.0671463478739605
np whose	2.039007283873737
across multiple	2.80219025320787
consider sentences	2.166102255087602
sentence includes	2.1774937848124125
transformational theory	2.083208212411497
use information	2.8556173714341915
prototype system	2.9351158172157166
usually involves	2.200598690831077
whose length	2.4119259404210314
many patterns	2.1119430342456558
null figure	2.7071566472721313
logical connectives	2.4260324477975805
experimental evaluation	2.6871477900175957
last iteration	2.075729255121515
extracted phrases	2.0817120252117016
different parses	2.060145782650642
graphical representation	2.8320846046861274
benchmark tests	2.0460964401335864
alternative interpretations	2.5165812465498982
take precedence	2.3088417794636857
training iteration	2.094727102798552
common prefix	2.000375974042675
existing lexicon	2.0846971567714903
data provided	2.8049222732220143
first define	2.7695539783780028
subsection 4	2.05168830636842
matching patterns	2.120473489839857
potential source	2.1479184330021646
type 3	2.2659011963673708
e de	2.2387955416348815
lowest error	2.193486839445044
tree rooted	2.5182852297878124
algorithm calculates	2.00238351760398
entropy models	2.8353282860038034
third example	2.33582780054359
small training	2.8344547311082033
uniform treatment	2.3918035520880734
input words	2.731673625116253
state machines	2.870707908047366
map onto	2.421865339792819
significant effects	2.122563170379007
selection criteria	2.4941507780763406
similar documents	2.27515069652231
longest string	2.060145782650642
two facts	2.4774416006747755
limited semantic	2.0074734400897967
dictionary word	2.0520370107650927
translation technology	2.292453688514974
additional lexical	2.2323699042331526
features listed	2.0023835176039797
processing research	2.4358964112583816
large classes	2.075729255121515
different experiments	2.3540470303697685
wit h	2.4637954520051095
appointment scheduling	2.448270568958935
system proposes	2.2032833372384237
likely state	2.096756183704444
use heuristics	2.2427935805269064
structures without	2.1774937848124125
words already	2.1597096509760467
extraction process	2.910157281927368
different phrase	2.174323003243204
processing algorithms	2.415551967562868
parser cannot	2.5285613055589633
structures may	2.6217762071382875
english joint	2.1166196741856655
syntactic parse	2.9582038402376014
given rise	2.2112071020073643
verb agreement	2.772419013245531
que le	2.3976644234252293
languages must	2.086884416965048
positive weight	2.0074734400897967
content planning	2.30191663273418
infinite sets	2.1678854032223676
specific information	3.3406508990250368
100 randomly	2.014014536653706
retrieval tasks	2.4581795841219467
relations r	2.00315042964679
lexical research	2.11638795219105
system retrieves	2.2512557276899194
every phrase	2.2380656204269957
morphological structure	2.58148468269631
test shows	2.014014536653706
full use	2.080557718300707
travel expression	2.014014536653706
syntactic patterns	3.1186101520111413
cl reference	2.645229720574196
sensitive grammar	2.1229216713036014
phase one	2.0031126076405927
related features	2.3551584362026334
words wi	2.1245637044373713
two procedures	2.2891619181755747
difficult tasks	2.014014536653706
matter whether	2.3644803559732956
development center	2.075729255121515
resulting corpus	2.282311649307406
sets using	2.335904293904669
viterbi algorithm	3.141981616821504
several works	2.172274429303222
use case	2.075729255121515
new categories	2.1539136878224143
linguistic basis	2.105322206137931
correct structure	2.2720616231260307
briefly review	2.800134991754313
statistical method	2.8842491747040566
n candidates	2.0157573240154925
lambek calculus	2.20729887017636
rich semantic	2.2976894783503656
unbounded dependency	2.228650047379652
using corpus	2.425593477889689
theoretical framework	2.876217273843913
performance evaluation	2.593979670821686
candidate pair	2.171685648350712
logical form	3.263579455973552
additional structure	2.1774937848124125
extraction applications	2.167252082381521
type definition	2.428632698533202
one considers	2.797546351031331
scope relations	2.1429137696693727
subject relation	2.0423259728447545
hybrid systems	2.2010503919436886
dice coefficient	2.2129777287932484
whose subject	2.273017465702768
roles assigned	2.012798451915916
generate several	2.1774937848124125
technical sense	2.0376102740271858
programming search	2.0887570102668747
uniform framework	2.0995450879838167
another interpretation	2.2112071020073643
foreign language	3.0482708025860505
select relevant	2.0699983678896388
processing work	2.0539861379893036
specific heuristics	2.1432314806484
selection method	2.541726839229071
error message	2.3641694683556356
dramatic increase	2.200598690831077
initial configuration	2.124245509180353
dependency relationship	2.251383797170349
selection mechanism	2.359317980477207
task domain	2.7162814527926766
object whose	2.045229961808099
structure provides	2.0733804997901433
multiple sentences	2.5922979619571165
brief review	2.2427935805269064
iterative process	2.5772487331134926
intermediate steps	2.0376102740271858
problem using	2.551177240893016
naval ocean	2.0157573240154925
data like	2.049069748176902
general interest	2.2380656204269957
certain properties	2.6426504149258223
important observation	2.3097823680289724
thanks go	2.834647584112479
important consequence	2.200598690831077
important concept	2.1773123985040854
specific phenomena	2.1543888787937475
real world	3.2740590897967725
use one	2.895724945976608
electronic dictionaries	2.410414775643157
tipster program	2.3377240116340143
make mistakes	2.338888780287266
one edge	2.2777975658644114
hyponymy relation	2.085864173944537
word group	2.215382437152059
basic unit	2.8451761856912134
direct relation	2.24825378073324
previous experiments	2.8957592626010156
entire structure	2.079019887609717
turing machine	2.406356737421012
best accuracy	2.620425154071785
perform experiments	2.3605881269336804
basic idea	3.569964325934749
clause types	2.0643620962409948
relationship holds	2.135844661161516
recognition hypotheses	2.0288959212522752
training utterances	2.150566681744153
based pos	2.1210870929251433
phrase followed	2.210786441785789
uppercase letters	2.185240400682336
rules directly	2.183647914993236
two restrictions	2.053986137989304
acoustic signal	2.5038669729982717
acquire information	2.2074757805024428
processing would	2.242793580526907
5 reports	2.3398800045580104
traditional information	2.37858059655019
specific case	2.5916675829755125
phrases cannot	2.135844661161516
analysis suggests	2.3981332997989453
three classes	3.0226935053786694
one minute	2.1120451055728906
tokens within	2.0074734400897967
every concept	2.1245637044373717
short words	2.0545945929659197
speech dialogue	2.231952224839077
following expression	2.357121432847829
internal organization	2.0074734400897967
probability greater	2.0376102740271858
becomes available	2.270613957971308
fixed set	2.7033849016234375
knowledge provided	2.200598690831077
larger numbers	2.0918364857418936
4 characters	2.272505380879168
makes use	3.7383092573675163
specific values	2.501400582953832
recognition process	2.8514473992878546
speaker dependent	2.1581868608430588
key aspects	2.178653004950429
becomes one	2.1543888787937475
generator produces	2.293917267737899
performed experiments	2.8012141794250303
rules extracted	2.155589295660503
whether x	2.0817120252117016
minutes per	2.3623688781715884
validation results	2.0312646501273113
gaizauskas et	2.059213640580528
raw text	2.9971192198731442
head driven	2.086884416965048
various possibilities	2.305458956322113
possible constituents	2.1295694626022152
different classifiers	2.531583007916822
real text	2.5677918185454667
processing strategies	2.526886590495809
nlp problems	2.6732493168523073
states government	2.186761001118405
acquisition bottleneck	2.3410112315031366
model adaptation	2.0753698875758126
brief overview	3.0319269135376414
different event	2.014014536653706
trees contain	2.045521226679158
ted pedersen	2.014014536653706
chinese dictionary	2.1342242464174515
symbol p	2.017570716396185
corresponding node	2.468819905483623
linguistic criteria	2.232476706943533
rather simple	2.8948042841521304
chinese text	2.7914038707630846
initial tests	2.089026915173973
single entity	2.5356828051672813
handle cases	2.4251394527998116
syntactic arguments	2.5524565739808436
model trained	3.0123089218718353
semantic selectional	2.0213043660700594
tagging problem	2.562988768816969
accuracy obtained	2.5431786107392598
immediate constituent	2.233065226075771
cases must	2.1023887620661283
experiments reported	3.2164666134519906
experiment shows	2.7113142160841357
preliminary test	2.089026915173973
discourse tree	2.2600812085392854
fo r	2.519262891192545
particular kinds	2.3005523645852994
parsing process	3.37999619302037
text documents	2.8387608785288854
lexical category	2.8480200636250106
standard linguistic	2.296234813805337
structure f	2.1267836630592134
second requirement	2.026260250766906
without explicit	2.6448556435754913
0 proc	2.5871936349478175
knowledge representations	2.51479161756516
use tile	2.264070934029513
percentage points	2.190533715243349
single rule	2.7985609570364494
text rather	2.314007818435079
important fact	2.135844661161516
every syntactic	2.153552027758611
spelling correction	2.800710237926169
one binary	2.032982255991816
full search	2.063292512210286
recognition applications	2.165544524420019
additional mechanisms	2.1774937848124125
functions may	2.2207046594768247
interpretation processes	2.072290098419245
summarization techniques	2.152105323847251
version 2	2.5273340608472292
value returned	2.064139780413935
cannot model	2.109437912434099
base noun	2.4512309612160132
parses produced	2.4089829611468265
mixed results	2.233997204181393
significant advantage	2.2955083501313895
analysis stage	2.1745351394738437
confusion matrix	2.6261025899340056
corpora annotated	2.3741063217126674
hobbs et	2.7339998861885464
outside probabilities	2.1764615659310973
set consists	3.0479297499366136
poorer performance	2.0918364857418936
method shows	2.140860171621032
different number	2.8255375552154924
control structures	2.2307091620227286
automatic word	2.5408336377554264
phrase structure	3.6830656443520544
similar lines	2.067747107964575
four times	2.6434929258879203
resulting classifier	2.140512028679813
state based	2.022410241232288
validation data	2.001066303399398
linguistic intuitions	2.331780823064821
likelihood function	2.3417233777739757
data types	2.638730105617867
get information	2.3841838643071576
limited extent	2.3398800045580104
good accuracy	2.4629615424710254
practical terms	2.2062448606993037
inui et	2.0215688023747362
linguistic environment	2.29350266905272
sentences included	2.014014536653706
automatic segmentation	2.3958934423869556
make explicit	2.874941591119322
capitalization information	2.06468203219145
spoken utterance	2.365645084287615
additional sources	2.1413469986390323
corpus includes	2.4303117287858482
data retrieval	2.1000868080586823
extraction step	2.0880414073297953
whose head	2.619865107063044
one contains	2.0376102740271858
english training	2.2193006070212657
different prepositions	2.272505380879168
selection approach	2.012701941512196
dagan et	2.7866066052182616
new parameter	2.12319209177683
sense inventory	2.475328567443996
cannot assume	2.4816559643828815
times faster	2.7730600255327595
measure described	2.045521226679158
al l	2.289161918175575
different point	2.1479184330021646
initial analysis	2.2020202169645566
different argument	2.2577963294869763
order markov	2.6419449959403534
method makes	2.500962469073721
information pertaining	2.3271110475429713
language data	2.946155420646467
current method	2.168967603804668
decision theory	2.0799084977218767
axis represents	2.315423472846696
last word	2.913307165717282
constructive comments	2.1222463152036486
different perspective	2.3707749680500125
test would	2.2492578335740094
random variable	2.8581381369687464
system since	2.3567860333521553
english language	3.20115876002272
another sense	2.388574780613938
work presented	3.2009416521367333
di erence	2.128087921056808
systems make	2.435632598873795
zhang et	2.558644121631012
dependency structure	3.0530889976130395
component technologies	2.084486237350305
grammar cannot	2.309782368028973
important goal	2.1004669767659387
takes care	2.850202159086066
three types	3.7069545829891264
relations include	2.2427935805269072
structural representation	2.2129211826900175
pragmatic considerations	2.0355401813680145
lexical type	2.000548223073574
working system	2.540273427854295
search space	3.640023050768479
error type	2.2241293084292693
verb frames	2.300237867940787
additional words	2.22672986885133
computational systems	2.397635513625495
relative scope	2.0984138679225444
nlp community	2.784908614688687
rules using	2.70554448771565
knowledge automatically	2.060145782650642
including words	2.166102255087602
representing knowledge	2.1543888787937475
system considers	2.249257833574009
syntactic units	2.5434453234632652
since f	2.0880414073297953
2006 conference	2.6383330595080268
processing methods	2.3372298137486975
best system	2.8296678530000228
chinese nlp	2.08469715677149
2 let	2.289161918175575
performance obtained	2.4711372947173453
tagged sentence	2.4380833758655096
daughter categories	2.2369671133979248
string match	2.2890085719426256
certain point	2.6375421544020945
second clause	2.804339019326108
two persons	2.128149293488139
real applications	2.567595533408169
surface text	2.3830687504612387
different representations	2.6161488971406985
complex relationships	2.0880414073297953
exponential number	2.4463686632506234
generic summaries	2.133165345977436
order models	2.0940990478094452
level rules	2.5734243675773505
language also	2.181370500368545
possible contexts	2.2916287452490876
one module	2.228225530606692
r n	2.3334800075054867
large difference	2.1946340157728974
times slower	2.147741798365251
gram statistics	2.478789582982379
condition part	2.136091232530588
order model	2.31486854760065
technical report	2.3734438459624023
alternative paths	2.026260250766906
grammars using	2.3213583873662316
work represents	2.0817120252117016
processes described	2.0817120252117016
difficult problems	2.795136363085671
dialogue state	2.1513594604785817
lisp functions	2.168608828146297
satisfactory way	2.1774937848124125
resulting clusters	2.3494271666348956
involves three	2.4659128163621613
choice would	2.022261218861711
users may	2.7359705823925826
statistical estimation	2.2318569333019087
uses rules	2.067747107964575
language without	2.431764288227561
clear separation	2.3955781761503503
one verb	2.831155264611875
since e	2.0376102740271858
features described	2.8763706966437677
category assigned	2.021237938105666
syntactic realizations	2.481244766950694
specific set	2.6927179621980586
surrounding words	2.8148156036900867
sometimes use	2.014014536653706
morphological analyses	2.5812432496755937
language may	2.8740748254211543
typical cases	2.1479184330021646
order within	2.296234813805337
maxent model	2.1251360315631636
tagging model	2.4021461731566776
many nlp	3.0408443206314977
information sources	3.034833825224972
5 times	2.8446146871473403
brings us	2.3725247035269392
scheme described	2.380830363798011
surface realizations	2.417009031491127
clear idea	2.070902079606615
three layers	2.2784990018139752
directly model	2.031264650127311
neural net	2.1798891926562147
nouns based	2.032982255991816
language requires	2.2917594692280554
specific modules	2.0074734400897967
correct analyses	2.279425300453922
current token	2.1678299255012967
one morpheme	2.122563170379007
susanne corpus	2.0443301570456933
providing evidence	2.1222463152036486
system performs	3.1023304161807417
data needed	2.296234813805337
partial translation	2.086507927101117
test collections	2.3675656741410087
underlying text	2.007473440089797
structures produced	2.520536044103427
use standard	2.5036665926162365
entire sequence	2.2533102790483586
equal size	2.452321695174711
president clinton	2.1108493956289704
tile semantic	2.3213583873662316
input information	2.3693884553419133
conditional model	2.1474781884105925
database management	2.189855481372695
first manner	2.0452143290890312
three categories	3.084955581281196
different classes	3.100300448918299
best parameter	2.185240400682336
entire utterance	2.3263148914896092
level context	2.072697162060231
phonological representations	2.080642293093584
resolve ambiguities	2.5683622307986216
galley et	2.0959124379297216
slot contains	2.16646831643446
average sentence	2.851204747328591
save space	2.509365157065348
truth conditions	2.6516588713785203
ne task	2.143621348255242
additional problem	2.305458956322113
distributional analysis	2.2473051805620425
3 corpus	2.222051840138173
local minima	2.1969231173101638
structure built	2.436369723844453
different parsing	2.572255272225193
one per	2.3307617954431006
syntactic configurations	2.2751250090009747
airline guide	2.0301435463043473
lemma 2	2.316613529565746
uses context	2.2062448606993037
particular use	2.227249320716272
structured language	2.032548595356089
functional word	2.0594417027446488
per question	2.065560069939466
system development	3.166948678382706
one interesting	2.719113797190007
learning tools	2.11638795219105
level description	2.2773875806865567
weighted finite	2.5089982986718997
dependent words	2.03518247373598
sun 4	2.0355431529026395
search problem	2.6006071075352524
9 shows	2.983506344592168
three judges	2.140856954368349
actual process	2.022261218861711
one element	3.1083503667101153
metrics used	2.4089829611468265
recognition result	2.2072086075217623
provides evidence	2.575721321074145
p must	2.1362777721376576
feature vectors	3.1288687902235437
recognition error	2.73967590538454
small amount	2.9004119820355827
compositional semantic	2.223138970458979
repeated application	2.116591742190463
list contains	2.760228409853269
system assigns	2.4197547133270083
chen et	2.676671672565844
ask whether	2.4614211265786925
semantic processes	2.0940234533427304
hajivc et	2.029677424295417
complete understanding	2.0172047205465917
method differs	2.166102255087602
likely interpretation	2.2826385442221477
analyze sentences	2.1312839727750825
brief introduction	2.6310181695457535
28 aol	2.022261218861711
simple idea	2.1774937848124125
feature name	2.094727102798552
general methodology	2.0817120252117016
component within	2.0376102740271858
original words	2.27225588938143
intended effect	2.0813270225215517
word selection	2.5541046147780873
tree transformations	2.235958347389501
alternative analyses	2.5277038268947223
top element	2.251043660476318
ad hoc	3.110467228431033
inductive learning	2.385866439905955
three tokens	2.007315006862137
similar considerations	2.1023887620661283
second line	2.5117780976151822
label l	2.205035232896739
good indicators	2.521311760953824
q x	2.255342325720455
relative frequency	3.2174579570254216
retrieval community	2.159746288077376
tile sentence	2.572812352683083
knowledge acquisition	3.2535026677789998
essential information	2.5869164183612403
run without	2.172274429303222
2 depicts	2.305458956322113
constraint equations	2.032523495804993
semantic field	2.271142139674707
interesting observation	2.462292105818408
phrase refers	2.0379203068673784
problematic cases	2.558787148135115
huge number	2.7588465510867373
vertical bars	2.0376102740271858
propositional logic	2.175311106187253
inversion transduction	2.0431377538298903
computer dialogue	2.2557870308088104
strategy employed	2.067747107964575
various levels	3.1912330284567587
makes decisions	2.089026915173973
ambiguity inherent	2.014014536653706
time linear	2.2756440808404257
system contains	2.656355958308939
target verb	2.2991030892221174
generalization performance	2.259710049085058
typical situation	2.0817120252117016
communicative goal	2.653641397610502
level knowledge	2.4635892042029712
potential ambiguity	2.1012225606144637
syntactic alternations	2.2455588337315464
two queries	2.0781754103273498
user responds	2.0551182606938574
language terms	2.048858931206226
event occurs	2.267991541156711
one document	2.694768103256239
terminology extraction	2.287961927103912
features could	2.6671758963459182
financial domain	2.0517691345478033
plural pronoun	2.040238153498075
bigram probability	2.2517149257502638
first noun	2.580023303032755
exact definition	2.183647914993236
memory requirements	2.6093852424694677
uses knowledge	2.2380656204269957
computational task	2.0817120252117016
lines represent	2.0831773849898116
optimization problem	2.776777475099747
many research	2.4459112107066288
briefly explain	2.126823141441437
another instance	2.1273754762918986
common assumption	2.109437912434099
classification method	2.5083728772652902
level system	2.28193063938493
case arises	2.178653004950429
parser achieves	2.197200382558977
corresponding input	2.0965741974051317
new entity	2.050174283106106
answering track	2.199100943769751
clusters based	2.2698137833474052
evaluation using	2.646575138759316
semantic information	3.8556051353201353
syntactic trees	2.7999179996940575
identi es	2.1963874985108163
many algorithms	2.2107864417857894
rule 6	2.266068056857324
score would	2.0965741974051317
research institute	2.1378947601730776
model since	2.1946340157728974
engineering point	2.049069748176902
align words	2.0844862373503052
recall curves	2.057694696557903
selected word	2.2463272029225663
network architecture	2.0355401813680145
several models	2.422926948621651
learning tasks	2.5372287993064258
paper investigates	2.7467109305722683
two errors	2.0402381534980756
proper name	3.0910316309600674
different parameters	2.4838133882040077
evaluation campaign	2.0369910881049584
resolution method	2.124198414029314
texts without	2.368834809141683
answer given	2.222406898745732
algorithm reduces	2.00238351760398
common cases	2.11638795219105
appropriate input	2.014014536653706
word overlap	2.2454700546611295
adequate description	2.159746288077376
refers back	2.108179066756418
next consider	2.2062448606993037
distance metric	2.651917908970546
new rule	2.5896979726368223
practical applications	3.163928757060651
task includes	2.026260250766906
optimization criterion	2.100258647138732
representation theory	2.8190841124868706
work reported	3.4048094250122363
particular source	2.0376102740271858
texts may	2.30606794696972
corpus consisted	2.659652700262078
coordinate conjunction	2.050573094363604
limited resources	2.4198340443080917
topic shifts	2.3596018716097173
varies widely	2.319324296244544
semantic primitives	2.6089131802644285
current project	2.002119294050899
extraction pattern	2.004399472273601
two possibilities	2.8345050377367764
resolving ambiguities	2.245558162117437
rules allow	2.4739112856245806
planning component	2.3161323053063243
grained sense	2.3346919580309033
major components	2.686385028497094
model considers	2.2470811344797332
recall score	2.3807019848689257
original problem	2.0224102412322886
knowledge needed	2.71342885791868
smaller units	2.2767822834620834
several others	2.2224068987457315
alignment procedure	2.271216308915548
thank michael	2.2631802623080817
principled way	2.8825671263316273
later sections	2.380830363798011
interesting fact	2.159746288077376
hard problem	2.4917756927623538
major challenge	2.172740644848404
approaches include	2.1774937848124125
different question	2.1597096509760467
development system	2.0580737128742386
similar languages	2.0763533583416773
model estimated	2.1227137463544357
complex linguistic	2.6890651646017547
relevant passages	2.203914695591317
stochastic language	2.3801775839440227
verbal head	2.3427116348708754
following assumptions	2.5270204646552323
simple top	2.019557844009582
stage 2	2.1929597029716836
entity x	2.0379203068673784
tagging tool	2.000178283129414
work best	2.3922788066689504
first approach	3.0865690393640812
name followed	2.014014536653706
oov words	2.249126132145285
conceptual representation	2.7364284304842466
main role	2.109437912434099
koskenniemi 1983	2.1356018195482047
relevant work	2.2631802623080817
time needed	2.4525765590889126
computational lexicography	2.269813783347405
certain parameters	2.014014536653706
particular aspects	2.075729255121515
functional grammar	3.118777562732792
robust language	2.0817120252117016
using transformation	2.0074734400897967
particular term	2.0390072838737376
concrete objects	2.000565790641362
three phrases	2.012347776929124
parse sentences	2.6343403784238224
similarity score	2.9217617113674135
partial interpretations	2.079249964537768
lower levels	2.440076772254309
terms extracted	2.1995346404612253
public domain	2.4196398436860287
semantic lexicons	2.325093064405023
current stage	2.318793079863193
significant problems	2.075729255121515
initial position	2.8197400210941788
inflected words	2.3560740107228835
particular subject	2.339875472386836
lets us	2.3886619127043263
estimating parameters	2.022261218861711
tools developed	2.339880004558011
declarative sentence	2.6034102829993655
small numbers	2.468819905483623
annotated corpus	3.346869385079499
language dictionary	2.0396655620209403
universal grammar	2.37037199607361
achieve higher	2.0654028249338845
different formalisms	2.2380656204269957
various language	2.3955781761503503
basic level	2.367806488030155
four sections	2.146224553829869
remaining words	2.7973612945261226
small sub	2.1290482690107417
problems like	2.0875263606178973
focus structure	2.0117256023230983
solid lines	2.5408638659617
three characters	2.4471265460479596
language text	3.250755943221843
defined rules	2.051285638402277
one topic	2.5466175074107356
target document	2.0099182428982036
performance gains	2.6278077419098116
example might	2.189546252614866
model performs	2.7944882019531536
including machine	2.2169936022425736
binary classifiers	2.4776442137781363
lexical content	2.4151616361163413
results could	2.23249824886836
language pair	3.0755218756676452
difficult issues	2.022261218861711
semantic values	2.257793122602041
principal component	2.1087319714718387
language related	2.0824927688889554
rank correlation	2.062848267546476
certain rules	2.285913086717917
minimal number	2.7062997765235606
given application	2.517221409688823
task definition	2.4864847742842326
given position	2.385485707217658
optimal model	2.0429058304291114
called context	2.022261218861711
input features	2.5942378183423704
two subsections	2.3920948169591325
model alone	2.232062756869182
restricted number	2.0577375124403856
tipster project	2.0772399850700265
like wordnet	2.47655231274074
user knows	2.3574185110656543
experiments designed	2.045521226679158
w e	2.8229001277155286
little training	2.2380656204269957
verbs based	2.143898852826872
algorithm depends	2.0824927688889554
general method	2.7216083857236715
links may	2.061969254719088
constituents whose	2.1479184330021646
reduced set	2.2823116493074065
20 hours	2.166102255087602
string length	2.2333282907383465
possible orders	2.0604764969548435
semantic forms	2.1354872861769416
standard approach	2.725274627770086
systems used	2.419705521055993
event descriptions	2.2680363009997677
question answering	3.4374600169595433
last condition	2.060145782650642
baseline models	2.2109095148312528
true probability	2.133165345977436
thus need	2.2169936022425736
baseline model	2.853130930303926
lin et	2.4375173275377735
text alone	2.0883166975146934
qa track	2.426198587044934
translation evaluation	2.2960858459243148
rules within	2.275125009000975
algorithm constructs	2.060145782650642
categorization task	2.201697277329875
preceding words	2.677878488282143
company names	2.706346891396743
diathesis alternations	2.1071178503328314
n u	2.333178632543555
structures would	2.126823141441437
daily life	2.204719497546924
additional information	3.477375869862533
sentence lengths	2.52087148891416
subcategorization features	2.13638592708426
translation approach	2.5111332454830833
new documents	2.3702121294188494
e denotes	2.085074160737753
including information	2.6760156204978185
functional unification	2.5686876998752335
specific facts	2.049069748176902
strong bias	2.060145782650642
four steps	2.707246013676819
first corpus	2.4039418597775994
formalism provides	2.1222463152036486
better representation	2.172274429303222
certain context	2.4922686900186415
tasks including	2.172274429303222
works well	3.1248015356198198
large portion	2.297162330154481
let f	2.6980442205844093
possible paraphrases	2.0445276499269367
parser needs	2.218434485750926
sentence form	2.234561366831872
de meulder	2.099458264360342
3 summarizes	2.7154083994216545
differences across	2.1366713893936176
induction algorithms	2.0236385391388776
formal representation	2.7839125339781146
essential features	2.230524125991308
window size	2.9898247759239753
every character	2.27458041023388
ii system	2.021135667951535
original model	2.443854725009737
categories would	2.060145782650642
model contains	2.5869164183612403
unigram model	2.461423672361511
parallel processing	2.5847501227270766
compound sentences	2.244801177116414
following constraint	2.408973091929181
mapping rules	2.5373798220016464
following discussion	2.7532929632105896
sense ambiguities	2.1119430342456558
three metrics	2.24288063176538
preceding utterance	2.342795459736605
two parallel	2.500962469073721
morphological phenomena	2.2424226425547475
simple enough	2.468819905483623
line corpora	2.016336526582215
analysis indicates	2.020363647132216
mitch marcus	2.49121940994721
individual differences	2.0802181005974956
syntactic category	3.4795132800808775
method uses	3.0897017564164964
let e	2.466199345581235
consider figure	2.435449594411215
reference set	2.132139966398633
strings whose	2.1413469986390323
sentence order	2.1575197075949495
lower frequency	2.460398372654627
adjacent constituents	2.026260250766906
previous context	2.45821324271322
different system	2.4137801326343
mainichi newspaper	2.2189920115289636
strategies used	2.4603482036646533
vogel et	2.4776771834746203
following models	2.109437912434099
logic programming	2.906912233513119
another person	2.121422069678421
different slots	2.0656667908183506
tile next	2.1413469986390323
zero values	2.183507263144583
text contains	2.4100484574566545
constant value	2.2776740307447065
question processing	2.0633969806750843
common source	2.189546252614866
little effect	2.1651344456319714
partial structure	2.1806061206095273
2 describes	3.2761132555812305
occurrence frequency	2.703315441776496
first sight	2.7156890376184353
permet de	2.053385468380775
next line	2.0355401813680145
communicative intention	2.062833829642103
target sentences	2.758876265268346
two paradigms	2.1119430342456558
various relations	2.166102255087602
takes place	3.368588673141248
automatic dictionary	2.060145782650642
srilm toolkit	2.176234939134731
syntactic processing	3.012145370197936
system designed	2.7304067915345684
standard ir	2.1051956698941954
direct approach	2.152791091267105
single object	2.224348485612061
similar problems	2.682401390823562
common data	2.0820460914892704
vector machine	2.890821981983456
various ways	3.359098918482294
features based	2.8699165994365736
manner similar	2.738852396948924
certain knowledge	2.0601457826506424
system incorporates	2.4816559643828815
standard corpus	2.3753959409621817
link clustering	2.0202561713341427
uniform way	2.747376523465468
bayesian framework	2.0013487161900905
three years	2.2077704599454604
main contributions	2.2895265042698942
punyakanok et	2.0828462539151458
multiple choice	2.0730360943881685
one segmentation	2.1290482690107417
achieves good	2.109437912434099
written text	3.0698034393216074
whole range	2.1829781245409827
errors may	2.607202102311032
art machine	2.1592457362625144
every item	2.1185047675079156
best solutions	2.0022853126259594
little time	2.3879279742060486
correct values	2.099458264360342
languages using	2.357121432847829
greedy algorithm	2.4547836483727297
grammar productions	2.135844661161516
scheme based	2.403472566572053
across domains	2.6963633764356505
many methods	2.433738217595396
following subsection	2.1290482690107417
learning problems	2.34559161373679
penn tree	2.9299632881283775
universal quantification	2.1544978067611362
english mt	2.3346140415901244
function word	2.7470237966627162
dependent information	2.336348917945085
systems still	2.1413469986390323
also handles	2.3725247035269392
integrated system	2.609857613318595
several techniques	2.6715524595376996
whose right	2.318222716608852
technology research	2.389326705744299
thank dr	2.6044434158658247
larger constituents	2.255593583249476
two fields	2.327403995660175
good agreement	2.404639610987332
de col1ng	2.28182140187005
depth one	2.160181161519838
ascii characters	2.2801050794522797
intonation patterns	2.099458264360342
matching methods	2.232062756869182
first phrase	2.3853931341068737
morpheme boundary	2.209954180635901
input string	3.451695707754639
language makes	2.109437912434099
method improves	2.289161918175575
among systems	2.067081229293324
simplified version	3.034406270755324
possible target	2.4770330173868644
since p	2.3571838468625623
different parameter	2.5508919667548535
july 2004	2.5302215052732078
syntactic type	2.531046085322244
important function	2.060145782650642
c represents	2.105195669894195
occurrence count	2.1127786641962607
previous knowledge	2.156980850785844
significance test	2.3180361567683185
trigger word	2.1727031237611945
final step	3.057838114601991
available today	2.2532512925302806
much improvement	2.116591742190463
formal treatment	2.2512557276899194
intermediate level	2.6804718085535284
method introduced	2.0074734400897967
compute probabilities	2.060145782650642
recursive definition	2.2371931519418524
many dimensions	2.014014536653706
bos et	2.1813446704867467
one situation	2.0074734400897967
words correspond	2.0376102740271858
single value	2.5195824672405656
differ considerably	2.369115447338463
parse results	2.2822386297586945
carreras et	2.272527835925634
network representing	2.1809543568234924
general view	2.0376102740271858
system utterances	2.2055715344564746
news corpus	2.724259065363733
particular tag	2.0264003656513117
significantly increases	2.166102255087602
sentence x	2.3389387632128473
tools described	2.00238351760398
different characters	2.0452143290890312
using character	2.080606834271845
grammar formalisms	3.0878179497789255
text chunks	2.0462485577187906
validation test	2.0131719474230962
information added	2.053986137989304
morphological variations	2.248968785663327
response times	2.289228240925313
matching technique	2.1816335351833818
ones shown	2.089026915173973
multiple meanings	2.490534521076478
initial performance	2.014014536653706
example consider	2.3997947207672894
semantic well	2.059213640580528
tnt tagger	2.00212253338633
000 word	3.0504829979888406
office environment	2.012347776929124
state q	2.3578179176098635
ordered sequence	2.577728364928994
final result	3.1216361736150118
p n	2.0612515863535696
part 2	2.0573053700718145
given sentence	3.4803420543634407
sense tag	2.1517696698555557
recognizer may	2.0074734400897967
next state	2.3006182184996105
governing verb	2.293773007231559
errors occurred	2.2823116493074065
independent word	2.470424554407324
still needs	2.196624013185909
work provides	2.172274429303222
prague dependency	2.302639744133172
passive transformation	2.0423259728447545
joint probabilities	2.3207588971466047
texts produced	2.351307564583778
test different	2.0817120252117016
operations required	2.255593583249476
static knowledge	2.2778616523026356
particular choice	2.403472566572053
objective function	2.8520255590115267
entities referred	2.2742912111882294
semantic conditions	2.1222270206314313
pragmatic issues	2.0817120252117016
standard phrase	2.0592191196139593
poorly understood	2.21826141526814
latter approach	2.69490660923505
section iv	2.0433576362240267
analysis based	2.7596959428695147
particular pair	2.172274429303222
probability 0	2.1013400484033893
large variety	2.7858079372275224
information values	2.012481064077442
depth first	2.080606834271845
leftmost child	2.029677424295417
two principal	2.3755396070923673
first observation	2.1479184330021646
tagging algorithm	2.043521175114013
results based	2.4360378007635592
detailed study	2.2631802623080817
genitive case	2.1886175034257573
xml markup	2.061697263752646
overall frequency	2.2571860900326115
modeling techniques	2.6448876506536365
speech systems	2.661870174713173
definition 4	2.6259988907338934
four classes	2.732634244567653
domain dependent	2.835783432190313
varying amounts	2.139044594632718
tree based	2.515566903354635
formal framework	2.464532383816498
null using	2.2169936022425736
learning community	2.1222463152036486
f n	2.103464715009379
second part	3.253026283677032
possible interactions	2.109437912434099
time speech	2.2394738813759885
frequency estimates	2.1148419198247215
better translations	2.0968154690268097
probabilities based	2.3005523645853
choosing among	2.443490996253451
analysis involves	2.2631802623080817
answer types	2.1546191156034222
np would	2.108179066756418
wordnet 1	2.660364004785595
singular noun	2.65149003573185
typical text	2.067747107964575
pour la	2.420863332119162
one source	2.405849289416855
parameters p	2.125327459817784
occurrence statistics	2.762776848474791
lexical elements	2.654323077022815
relevant aspects	2.514014135006877
generated word	2.133165345977436
system whose	2.558787148135115
object nps	2.0709020796066153
language analyzer	2.2729735178559443
type hierarchy	2.818255898900074
competitive results	2.1949289510852124
speech analysis	2.363254272817871
one data	2.193056710825906
different metrics	2.4120034170101396
tile tree	2.070902079606615
question might	2.089026915173973
one crucial	2.089026915173973
en effet	2.1791899026309856
user profile	2.22465628572427
english input	2.453474368022782
two subtrees	2.0236385391388776
additional effort	2.3097823680289724
extraction using	2.3448347819209543
topic changes	2.129501083087358
automatic classification	2.571052879368418
subsequent stages	2.1828884764914416
many examples	2.838566573545308
human knowledge	2.40097441318663
linguistic annotations	2.1381685381262647
reflexive pronoun	2.3702048935119695
two language	2.6898587154661016
relevant examples	2.022261218861711
word belonging	2.014014536653706
missing words	2.2211453119630327
approach offers	2.3523313048426364
contain words	2.6441885228526156
semantic parser	2.4387063436797343
common terms	2.0001782831294137
ambiguous words	3.063914404962998
whose part	2.096756183704444
pcfg model	2.1057396153134382
additional set	2.3725247035269392
corpus used	3.2753310051914557
precedence relation	2.350674145792996
generation strategy	2.1312839727750825
second interpretation	2.190982236626934
without context	2.4316477510177266
grammar system	2.213665579272571
research systems	2.219474104888321
distinct levels	2.1312839727750825
outside probability	2.0375746510034185
general picture	2.045521226679158
word sample	2.136671389393618
100 test	2.175400407836868
strategy used	2.653085657410411
negative log	2.462199100120177
language would	2.6538988880551524
paper briefly	2.227249320716272
less work	2.3644803559732956
based disambiguation	2.23712941840118
run test	2.052352800514945
relationship among	2.6024460946269072
parse may	2.0709020796066153
provided training	2.0023835176039797
method works	2.6658978865245158
symmetric relation	2.0460967409737787
based document	2.1297846672161187
seed examples	2.003240579996545
aligned parallel	2.5503333727170916
equal weight	2.714127378147386
dialogue structure	2.3955359638997424
synthesized speech	2.3491362157121403
use domain	2.1789148691025395
research literature	2.037402853550243
first number	2.116591742190463
formal grammars	2.2110128674632685
60 words	2.026260250766906
user model	2.56064507500462
top k	2.273001955576606
alignment program	2.1228171960130373
grammatical sentences	2.6445658119224023
scale grammar	2.228382969865897
radev et	2.3057310977884065
deeper level	2.3448347819209543
syntactic aspects	2.1762349391347313
initial weight	2.008083705228559
1 x	2.1325100589991792
one parse	2.8221928795176723
entire set	3.0110919724631904
column 3	2.3224723271667544
darpa community	2.023617009900904
small corpora	2.4211577250508505
long term	2.45551534965744
processed text	2.3483606540998085
syntactic subject	2.328295822355019
linguistic unit	2.4613417883332085
single correct	2.189546252614866
implicit information	2.3987427051706645
que l	2.3056339627510445
syntactic behaviour	2.14388136866975
possible approach	2.4972046802134193
thank three	2.089026915173973
first point	2.5747371081275414
another case	2.5871936349478175
value associated	2.101240253546522
present results	3.213747203401136
order predicate	2.6579427558481177
english name	2.0502571300418837
syntactic tags	2.425955348725714
single constituent	2.357947172648072
particular applications	2.3332911335775464
model containing	2.3100369696113856
lookup table	2.0429410169265063
standard syntactic	2.109437912434099
rules specify	2.3100369696113856
individual systems	2.372668390946239
many ambiguities	2.282311649307406
four types	3.2079147361199323
empty sequence	2.207380515164456
bootstrapping process	2.2679959814824544
recall performance	2.147935317133652
information required	2.798626562195748
relative order	2.7470070467924557
segmentation accuracy	2.3288285208860016
text generator	2.5388663591650356
closure properties	2.037402853550243
new tools	2.233997204181393
experiments performed	2.5848980647393134
modified version	3.210149168330244
algorithm operates	2.2577963294869767
two learning	2.152105323847251
paramount importance	2.067747107964575
processing must	2.1413469986390323
many researchers	3.252566033611196
dictionary lookup	2.757756423561441
without knowledge	2.3229739757638654
broader range	2.108034268305181
tool developed	2.1543888787937475
minor modifications	2.0256036505337733
similar word	2.636324089427963
evaluation tool	2.0721570056738003
test suite	2.3726221960574247
let g	2.802499543222853
original features	2.051405404013556
multiple senses	2.70798805585506
first goal	2.504497044060369
object would	2.2169936022425736
systems typically	2.5765298321699723
rule ordering	2.2983418593321767
produce good	2.4251394527998116
approach suffers	2.0376102740271858
text containing	2.468819905483623
noise ratio	2.154289253254081
many decisions	2.0376102740271858
order constraints	2.3778937982931474
new object	2.342454119591512
based annotation	2.1028168996049788
rule interpreter	2.069576901487641
different rule	2.306224889036373
free phrase	2.7371298153443253
estimating probabilities	2.026260250766906
proposed models	2.233922353806755
another one	2.790616566404616
query results	2.275644080840426
training algorithms	2.340356317561991
learning word	2.1312839727750825
word dependencies	2.166013936376663
data presented	2.515272207357606
proposed model	2.6975781767581237
recognition using	2.573562255840437
4 contains	2.4421485591028738
management task	2.42620205812195
grammar consisting	2.233997204181393
word probabilities	2.2733751222913563
particular theory	2.1072480253375243
method using	3.031315058364704
planning systems	2.2718366326574806
model 2	2.680841945501968
class distribution	2.158230686665582
underlying meaning	2.064310474990315
whose meaning	2.8035501086382646
scores produced	2.1911147757555276
set sizes	2.403286209924487
distinctions made	2.4406532125653317
separate components	2.189546252614866
contextual information	3.4696463903429398
corpora could	2.089026915173973
different distribution	2.3322409522263214
correct predictions	2.3901628362899903
transcription system	2.0027830163140345
inflected forms	3.0489360639265146
biomedical texts	2.0548191422527147
radial basis	2.0477789160236313
checks whether	2.880928409919221
vp nodes	2.0728089524061724
errors produced	2.233997204181393
search algorithm	3.0302429288606
representative set	2.1413469986390323
syntactic ambiguities	2.707067227055051
line dictionaries	2.441300693078526
two relations	2.6673798220856435
possible outcomes	2.3906121063052925
free variables	2.5235692532218144
ov coling	2.2273864463788637
result shown	2.1563079384871173
estimation techniques	2.355337380520586
relational data	2.46819184797553
two ways	3.737578924228997
terms occur	2.096756183704444
finite clause	2.244015212274193
extract relevant	2.3707749680500125
tile linguistic	2.067747107964575
disambiguation based	2.3494271666348956
partial ordering	2.823957773601003
findings suggest	2.172274429303222
different pos	2.3817316286655625
input representation	2.5176845233017606
new value	2.436107672015553
german noun	2.076473429722452
mutual translations	2.002576330670572
content words	3.461851964967579
phonological rule	2.195317618648504
four categories	2.8396477756680243
hidden variable	2.3273751138300582
particular test	2.2062448606993037
comprises two	2.185240400682336
new tasks	2.355695717315683
linguistic specifications	2.1253274598177834
separate rules	2.1543888787937475
small fraction	2.7053773320256242
vast amount	2.4658585645856936
second sense	2.310633585471453
approach avoids	2.075729255121515
following formula	3.1294191984406354
wordnet semantic	2.2654812855760516
produces two	2.4103259885026898
show results	2.417618427399807
lower score	2.4242418191018458
textual context	2.1110509596334177
constant c	2.1588487997915733
human mind	2.2959583628072817
meaning may	2.3623688781715884
whole word	2.359876038453363
deep case	2.1022265084753875
improved version	2.4421485591028738
english generation	2.2512557276899194
features obtained	2.1462245538298697
using speech	2.554006539039166
generated output	2.0923172520507163
input signal	2.1312839727750825
model whose	2.4251394527998116
verb whose	2.233997204181393
structures derived	2.0421480663134535
classify words	2.3448347819209543
specific ones	2.5747371081275414
syntactic parsers	2.6847500722988515
informal description	2.2073697906566263
english query	2.1425979537242066
one simple	2.6780005804409055
word containing	2.007473440089797
algorithm described	3.40250910254974
local information	2.7453050821941627
features cannot	2.022261218861711
becomes apparent	2.166102255087602
object type	2.101711586587112
document frequency	3.0376425861148078
fernando pereira	2.531391211068655
optimization algorithm	2.423740771451185
different solutions	2.1638349431134714
computational burden	2.1023887620661283
documents per	2.336758443421073
specific form	2.2895265042698942
words also	2.4250738008550288
leaf node	3.0332317946462197
word boundaries	3.087297102725486
language interaction	2.428841513639201
positions within	2.3633976741456193
bc used	2.109437912434099
sheds light	2.2112071020073643
list may	2.1222463152036486
possible parses	2.81428796931232
system output	3.05340254999964
model word	2.159746288077376
loan words	2.2369721410572003
good starting	2.2009635494148077
annotator agreement	2.9491681805414838
last category	2.1479184330021646
matsumoto et	2.6079530307762155
useful data	2.1023887620661283
coreference links	2.1874388446194004
several sources	2.7071617172136513
aligned training	2.0704460625635814
zero frequency	2.00238351760398
transformation rule	2.1852902825441163
limiting case	2.0601457826506424
new languages	2.7589985647519875
words may	3.2574217026572674
much research	2.694649565144252
abstract data	2.1647786971648557
use english	2.1543888787937475
2 illustrates	3.1784396417711744
standard format	2.1023887620661283
wu et	2.2163324640067588
key concepts	2.126075820314081
handcrafted rules	2.1000868080586823
another set	3.0155029879138056
tag sets	2.648441160592934
generate candidate	2.0355401813680145
criteria based	2.022410241232288
different documents	2.4654868758369037
preceding discourse	2.6752930496349814
attributes may	2.2458302187516197
original string	2.2147431158325057
several paths	2.060145782650642
improved results	2.086804311199347
string z	2.1060856897808726
compound terms	2.0645344172052713
structure annotation	2.088519749378327
original algorithm	2.5176175237714196
task becomes	2.0965741974051317
using rule	2.300252528032397
performance metrics	2.2225315747955348
system without	2.8936592063379742
learned rules	2.411755076031854
substitution grammar	2.062254248416407
f l	2.412081342249917
alignment method	2.2224958077611783
task used	2.075729255121515
similar data	2.2578698255858516
right context	3.00004383003393
standard parsing	2.3612865440695145
one grammatical	2.075729255121515
orthographic information	2.0355401813680145
main concern	2.342613615207778
theory provides	2.3322409522263214
one attempts	2.0817120252117016
np subject	2.0606402530359342
recursive structures	2.3645836211524633
semantic distinction	2.1432314806484003
reversed order	2.0376102740271858
word co	2.8955472789964736
possible phrase	2.2020202169645566
database system	2.5496990648043236
precise information	2.061969254719088
semantic restrictions	2.719822135868116
semantic representation	3.644735521734186
instructional texts	2.0387114885528606
speech interface	2.0621389048534704
decision problem	2.0185538120547135
50 times	2.24583021875162
example 10	2.1884816114309817
several places	2.082492768888955
results clearly	2.2459839572002176
shows tile	2.0539861379893036
recognition problem	2.696836570519369
structural relations	2.5946585479222612
english corpora	2.585263558902455
terminal strings	2.376858326222075
improve parsing	2.3876270297706563
syntactic tag	2.2124640066341725
following structure	2.52765372432748
use x	2.022261218861711
nlg system	2.4907980840157506
par les	2.133889772832034
evaluating performance	2.014014536653706
one dictionary	2.1295694626022152
thematic structure	2.309505450617034
three components	3.016623336924599
component parts	2.5128964004670493
semantic understanding	2.1414753566437956
main stages	2.159746288077376
planning task	2.128389388126583
one choice	2.3523313048426364
user feedback	2.2974668198166297
adding features	2.1976485766852405
one text	2.7261153767859603
based segmentation	2.2124903162114187
disjoint subsets	2.099458264360342
two similarity	2.021422743908272
frequency count	2.5427861404865473
main sentence	2.161097326348483
among objects	2.140860171621032
definition 1	2.9719922897191267
2 f	2.1174510428502895
system begins	2.109437912434099
based extraction	2.1317142331609915
sentence pairs	3.097278506373467
words chosen	2.0648406357378897
took advantage	2.2532512925302806
acoustic analysis	2.042325972844755
much work	2.904511824981509
rewrite rules	2.8533328031948826
useful comments	3.006158213237917
system design	2.884598081840087
time stamp	2.048809151381164
contain either	2.2169936022425736
hpsg parser	2.0683386567780526
results cannot	2.22672986885133
automatic methods	2.81926772285024
two decades	2.1920301070803174
input list	2.048033424136163
models like	2.32391019643238
based version	2.015795287891615
thank mr	2.099458264360342
speech transcription	2.1651154718617
two sets	3.7611327687068288
degrade performance	2.1774937848124125
scores across	2.0421480663134544
set r	2.424288674587337
first question	2.8111994399199958
element x	2.080340914984138
grammar rule	3.159493646979939
initial tree	2.5629368978761384
combination methods	2.1335890251440035
three rules	2.6820128203213565
valuable information	2.6610945516467606
parsing technologies	2.227249320716272
whole grammar	2.1000868080586823
overall score	2.670198644102247
speech recognition	3.8652176050770404
large value	2.28182140187005
similar patterns	2.3841838643071576
partial information	2.394604039883415
conditions must	2.289161918175575
pietra et	2.448466182223407
ibm models	2.634287807133465
elementary trees	2.917520905890566
representation using	2.2511939003694694
church et	2.384173238929259
given frame	2.295599247235722
grammar writers	2.2317229284266027
applicable rule	2.1678854032223676
adverbial modification	2.0817120252117016
ibm model	2.862436178408788
contain information	3.0704741806305753
german corpus	2.226786625183542
inside probability	2.070384658265511
difference may	2.166102255087602
controlled experiments	2.164235940665368
phrases rather	2.067747107964575
new constituent	2.309172255166361
perform two	2.3005523645852994
contextual features	2.6116917285728354
reported performance	2.194634015772898
linguistics research	2.3963829963262837
x e	2.564305130103813
train classifiers	2.035540181368014
text stream	2.313900874640595
practical importance	2.2380656204269957
compute p	2.1621373145487084
bilingual lexical	2.0162790718821597
four characters	2.3981922091327075
phase 3	2.0534430737113434
driven approaches	2.2209236662255827
given tree	2.267417282884252
funded project	2.075729255121515
human agreement	2.2389535927952933
based user	2.070902079606615
different frames	2.096814526516021
local discourse	2.175670312743967
subcategorization list	2.052445756191357
5000 words	2.171178031468078
marcus et	3.3891558470440923
extra work	2.153552027758611
large volumes	2.2046737533515723
corpus showed	2.089026915173973
linguistic clues	2.299918657112939
finite state	3.401928346121868
many elements	2.089026915173973
several parts	2.4005597282692768
building blocks	2.7391059129398685
singular nouns	2.2062448606993037
acoustic score	2.051113524185823
preferences among	2.103041056581321
framework presented	2.143898852826872
graphical form	2.0074734400897967
generative power	2.5541230544229188
lexical correspondences	2.075496024764096
example 12	2.009868063569498
language expression	2.578039544355876
definite nps	2.4896182575500023
critical role	2.2112071020073643
scores would	2.1165917421904625
japanese verbs	2.2234403544542047
interpolation method	2.0566156493467185
overall task	2.226511030374328
sidner 1986	2.444162397011934
give results	2.431764288227561
complex syntactic	2.64978357575092
order among	2.3310204571264768
orthographic forms	2.0376102740271858
based studies	2.116591742190463
general idea	2.9166964243012963
processing techniques	3.0622386733660023
broad spectrum	2.0965741974051317
two strings	2.890384363683607
asian languages	2.4251867336040833
key templates	2.15290426706119
one variable	2.4498459736561693
intonation contour	2.0170841037187204
reyle 1993	2.0158422857843687
different effects	2.135844661161516
application system	2.378124960670362
probabilistic models	2.952294030862857
improve retrieval	2.2976894783503656
single word	3.593759488613043
first describe	3.129020143241968
probabilistic model	3.151999562054411
frequency statistics	2.12160185171063
information overload	2.11638795219105
errors would	2.061969254719088
final syllable	2.1994143718803016
target text	2.71275940568723
also create	2.272505380879168
takes values	2.0918364857418936
distinguishing feature	2.3763414719464064
maximum entropy	3.4406020908650294
acl 2006	3.011940260423138
aligned pairs	2.1015773992236024
effective method	2.6163224599499784
correct class	2.1952935972750742
reference translations	2.5856293713261933
research programme	2.122563170379007
local features	2.5269112514838574
bigram counts	2.0004873366203295
improve accuracy	2.759789179262079
new problems	2.3202936149271833
old information	2.0690491846338652
salient entity	2.008878027810602
node may	2.5983758925109566
compare various	2.089026915173973
length normalization	2.0957092588056296
translation accuracy	2.626372432853271
positive correlation	2.310207631436551
recognition model	2.0988437308558776
following hypothesis	2.189546252614866
additional restrictions	2.012347776929124
typographical errors	2.170560999202202
language researchers	2.096756183704444
discourse parsing	2.0324779749733084
local maximum	2.2367883267124533
relation p	2.1245637044373717
following experiment	2.4103259885026898
c f	2.2152450059576303
salient properties	2.0376102740271858
various factors	2.2035464656964545
basic knowledge	2.1549242316179624
structural similarity	2.098347704905195
like noun	2.1543888787937475
used words	2.4200952980083943
discourse unit	2.3729032717475773
tile current	2.4115483316123614
noisy data	2.597909015028267
also compute	2.468819905483623
degree 2	2.1267965768517323
undergraduate students	2.012347776929124
line indicates	2.2455581621174368
writing rules	2.2147431158325057
system learns	2.498621589417806
first character	2.6583360874957678
nlp processing	2.0074734400897967
free parsing	2.584587423291659
fundamental assumption	2.189546252614866
hmm tagger	2.193645545679375
present implementation	2.4846223918635233
independent feature	2.0355401813680145
syntactic elements	2.4120311976299664
often cause	2.089026915173973
hard constraints	2.3597633839437915
possible number	2.159746288077376
1000 words	2.4920505435324705
short list	2.217704436319682
basic system	2.1704027043784135
earlier draft	2.631339938520658
contains words	2.3955781761503503
rule states	2.6306827317107158
based data	2.2168408494647585
illustrative purposes	2.307382090261814
feature types	2.3118268999074347
exact nature	2.348360654099808
algorithm allows	2.429066359168927
parsing models	2.7200477869068767
three tasks	2.627779865800701
particular part	2.3332911335775464
body part	2.201318854477547
second issue	2.0399904913383486
rule 5	2.4341840376082593
parsing model	2.909242813859805
empirical observations	2.022410241232288
companion paper	2.2222193010455857
system employs	2.6239319852280314
v np	2.5903630657161902
feature specifications	2.297613977329401
unsupervised methods	2.6600586248690945
word wi	2.8260549058348965
modeling problem	2.0301435463043473
developing methods	2.1597096509760467
formal definition	2.9520993205502633
methods employed	2.060145782650642
classification based	2.524119935499075
structural analysis	2.4614165333812883
text output	2.045229961808099
particular pattern	2.2776740307447065
words appearing	2.987779510780442
resource limitations	2.0551182606938574
data concerning	2.2380656204269957
10 runs	2.029677424295417
preprocessing steps	2.3203520484011997
partial credit	2.3860158255253854
see e	3.217101392991735
three functions	2.28193063938493
graphical interface	2.667884712960831
possible argument	2.109437912434099
trec collection	2.177338384924136
contain one	2.7449993637568837
statistical properties	2.408317990384386
linguistic level	2.535803077376552
much use	2.2631802623080817
objects whose	2.1479184330021646
multiple tags	2.0799644973902938
enable users	2.30606794696972
bresnan 1982	2.362352988202639
resolution based	2.017570716396185
e ect	2.2332655149968126
verb requires	2.020363647132216
previous paragraph	2.59583142385398
output word	2.1895967860268843
sub j	2.606904310215642
important components	2.022410241232288
encouraging results	2.6349746298108108
direct mapping	2.3540470303697685
extraction tool	2.0779689325499326
obvious advantage	2.0817120252117016
system implementation	2.1774937848124125
various stages	2.6044434158658247
binary rule	2.032136418263975
example consists	2.1072480253375243
significant impact	2.233717797956282
collection effort	2.1694718574205556
following sequence	2.6723975919703395
large range	2.369841578143437
individual objects	2.3506228458883727
word grammar	2.2403798268460267
role assigned	2.014014536653706
two numbers	2.417618427399807
syntactic theory	2.5342472954318085
context dependent	2.772182399476773
semantic domain	2.5584211547832028
sentences including	2.2364497093483218
second problem	3.1208570946596894
german newspaper	2.067037002975564
cases one	2.067747107964575
three languages	2.6982581052416923
precision error	2.053986137989304
training examples	3.3109987888453825
large sample	2.480027435464728
temporal adverbial	2.14057385655998
constructions used	2.089026915173973
template structure	2.2814341659531454
computational load	2.1342024470118135
evaluation experiment	2.1701156354131186
command line	2.0671902152009594
within words	2.017570716396185
constant k	2.1073394803862087
grice 1975	2.1234143692948715
place relation	2.1214166842517015
values associated	2.2328679513998626
conceptual relations	2.445692659684936
question posed	2.1487569736966314
pcfg parser	2.100061206411476
use language	2.417618427399807
relative time	2.1353034784701492
feature bundles	2.167098121771539
recursive application	2.2458216818148236
one test	2.573562255840437
achieves better	2.469909103548966
size 2	2.14183096411683
good summary	2.280088143077826
left branching	2.060765272950003
rule set	2.8760559187611534
features automatically	2.116591742190463
given candidate	2.049069748176902
items used	2.22672986885133
tagged corpus	3.2092401421750534
experiments demonstrate	2.3920948169591325
explicit model	2.0817120252117016
similarity relation	2.0843177461209628
ou r	2.2133187583704625
inference engine	2.5224929525135846
second criterion	2.1119430342456558
several machine	2.2863172876464515
feature based	2.4494924368402
syntactic parallelism	2.133389300142757
better way	2.4290948438017668
extensive analysis	2.1946340157728974
shows promise	2.109437912434099
single grammar	2.3092345630398707
computer interface	2.1708197166165597
recognition vocabulary	2.033612577484682
subject np	2.863537554637637
approach adopted	2.5300278128180302
matrix sentence	2.136610901448027
sag et	2.3769793063932445
formal way	2.2974668198166297
automatic content	2.393350325443479
total probability	2.604900418259717
process results	2.067747107964575
small samples	2.1023887620661283
introduce noise	2.109437912434099
structures must	2.3483606540998085
vector representation	2.6658923040718214
simple question	2.221537802998995
labeling system	2.1356018195482047
system produced	2.517058807274161
one individual	2.260688804545853
broad categories	2.1595944517453205
existing data	2.3823135849845682
particular meaning	2.3546421252999927
following parts	2.2112071020073643
entity recognition	3.303560765842928
common type	2.460723249657633
warren 1980	2.0435211751140128
identity matrix	2.061212299166531
important advantage	2.6671758963459182
candidate translation	2.2482408867643664
unambiguous words	2.093143489625991
specific training	2.295917867106508
artificial language	2.409550753455462
academia sinica	2.410881456137192
key issue	2.177563875982708
syntactic variants	2.3971827599709448
chart parsing	2.999255136691668
interface allows	2.458989459396439
possible classes	2.388661912704326
estimating p	2.278461809175595
node b	2.2695261903768698
similarity calculation	2.32293794955719
concepts like	2.3489571078839537
short queries	2.045541862212022
joint probability	3.153761157311599
data sample	2.055238065375353
good generalization	2.0676325559456012
word immediately	2.154924231617962
verb arguments	2.4352620852833855
task performance	2.3253734749481803
generate multiple	2.24583021875162
lexicalized grammars	2.314235577881132
high precision	3.397337490840478
parse would	2.11911577017412
identi cation	2.212575850860245
semantic ones	2.1597096509760467
specific language	2.8336074004250884
language using	2.5969837767955446
also presents	2.4211599391922816
interpretation component	2.004867676202373
paragraph level	2.357763220526021
comparable corpora	2.3787318716065156
structure proposed	2.053986137989304
abductive inference	2.011798663690109
single output	2.1939531715857408
arguments must	2.166102255087602
core component	2.1761801669559984
two input	2.4522357181311802
shared task	2.997636069813236
certain points	2.0817120252117016
subject matter	3.0575213414490943
method achieved	2.305256117322403
default rules	2.4073308992474916
words respectively	2.2533102790483586
algorithm assumes	2.3417181621687195
techniques discussed	2.1023887620661283
current experiments	2.3121495851644385
mixture weights	2.031659239081418
limited number	3.123737699999974
short ones	2.154388878793747
every object	2.237670801565959
order variations	2.0516186188527366
time system	2.295328311242409
lexical conceptual	2.4031355592328256
usual way	2.8182197630943233
research laboratory	2.501640589032693
grammar g	2.940510502676375
possible orderings	2.0763533583416773
simple context	2.3623688781715884
language l	2.8429512393927494
corresponding target	2.5845845588192393
similar meanings	2.554006539039166
clustering algorithms	2.6374035193966425
web browser	2.4503004510758957
document size	2.178914869102539
lower performance	2.6768614006979266
zhou et	2.2185399724593973
arises whether	2.0965741974051317
parser performs	2.382300983918008
approach described	3.0708933571996044
key problem	2.3229739757638654
1 displays	2.26358463877878
include words	2.512514922083054
data provides	2.183647914993236
investigate whether	2.7568766051679083
lose information	2.0074734400897967
w c	2.165860272348702
model learns	2.233997204181393
contains entries	2.200598690831077
constraints defined	2.042481049268214
previous study	2.2274490480752105
texts written	2.441300693078526
linguistic properties	2.8448457035664663
k x	2.1152653888000224
briscoe et	2.1317808304403396
output alphabet	2.0844862373503052
structure also	2.200598690831077
active edge	2.3029188583259605
several heuristics	2.3879279742060486
data problems	2.670934129205818
model probability	2.495970466700707
main clauses	2.642725453230336
task easier	2.116591742190463
suitable representation	2.116591742190463
many reasons	2.3806000578467827
second model	2.5096496577808725
different value	2.331780823064821
briefly sketch	2.109437912434099
translation function	2.0373156863450474
common pattern	2.1563079384871173
tile corpus	2.183647914993236
common technique	2.089026915173973
crossing dependencies	2.2143488331901895
highly domain	2.2328679513998626
structure without	2.368834809141683
intensive task	2.272505380879168
attribute value	2.733245252765788
present research	2.378967493188815
nodes representing	2.408571851004785
morphological processor	2.0720053006339114
different tasks	2.9369488931984655
linguistic resource	2.2166926726559373
tree represents	2.6262393868602247
significance testing	2.164769309748872
clause containing	2.366082551445672
et us	2.014014536653706
nirenburg et	2.361906894674326
structure may	2.8445493866569964
several phenomena	2.0601457826506424
special purpose	2.481519830798549
root word	2.2427119120937076
two evaluations	2.238945374754718
n order	2.2112071020073643
process also	2.314007818435079
penn chinese	2.3287959010410724
new word	3.1216456276622266
features need	2.0817120252117016
parser assigns	2.159746288077376
spoken dialogue	3.0046063182766876
sign test	2.205516715622197
different weighting	2.135844661161516
clause boundaries	2.508334299940159
vast majority	2.9679970111383795
end times	2.105083838552665
brief sketch	2.1275552563638707
complex types	2.0352110047855145
40 sentences	2.1112603678094644
early stage	2.6818477874181035
noun relations	2.022410241232288
next words	2.0361161293026124
proper noun	3.074392336023147
without consideration	2.1222463152036486
various attributes	2.178653004950429
three problems	2.6010996188684152
wide variety	3.3558314515591943
automatic discovery	2.209192498501841
run using	2.529105055367556
sentence 1	2.7187784375360127
several values	2.0224102412322886
high error	2.3560259159648136
conceptual graph	2.0641243870362462
open problems	2.184223228921038
significantly reduces	2.4784612405958746
information included	2.2492578335740094
20 documents	2.0751917915960365
conceptual analysis	2.192655007077592
varies according	2.288562847637128
class contains	2.429409673704491
experimental study	2.1432314806483994
plural form	2.690075675133152
stop word	2.58149630082237
process requires	2.475621859290715
let c	2.9610507627788856
directed towards	2.0378658638277374
control mechanism	2.4103038708414894
systematic way	2.822250952736013
new version	2.805453599310287
million words	3.5284234974433275
larger corpus	2.976593494153743
text units	2.1411796033927075
simple system	2.222406898745732
unary rules	2.2610006697605396
recognizer uses	2.075729255121515
selected examples	2.1982506992808917
analysis components	2.3322618602644836
default value	2.7269499572985207
technical domain	2.1234407732397895
different languages	3.5397023591184995
several measures	2.2248375176886155
argument position	2.841117494879423
complex domains	2.255349860587285
sufficient detail	2.1479184330021646
augmented transition	2.8040617968558785
next utterance	2.404183885189399
h c	2.000178283129414
conceptual structures	2.7715162740882895
process begins	2.594827371013212
definitions given	2.159746288077376
search queries	2.1679047592232426
experiments show	3.3378256985323667
one type	3.303057361899439
traditional notion	2.1642359406653675
expressions used	2.4547526570363916
morphological form	2.099554836557906
des mots	2.075010183763798
step 7	2.1644299823214186
correct errors	2.441300693078526
essential component	2.3398800045580104
official policies	2.7033596236321222
exact matches	2.603692804935942
atomic feature	2.070888324276621
special properties	2.045521226679158
based text	2.6949476882757466
standard metrics	2.0483937955874474
two factors	2.7599451931961574
structural position	2.0225191486479366
contains either	2.014014536653706
approach could	2.8591999734006524
linguistic semantics	2.26108506800991
local constraints	2.3949536187572127
third stage	2.086876805852392
section reviews	2.0074734400897967
providing information	2.609451926984696
forms like	2.377282352832986
new user	2.153552027758611
morphological analyzers	2.517125943143095
good reason	2.1320748009564876
verb meanings	2.0090586092742804
correct form	2.3404899548525275
perform tasks	2.183647914993236
attachment points	2.12450216397339
x p	2.3625463139324703
select appropriate	2.3706473607714544
data containing	2.1946340157728974
simple heuristic	2.8434556050567545
compact representations	2.067747107964575
similar topics	2.043229651767235
components used	2.1774937848124125
documents without	2.0023835176039797
use local	2.135844661161516
subordinate clause	2.909681899293271
full morphological	2.097389012909068
full account	2.0074734400897967
cannot determine	2.5778365599508897
relevant part	2.4211599391922816
people use	2.7284890352100577
semantics must	2.185240400682336
surface syntax	2.463272745121525
problems faced	2.0490697481769016
work discussed	2.145668076397672
module based	2.009868063569498
documents may	2.3503564505425754
spoken english	2.6380785333978904
add information	2.4658668186702535
complex categories	2.3216595274256004
constituent types	2.1894127146641735
constraints given	2.26358463877878
data bases	2.6600181678950965
np must	2.2512557276899194
word recognizer	2.124022677298511
contains information	3.149601122977319
word probability	2.412872105987849
several variations	2.135844661161516
level annotation	2.1140178105894183
list would	2.3310204571264768
first rule	3.017288144391901
articles used	2.135844661161516
th word	2.548170782985305
relative contributions	2.02585040092793
using cross	2.5480559942730014
unannotated text	2.276532200004352
actual translation	2.307091302036315
discourse function	2.0954421535324146
correct order	2.137726762436956
internet search	2.1112053387580056
best word	2.5671418055509023
sentences contain	2.7249240955372054
context sensitive	2.7844704130279605
three phases	2.8008873579979054
called feature	2.0781754103273498
regular set	2.203154941565432
probabilities assigned	2.286552317547569
psychological experiments	2.3358996282363322
general level	2.1122681466734634
martha palmer	2.1047787403569425
set consisted	2.594550888343552
figures 1	3.106280408100332
final model	2.4905108962607465
low precision	2.8929456987850277
either words	2.0965741974051317
sentence recognition	2.2660680568573235
many techniques	2.145668076397672
improve speech	2.1954890274836183
complex np	2.4005083165968553
place name	2.2770801196896384
query would	2.2380656204269957
system intended	2.1479184330021646
classification experiments	2.2888673625887526
reduction technique	2.012347776929124
word units	2.513885054810782
pragmatic processing	2.199802080679639
rather ad	2.022261218861711
speaker may	2.67877916648406
line 8	2.094023453342731
detailed comments	2.067747107964575
whose syntax	2.014014536653706
two stages	2.956583598923383
two directions	2.521656741004332
formal analysis	2.184223228921038
expressions including	2.026260250766906
one pattern	2.4106910592380197
system would	3.215234488074766
computational method	2.135844661161516
current point	2.086551350552231
learning algorithms	3.4003038801289676
one technique	2.109437912434099
matching techniques	2.7047569593246434
two sentence	2.3919334744960725
new grammar	2.689728059243937
relevance feedback	2.5105372023031314
crucial point	2.1731109929399572
verbs belonging	2.0853665562579624
best understood	2.1479184330021646
full model	2.2030908323643548
category c	2.789366980454311
discourse level	2.7350530268204363
syntactic part	2.304672028095348
image processing	2.157206459959845
la langue	2.2698204427440807
lexical words	2.2005344204707433
active verb	2.275125009000975
cepstral coefficients	2.304663673817692
related task	2.282311649307406
trained system	2.015616055186902
performance reported	2.122246315203648
annotated documents	2.132720249003027
tools provided	2.0817120252117016
training process	3.0114521361584443
following heuristics	2.257186090032611
whose translation	2.031264650127311
increased accuracy	2.02585040092793
also becomes	2.022261218861711
young et	2.0520370107650927
noisy channel	2.6125244843159914
extended version	2.820400888050602
wsj data	2.231114933170004
certain verb	2.154388878793747
common characteristic	2.089026915173973
grant iis	2.3879279742060486
every constituent	2.3205010232255585
similar work	2.3920948169591325
based nlp	2.555756637749394
spelling corrector	2.080876643932938
c p	2.140743048808871
existing text	2.3229739757638654
entity tagging	2.188953746393379
verb classification	2.196800697141404
rewriting process	2.087352091357726
tight coupling	2.0236385391388776
words appear	2.8247147873168705
arbitrary context	2.1914609895286294
reuters corpus	2.3118790424824525
clustering methods	2.5345941928465017
original ones	2.2320627568691815
inactive edge	2.281109802369453
rule looks	2.0376102740271858
word feature	2.4322502186829564
nigel grammar	2.0665126236063323
text collection	2.684830286468507
test instance	2.516065898061819
parameters may	2.1414753566437956
null hypothesis	2.6276693784857557
many nodes	2.172274429303222
initial evaluation	2.253074072245069
single measure	2.1516988527908136
document summarization	2.798614733949234
meyers et	2.0707654311661052
translation rule	2.1618890519095633
better score	2.1774937848124125
semantic levels	2.348359550546241
presents one	2.022261218861711
various contexts	2.4943690648994172
slight modification	2.443147787685106
grammar development	2.9397812246064454
whose leaves	2.3676013596092336
following case	2.2754268053105045
average number	3.447648469286496
several modules	2.375180844916545
direct speech	2.0606355586667684
acceptable translations	2.178914869102539
head constituents	2.015759442681759
f j	2.0977012022428148
ir systems	2.6351554719626815
syntactic interpretation	2.0301435463043473
large amounts	3.1352980251344196
figure 7	3.661937425534435
lambda calculus	2.1813446704867467
important problems	2.420009569037414
context word	2.335050581538012
general purpose	3.0659037810142395
identify relevant	2.4584934740439657
recognition algorithm	2.7581228668987468
xerox parc	2.201550413970512
appropriate level	2.446641693049118
various resources	2.045521226679158
dimensional vectors	2.1651154718617
text classification	2.8028552159193687
following values	2.2107864417857894
tile syntactic	2.1852404006823356
also supports	2.3513527102951746
grammar assigns	2.264070934029513
per topic	2.1779177772537377
different genres	2.6496128205992484
form f	2.045097872821157
different problems	2.3380561048888584
object must	2.5144589935023425
linear discriminant	2.215424658358881
negative examples	2.99034384369504
compound nominals	2.184547706054579
word occurrence	2.5014591567392044
contains one	3.010914190662333
standard pos	2.0823194846768738
use mutual	2.014014536653706
preliminary experiment	2.7434607929495183
sun sparc	2.060145782650642
methods used	3.0015509290776943
present methods	2.070902079606615
clustering technique	2.289694481867341
low error	2.0424793004770945
attachment decision	2.080830633228019
single feature	2.865032971777936
analysis system	2.90641899132854
intonation contours	2.040153179281739
channel model	2.494218026960666
new patterns	2.29521230639727
many concepts	2.1774937848124125
based classification	2.229850643094573
research grant	2.501400582953832
relevant concepts	2.429800097281894
n increases	2.0312646501273113
knowledge bases	3.3253959613783417
tile fact	2.2380656204269957
speaker says	2.0595811562598763
test words	2.3880710937743306
overall time	2.020363647132216
10 hours	2.1312839727750825
sentence generation	2.791089682302191
orthographic representation	2.152105323847251
certain distance	2.159746288077376
maximize p	2.061212299166531
representations based	2.189546252614866
discuss one	2.318793079863193
general tendency	2.109437912434099
agent architecture	2.080606834271845
three issues	2.285913086717917
john wanted	2.0144251740288075
human cognitive	2.076473429722452
grammar contains	2.8368062943021157
argument slot	2.2113484477087546
phrase attachments	2.0896950270155754
formalisms used	2.0376102740271858
following approximation	2.089026915173973
main idea	2.999612897393429
corpus 1	2.170453540927234
learning strategies	2.0959116368322723
syntactic nodes	2.2484953082437333
obvious reason	2.166102255087602
sentence according	2.5901978386358078
open issues	2.067747107964575
final translation	2.4762578289420167
output units	2.0400727234735476
complex phrases	2.211790418408607
method yields	2.2939172677378985
statistical tools	2.022261218861711
articles published	2.1414753566437956
four components	2.6153148829830504
single occurrence	2.3506228458883727
three word	2.4254019176185757
success rate	2.9407112360595558
different subsets	2.5115297397318246
past decade	2.2028908796704334
research prototype	2.0352110047855154
sample texts	2.3299092143753963
include information	2.708751169806749
produce summaries	2.212743797827991
natural consequence	2.1563079384871173
structure produced	2.4154478605449703
clause grammar	2.606852219891702
show similar	2.1222463152036486
john gave	2.511731782982432
used features	2.135844661161516
prototype implementation	2.334516541622288
many thousands	2.1678854032223676
full translation	2.169927587063312
trees based	2.176234939134731
existing nlp	2.1405120286798134
segmentation rules	2.1064499468417246
2 nd	2.038871200953019
based research	2.133165345977436
disambiguation accuracy	2.302930580190464
example words	2.0445276499269367
falls outside	2.181633535183382
one obtains	2.05559768656669
grammatical roles	2.530327778999997
careful analysis	2.3417181621687195
exactly match	2.4303117287858482
theoretical interest	2.5063198783573477
parse fails	2.0023835176039797
idf weighting	2.2058102909449806
state automaton	2.836038112852502
several basic	2.0817120252117016
roles may	2.1312839727750825
supervised machine	2.7297315244256306
also facilitates	2.1774937848124125
large grammar	2.5295570913094396
foundation grant	2.389326705744299
section 6	3.940589942331048
index terms	2.142997539725231
distance dependency	2.3994297233785593
discourse entity	2.5949441407065112
larger number	3.2036092681905517
best output	2.431038832879589
automated knowledge	2.0116819766087874
utterances using	2.022261218861711
nodes contain	2.086884416965048
perplexity reduction	2.119859193782564
approach based	3.1352485411925355
saw mary	2.039715365209628
similar type	2.031264650127311
technique proposed	2.1222270206314318
aided translation	2.4113614298834563
english expressions	2.3586440878837003
french corpus	2.298645289712497
bilingual lexicons	2.404925600980061
algorithm given	2.5086770708597506
generative grammar	2.6594272611303817
remaining ones	2.183647914993236
information sciences	2.163971947598413
comparative experiments	2.1497135154557805
remaining errors	2.6927179621980586
text passages	2.29368411071382
two attributes	2.5177516217480522
language syntax	2.621698572525026
evaluation measures	2.7942695267231343
class c	2.59361540970502
text fragments	2.4791560717978314
attention recently	2.045521226679158
given corpus	2.9063754185764865
one characteristic	2.045521226679158
best parse	2.681499883745598
partial understanding	2.2187954630349687
dictionary consists	2.352331304842637
important reason	2.2776740307447065
two vertices	2.0904334515920633
constraints like	2.0376102740271858
many linguists	2.1859857899642225
lexical entries	3.6017197767225952
represent relations	2.0376102740271858
recognition hypothesis	2.130003158009847
different topic	2.1529233742771527
verb would	2.1268231414414376
clarification dialogue	2.0004462982469926
output produced	2.1247287676131994
average parsing	2.1339258481610717
tree construction	2.1637170818433074
last step	2.939935977224859
sun workstations	2.067747107964575
cannot make	2.413667391558569
word cluster	2.0073178378257417
set included	2.0376102740271858
per word	3.2243600950157707
performed worse	2.116591742190463
vector x	2.2230437030927894
definite clause	2.817491616494593
classifiers based	2.313258134832421
word identity	2.190224074730801
training database	2.073707048008291
problem areas	2.0961783645192047
gradient descent	2.4353667574683575
distinction among	2.253310279048359
processing model	2.4951118749880017
language translation	2.8102419476045153
latter group	2.109437912434099
also plays	2.414320698244546
linguistic components	2.2133812259712267
recorded speech	2.367811528385638
binary value	2.099458264360342
similar methods	2.3398800045580104
approach assumes	2.30606794696972
statistical knowledge	2.296970247875455
recognizer using	2.0074734400897967
5 describes	2.885342312232834
objects mentioned	2.167885403222368
existential quantifier	2.438763937192797
implementation used	2.045521226679158
within tile	2.045521226679158
news documents	2.0561803289580536
ir task	2.0630512535488963
final consonant	2.2480484620456886
pronominal reference	2.510331905608049
state p	2.0161876059012593
system outputs	2.643532998388591
nlp applications	3.3798954766926395
spoken language	3.541985171616702
system might	2.736580953457731
individual speakers	2.0539861379893036
est le	2.174889941761219
structure makes	2.166102255087602
differs considerably	2.022261218861711
learning methods	3.243421837787493
two constituents	2.868552790763745
technique may	2.060145782650642
surface lexical	2.007038640285683
one pass	2.5480824318223743
frequent class	2.2939172677378985
syntactic labels	2.3114171860327613
statistical system	2.2577963294869763
annotation style	2.00238351760398
aligned sentence	2.1449945562093906
proposed framework	2.3046445225122545
new kind	2.393598215947388
metaphorical uses	2.029927880067892
say something	2.486660417818362
mental states	2.25133704225295
sense tagged	2.4677495581852553
models trained	2.8247650684916144
provide reliable	2.1774937848124125
label set	2.2374811801071655
single cluster	2.1871767584227904
times higher	2.0763533583416773
current question	2.0164025985712373
following modules	2.2427935805269064
certain kinds	2.794098893560588
text fragment	2.355437665846927
relation extraction	2.4643801379378014
linguistic rules	2.7251003894465993
similar technique	2.2169936022425736
interpretation systems	2.0376102740271858
several areas	2.069556602923933
xml structure	2.0379203068673784
proper nouns	3.346531994340413
readable form	2.689639198988436
exponential complexity	2.019557844009582
key features	2.447549517079291
pertinent information	2.1315696211503674
assign probabilities	2.431139609951431
cue words	2.5570420611949722
dimensions along	2.042325972844755
semantic indexing	2.480785241420839
strong association	2.0355401813680145
decision whether	2.088762691620893
certain sentences	2.200598690831077
new probability	2.014014536653706
lee et	2.565878248558516
words represent	2.0355401813680145
child nodes	2.58558314653239
techniques proposed	2.0965741974051317
learning technique	2.773968646270177
basic principle	2.3841838643071576
analysis provides	2.1859857899642225
several differences	2.0376102740271858
position within	2.3584279053719435
value pairs	3.2381609444788024
text processing	3.378419789259185
linguistic formalisms	2.1959322627493645
anaphoric elements	2.0312319370715617
1997 association	2.2631802623080817
1 sentence	2.1678854032223676
given phrase	2.3629729310777696
four tasks	2.221272634988064
surface structure	3.053550052228725
previous topic	2.1646123301199256
28 aoi	2.031264650127311
r u	2.2069049102021454
based smt	2.0100952613475203
sentences describing	2.1678854032223676
total length	2.304050011796824
second algorithm	2.353291860340023
syntactic characteristics	2.2231389704589795
occurring word	2.2222193010455857
speech recogniser	2.1429137696693727
verb type	2.253753182655405
method generates	2.1984572101092845
semantic relations	3.463337711468738
previous utterances	2.530947562199107
important issues	2.4935138875084166
common elements	2.102902855044873
reference resolution	3.0158634474968116
sentences correctly	2.233997204181393
worse results	2.4620764930126606
disambiguation system	2.51247660124379
dowding et	2.1754093285263174
particular model	2.314007818435079
rules determine	2.0074734400897967
3 contains	2.5191858325594803
kappa statistic	2.5964753062124153
previous sentences	2.7578791021160134
linguistic context	2.8766679056454074
current context	2.988435781623599
et en	2.1991009437697504
english noun	2.708573798692394
boolean value	2.2234532051296254
consistent way	2.3763414719464064
retrieval process	2.482189650856599
wsj text	2.2247622060406442
weight w	2.3125811104238214
np nodes	2.1999592456502097
present perfect	2.286401522677729
general point	2.183647914993236
methods require	2.50352395005998
possible permutations	2.2275895034737263
another strategy	2.3005523645853
important word	2.014014536653706
class word	2.346931377965751
predictive power	2.764034995718061
argument pairs	2.0499885992847338
evaluation set	2.6886265236010964
parse using	2.330318201869984
different implementations	2.2427935805269064
morphological root	2.0460967409737787
discuss future	2.305458956322113
right edge	2.1696576887220367
linguistic description	2.9843945254028497
wordnet noun	2.197108675172731
entire training	2.704684146465351
text type	2.6073251307003957
different word	3.236588128235447
average accuracy	2.8293151548204962
lexical information	3.6258521957029592
using c	2.1222463152036486
summaries generated	2.110765862739891
distance features	2.015972900817892
underlying structure	2.542282406607695
whose structure	2.296234813805337
certain cases	2.740176549109128
simulated annealing	2.268345357483142
models presented	2.2885628476371283
different segmentation	2.1940172623289786
one clause	2.5667043027512415
coreference information	2.125094321505438
salton et	2.370333738795782
closer inspection	2.2917594692280554
world application	2.0641397804139356
development data	2.9139341274943686
systems developed	2.762711211081032
four languages	2.4469899066325125
semantic role	3.0473057885733126
database contains	2.5909817942905016
two clauses	2.8556516585849443
every word	3.51621668479792
web interface	2.2340444687225673
implementation details	2.503628568806093
general background	2.031264650127311
mixed initiative	2.387745737063577
chinese words	2.9193362690277813
corpus according	2.3725247035269392
possible rule	2.1413469986390323
special rules	2.40374225252481
factors may	2.314007818435079
data points	2.903459826523414
one extra	2.1413469986390323
word similarities	2.0553852839257445
retrieval applications	2.488048946923766
increasing attention	2.022261218861711
also specifies	2.2631802623080817
dependencies within	2.135844661161516
phrasal structure	2.1157313950935537
sekine et	2.157822704739513
interesting problems	2.4235791710490444
new feature	2.9153134524728497
transformational grammars	2.3056277295196654
stage process	2.6339219017704414
performs comparably	2.026260250766906
level may	2.1222463152036486
journal article	2.089026915173973
corresponding number	2.1222463152036486
van den	2.5550664191639507
similar examples	2.4873390340894383
initial steps	2.109437912434099
sentence constituents	2.335383050109153
subsequent experiments	2.2578698255858516
new ideas	2.19037043505816
crucial difference	2.481655964382881
two contexts	2.20581029094498
given concept	2.560482058477468
also extracts	2.089026915173973
research program	2.4717250847823324
verbal forms	2.355666607622057
corpus sentence	2.061969254719088
also defines	2.282311649307406
information attached	2.21826141526814
logical expressions	2.301874657053646
genetic algorithms	2.1318410553915705
different thresholds	2.329066040124741
porter stemmer	2.133003568356231
two occurrences	2.6804543566035286
word processing	2.6122083632725395
linguistic knowledge	3.635951173625262
rules associated	2.4241657936680436
wilks et	2.447792476968474
current knowledge	2.288085882671463
review previous	2.022261218861711
category may	2.372251120681404
main noun	2.067081229293324
human annotation	2.5792024035121446
two resources	2.1399536659528025
summary generation	2.164429982321418
first study	2.1563079384871173
either case	2.9461668876686797
words directly	2.1119430342456558
capture important	2.1946340157728974
row 3	2.03938062346223
also contain	3.153749560199884
som e	2.0577375124403856
functional roles	2.335614426270997
ditransitive verbs	2.135844661161516
search path	2.0595811562598767
document pairs	2.1224701999930353
many words	3.3353895264806184
relevant words	2.4217893818277947
structure would	2.59056199895746
input structures	2.170993433660292
based methods	3.313979218684239
newspaper corpora	2.114299018453181
original definition	2.253074072245069
thompson et	2.3147871986925233
another aspect	2.5577806922873396
monolingual corpus	2.275599843719717
linguistic system	2.3471701387225035
current system	2.9901386246379116
left corner	2.531054875983764
principled manner	2.1023887620661283
underlined words	2.096756183704444
pitch accents	2.31284069347946
words extracted	2.5125561039231155
matching words	2.2825360535094297
dans l	2.099496732825468
possible pos	2.246967509856712
different user	2.2601598698812575
tous les	2.0781754103273506
systems attempt	2.045521226679158
9 words	2.089026915173973
hand side	3.630195221724507
also investigate	2.561626483276044
tasks involving	2.380830363798011
tagging systems	2.1039800387717347
accurate models	2.1380488015835484
containing n	2.00238351760398
hav e	2.1275552563638707
word extraction	2.1148954127440076
accurate model	2.267762144732373
inference based	2.0965741974051317
process would	2.6887069373952794
system takes	2.9138836729018625
general classes	2.3333329581503173
perform word	2.322778367371739
4 compares	2.257869825585851
second hypothesis	2.0244533425815945
2 sentences	2.260142018802944
automatic thesaurus	2.0844839817428333
model might	2.5429423198957037
choice among	2.512514922083054
x c	2.460113602025311
last part	2.560964388468025
first object	2.12063965644779
briefly present	2.3229739757638654
column 4	2.099458264360342
driven approach	2.6828088844550555
two articles	2.1161051295129982
training part	2.0024177265449423
based technique	2.3239101964323794
automatic techniques	2.2107864417857894
line represents	2.227812214796686
correct antecedent	2.3723517222616897
semantic similarity	2.9861772641245414
penman system	2.047778916023631
world objects	2.1792706702716673
node n	2.852170272191933
dependent upon	2.4720176514115995
similar content	2.0224102412322886
longer sequences	2.019557844009582
next set	2.2924536885149736
additional relations	2.135844661161516
particular translation	2.067747107964575
efficient algorithms	2.5709996213863664
generative probability	2.0751483849796104
use semantic	2.6497835757509205
program used	2.060145782650642
content planner	2.1367640752995145
word use	2.108179066756418
one line	2.47396486010276
intuitive way	2.1413469986390323
li et	2.4498404910698284
vary significantly	2.1954890274836183
related approaches	2.166102255087602
rule used	2.5613167061124775
new york	3.015131864313302
propositional attitude	2.1782697579799617
iterative scaling	2.550313003547375
possible solutions	2.6984677094911786
following requirements	2.260688804545853
de nitions	2.0351887168996523
generation procedure	2.2103499632097234
propositional attitudes	2.0352622408596655
lexical forms	2.665459265129915
previous version	2.4303117287858482
kl divergence	2.0510929329227183
resulting word	2.3784963844836295
special emphasis	2.30606794696972
independent phone	2.0428099383225526
th feature	2.009868063569498
separate module	2.3290814369082278
pointwise mutual	2.4322368474736327
asking questions	2.412417536370418
unrelated words	2.181633535183382
detail later	2.1479184330021646
using word	3.0828882180758406
explicit representation	2.7798800404598394
reciprocal rank	2.294951456991246
surface words	2.3694628349497235
issues related	2.392203246999948
take advantage	3.3180311770858673
translation rules	2.4684295770056153
work also	2.5715673631957685
relation may	2.617891242311779
learned decision	2.0052694167148832
different grammar	2.22652117801584
address issues	2.166102255087602
big problem	2.183647914993236
semantic analysis	3.7148438034956723
structure similar	2.166102255087602
english corpus	3.0463438491470662
initial goal	2.120101630234125
utterances within	2.194325391320961
promising results	3.1117169031184178
one explanation	2.2709576744960343
atn parser	2.2517149257502638
connects two	2.1739387095397857
graduate students	2.7044755448237936
major bottleneck	2.022261218861711
phonetic alphabet	2.273939470705714
errors occur	2.5576830384474456
matching problem	2.249945344501949
obtain information	2.4619447088476862
better performance	3.5521641283049576
one symbol	2.409668321623291
selecting words	2.0297445546210575
common error	2.0763533583416773
sentence w	2.3971002219744073
work may	2.313328091895502
modal operator	2.0494172105712174
every grammar	2.1588487997915733
identify non	2.1816335351833818
semantic structures	3.089867169991554
uses simple	2.089026915173973
form p	2.6050400774384954
decision list	2.341975878670305
indirect object	3.1385727830790664
improving accuracy	2.049069748176902
defined threshold	2.154388878793747
following definitions	2.536987862957993
major source	2.3731678494098376
jerry hobbs	2.332240952226321
grammar whose	2.2492578335740094
ill tile	2.653611405343162
use sentence	2.0376102740271858
corpus tagging	2.026260250766906
improves accuracy	2.115924756446767
new topics	2.3050471301632443
word length	2.686478732278914
two lists	2.7881572895895097
alternative ways	2.6584018814021215
table show	2.067747107964575
features indicating	2.1563079384871173
structural relationships	2.2911948428372915
rule applied	2.318222716608852
classifier used	2.22672986885133
worth pointing	2.414320698244546
following facts	2.3229739757638654
events occurring	2.045521226679158
clustering process	2.360112931171831
certain threshold	3.1073391058390443
one chooses	2.0880414073297953
anaphoric links	2.216433911233839
probabilistic framework	2.605461530789095
allows users	2.7985591799697853
sequence labeling	2.097234748821733
improve overall	2.2427935805269064
onr grant	2.24825378073324
affect performance	2.200598690831077
among verbs	2.0297445546210575
grammatical formalism	2.4597920863592893
previous results	2.5755765135410424
large feature	2.4047215428666187
words using	3.0311603899477153
probability estimation	2.5625566227454524
language structure	2.5812107910216766
constituents must	2.075729255121515
bar theory	2.26110911836725
approach cannot	2.272505380879168
time points	2.3524092212464067
explicit reference	2.045834551013545
heuristic based	2.1479184330021646
previous studies	3.171322714253944
application dependent	2.0919612108326557
different output	2.1499148428335464
another solution	2.3763414719464064
higher coverage	2.202020216964557
subsection describes	2.00238351760398
multiple interpretations	2.4741288466438416
another language	3.191686316581336
full coverage	2.358766725317489
efficiently compute	2.135844661161516
processing required	2.32313543527454
kamp 1981	2.028939001938722
argument relation	2.2524051318160296
list l	2.11557814765122
richer set	2.472505635726833
4 show	2.848788382993413
new algorithms	2.3710100156961955
wsj test	2.1368404379378694
smoothing methods	2.3630172674300165
tree built	2.045229961808099
predicate calculus	2.812200599894546
one state	2.5219259947627792
different parse	2.1165917421904625
tensed verb	2.228349692170916
simple yet	2.1312839727750825
last n	2.079019887609717
target audience	2.0312646501273113
see equation	2.210100193741667
parser without	2.3130951516759186
word might	2.3229739757638654
arabic text	2.1035965704006427
correct reading	2.1795788493780517
modifier relations	2.2308031572326708
reliable information	2.2224068987457315
question arises	2.858230663826542
old ones	2.0114577897060197
specified number	2.200598690831077
becomes difficult	2.166102255087602
rules make	2.3271110475429713
agglutinative languages	2.3525745418694655
general question	2.4223028454014885
different patterns	2.7359002960242673
developed based	2.2074757805024428
one percent	2.0692336389510193
hovy et	2.2791350886609054
context feature	2.1508398570977194
lexical preference	2.157742540229691
internal structure	3.3323097940047504
parsing framework	2.365242673797953
last names	2.4550327354752923
many entries	2.2427935805269064
tagging task	2.743418751592424
rule x	2.28318670609653
text chunking	2.313999201322282
computer program	2.8501310243900693
parser requires	2.159746288077376
developing techniques	2.1543888787937475
clear advantage	2.3879279742060486
sentences annotated	2.257796329486976
smoothing technique	2.487303046126164
two observations	2.25398049501041
phrase pairs	2.343221570334814
system found	2.2047194975469235
first train	2.1023887620661283
best classification	2.146224553829869
translation tasks	2.3029148383681344
uniform manner	2.0817120252117016
object positions	2.0082269995432465
efficient processing	2.7430671254456223
speaker utters	2.0423259728447545
avoid problems	2.200598690831077
interesting issues	2.2112071020073643
larger system	2.406532320205305
7 points	2.075729255121515
answers given	2.0376102740271858
hmm models	2.3832110647153515
subjects would	2.053248452323013
lowercase letters	2.1380488015835484
another action	2.146224553829869
classification system	2.5984102876319524
single source	2.296131235086933
quality translations	2.3322618602644836
straightforward application	2.0376102740271858
k 1	2.0473639765896525
hmm model	2.5493834095574317
example one	2.0376102740271858
table lookup	2.218555802555602
feature description	2.2329657360128907
depth analysis	2.1984572101092845
resulting grammar	2.551293087826894
automatic sense	2.232062756869182
e c	2.7001717825308735
automatic lexical	2.1637030579857273
modified versions	2.0577375124403856
descriptive adequacy	2.111260367809465
serial dependencies	2.104447110576919
set 3	2.0007936461763087
random field	2.46706573212836
text production	2.19476888680063
ai research	2.2758754002363615
base containing	2.183647914993236
resolution system	2.5540815700706307
understanding conference	3.0841995816710095
particular features	2.524067375092386
corpora consisting	2.014014536653706
large collection	2.9673059239101685
25 sentences	2.089026915173973
model takes	2.5327107950191152
free rules	2.9821074773791603
halteren et	2.1156191174588637
verb takes	2.4160787034667823
algorithms may	2.406719372930616
expression like	2.2918809527346156
text segment	2.576411606673064
generalized phrase	2.665188666903419
michael collins	2.345591613736791
model derived	2.093294558436213
additional analysis	2.060145782650642
using hand	2.4148184100839467
accurately model	2.014014536653706
general context	2.398910627626017
one constraint	2.462216415772723
double quotes	2.063423065972687
three feature	2.2443687102109093
regular grammars	2.2772567071998067
roughly corresponds	2.4943690648994172
elements may	2.487258520884037
information stored	2.8515294953110297
figure also	2.318793079863193
driven phrase	2.90349273180833
adjacent characters	2.014014536653706
based formalism	2.739571084221698
less importance	2.022261218861711
validation experiment	2.2256816819452245
common practice	2.616250359361909
2 results	2.2062448606993037
single term	2.33442808544182
spreading activation	2.5268178961040983
general description	2.4682791781671014
two orders	2.3725247035269392
discourse markers	2.3876157899843977
current address	2.1607388591823553
speech act	3.006666096686702
connected nodes	2.070902079606615
phrases without	2.2275895034737268
et el	2.6658421449985443
grammar writing	2.2844168906394247
evaluation procedures	2.3454568729702805
one error	2.4892605717182814
nsf grants	2.631339938520658
root category	2.0342985541158325
accuracy compared	2.296234813805337
phrases may	2.5980817446720854
recursive transition	2.2183912844168048
resulting output	2.0376102740271858
best match	2.6416219068063063
new category	2.2857443413397442
new strategies	2.014014536653706
1 v	2.2013688381809358
tasks like	2.5593166380164063
typically contains	2.0817120252117016
efficient use	2.3956433384568747
first constituent	2.353936130320921
algorithm needs	2.4251394527998116
negative logarithm	2.0880414073297957
cannot express	2.2224068987457315
value 1	2.4931783620473804
empirical question	2.478461240595875
processing algorithm	2.0918364857418936
text written	2.1165917421904625
condition 3	2.0594417027446488
every edge	2.158875372008553
expected values	2.3794289964955144
chinese information	2.2640242571042535
test set	4.028482879244477
baseline systems	2.389324756276414
feature system	2.5648287557013867
specific permission	2.850240182896208
present state	2.6145318066831376
case analysis	2.1875733214545923
syntactic model	2.263608750737464
efficient methods	2.2328679513998626
different arguments	2.0074734400897967
heuristic approach	2.2541760535899478
consistency checking	2.299560947587238
task specific	2.06468203219145
work attempts	2.0074734400897967
user requirements	2.1356018195482047
sets used	2.6946495651442515
example 6	2.5232209369967418
much simpler	2.90764806215907
earlier systems	2.166102255087602
recall scores	2.4933637691485817
tagger trained	2.224181951210027
asr systems	2.000097843578679
biomedical text	2.214939637453285
general knowledge	2.970248465677351
first element	3.1128628973190313
packed representation	2.0139246851514185
english nouns	2.5335020901050336
english equivalent	2.273939470705714
probability given	2.57276412044517
9 proc	2.318793079863193
relative weights	2.1946340157728974
using unification	2.2937733624949908
current status	2.675152689970683
2 data	2.241556731652663
restricted version	2.257281395364781
learning process	3.195045830505929
using patterns	2.026260250766906
understanding components	2.0530842980529735
discourse situations	2.0743965972347684
line system	2.397957210424912
sentence could	2.6015241666705364
algorithm combines	2.1312839727750825
language words	2.6390905340671265
sentenc e	2.022410241232288
many information	2.178653004950429
like speech	2.1774937848124125
relevant information	3.3269116569281008
use wordnet	2.6015241666705364
metrics based	2.006571617361778
score values	2.049069748176902
modeling framework	2.1760577188495196
different kind	2.7703992763303584
top right	2.255349860587285
appropriate context	2.478143211430341
general system	2.22672986885133
analysis performed	2.026260250766906
rich variety	2.0312646501273113
work related	2.1479184330021646
plan operators	2.2372487832695853
individual verbs	2.320773549445688
vector similarity	2.0435975245753424
separate sentences	2.351334739093333
generating appropriate	2.0074734400897967
communicative goals	2.712097802318716
detailed specification	2.1023887620661283
final representation	2.2285093470807924
likelihood ratio	2.859223224459874
similarity scores	2.770380541221578
long vowel	2.1470265484114366
words tagged	2.5552643227954897
unification procedure	2.054579666867637
object oriented	2.362505415324706
obtained results	2.4755931291616378
japanese newspaper	2.1763061798931354
information structures	2.245199749514539
verb stems	2.1198235598457993
hand sides	2.871975501547667
score measures	2.012347776929124
class probability	2.133623440963199
first iteration	2.629568081286645
strategies could	2.0376102740271858
discourse functions	2.083059647949183
common denominator	2.296234813805337
test conditions	2.2339270481759206
opposite order	2.022261218861711
correspond closely	2.1479184330021646
scoring systems	2.042481049268214
p u	2.025073621873738
semantic input	2.37181358452883
verbal predicate	2.1649596527500012
entries whose	2.045521226679158
word terms	2.439264633062927
treebank grammar	2.0276399489172547
language utterance	2.0776713017441484
implicit relations	2.0727217266824014
semantic interface	2.012036422050798
event types	2.604447065192607
making use	3.4697959863345726
always use	2.1852404006823356
parser attempts	2.075729255121515
word matching	2.221823019603162
start time	2.112778664196261
times larger	2.4851929011288982
representation framework	2.216598368661922
reasonable approach	2.109437912434099
real values	2.089026915173973
specific attributes	2.116591742190463
representation schemes	2.0853752948128275
grammar also	2.4459101490553152
speakers may	2.4005597282692768
relative clauses	3.4569105366176913
x k	2.0828211823182325
strategy adopted	2.1543888787937475
complex rules	2.2758754002363624
two expressions	2.5754458096871717
performance compared	2.2470362724777164
significant result	2.1072480253375248
single segment	2.307091302036315
present discussion	2.4658585645856936
construction method	2.0643104749903145
distinguish among	2.5577070560524944
word counts	2.4994489180382855
fig 1	2.2955083501313895
particular structure	2.200598690831077
main steps	2.7782787052139843
description given	2.227249320716272
speaker uses	2.3127450300423407
grammar without	2.386958661868031
several possibilities	2.3723877389097376
since none	2.4560115027140723
practical machine	2.0067061523912564
human listeners	2.2147431158325057
commonsense knowledge	2.3252102758244604
kappa value	2.225183780069205
precise definition	2.7249617453217225
four word	2.0824927688889554
whole process	2.905514734632954
simple decision	2.1119430342456558
earlier example	2.3130951516759186
early experiments	2.296234813805337
data annotation	2.075729255121515
technique would	2.2112071020073643
two daughters	2.1462245538298697
phonetic units	2.0142838840013058
system must	3.4798259906372344
gao et	2.17674258762191
two target	2.1621373145487084
preceding section	2.7465297235368333
appropriate knowledge	2.1268231414414376
valuable suggestions	2.2631802623080817
table also	2.7190982938069155
example shown	2.668421914508176
single terms	2.100387256098407
japanese nouns	2.03820918056356
grammar requires	2.172274429303222
existential quantification	2.252290089836351
previous phase	2.1275552563638707
case occurs	2.1886535279817885
1 lists	2.9788062204439414
following questions	2.752657511445583
surface information	2.133337178648789
gaussian prior	2.496303158018334
axis shows	2.1461787237202614
one dependency	2.159746288077376
two runs	2.334465930933078
time taken	2.596865193775877
represent semantic	2.3417181621687195
input semantic	2.1342024470118135
match exactly	2.319324296244544
efficient search	2.516777682323169
many purposes	2.3523313048426364
combine multiple	2.0421480663134544
inflectional suffixes	2.1897386758010873
serious problem	2.866812442250025
speech assignment	2.120473489839857
word b	2.2031599912695
x may	2.0932945584362126
probability estimate	2.5792284646070134
special status	2.422302845401488
learning theory	2.3069618176552806
inchoative alternation	2.0262602507669065
information corresponding	2.1678854032223676
confusion matrices	2.205037006299681
esprit project	2.296591647376872
qa system	2.6712192772209935
features alone	2.330393402432155
spoken input	2.6804705866557295
much harder	2.6273476118034016
training using	2.307091302036315
multiple sources	2.7728475381954163
time per	2.4951523092890886
collected data	2.5541530076379093
different realizations	2.043229651767234
three algorithms	2.3337395385196897
f r	2.0893925150367654
extracted terms	2.114820586290371
new technique	2.5489983832234455
translation problems	2.325823278732151
theory may	2.075729255121515
possessive pronouns	2.5843610058726454
grammar design	2.053986137989304
cognitive psychology	2.3299386891424736
important feature	3.0347916828193346
confidence intervals	2.5633259927653924
computer science	3.438837823315411
general lexicon	2.1522130098167818
multinomial distribution	2.031944624567844
examples shown	2.0376102740271858
following example	3.6862077087063376
multiple paths	2.043373958776237
growing interest	2.369094096198505
resolving anaphora	2.097490679815661
detailed examples	2.0074734400897967
corpus described	2.4682791781671014
detect errors	2.2625480205121296
candidate set	2.6078794693187177
certain text	2.1774937848124125
classifier performance	2.1939531715857408
words within	3.219596719695601
strategy described	2.37849638448363
many forms	2.5387687219528585
given character	2.0236385391388776
system error	2.1117964579724733
objective measure	2.139044594632718
middle ground	2.1852404006823356
dictionary entries	3.1800150261876685
process takes	2.431764288227561
dependency relation	2.886147039890416
previous attempts	2.386117680138509
passive voice	2.9426860579465988
word bigram	2.2805978955100557
different meanings	3.099603579442641
agreement constraints	2.2093487881958334
null tence	2.2328679513998626
word choice	2.4668888439148766
work shows	2.3229739757638654
positive value	2.382648864965101
translation probabilities	2.694840263848446
methodological point	2.022261218861711
improved precision	2.1852404006823356
every feature	2.650226886500534
particular phenomenon	2.075729255121515
recognition component	2.6238861677276923
semantic objects	2.4078056786225455
multiple words	2.65692868135863
examples include	2.9644648325917475
three kinds	3.264710259487658
every verb	2.498621589417806
message processing	2.200661481121184
first reason	2.045521226679158
subjective evaluation	2.284098357162916
v e	2.509676871061286
six times	2.3005523645852994
word appearing	2.3639525993606796
described herein	2.160181161519838
bigram models	2.2264379317369563
single tag	2.291880952734615
complex situations	2.067747107964575
word pair	3.0258754570969493
bigram model	2.8126353893350107
references therein	2.32391019643238
precise way	2.189546252614866
per cluster	2.0355401813680145
formal notation	2.1290482690107417
document vectors	2.1853909161341183
top part	2.138048801583549
understanding tasks	2.280942543356468
many discourse	2.154388878793747
surface case	2.089857964163576
possible situations	2.061969254719088
best use	2.0199612517787866
main goal	3.0782249726748323
dialogue strategy	2.1683391038892594
cannot contain	2.1816335351833818
finite set	3.352474628295824
hard task	2.3855808741537103
complex knowledge	2.0817120252117016
techniques would	2.278461809175595
possible value	2.5445384238364364
computational tractability	2.257281395364781
utterance containing	2.114299018453181
data show	2.0940461961350008
rules used	3.2027566465895996
probabilities estimated	2.2098706669249486
every occurrence	2.4922686900186415
carletta et	2.377654446674476
relations described	2.0390072838737376
phone numbers	2.001339386889736
word endings	2.2339270481759206
additional input	2.1543888787937475
three subjects	2.074810538309736
basic assumption	2.607007640986166
common sense	2.8363221736179063
training materials	2.2544389428349207
variation across	2.158407162326257
degrades performance	2.0376102740271858
new search	2.052037010765093
see also	3.4652284796230726
reference sentence	2.098041559116367
two noun	2.6405032524754195
possible attachment	2.2597813303043677
makes reference	2.2256816819452245
two times	2.6217762071382875
memory capacity	2.0580417686786436
default feature	2.1051277246894857
one interpretation	2.816393457801258
c may	2.1290482690107417
utterance boundaries	2.0778348055790197
random set	2.0388643487004234
given item	2.137726762436956
word vector	2.185216671905356
lower perplexity	2.346689061992337
syntactic forms	2.632961085130901
ordered pairs	2.4823860341383233
single pair	2.070902079606615
base grammar	2.012038826400085
young children	2.153552027758611
similar result	2.109437912434099
speaker turns	2.067174455413347
model theory	2.12388417950383
new term	2.3255541479294672
word found	2.166102255087602
longer phrases	2.1154706826180134
grammars used	2.384183864307157
focus shifts	2.1185047675079156
systems based	3.16290410511748
inheritance hierarchies	2.2143442211259234
word hypotheses	2.4538217447394786
surface forms	3.044412514237024
many features	2.873247633964736
highest precision	2.4503922558383535
relevant features	2.950147609911342
whose left	2.4529748828682516
category would	2.045521226679158
approach achieves	2.4663690826116857
ken church	2.293917267737899
source texts	2.5665207501102527
sample sentences	2.7632126312834813
entire input	2.4683935743557397
share similar	2.3448347819209543
three strategies	2.191114775755528
new examples	2.400082656453792
composition rules	2.273293646034834
contains 100	2.014014536653706
specific contexts	2.282398402847935
feature f	2.7794582526195826
untagged corpus	2.192925778410105
text theory	2.2651662548541567
class classification	2.416220539541077
corpus could	2.4103259885026898
paper deals	2.7403035930515904
takes time	2.0024615186367023
ntt communication	2.045521226679158
common structure	2.2020202169645566
efficient parsers	2.007315006862137
different trees	2.2003452960847527
textual data	2.7444240604894037
process terminates	2.3263148914896092
specific resources	2.072005300633911
different classification	2.339880004558011
strategy could	2.0312646501273113
sentence based	2.5385603545976756
main classes	2.4197547133270083
tagging performance	2.179209876010537
nominal form	2.1342024470118135
start node	2.1827524849847553
following form	3.187874871968995
times real	2.121466355343561
considerable amount	2.6241285425347263
gram based	2.3520026230586053
two evaluation	2.3997947207672894
sequence x	2.27515069652231
insertion errors	2.165544524420019
data extraction	2.610160338739814
resulting feature	2.153552027758611
square test	2.188983978167193
di er	2.064139780413935
quantitative analysis	2.4260324477975805
techniques similar	2.305458956322113
similarity based	2.4773246167686063
every document	2.409229130313168
time complexity	3.1045831849072703
robust syntactic	2.084486237350305
good use	2.333950776258771
number generator	2.0074734400897967
top nodes	2.06468203219145
using feature	2.463090923310541
list consists	2.218434485750926
potential referents	2.162715511048699
task involving	2.089026915173973
vector representing	2.175292712224692
particular sequence	2.2532512925302806
poesio et	2.161158232087539
model must	2.7785470641867387
short sentence	2.216652831431069
actes de	2.3869586618680305
verb must	2.4148184100839467
another alternative	2.357121432847829
lexical representation	2.829366785755765
different topics	2.680914730746545
values corresponding	2.039007283873737
strategies based	2.140860171621032
single process	2.0022853126259594
speaker intends	2.5422417712094605
modi er	2.0445853050550293
test whether	3.100995806272086
physical location	2.022261218861711
first word	3.379770318679551
syntactic features	3.37348575772048
section 5	4.230338028010943
flat structure	2.343506606386406
arithmetic mean	2.040238153498075
following order	2.5036665926162365
basic version	2.030143546304348
similar reasons	2.2328679513998626
increase performance	2.1245637044373713
feature agreement	2.020363647132216
lexical association	2.1392509863687934
syntactic constructions	3.2046161469156518
separate data	2.2020202169645566
novel approach	2.9862013406755357
tree contains	2.369841578143437
audio files	2.1694633268926293
also introduces	2.189546252614866
reduce computation	2.00238351760398
pos sequences	2.1903440715835174
rst step	2.2231389704589795
given level	2.4871944242259447
capture different	2.2112071020073643
html files	2.022261218861711
different research	2.1275552563638707
best search	2.06173034965947
per category	2.1222270206314313
text must	2.4949866697330707
system consists	3.2460399958884905
negra corpus	2.126007302930681
figure shows	2.68054029211252
former approach	2.2169936022425736
making decisions	2.3955781761503503
singleton set	2.2321022309210736
allows us	3.9967402363189404
primary focus	2.3866435672388717
database may	2.0074734400897967
standard chart	2.094727102798552
brown university	2.1678854032223676
solid line	2.421619438140385
relation would	2.264070934029513
different algorithms	2.743204330728333
tree kernel	2.1448261597506284
user questions	2.2677621447323726
system produces	2.8979387463802193
topic detection	2.5212586571763325
level alignments	2.1460226422107826
r example	2.200598690831077
also help	2.305771163258715
original position	2.1290482690107417
first version	2.9401009904500595
problems mentioned	2.4089730919291816
work differs	2.57917382991629
new segment	2.1381987140409975
substantial reduction	2.189546252614866
major research	2.2328679513998626
language information	2.9512738983171074
binary branching	2.6511542834695034
logical representations	2.082836083424519
text database	2.246856271105669
expert systems	2.7929369344685604
work would	2.185488621164242
maximal set	2.0969091179100516
conceptual framework	2.1678854032223676
following subsections	2.8891240381758125
manual work	2.3310204571264768
performance measure	2.261048002666622
analysis module	2.5680556628561537
morphological generation	2.198813440424927
sets contain	2.086884416965048
verbal stem	2.0753388214874464
third argument	2.348375122351955
additional semantic	2.5056712599262134
document clustering	2.2655096485031683
speech sequences	2.2601220552236354
long history	2.515272207357606
different distributions	2.4144226718804758
section three	2.0918364857418936
fill slots	2.1904989417851644
independent meaning	2.1353034784701492
10 documents	2.3065837960095763
feature co	2.023833476279208
path p	2.280620608865212
syntactic bracketing	2.0452299618080985
model corresponds	2.109437912434099
values 0	2.159746288077376
standard word	2.5774906910825686
evaluation used	2.075729255121515
deletion errors	2.1000868080586823
large search	2.2631802623080817
higher values	2.3052677840330396
since l	2.0817120252117016
c b	2.0523292458965874
program committee	2.3671782180475036
grained analysis	2.1112053387580056
particular attention	2.4689553350630398
previous papers	2.2380656204269957
parse structure	2.2784191688918103
improve word	2.1806129413887136
small lexicon	2.144865366160583
translation word	2.079019887609717
structures shown	2.1859857899642225
second component	2.67337364239059
phrase recognition	2.1707715823840887
one conceptual	2.060145782650642
pilot study	2.749255957490495
parsed corpora	2.2532114962722387
kim et	2.491825394888175
attachment ambiguity	2.5756346605538267
word test	2.1105162663880916
two summaries	2.042809938322552
software tool	2.3162726447741373
stack contains	2.085074160737753
patterns match	2.013393050128415
corresponding rule	2.422664184151837
matching algorithm	2.777634309879703
models built	2.3079188680593137
use linguistic	2.3540470303697685
multiple events	2.0022319013701297
state may	2.055118260693857
compare results	2.4917756927623538
high percentage	2.5448904488010253
clause may	2.258887940509119
language phenomena	2.6475561571598956
based error	2.399589008400031
medical domain	2.586735181807458
different subjects	2.2965916473768724
data source	2.6025545966570696
main question	2.020363647132216
previous research	3.3145682533850263
particular person	2.1117654159915116
syntactic conditions	2.037402853550243
infinitive form	2.4846411894262057
alphanumeric characters	2.045521226679158
topic identification	2.177801413321817
word senses	3.484971988003087
score associated	2.055118260693857
retrieval using	2.2687087058757234
simple sentences	2.871588425653309
computer aided	2.1039800387717342
parses generated	2.1816335351833818
online dictionary	2.0648905011597334
parser would	2.835804158729712
results discussed	2.116591742190463
including sentence	2.0965741974051317
minimal units	2.1234143692948715
previous algorithms	2.338901527421961
one structure	2.408317990384386
design philosophy	2.0965741974051317
online text	2.0965741974051317
hard copy	2.0975428041023823
following strategy	2.2917594692280554
15 seconds	2.0743965972347684
also captures	2.1023887620661283
confidence score	2.6236532953246106
given speech	2.067081229293324
basic data	2.577151220460659
statistical decision	2.0345605107603677
interesting feature	2.1479184330021646
relation type	2.413502408236917
another application	2.631339938520658
approach outperforms	2.330318201869984
method employed	2.329598655974965
reasonable number	2.380830363798011
7 show	2.3567860333521553
annotation system	2.152791091267105
two parses	2.2310878058681087
agreement among	2.2586439211853158
massachusetts institute	2.045521226679158
new tag	2.2566978107196425
approach combines	2.1023887620661283
great promise	2.166102255087602
see text	2.022402441477414
research goal	2.1943253913209606
template task	2.086179585468413
fairly straightforward	2.724620719270728
derived fl	2.022261218861711
common words	3.064230908355333
complete noun	2.0577375124403856
sentences involving	2.4196519231039106
different strategies	2.99353311452595
2 shows	4.202890026210513
several language	2.3322409522263214
system selects	2.5294407809910826
descriptive power	2.315148819164931
dominance relation	2.3267583189922774
dependency graphs	2.2934131970157
stress assignment	2.030076319647331
corresponding pos	2.022261218861711
stochastic parsing	2.0792499645377673
empirical distribution	2.299277559754648
zelenko et	2.2432061400370964
parameters using	2.3725247035269392
action part	2.035079038835738
good examples	2.2776740307447065
sample results	2.014014536653706
semantic attributes	2.219584087278643
disambiguating information	2.182888476491442
allowing users	2.1739387095397857
similar techniques	2.5777505121879893
one place	2.7778295431923445
entire sentence	3.1704543957192906
remaining input	2.012701941512196
two authors	2.2147431158325057
formal basis	2.001705936914745
open source	2.6626849626319853
surface string	2.8459918959130537
make significant	2.053986137989304
nlp technologies	2.151302016846157
training dataset	2.0636599101335804
set size	2.9129770166965963
robust parsing	2.727652826609105
various classes	2.1413469986390323
natural deduction	2.0890495593733682
main categories	2.5144589935023425
third columns	2.0965741974051317
statistical measure	2.3681423926547103
first conjunct	2.3272373825582653
raw frequency	2.0630024689277313
given entity	2.2222675308868496
case slot	2.102375733343658
time efficiency	2.2156848478774354
single individual	2.135844661161516
possible improvements	2.3777902240579687
steps 3	2.0743965972347684
different processing	2.351334739093333
alignments produced	2.196534791510497
mistakes made	2.3605881269336804
fixed expressions	2.093529289836999
tag given	2.008226999543247
computational cost	2.8870408529069893
table shows	3.24885045973003
two phrase	2.1852404006823356
automatic information	2.049069748176902
rich languages	2.265377352213068
extensive evaluation	2.1739387095397857
earlier paper	2.2447151258928626
bigram features	2.0473368898884488
single step	2.317189433640852
head daughter	2.4784694282277795
efficient access	2.104065875772636
simple cases	2.686874393443033
patterns would	2.1012225606144637
derived words	2.2852658354069004
syntactic parser	3.1023896974338223
worth considering	2.2578698255858516
first k	2.200183409315332
possible subsets	2.0577375124403856
identifying words	2.026260250766906
substantial improvements	2.488045314009769
hacioglu et	2.0333762149816526
three aspects	2.602446094626907
english system	2.421308158678956
al language	2.1479184330021646
viterbi alignment	2.3040663972803026
data may	2.4908676320196843
many candidates	2.271753499640152
word following	2.1615556184700297
original paper	2.0460967409737787
first page	2.9205999196116843
corpus based	2.76650496423089
sgml tags	2.280211081379637
key insight	2.0817120252117016
darpa communicator	2.155053809662567
generation approach	2.135844661161516
strategy based	2.147816928703614
default assumption	2.03820918056356
linguistic considerations	2.318222716608852
weight given	2.292453688514974
left end	2.1051987238955414
key component	2.5791738299162903
children nodes	2.180390457980577
wider range	2.864040711623857
errors caused	2.5288019059241145
phrases would	2.020363647132216
process must	2.6090459237723915
sentence corpus	2.083918959489316
three examples	2.776168663533263
two variables	2.7470098239646656
sample data	2.5052793556405
existing models	2.0653770836899343
first parse	2.324016176388633
smallest number	2.3263148914896092
evaluation strategy	2.2288797259114883
parameter tuning	2.579478526188788
occurs within	2.5815863676268798
anaphora resolution	3.1221861913486104
selection module	2.051978814850812
tables 3	2.7785217538312694
limited range	2.299686872045762
model consists	2.782470001538354
left part	2.5549271613027438
method outlined	2.045521226679158
linguistic motivation	2.3992522812758517
different type	2.661679342850055
different usages	2.0774165797489923
main difference	3.226631529424918
function returns	2.266519245198103
baseline performance	2.7825303591132173
ambiguous structures	2.0316144144734802
target structure	2.0986209644574094
rule corresponding	2.1543888787937475
new versions	2.154388878793747
engineering research	2.566519959895443
model produces	2.233997204181393
extraction systems	3.0188341883859193
substitution nodes	2.274558876391925
lower scores	2.3182227166088523
yields results	2.022261218861711
2 test	2.279673679644188
western languages	2.07198488729853
information helps	2.200598690831077
common parts	2.104727503321801
individual scores	2.1042904425091606
tutoring systems	2.3048834426823657
horse raced	2.073371587471194
oriented approaches	2.070902079606615
links among	2.4587293103141286
testing results	2.045521226679158
correct tagging	2.1222463152036486
log likelihood	2.6510436881129396
induction algorithm	2.4041183200133247
experimental work	2.223453205129625
based dependency	2.0307439411553245
underspecified representations	2.009134796779866
figure 2b	2.284035923185844
basic design	2.2827808432516172
structures generated	2.399822042602986
text consists	2.1023887620661283
knowledge might	2.2112071020073643
two hours	2.1619381542689116
one instance	2.9059221902366326
bilingual parallel	2.035409656077611
2 senses	2.012347776929124
stative verbs	2.0111161823711523
rules contain	2.222406898745732
best tag	2.1039800387717342
something like	3.17350880243249
systems produce	2.227249320716272
function using	2.1290482690107417
intermediate results	2.7440485256582177
one experiment	2.4658585645856936
possible reason	2.514628276488317
tile rules	2.2571860900326115
produce two	2.1497902212099103
simple extension	2.4103259885026898
distributional properties	2.3645553633450382
dependencies among	2.6727205136289283
different methods	3.095593000147087
consider tile	2.1023887620661283
similar concepts	2.1627141798565566
v p	2.128439845663996
simple version	2.159746288077376
words contained	2.6707546915695466
possible utterances	2.035211004785515
list structure	2.2419529662889204
10 ms	2.008226999543247
across sentences	2.3692777127487306
describes one	2.0817120252117016
negative words	2.093859016151434
validation set	2.6028258655253387
certain type	2.491685947589982
relational structure	2.156587076024614
precedence rules	2.16202161159633
que la	2.284817167364965
longer documents	2.1724093943740512
work uses	2.278461809175595
poor performance	2.661109733843909
japanese system	2.005674037863268
features whose	2.5361198428948413
unlabeled data	2.5653433214516883
boguraev et	2.055385283925744
mapping table	2.13821635516409
four feature	2.061969254719088
standard parse	2.077635395584168
translation output	2.4419092495953416
macleod et	2.007315006862137
information gathering	2.2020202169645566
memory based	2.3803748847608253
news data	2.335122819457052
several ways	3.393082063413206
input tree	2.2392353595201646
paper concerns	2.1983168585210526
two classifiers	2.3159730641579004
different viewpoints	2.122186550472634
english lexicon	2.807408183273531
additional attributes	2.0074734400897967
entropy modeling	2.412079749663824
earlier approaches	2.271753499640152
languages used	2.2891619181755747
training purposes	2.1543888787937475
standard unification	2.1642359406653675
level formalism	2.0730662534060538
treebank annotation	2.4108027272478347
different dimensions	2.0802362953533846
two drawbacks	2.022261218861711
third question	2.026260250766906
testing data	2.967181307883104
rhetorical structures	2.1318262623434623
situation semantics	2.1459372052175967
word ends	2.0904334515920633
5 decision	2.299071968354055
new applications	2.2626443351441323
two clusters	2.610392027212772
spontaneous utterances	2.082836083424519
probability function	2.226556581782148
alternative view	2.1268231414414376
value v	2.1766360892670584
experiments using	3.4115525833417197
database records	2.160181161519838
complete corpus	2.0722900984192454
related verbs	2.2093487881958334
better method	2.045521226679158
classi ed	2.3739190617216837
aligned corpus	2.570126925694754
subcategorization frame	2.5953338353525357
sentence cannot	2.4384100760218654
undirected graphical	2.082836083424519
expert users	2.178380140392139
cpu seconds	2.0381169104851677
vector quantization	2.159655983513591
computational semantics	2.420389283235054
formal query	2.1013886752369264
true iff	2.274324312525912
first condition	2.4793959105551036
syntactic constraints	3.224928147056488
text interpretation	2.2415552840789994
generate rules	2.045521226679158
feature principle	2.349243511334179
grammar would	2.7138288425439936
term x	2.1875174872369643
bayesian approach	2.1041150160072135
ill order	2.489410016064389
bound variables	2.235218379466123
case time	2.173756444575611
linguistic choices	2.0267686188135974
another sentence	2.607470301894714
binary trees	2.3187745257410235
main function	2.3015248324217983
binary classification	2.8841329744564135
semantic contexts	2.031264650127311
tile best	2.222406898745732
based techniques	3.0546888534400636
tree correspond	2.0918364857418936
human would	2.020363647132216
three terms	2.1739387095397857
one utterance	2.7069331258434848
number k	2.1982506992808917
recognition systems	3.2179595662244362
written discourse	2.225142795286369
smt systems	2.1612501636860486
parser outputs	2.2427935805269072
like agent	2.0433739587762374
second way	2.3725247035269392
parser might	2.2742912111882294
fair amount	2.1904049173351097
node j	2.0860645961696633
semantic resources	2.4656947599290593
based interpretation	2.1521748154008318
order relation	2.13700765902827
trigger words	2.3144377771179476
incorrect analyses	2.0157594426817584
previous methods	2.589844090060289
evaluate whether	2.487920588919004
abstract structure	2.1513738263092224
first construct	2.2458302187516197
x would	2.067747107964575
time without	2.272505380879168
content analysis	2.5658445223883772
models p	2.1051956698941954
whose frequency	2.425593477889689
best hypotheses	2.3946688830066574
testing material	2.131569621150367
carbonell et	2.0235371114279648
particular case	3.1247460667660034
speech categories	2.515043216909394
new theory	2.271753499640152
rule 1	2.4056937225546227
branching structures	2.2400076283322674
test queries	2.233432577790629
including pronouns	2.045521226679158
approach suggested	2.0074734400897967
discourse grammar	2.009667338694342
form without	2.1023887620661283
acceptable sentence	2.031264650127311
interesting work	2.053986137989304
also combine	2.183647914993236
various categories	2.257869825585851
general principle	2.6605572865586105
2 table	2.1119430342456553
accuracy even	2.109437912434099
complete parses	2.195697036372275
human dialogues	2.365438535231822
synthesis system	2.3251588063649455
idea behind	3.2099238888313697
form may	2.4873413864592644
building semantic	2.0577375124403856
parsing systems	2.744446232460081
ad hoe	2.2175247055641627
inference system	2.2478248271167076
different search	2.3933315819208887
languages including	2.172274429303222
task domains	2.20379976615122
free grammar	3.565513849318597
first kind	2.3955781761503503
overall scores	2.2142371090381427
entity classes	2.2309685818939657
important aspects	2.8559716689887686
word associations	2.3567058799617415
system returns	2.4168518426735073
possible word	3.14978540733032
system applies	2.318932106792177
algorithm proceeds	2.6051982776453304
simple data	2.2389453747547177
computational point	2.619994124514501
pattern match	2.1677847630386546
extraction task	3.013666762520586
system knows	2.6254683678931134
different aspects	3.0446426270708944
prosodic information	2.6067687944056757
present proposal	2.017570716396185
let l	2.640438958811599
sentences like	3.2027048729147545
phenomena observed	2.0074734400897967
machine communication	2.315147078756572
syntactic analyses	2.888273795800927
ambiguity may	2.2020202169645566
similar characteristics	2.022261218861711
terminal symbols	3.1760921651319616
typical case	2.536119842894841
different subject	2.067081229293324
retrieval models	2.0432296517672346
edit operations	2.2691298789509613
contextual factors	2.369854663379711
reasonable accuracy	2.2470811344797337
ones described	2.145668076397672
retrieval model	2.2706443250545325
evaluation showed	2.249257833574009
different tags	2.461690274557259
passage retrieval	2.1636027948451164
language includes	2.067747107964575
target words	3.12107184151974
single part	2.1499148428335464
labeled training	2.6638263964927127
local coherence	2.2508207662853987
structure consists	2.4115483316123623
every term	2.28010507945228
second mode	2.0074734400897967
constraints apply	2.1816335351833818
parse forest	2.485935008096779
collins parser	2.334201957217112
verb followed	2.2658748144574723
acoustic scores	2.118220170284089
brill et	2.177026239901176
generative probabilistic	2.1895967860268843
soft constraints	2.1692338916982563
high inter	2.009868063569498
two representations	2.5621006029270164
3 compares	2.5327107950191152
user wishes	2.2328679513998626
hopkins university	2.24583021875162
optimization procedure	2.1240946476680165
knowledge encoded	2.375180844916545
sentence comprehension	2.0864276890759808
noun noun	2.117892309685102
task based	2.0743965972347684
finite list	2.0460967409737787
syntactic representation	2.9529539363430994
threshold value	2.8656438218933977
generative process	2.410303659044374
maximum score	2.4124791196839253
uses different	2.166102255087602
process consists	2.7609962621604445
makes predictions	2.0376102740271858
verbs may	2.507668226348355
log probabilities	2.465039734277093
normalization constant	2.2996115885042303
adjectives like	2.0093760756009043
case 4	2.060640253035934
river bank	2.022410241232288
orthographic transcription	2.2465865162305816
unified treatment	2.067747107964575
trigram probability	2.1012225606144637
formal evaluation	2.572095578316232
second section	2.438730573987262
grammatical structure	2.923913513240278
robust system	2.3083896201452303
second class	2.6409979853906367
verb sense	2.540493833903573
new database	2.0965741974051317
procedures described	2.24825378073324
several sets	2.3992522812758517
process produces	2.2328679513998626
input file	2.134799608450785
recent successes	2.0376102740271858
systems center	2.28010507945228
procedure called	2.1413469986390323
one person	2.6720467953151794
good overview	2.178653004950429
coordinate structure	2.2651187405535844
edge weights	2.122599487702517
name recognition	2.533714044805604
simple patterns	2.063313286103995
common information	2.0525092014357855
task involves	2.421360589371111
processing component	2.3966811551228147
rule name	2.122861153066772
sixth message	2.159746288077376
many adjectives	2.020363647132216
output consists	2.2776740307447065
abstraction hierarchy	2.038764723160757
special feature	2.1678854032223676
performance even	2.2776740307447065
specific noun	2.070902079606615
word meaning	2.571472689195207
following ways	2.5554369320866543
higher frequency	2.704275040063897
two classes	3.2027335011795093
query words	2.252536759677753
alignment probabilities	2.272957657107618
distinguishes two	2.075729255121515
little difference	2.2476262854707003
phrases headed	2.0355401813680145
sentence number	2.027614244722898
following pairs	2.1023887620661283
local semantic	2.2047194975469235
later work	2.5300278128180302
character sequences	2.4094086207707006
smoothing techniques	2.4656992176265424
classification decision	2.1039380034279427
word n	2.713905526090816
uchimoto et	2.1180751496779293
provides sufficient	2.053986137989304
current document	2.0983798389015877
lexical component	2.2689486128575904
systemic grammar	2.372900730281045
two parsing	2.2616778639909247
best precision	2.3066832206917827
de deux	2.0920241304388747
stressed syllables	2.12749000458005
category membership	2.0172047205465917
significant problem	2.5713098687748897
problematic case	2.060145782650642
methods developed	2.377838240407285
different researchers	2.305458956322113
previous tags	2.1339258481610717
scheme proposed	2.022410241232288
recognition task	2.9765332472779433
template slot	2.005505341202414
translation probability	2.761511355993775
text window	2.1403073737178895
whose referent	2.1222463152036486
see table	3.580597103421441
testing phase	2.440990817851895
compression rate	2.1951906713486338
conventional dictionary	2.0022828335349505
bbn systems	2.0388643487004234
two symbols	2.478876998805241
comparative study	2.5523780372914953
sun microsystems	2.2608962684880867
four kinds	2.4921727334245958
algorithm performs	2.8189168809524294
single nodes	2.116591742190463
operations described	2.045521226679158
cross entropy	2.030642162609248
grammatical coverage	2.457778929037977
language generated	2.6957481731202217
construction types	2.112778664196261
usual approach	2.0074734400897967
analysis results	2.5505888876953917
human interaction	2.6002767005315617
thematic role	2.6965001205621877
grammar specification	2.2595688265973184
unified framework	2.390242508524609
electronic mail	2.3707749680500125
cost function	2.553564154575315
system utterance	2.0942997329578903
mechanisms used	2.286317287646451
concrete object	2.061969254719088
various parameter	2.0376102740271858
knowledge discovery	2.2377474270561715
separate test	2.310036969611386
find words	2.3589708974356802
different parsers	2.3538885971421086
language differences	2.017570716396185
first occurrence	2.7119980974598876
using term	2.14183096411683
10 iterations	2.32391019643238
multiple references	2.3117016392411247
english pos	2.1013309453017137
automatic creation	2.3537384159980346
translation units	2.0948282790922503
parsing task	2.423002378158408
different settings	2.468800610911404
single speaker	2.391236445720441
unique word	2.3484504851702557
parser takes	2.4103259885026898
tile correct	2.031264650127311
trees rather	2.1462245538298697
therefore need	2.172303059482051
interactive machine	2.023537111427965
features without	2.341285221188147
organization name	2.4917583544052837
common case	2.345768269478816
appropriate points	2.053986137989304
empirical study	2.78339241102113
3 show	2.911937668206805
set performance	2.0310970943079423
answer extraction	2.3002768230818895
nonterminal node	2.242265172422881
using examples	2.4282856154916748
representation systems	2.3067309653553387
search tree	2.149905015350811
york times	2.417335249418879
similar words	2.9808555889688386
linguistic annotation	2.4947695309961446
german word	2.5620045580207957
human expert	2.562491051566038
grammar might	2.2640709340295135
5 compares	2.2776740307447065
process repeats	2.2512557276899194
realization component	2.304518370767248
overall success	2.0782768626411183
dans les	2.2919797422736625
standard feature	2.252261568910547
method used	3.0923763862239424
log 2	2.1266359682615787
judged correct	2.2117261762154827
large test	2.1949289510852124
terminal categories	2.207056318004464
string comparison	2.072192581899669
human judge	2.495895924583971
rules describing	2.2965916473768724
resolution module	2.4602604402838475
black et	2.628377141080747
automatic parser	2.159104637324296
bayesian classifier	2.096255309490079
frame based	2.0919612108326557
row 4	2.0175707163961847
available tools	2.2107864417857894
parsing errors	2.678893480631402
properties may	2.159746288077376
issues must	2.060145782650642
increasing order	2.362740142511377
case frame	2.8170985540366154
possible alternatives	2.127020455996988
define one	2.022261218861711
shallow text	2.0390072838737376
use corpus	2.089026915173973
make use	3.9517618250243687
one field	2.116591742190463
flight number	2.1397232180818557
different perspectives	2.330522638208586
good precision	2.3028207919454977
information resources	2.204719497546924
selects one	2.4728837265095613
possible output	2.3494271666348956
previous researchers	2.102921514525577
negative information	2.124245509180354
final section	2.775943661792309
retrieval purposes	2.216598368661922
parsing becomes	2.022261218861711
pair consisting	2.6144935270925282
one human	2.2717132851229636
pure bottom	2.0728585969279347
preference ordering	2.040565299299733
lies within	2.096756183704444
work using	2.6629950869381944
u e	2.3012283429236895
limited amounts	2.070902079606615
current topic	2.200332785903859
markov assumption	2.3893421242619524
many domain	2.0423259728447545
section briefly	2.4459101490553152
relevant domain	2.116591742190463
best result	3.0984070239431705
word window	2.641098358564001
1 denotes	2.1852404006823356
english texts	2.9705174700054746
case relation	2.0803889091736867
prosodic features	2.614884729120491
brown et	3.4905906696025055
larger sample	2.1946340157728974
intensional logic	2.3483122592533445
practical systems	2.406676464637198
second analysis	2.122227020631432
relevance score	2.1353277962152895
possible parse	2.4818129486717257
extra features	2.212862020185457
category symbol	2.0302303699278212
independent models	2.2854607589299643
large database	2.4556239768977663
different tag	2.341285221188147
hybrid system	2.426338666367838
information technology	2.434572318855365
next action	2.3111374396732023
independent model	2.2116305718004394
occurring text	2.2058829837008718
1 summarizes	2.9982597239647175
syntactic framework	2.0763533583416773
temporal expression	2.2330817247128647
graduate student	2.1018382138609883
selected features	2.4393549949108246
possible patterns	2.380830363798011
straightforward manner	2.4600880482642387
entropy h	2.330045333374649
contain semantic	2.045521226679158
sentence parsing	2.523141783955899
4 seconds	2.102921514525577
necessary condition	2.568849650043093
subject verb	2.0865513505522313
grammatical constraints	2.8013115896185607
traditional approach	2.521534570594051
per line	2.1786530049504287
related concept	2.021422743908272
derived forms	2.2667285397850265
initial context	2.021965172158874
5 sentences	2.1018382138609875
analysis also	2.452321695174711
method gives	2.3229739757638654
low inter	2.016336526582215
electronic text	2.099458264360342
objects involved	2.3162726447741377
null second	2.200598690831077
manual review	2.0221951140579186
semantic basis	2.0379203068673784
basic research	2.58800007950865
phrases extracted	2.409790394279902
results also	2.611391246332236
closely resembles	2.4943690648994172
similar problem	2.7033596236321222
six categories	2.0743965972347684
science foundation	3.4679233521799753
specific corpus	2.253918327663226
bayesian classifiers	2.082887409670474
analysis without	2.1479184330021646
later use	2.3644803559732956
discrimination net	2.060640253035934
latter kind	2.1479184330021646
first mention	2.2265794848395717
incorporating information	2.0376102740271858
results without	2.166102255087602
analysis may	2.611029671005839
independent processes	2.0817120252117016
philip resnik	2.031264650127311
two tables	2.489932277147995
words present	2.257869825585851
e use	2.045521226679158
theoretical level	2.014014536653706
either language	2.0215688023747362
supervised learning	3.2675290361135
given object	2.372124146446556
results may	2.7267999291464706
original tree	2.1241984140293138
learning problem	2.595586113909243
standard hmm	2.215156728075037
equivalence classes	3.102920550956175
two layers	2.317189385166775
statistical results	2.0155185883183986
structure building	2.291262354116939
relations involving	2.3109116208105
identifying discourse	2.0426613721341615
tree without	2.060145782650642
chinese treebank	2.534809735074269
available knowledge	2.286725248739412
analysis requires	2.189546252614866
simple dictionary	2.1982506992808917
agglomerative clustering	2.4739677238100746
existing structure	2.0312646501273113
unsupervised algorithm	2.193957026760592
simple text	2.4212979507023116
tree may	2.609763958625415
interesting aspects	2.172274429303222
three test	2.191087968591127
every pair	2.9573339942999906
partial sentence	2.116591742190463
person name	2.70430771111054
lower level	2.785212782954169
new part	2.1023887620661283
linear logic	2.0693117606956983
relevant ones	2.379465786549987
5 show	2.6383330595080268
two categories	3.2671818972814983
probabilities may	2.0601457826506424
pages containing	2.174809824991888
scoring software	2.227761599301478
standard classification	2.1312839727750825
simple implementation	2.159746288077376
ambiguous sentence	2.6196637551052513
computational costs	2.1886942562051273
open test	2.3442723190949346
japanese texts	2.539376526067022
three senses	2.5480824318223743
put forth	2.357121432847829
assigning probabilities	2.172274429303222
additional linguistic	2.523405255352561
unsolved problem	2.2492578335740094
syntactic grammars	2.070902079606615
three variants	2.1039380034279427
experimental settings	2.2974668198166297
last experiment	2.045521226679158
interesting examples	2.116591742190463
character types	2.2155385920338264
different versions	2.835780507366547
noun phrase	3.921607967623253
mean either	2.21826141526814
language understanding	3.7104728492958134
allows multiple	2.314007818435079
column indicates	2.453169315282728
improve classification	2.0530842980529735
general applicability	2.2328679513998626
one case	3.129468217317903
point scale	2.513157418092662
linear chain	2.0593134444963326
presents several	2.3398800045580104
pang et	2.315449448657521
methods described	3.003014159549689
real users	2.4770382299629663
de nes	2.208897842265035
sufficient data	2.3417181621687195
transfer system	2.0353962568678874
syntactic analyser	2.0213043660700585
augmented context	2.3129850099779756
approach extends	2.022261218861711
three individual	2.017570716396185
discourse module	2.0245795910087923
interactions among	2.598333431703275
module performs	2.2224068987457315
category must	2.0817120252117016
different hypotheses	2.102921514525577
common set	2.3955781761503503
allow users	2.5546433695313278
lfg grammar	2.1822126749956308
traditional ir	2.2326548743270425
one column	2.0074734400897967
reflexive pronouns	2.490421036997854
text words	2.0657827952454104
ongoing research	2.790546467087736
given value	2.153552027758611
example 3	2.962885881869232
possible improvement	2.159746288077376
atomic elements	2.031216633777997
event description	2.053222102967216
distinct types	2.543359598838406
whose score	2.1642359406653675
speech group	2.1413469986390323
algorithms using	2.3955781761503503
significance tests	2.3587448478967863
recent systems	2.24825378073324
two character	2.067081229293324
second task	2.485308639878679
statistical data	2.6978118805032048
source sentences	2.4194871012262142
domain using	2.3448347819209543
un texte	2.0016294936642725
disambiguation results	2.1386681817934496
five classes	2.2673299858792415
first author	3.092384191548698
two tokens	2.57054505391395
substantial improvement	2.3427283802008265
first work	2.171178031468078
words among	2.2429157999161102
previous clause	2.0922634241637468
possible arguments	2.063693154062279
word delimiters	2.0659571110450177
always lead	2.022261218861711
word using	2.5085220297816346
less attention	2.1956201825353343
data necessary	2.1774937848124125
systems need	2.5642983021946666
classification scheme	2.5640809254566164
many advantages	2.5445384238364364
atomic symbol	2.0490697481769016
syntactic ones	2.417618427399807
system processes	2.4028451845347893
third rule	2.222406898745732
c n	2.536110709530076
space using	2.307091302036315
hypotheses produced	2.0312646501273113
final analysis	2.157519707594949
sense definition	2.1456793203578766
extreme example	2.2631802623080817
standard algorithms	2.00238351760398
corpus provided	2.286725248739412
fixed number	3.0328049285702914
present information	2.2447151258928626
names like	2.4629615424710245
sentence accuracy	2.313661850585649
gives results	2.3605881269336804
automatic procedures	2.1380488015835484
rate reduction	2.1198094239462697
les deux	2.0848570099691117
size n	2.5492084474393555
null able	2.045521226679158
wide web	3.0472196289504607
basic category	2.1366713893936176
current version	3.276658834570914
systems reported	2.020363647132216
4 discusses	2.780209642029424
grammar written	2.2891619181755747
different elements	2.3886619127043263
case 1	2.5857841578696186
important applications	2.0376102740271858
first generate	2.0539861379893036
extraction performance	2.0314523739242913
answer may	2.288359372860425
patterns found	2.301874657053646
van benthem	2.2073805151644557
core engine	2.0907374619642662
whose task	2.272505380879168
good reasons	2.6072490531462535
relational information	2.44815860064319
fixed point	2.1688842944544273
error messages	2.5072026096501143
root nodes	2.7104096120059977
two slots	2.4161410260493708
features related	2.3739907005522554
full analysis	2.2532512925302806
tagging method	2.124055529249379
representation used	2.8372241679971872
physical world	2.2635846387787795
similar model	2.166102255087602
straightforward approach	2.3005523645853
system receives	2.075729255121515
heuristic rule	2.408789747896252
appropriate verb	2.0381169104851677
abbreviated form	2.2489687856633274
figure l	2.5383449490693457
new nodes	2.515730795588298
ongoing work	2.7667441373303556
special cases	3.0667240518017516
research group	2.6047759126563683
volume 23	2.0213340733835565
current situation	2.178132442751555
analysis phase	2.652274753582389
7 shows	3.281895157892513
text encoding	2.331932572390108
rules 1	2.345079850850462
first show	2.326156671484873
previous versions	2.3644803559732956
computational system	2.5915614634685893
anaphoric references	2.7187703454697987
language independence	2.30018490760141
irrelevant information	2.3333329581503177
path length	2.462374913643714
learning models	2.528433711515593
knowledge must	2.523210818280746
final example	2.368834809141683
learning model	2.471759222198128
reference times	2.04701768515259
manner analogous	2.1023887620661283
current user	2.015353596841515
new speaker	2.1158750191511198
relation must	2.329081436908228
like word	2.542316220970792
army research	2.049069748176902
context free	3.1923904692257206
average word	2.614744039235189
requirements imposed	2.045521226679158
use syntactic	2.667664449647699
human beings	2.91427401626963
phrase could	2.2062448606993037
frequent verbs	2.265597167471218
multimodal systems	2.0167612951521843
easy way	2.783477525020168
produce correct	2.172274429303222
final score	2.6039869481424525
utterance would	2.0157952878916157
sense reasoning	2.2202514461806047
methods could	2.135021879186851
sense definitions	2.3380193664729756
also incorporate	2.2328679513998626
lexical relations	2.6461717634777613
readable dictionaries	2.994047094149171
specific concept	2.430104237749728
current results	2.0117974635868396
new concepts	2.5741889277192076
corresponding element	2.0643620962409948
underlying form	2.1191052625658555
sections 15	2.0344778458477064
coreference relations	2.3841802834313377
formal run	2.098030941104246
slot values	2.278158802900108
main conference	2.8963289880997483
linguistic studies	2.5267279868836843
little agreement	2.1023887620661283
present case	2.475593129161638
person plural	2.216539075999985
features mentioned	2.045521226679158
aravind joshi	2.6610945516467606
geometric mean	2.5999264138223417
difference among	2.1016722214456514
large part	2.6536316872649
select sentences	2.0676325559456012
basic type	2.1315696211503674
joint model	2.1262189803176934
best interpretation	2.143902638500865
grammatical theories	2.1982506992808917
lie within	2.1023887620661283
make decisions	2.405174295264331
find information	2.0947271027985517
useful information	3.3819578092924907
theoretic measure	2.184223228921038
noun modifier	2.386272445091726
authors use	2.1071306144909485
possible explanations	2.2827808432516177
lines indicate	2.4286000999711335
elementary structure	2.050340229766856
auxiliary inversion	2.0880414073297953
linked words	2.042325972844754
pp attachment	2.5665494938072664
contain fewer	2.089026915173973
certain assumptions	2.272505380879168
tells us	3.1868977602803192
word usage	2.5459797601527674
collocation extraction	2.218973938358171
iterative training	2.008226999543247
general scheme	2.403472566572053
two characters	2.7272462112995854
specific event	2.1240946476680165
last year	2.507922689555792
possible realizations	2.2685844072039183
q e	2.254365267238734
verb entries	2.3799618056633616
lowest node	2.1312839727750825
pronouns may	2.075729255121515
approach considers	2.089026915173973
performance metric	2.1395795253346153
lexicon acquisition	2.149905015350811
first type	3.040946120661769
complex feature	2.6938216077819983
using support	2.398311654256113
one set	3.049847602102917
pairs involving	2.0992185857391816
complex verb	2.0715111049808934
experiment showed	2.4421485591028738
corpus automatically	2.0601457826506424
similar information	2.3207800473379914
second application	2.0965741974051317
knowledge necessary	2.4212679803341364
right corner	2.313990269173465
explanatory power	2.3612865440695145
recognition performance	2.959289538926903
second position	2.574130338775189
generate texts	2.0539861379893036
correct target	2.0880414073297953
oflazer et	2.0817120252117016
new sense	2.1249590847963864
words occurring	2.9888253110057357
referential expressions	2.2142001434903538
phonological theory	2.0133763638918083
quirk et	2.6928031789893523
beeferman et	2.1589381903612557
technical reports	2.1789148691025395
human agents	2.0133763638918083
linguistic data	3.380873994667522
frequency based	2.0341126135884817
given constituent	2.275125009000975
given sample	2.0577375124403856
parser must	2.926967164456128
l e	2.3181136992115494
log p	2.409703813564276
coreference resolution	2.747507648890434
computational lexicon	2.46242971943526
first use	2.6428084040853124
different variants	2.2541760535899478
scoring program	2.4419939465780915
consistency among	2.2020202169645566
certain state	2.122246315203648
concepts mentioned	2.027950123809551
grant 01	2.089026915173973
work within	2.0743221253592274
2 l	2.248968785663327
mt evaluation	2.478313546828488
information allows	2.272505380879168
type coercion	2.0809684663889367
palmer et	2.5586258972523206
pattern matching	3.405730880608577
user interfaces	2.671143360153593
arbitrary text	2.117020269999787
null ture	2.2328679513998626
discourse annotation	2.094984910802675
embedded structures	2.2020202169645566
relation links	2.0743965972347684
functions used	2.335904293904669
important step	2.3293407523010043
consider example	2.440873099962798
consonant clusters	2.0551948345260858
possible configurations	2.1266359682615787
form would	2.3955781761503503
high scoring	2.1762349391347313
relations like	2.6485282795146685
prior context	2.075729255121515
particular class	2.7190848018099607
processing architecture	2.071008230235014
independent features	2.3605051570246514
utterance type	2.0134036822475294
also look	2.0484178732091096
pas de	2.1356320279912855
semantic evaluation	2.1046522564716277
prepositional objects	2.251465329281997
knowledge acquired	2.147741798365251
foreign languages	2.619461070694784
sentential level	2.380224455873114
x l	2.442812985516855
several stages	2.501400582953832
several directions	2.4103259885026898
approach performs	2.375180844916545
every individual	2.082492768888955
syntactic component	2.7182793798651295
semantic approach	2.1690068894541996
future plans	2.113154262164259
complete description	2.7771345941265477
methods rely	2.200598690831077
systems employ	2.27225588938143
processing systems	3.2587313511197946
every step	2.449699911611846
previous state	2.4388398028290212
describe three	2.3644803559732956
lexical cues	2.145891847496417
language processor	2.4074912438846106
word tagged	2.016336526582215
important result	2.2112071020073643
issues involved	2.5591493350506007
computer speech	2.0965798710124774
smaller ones	2.4005597282692768
textual content	2.1689107931405918
skut et	2.208897842265035
cfg rules	2.4276588195328674
inferences based	2.08469715677149
plural noun	2.835245176058946
results produced	2.6274132965151047
pragmatic features	2.3228160871806045
research center	2.736616913737815
initial probabilities	2.072697162060231
based theories	2.313900874640595
full form	2.2695554962447586
many levels	2.6324973499642152
additional mechanism	2.200598690831077
two metrics	2.5051296017851032
large n	2.0074734400897967
length 2	2.3451609435735197
annotation tasks	2.147357741864222
linguistic means	2.319679830438633
information gain	2.7583779280241214
relations hold	2.4525318180835374
search query	2.2823877774700367
actually use	2.286725248739412
larger amounts	2.438730573987262
tree produced	2.257186090032611
lexical similarity	2.340023045200174
functional representation	2.00499466256068
search method	2.4960954978856753
specific goal	2.099218585739181
local tree	2.3076252146637293
automated text	2.192729087350866
reference corpus	2.2794059180375843
explicitly represent	2.4043111027203574
square brackets	3.1113797230651765
since words	2.478461240595875
successful parse	2.4069366191070882
occur less	2.489410016064389
language family	2.0593134444963326
one string	2.6545782769584503
associated probabilities	2.267708215490379
scenario template	2.2103092492273597
general assumption	2.0376102740271858
symbols x	2.045521226679158
one algorithm	2.1023887620661283
important roles	2.357947172648072
simplest way	2.652641537249688
verbs would	2.109437912434099
useful feedback	2.1222463152036486
fine grained	2.6510543490087044
conventional methods	2.187925036346752
combined model	2.4366084561621655
prior knowledge	2.9286640082633846
reference summaries	2.002305154739221
standard methods	2.319324296244544
higher order	2.965525103588669
using precision	2.1275552563638707
phrases must	2.2427935805269072
seven words	2.053986137989304
daughter nodes	2.5849593178424395
annotated examples	2.3253600194401463
questions posed	2.409537891678971
theoretical point	2.548593807244528
mechanism must	2.172274429303222
new edges	2.32492982832413
character level	2.2137532682130674
source code	2.6046966140866816
eric brill	2.3721241464465566
answer pair	2.2145424505070994
irregular verbs	2.217704436319682
general cases	2.075729255121515
c using	2.060145782650642
based machine	3.0898438735457208
irrelevant features	2.247747511289578
transfer rule	2.1609468162550862
lexical analysis	2.829057442767429
noun classes	2.3122906207191716
source data	2.3344977300862877
possible trees	2.319324296244544
grammars provide	2.045521226679158
vertical bar	2.2381951098381685
much attention	2.7683360653146267
two years	2.5219473947066975
tile grammar	2.216598368661922
error analysis	3.0664096663917886
complete system	2.744661973507811
main advantage	2.889977862936158
analysis shows	2.7532500549171735
grammatical phenomena	2.3707749680500125
current design	2.0539861379893036
system first	2.782470001538353
noun like	2.153552027758611
one article	2.2099973680539073
4 words	2.593768486981838
full stop	2.3639525993606796
also describe	2.8766014030980003
real language	2.399794720767289
feature extraction	2.9179662066276055
results shows	2.4489623523461015
practical experience	2.230590646944645
using test	2.0376102740271858
language dialogue	2.5117062568398656
domain models	2.2609340119652455
structural level	2.189546252614866
provide clear	2.079019887609717
expository texts	2.1515687946704993
consecutive sentences	2.1222530938767195
question asked	2.270820552349159
classification results	2.690151982931496
lexical structures	2.0435772799719114
domain model	2.874334110121797
previous discussion	2.3161212503528343
syntactic levels	2.3871952135040706
final position	2.408208657496615
rules according	2.309782368028973
word models	2.6449733463758425
features across	2.0965741974051317
evaluation methodology	2.844868659401012
also enables	2.435632598873795
grammatical case	2.060640253035934
chinese language	2.887392378487869
standard technique	2.3725247035269392
word model	2.519847015454509
first search	3.05611479777534
optimization method	2.1353034784701492
questions must	2.0965741974051317
theoretical background	2.455983846584365
problems posed	2.2224068987457315
english newswire	2.079019887609717
unique feature	2.078276862641118
baseline method	2.6419664387324975
grammar theory	2.2470811344797332
space model	2.924644484342031
different reasons	2.2458302187516197
dotted lines	2.766615680053916
risk minimization	2.1212265770012984
different random	2.332240952226321
second paper	2.0390072838737376
elements within	2.5252894181985917
issues raised	2.082268497632213
central notion	2.200598690831077
specific classes	2.1773123985040854
first term	2.645026681740856
single constraint	2.0421480663134535
serious limitation	2.1023887620661283
conceptual categories	2.3559971050248327
verbs appear	2.2169936022425736
word within	2.608135621477854
expanded query	2.049069748176902
frame semantics	2.216951171322193
three nodes	2.2974668198166297
discourse processing	2.9862419647562226
timing information	2.0763533583416773
word frequencies	2.7781207326099406
systems usually	2.399252281275852
research presented	2.4144226718804758
useful features	2.648098422701773
probability value	2.3110198845665235
parsing time	2.817419859080593
document management	2.0563704075215554
factors involved	2.230590646944645
tile output	2.031264650127311
based information	2.8291903514903405
future work	4.115120083979064
r 1	2.2020596809205397
ordered lists	2.0965741974051317
certain combinations	2.2380656204269957
methods based	2.949506986458899
dramatic improvement	2.3005523645853
document level	2.464029003206307
proper word	2.026260250766906
two rule	2.0840523766681063
give information	2.4212861489765967
linguistic terms	2.3469240793377675
original query	2.61485893653962
problems must	2.022261218861711
clustering algorithm	3.051597485466126
information gained	2.194634015772898
step consists	2.541926234035544
existing theories	2.031216633777997
reading comprehension	2.10185429551906
depends heavily	2.2173716512574106
original method	2.075729255121515
diagonal covariance	2.0042805385857125
among rules	2.0402381534980756
single model	2.5373837673968147
article presents	2.178653004950429
language users	2.500231523443145
small improvements	2.260688804545853
web sites	2.590608892317342
log probability	2.564421622363185
candidate pairs	2.169789361611463
information science	2.7647286752696285
corpora may	2.1222463152036486
test sets	3.5826334676547287
word combinations	2.5812547197070206
target string	2.3539075078196587
event e	2.22019195348457
similar features	2.389326705744299
decision procedure	2.4373088402134693
input chinese	2.070902079606615
main tasks	2.459228001613674
high levels	2.0849542955934797
columbia university	2.515998525921566
features would	2.6058182593196637
wordnet contains	2.22672986885133
dependency based	2.0074734400897967
verbal form	2.4950919650787906
level concepts	2.2133187583704625
tags using	2.0539861379893036
processing task	2.3861511774041837
following query	2.2959583628072817
figure 12	2.8670783586800552
actual performance	2.3523313048426364
rare cases	2.1956201825353343
based heuristics	2.1636608548876186
rich feature	2.093294558436213
strings generated	2.1161528435313297
probabilistic finite	2.055118260693857
common representation	2.3623688781715884
scoring method	2.6530791982296584
grammar must	2.6811267557990384
morphological level	2.1813446704867467
alternative readings	2.020363647132216
search engine	3.1936270553741117
present algorithm	2.099218585739181
real time	3.132897337798838
existing machine	2.3161212503528334
name lists	2.1631479579162924
assisted language	2.2756755572061147
better understand	2.5677256989845296
corresponding word	2.791447905449822
complex sentences	2.997825867242478
experimental result	2.5120747860790003
selection process	2.9793469703394435
characteristic features	2.2072344776290143
error occurs	2.3821137716479885
context window	2.62847336445191
following stages	2.1479184330021646
concept may	2.320501023225559
statistical nlp	2.567952199587176
parser consists	2.296234813805337
particular example	2.533874621058591
whole discourse	2.00238351760398
pairs like	2.485841069839246
symbolic features	2.0439237932091396
process model	2.260816033742312
helpful comments	3.4800078024504497
similar set	2.2169936022425736
prime minister	2.030473450100184
verb forms	2.983623870681409
ron kaplan	2.0329822559918163
et ah	2.0850741607377534
x must	2.1051956698941954
parser produces	2.794800753550539
traditional parsing	2.190982236626934
discourse situation	2.3481975708849756
information specific	2.30606794696972
overall number	2.040588147160348
space limitations	2.963551887407285
results across	2.4943690648994172
important part	2.7590537510776096
approaches like	2.2601220552236354
simple methods	2.329081436908228
blind test	2.456002818208387
gram probabilities	2.502004371516393
system include	2.480027435464727
temporal adverbials	2.393674755856475
mean something	2.086884416965048
tag ti	2.1583344684158763
different part	2.5979264194982656
per speaker	2.0808215024313803
models proposed	2.1413469986390323
est une	2.1848665875808617
whose primary	2.314007818435079
segmentation process	2.504215872446623
specific parameters	2.124563704437371
hearer may	2.070264663841787
word vocabulary	2.742751298703189
develop algorithms	2.267991541156711
weischedel et	2.4721851851439225
selectional preferences	2.387509549959245
background material	2.1950240152843934
different scales	2.000178283129414
translation examples	2.30167019775487
integral part	2.5621783563775002
data extracted	2.5138672549211005
multiple possibilities	2.075729255121515
longer word	2.201150196887566
extracting features	2.0918364857418936
first segment	2.2305776301806874
central role	2.90849708527561
help disambiguate	2.3448347819209543
typical example	2.9975356982274337
potential solution	2.075729255121515
word translation	2.915072014049792
bahl et	2.1768662208817515
also function	2.2112071020073643
uses lexical	2.24825378073324
past work	2.393598215947388
normal way	2.2578698255858516
text files	2.5314045711322515
build models	2.0817120252117016
existing information	2.227249320716272
current text	2.425593477889689
based measures	2.350898390366852
detailed account	2.5469502675910904
tipster phase	2.2572885455459235
subsequent utterances	2.2619187344299303
structural properties	2.5593708642871795
speech recognizers	2.858026900342835
l g	2.089026915173973
accurate analysis	2.116591742190463
b c	2.848227302939414
complex cases	2.4532558885755127
current implementation	3.3976524917829436
rule could	2.514628276488317
possible category	2.0423259728447545
simple technique	2.30606794696972
specific question	2.275125009000975
newspaper stories	2.08166378125058
definite reference	2.2789109571657544
different techniques	2.7026946251444595
category labels	2.675116215176501
hierarchy used	2.0074734400897967
give one	2.1946340157728974
algorithm performed	2.0965741974051317
modified algorithm	2.031264650127311
rhetorical relation	2.4667454540351414
analysis would	2.7220867555208184
june 2005	2.916635691413861
underlying language	2.014014536653706
french word	2.60784894930548
models also	2.183647914993236
first requirement	2.053986137989304
one letter	2.367725858470009
particular task	2.911902926836748
two documents	2.593492704542386
implementation based	2.014014536653706
recent paper	2.4012688714330688
structure information	2.6874904286081005
sentence e	2.388502338627962
many relations	2.2251427952863683
associated word	2.1631899329302136
results would	2.2250895492608302
various tools	2.1462245538298697
different interpretation	2.3005523645852994
null tences	2.067747107964575
procedure also	2.045521226679158
gb theory	2.1814726629480035
linguistic realizations	2.184288267337701
term recognition	2.1523636626394556
graph g	2.444474855333356
comparative results	2.4043111027203574
level structure	2.4745303366457128
type e	2.018481010740122
subject may	2.119240389473862
single translation	2.1413469986390323
semantic concept	2.3898157952306303
model features	2.1911626735390426
entire corpus	3.042893242273443
models may	2.54911358032642
corresponding results	2.1413469986390323
tree would	2.386828305171403
comparative studies	2.079019887609717
common lisp	2.938996000906306
average probability	2.1315696211503674
e z	2.03820918056356
result suggests	2.481655964382881
include semantic	2.1946340157728974
new technologies	2.0880414073297953
correct parses	2.4427076953352773
verb constructions	2.386501328889681
another noun	2.552818450979276
subcat list	2.055051220501721
types found	2.1050838385526647
procedure may	2.4282856154916748
functional grammars	2.3058145169271436
le 1	2.222406898745732
muc task	2.114299018453181
individual components	2.7358828525235976
conversion process	2.2243484856120617
french version	2.119240389473862
increasing interest	2.0891832546571605
whose root	2.6598725545914568
feature may	2.4421485591028738
corpus consisting	2.887098028782817
detailed models	2.031216633777997
05 level	2.2357179378850733
similar algorithm	2.172274429303222
execution time	2.612662786165751
candidate sets	2.1033569370576704
2 minutes	2.286751068972218
automatic scoring	2.012576357809039
interesting result	2.568611116983589
reference number	2.0262602507669065
voted perceptron	2.11587079995358
prior probabilities	2.523681981765445
detailed model	2.026260250766906
ungrammatical sentences	2.574462107986411
japanese verb	2.2973102717442613
procedure requires	2.022261218861711
statistical test	2.2443687102109093
text features	2.0900275361171667
branching tree	2.305635409098957
time expression	2.3547573976596032
2 reviews	2.200598690831077
indefinite articles	2.11638795219105
empirical studies	2.7927286070212087
final performance	2.1290482690107417
bilingual sentence	2.283387360784051
representing words	2.079019887609717
different nodes	2.423293735248201
particular form	2.5361198428948413
like units	2.04764014154256
con dence	2.2651210736926144
first blush	2.045521226679158
learning algorithm	3.5657581685746615
spanish language	2.037093214120776
och et	2.5465955635528372
context using	2.014014536653706
head child	2.055723206077514
link grammar	2.1555309175123787
words associated	2.5322539750765327
large space	2.318222716608852
one representation	2.303003413898794
1 illustrates	3.249640104042154
research reported	3.079147353783564
looks like	2.777632679143542
new mexico	2.132410868885472
results appear	2.3271110475429713
syntactic relations	3.153797053893217
individual rules	2.5390805203865305
decision tree	3.2910558470421893
intentional structure	2.2761983506922707
general patterns	2.2078449813130154
corresponding data	2.1312839727750825
pragmatic constraints	2.457720832613169
data using	3.2036520707533107
see discussion	2.417618427399807
system generated	2.3276232575917817
closest match	2.065429826730168
relation labels	2.180316110524265
sense used	2.109437912434099
generalized quantifier	2.1863864459065976
current nlp	2.3417181621687195
particular order	2.49449202328214
case assignment	2.158048788504796
alternative parses	2.158407162326257
finite sets	2.597474706422906
primary concern	2.4103259885026898
syntactic class	2.460104513666221
rules defined	2.255349860587285
involves finding	2.366082551445672
results similar	2.0817120252117016
various tasks	2.580901723965794
using part	2.5102902122306507
child language	2.01385242568157
possible use	2.1479184330021646
rules developed	2.116591742190463
l p	2.171178031468078
since information	2.159746288077376
play important	2.0577375124403856
lesser extent	2.179810072316421
appropriate data	2.315664817623035
attribute names	2.288005735345076
dictionary definition	2.452794472677314
first clause	2.7482188478757004
following conventions	2.1312839727750825
another approach	3.2007547834028265
different concepts	2.5922893931369995
systems built	2.2823116493074065
representative corpus	2.0490697481769016
naive users	2.3919334744960725
management system	2.487000926979956
capture information	2.2380656204269957
machine readable	2.9975269551036554
full training	2.2280649783291073
task description	2.155589295660503
next generation	2.448740164867852
broadcast information	2.014014536653706
correcting errors	2.0460967409737787
new entries	2.5505996287583548
selection step	2.0350538995209133
rambow et	2.0678810562532766
2 note	2.1479184330021646
empirical data	2.7701289602902355
othe r	2.096756183704444
paper focuses	3.1438949750043315
minimal recursion	2.3095161954667387
syntactic configuration	2.0223531762070586
e must	2.122246315203648
possibility would	2.314007818435079
discourse may	2.3676013596092336
head rules	2.00518896099143
entire phrase	2.2896944818673393
entropy based	2.1062149214048658
avoid unnecessary	2.329081436908228
become less	2.2183357382334514
disambiguation algorithms	2.1941275175579147
main problems	2.7571190872658984
template generation	2.3672976276600366
recent approaches	2.515272207357606
model shown	2.16202161159633
users tend	2.060145782650642
whole string	2.085074160737753
four test	2.270560754330841
returns true	2.277814944853371
features might	2.329598655974965
first review	2.2631802623080817
earlier stage	2.2492578335740094
end points	2.4021615107911027
general design	2.1298730493372275
de ned	2.8134870933933445
user query	2.6368410853531667
abductive reasoning	2.049653377978883
5 shows	3.781262358449544
corresponding parse	2.0918364857418936
grammar consists	2.852479257319972
word features	2.8350971775252187
system demonstrates	2.1222463152036486
analysis methods	2.300356048542672
material may	2.0817120252117016
four senses	2.2024639441989313
sentence rather	2.1023887620661283
next sentence	2.816922728613515
plural nouns	2.60376410378184
hierarchical information	2.086884416965048
algorithm called	2.453255888575513
many systems	2.9669276023645077
produce output	2.5445384238364364
new model	2.7063360112242996
multilingual corpora	2.147741798365251
two effects	2.116591742190463
full generality	2.1499148428335464
grammar produces	2.014014536653706
various languages	2.7108994176066705
back end	2.198530798948747
descriptions may	2.3398800045580104
certain concepts	2.200598690831077
clauses may	2.1816335351833818
text shown	2.0577375124403856
particular position	2.2251427952863683
foreign word	2.2353625590951047
given grammar	2.6229771217992903
sequence w	2.6296740053094187
latter may	2.286725248739412
different sense	2.7122348526418025
states correspond	2.1312839727750825
degraded performance	2.014014536653706
relevant literature	2.053986137989304
different notions	2.105195669894195
input symbols	2.2966998529150304
tile verb	2.2635846387787795
experiments conducted	2.5998248828102515
applications involving	2.0918364857418936
larger units	2.5454616944663853
class labels	2.6038485573731474
mutual belief	2.26374080812925
aone et	2.007315006862137
vocabulary sizes	2.37717276447754
arabic words	2.1496679549880566
target representation	2.2024639441989313
section 4	4.366881146834556
different labels	2.435632598873795
whose role	2.2062448606993037
attitude toward	2.104375523812563
analysis technique	2.070902079606615
cross validation	3.125669792409555
one half	2.4894100160643893
two participants	2.605040077438496
trees whose	2.4917828007509444
e par	2.1045924823818805
discourse segment	2.6675540265269206
basic elements	2.086490851564808
develop methods	2.569287500617407
text string	2.407590815338253
e following	2.007473440089797
possible outputs	2.022261218861711
complex data	2.446388901957011
tables 4	2.6371927748543427
english gloss	2.1312839727750825
constitutes one	2.109437912434099
cause problems	2.29597950616448
onr muri	2.109437912434099
another corpus	2.200598690831077
precision score	2.519440947920017
proposed method	3.2157283031511277
linguistic research	2.9399457611530955
incorrect analysis	2.0965741974051317
translation pair	2.574907260979113
walker et	2.6183608484879457
current research	3.184140829534177
recursive rules	2.3584168141644763
50 sentences	2.420568424770859
second reading	2.24436871021091
construction rules	2.0406281519261285
certain sense	2.318932106792177
first line	2.63226832661378
additional syntactic	2.3605160818311552
extracted features	2.2601584122950134
linear models	2.559914844513463
evaluate systems	2.060145782650642
using sgml	2.0539861379893036
using n	2.6925740897797996
tags within	2.061969254719088
linear model	2.8566773233954876
utterances used	2.075729255121515
one list	2.1884816114309813
2 seconds	2.3322409522263214
methods cannot	2.264070934029513
common ones	2.1222463152036486
based algorithm	2.8543763084478826
main phases	2.0376102740271858
parallel structures	2.1302296826955534
individual system	2.089838197048368
lexical semantic	2.9893757081235095
ones like	2.345768269478816
common errors	2.230590646944645
important characteristics	2.060145782650642
wordnet 2	2.162883830633518
component could	2.0817120252117016
existing set	2.21826141526814
positive examples	2.9240051061577037
rule based	2.8337304642698786
open issue	2.1543888787937475
experiment 1	2.5515136959091933
semantic classes	3.271748628799584
text parser	2.0800690951591854
different characteristics	2.5275580183983113
module also	2.102921514525577
human readers	2.442783570877376
mccallum et	2.356303115704617
problems involved	2.580570794461364
auxiliary tree	2.6043178572805505
distinctions among	2.4699091035489666
correct number	2.112778664196261
successful match	2.0918364857418936
information given	2.604392587037138
semantic parsing	2.5043946033351063
nearest neighbor	2.7106719876705907
main reason	2.6800636980583117
syntactic analysis	3.755195222960441
becomes possible	2.664670410828098
existing modules	2.073066253406054
weights using	2.0376102740271858
algorithm employs	2.085074160737753
latent semantic	2.9443722181437924
total sum	2.009868063569498
object language	2.226190716569853
insightful comments	2.4851459567760594
time using	2.605818259319664
typed feature	2.851828004305107
possible tags	2.5873421067270903
general world	2.361399844081622
limited use	2.352331304842637
high quality	2.972704081954467
small data	2.529356583063629
system seems	2.2112071020073643
problem could	2.50344848325348
expectation maximization	2.5928350912245843
best model	2.8112148667081303
based parser	2.94802868327456
estimation procedure	2.375008916580972
hold among	2.2258046888534917
bateman et	2.2090936426708474
general grammatical	2.0074734400897967
connected component	2.0214227439082717
method 1	2.155941020508723
language learning	3.281351156969555
much noise	2.177338384924136
cannot understand	2.045521226679158
syntactic structures	3.690389160467305
position paper	2.1226162390904557
needed information	2.272505380879168
corresponding arguments	2.014014536653706
new vocabulary	2.326012528775637
resolution process	2.6214856678034595
high similarity	2.5384022428724005
repeat steps	2.1380488015835484
recall rate	2.5765165410773667
line dictionary	2.3994826745260758
negative polarity	2.225360269668718
translation process	3.2844037040709653
third element	2.019557844009582
u c	2.165679121692736
grosz et	2.764002008031483
describe briefly	2.1479184330021646
japanese sentences	2.95699375156555
new translation	2.227256700460021
based learner	2.5621346915681906
line text	2.1563079384871173
manual checking	2.0319218395644905
word structure	2.154243577735084
general dictionary	2.0566213940037743
system finds	2.581629994238975
since tile	2.1930567108259056
prolog clauses	2.1825394469897104
classifier based	2.3162726447741377
general text	2.500942383522798
bigram statistics	2.048018671286978
string position	2.1383659453386112
minimum edit	2.065306306704894
another phrase	2.060145782650642
systems performed	2.1718237813971424
incremental interpretation	2.120737661603118
dictionary entry	2.961889511339757
real numbers	2.499004677693426
hierarchical discourse	2.1000868080586823
feature vector	3.272267323537079
english word	3.3351170186838592
use speech	2.101222560614464
second tree	2.0919512504091182
nl systems	2.2015434616930514
practical interest	2.384183864307157
false alarms	2.219093212707812
explicitly model	2.369115447338463
structure shown	2.6196326496608093
generative models	2.649030896831995
also implies	2.3920948169591325
features derived	2.768725325961829
generative model	2.8127483010421646
rules define	2.318793079863193
term weight	2.0138365075676035
nlp component	2.0047757332520497
structures corresponding	2.3421533972173108
various problems	2.3806000578467827
approximate p	2.0577375124403856
surface strings	2.663209367700485
vp node	2.5257198292072522
several observations	2.022261218861711
newspaper texts	2.6274758635454023
one thousand	2.0807650364935766
column represents	2.105195669894195
frame slots	2.0046688776984256
weighting factors	2.1017707458424697
important respects	2.2776740307447065
translation applications	2.0817120252117016
foot nodes	2.2048407631060267
common english	2.366209969973885
answering process	2.0082269995432465
united states	2.7411753331435698
simple clause	2.045229961808099
single structure	2.364432822528688
formal properties	2.787933409930542
written language	2.8440401493191114
multiple language	2.004280538585712
language pairs	3.2299035003902197
whole sequence	2.24583021875162
framework using	2.045521226679158
values obtained	2.389326705744299
sophisticated system	2.022261218861711
4 features	2.043521175114013
approach aims	2.0601457826506424
free text	3.05832995526904
english version	2.6702201412302826
greatly enhance	2.014014536653706
x j	2.120473489839857
basic intuition	2.166102255087602
domain coverage	2.0432296517672346
contingency table	2.4986865611719358
negative ones	2.154626642362153
context models	2.1211804427697247
value assigned	2.3523313048426364
semantic types	2.765334278647739
disambiguation methods	2.31674451463999
good model	2.472067708086249
senses based	2.230524125991308
uses statistical	2.30606794696972
constituent may	2.280088143077826
paper provides	2.7755413124176123
context model	2.1411698292198764
frequent categories	2.023537111427965
different sources	3.186095726222681
best answer	2.2817090651244336
location information	2.161576763245159
particular slot	2.161246853402207
batch mode	2.4276598679371597
thank prof	2.314007818435079
predicate argument	2.632500181603146
chart parsers	2.1903752569975983
2 discusses	2.6383330595080268
length feature	2.0074734400897967
overall system	3.172499613700066
distinct components	2.045521226679158
topic segmentation	2.0839981402228083
test sentence	2.6550945151057803
used tbr	2.1432314806484
context within	2.3322409522263214
temporal structure	2.235179185844376
language spoken	2.014014536653706
research problems	2.0490697481769016
information provided	3.186632471855108
representing information	2.0965741974051317
automatic way	2.5356198281017184
first step	3.753545502379291
database consists	2.064139780413935
major sources	2.200598690831077
several orders	2.296234813805337
another observation	2.183647914993236
infinitival complements	2.1097950308192193
input character	2.2509785271434346
absolute frequency	2.171672995767222
analysis fails	2.3097823680289724
another consequence	2.0539861379893036
another concept	2.103083082947048
dative case	2.133549549984013
numerical value	2.1960923814187767
run time	2.8512827564632266
appropriate text	2.1588487997915733
extraction method	2.6548023654660273
adjoining languages	2.0360975235288636
psychological research	2.0329822559918163
generalized version	2.1222270206314318
input tokens	2.1840129669193273
translation memories	2.0053762946230105
object relations	2.276003773337814
two situations	2.516572873225381
main difficulties	2.1774937848124125
class words	2.9415472032517695
unification operation	2.3759564443779952
improve precision	2.464836609822278
document detection	2.2182885082765047
model generates	2.3153401641306868
closed set	2.444518217198502
data within	2.049069748176902
different words	3.2905767988897843
head constituent	2.2801895527899143
user defined	2.0343808239087764
distinctive feature	2.25049428373789
scale grammars	2.0816654754454493
automatic learning	2.654501814228251
utterance must	2.135844661161516
one dialogue	2.067081229293324
total ordering	2.007315006862137
new discourse	2.6227675164943407
two tasks	3.1480057837002153
argument identification	2.1702247836053052
news wire	2.2280813991509
accurate semantic	2.032982255991816
right boundary	2.383940893953333
three models	2.637557563020592
first result	2.0965741974051317
normal form	3.051787703586057
structural rules	2.166822722514069
output string	2.548827546008932
crossing brackets	2.480353269657896
adjacent sentences	2.373764665980106
important issue	3.0285912458659094
software architecture	2.225681681945224
associated probability	2.08469715677149
3 seconds	2.264070934029513
rules could	2.5956566822166383
applications like	2.626893153747622
values given	2.267991541156711
another event	2.135294297555318
word hypothesis	2.1577627246289586
two functions	2.791447905449822
feature makes	2.045521226679158
maintain consistency	2.159746288077376
interpretation function	2.3050588293589054
use general	2.0312646501273113
greater variety	2.116591742190463
original goal	2.0301435463043473
small improvement	2.6378074479286577
introduce additional	2.074322125359227
line shows	2.5431102692130456
algorithm follows	2.1290482690107417
certain circumstances	2.1289533511364276
preceding sections	2.420009569037414
generic concepts	2.0175986483913877
bilingual resources	2.0343808239087764
large vocabulary	3.025884392880811
software systems	2.1236611520743356
coherent text	2.5862350649431427
genetic algorithm	2.338212754056447
broader coverage	2.272505380879168
specific rule	2.3496460750061963
formal semantics	2.8195442713861283
large margin	2.1758033131418872
semantic categories	3.135616220563526
crucial use	2.230524125991308
forms may	2.3605881269336804
interest group	2.0343808239087764
set perplexity	2.2475258420906217
french dictionary	2.199395816231769
tense form	2.2943212149342127
c j	2.1234143692948715
naming conventions	2.245558162117437
classes may	2.4421485591028738
computational power	2.3125352367853242
certain words	2.8752883162802068
structures representing	2.338978613401074
maximizes p	2.3804707576702646
trees without	2.194634015772898
current methods	2.1678854032223676
complex grammatical	2.007473440089797
nlg techniques	2.0354296340457756
new features	2.9663148988946952
lemma 3	2.16617598669601
final tree	2.1679945376767864
term weights	2.4037920454462274
individual probabilities	2.012347776929124
model seems	2.1479184330021646
many application	2.067747107964575
trees may	2.500374545686274
possible tag	2.4917828007509444
following lemma	2.187301331793693
ralph weischedel	2.190172125754694
sets based	2.0656667908183506
two languages	3.4682023591210376
nominal groups	2.02007728945488
alignment results	2.0248229969633322
occurrence probabilities	2.2855085173875844
short vowels	2.2610001004685807
adverbial modifier	2.0376102740271858
english data	2.8313046563221467
retrieve relevant	2.353041825540565
uses machine	2.0790198876097175
translation direction	2.0573459943171177
reasoning process	2.366936069156927
translation step	2.012798451915916
accuracy using	2.7748803789416088
important problem	2.830708422854557
equivalent words	2.062833829642103
class problem	2.0466937810787957
two iterations	2.2389453747547177
university corpus	2.1248179754169145
single predicate	2.0539861379893036
multiple ways	2.50352395005998
resolving pronouns	2.0994361843150067
right end	2.293489217517628
generate sentences	2.588076252515428
space constraints	2.458989459396439
structure generated	2.1543888787937475
vilain et	2.0688431445987794
different problem	2.33582780054359
dialogue participant	2.116180416769682
cfg g	2.260803580605681
information like	2.469909103548966
features found	2.227249320716272
structure associated	2.4882216501717878
text representation	2.4935288829576505
full tree	2.00238351760398
semantic hierarchies	2.070902079606615
low similarity	2.0817120252117016
e n	2.4641829816945684
research community	3.1075230593320184
system handles	2.260688804545853
previous words	2.5656414572148876
language learners	2.4723469999118537
one english	2.5552655330242287
queries using	2.2640709340295126
candidate generation	2.058872747877497
recognition method	2.000178283129414
clearly indicates	2.2169936022425736
dependency representations	2.1480601086854554
compare favorably	2.3229739757638654
make sense	3.2722211102853085
contains four	2.560565026996404
failure occurs	2.067747107964575
additional types	2.135844661161516
system simply	2.4250738008550288
word sequence	3.3988509609263686
information automatically	2.4328329620168185
medline database	2.067894093256776
algorithm compares	2.0817120252117016
derivation tree	2.8756365066565888
section 1	3.4907421765681397
systems development	2.248968785663327
associated feature	2.0604764969548435
last section	3.1940260595388836
efficient algorithm	2.7578924806400336
based representation	2.844393304382511
apply rules	2.181370500368545
use two	3.2753296869695805
adequate model	2.0074734400897967
microsoft word	2.200132603862311
following observations	2.5108095309453398
experimental evidence	2.650860138049
result indicates	2.4658585645856936
given feature	2.58158636762688
several target	2.031264650127311
major parts	2.292453688514974
immediately dominates	2.251465329281997
state techniques	2.391531866664422
given verb	2.835621230268185
level information	2.8455647287898747
communicative intentions	2.366852968883598
particular time	2.369115447338463
another goal	2.067747107964575
actual input	2.0257831451063106
syntactic position	2.358093672801348
previous utterance	2.714074656038973
greater detail	2.7347623091976097
candidate sentence	2.2462327573261076
lower recall	2.6112532300437925
learning framework	2.606502253298294
uses information	2.674149638674039
parsing method	2.7126517522924622
category information	2.401654304166575
figure 18	2.1103091777778507
90 percent	2.0390072838737376
output generated	2.110676283984369
knowledge within	2.014014536653706
unsupervised learning	3.0191854671575147
based paradigm	2.0423259728447545
argument slots	2.1884179338195446
interrogative sentence	2.0390072838737376
small text	2.1353034784701492
certain parts	2.4880489469237665
distance calculation	2.136423879344522
early version	2.3448347819209543
arrows represent	2.067081229293324
data flow	2.186006895518436
different modalities	2.167244231831556
score according	2.022261218861711
open vocabulary	2.0508951560700415
also describes	2.472067708086249
significant advantages	2.377838240407285
shared knowledge	2.6213060375099078
examples suggest	2.1072480253375248
one strategy	2.3866967846580778
let r	2.7487907561572804
understanding process	2.5641976280201924
unbounded number	2.269735520013838
language strings	2.0539861379893036
greatly increases	2.24583021875162
3 discusses	2.6910133173369357
future applications	2.24825378073324
cognitive science	2.9531534382055233
smaller size	2.314007818435079
words form	2.166102255087602
best features	2.250314711844597
complex process	2.4489623523461015
third word	2.2571860900326115
incorrect answers	2.29229328278686
words used	3.189490480407665
two sample	2.280088143077826
procedure would	2.4910639166060555
high confidence	2.6301156129617373
performance using	2.991470566899278
good job	2.120711732422718
corner parsing	2.3683984905976407
average length	3.2341806541601414
morphological description	2.0175986483913877
whose input	2.2328679513998626
various word	2.1290482690107417
preference order	2.1810671172835487
variable binding	2.374105083526454
time adverbials	2.018940998552095
attributes used	2.1119430342456558
feature would	2.1516028827357654
allows efficient	2.1290482690107417
human judgments	2.6006430134480145
different entries	2.2047194975469235
probability p	3.3892775341500943
spatial relation	2.010652943170383
style parser	2.255117584688273
many factors	2.5864645700626294
involves two	2.9346931205578652
liu et	2.364220337319731
space representation	2.1317142331609915
strong indicator	2.022261218861711
rule cannot	2.1023887620661283
classify verbs	2.1347704630505406
collier et	2.1407430488088703
defines two	2.045521226679158
information types	2.0414950192142793
formed sentences	2.567571109976615
boolean expression	2.1606544481704097
sentence contains	3.0728488591336447
data collected	2.869264194117341
automatic systems	2.3380561048888593
information retrieval	3.950362010685135
partial parses	2.654064974765372
text planner	2.57467927967663
6 months	2.197570621582594
mathematical model	2.0971123686795576
greatly simplifies	2.1774937848124125
disambiguation decisions	2.0107581307102835
verb stem	2.479534977819461
test procedure	2.2011686204719005
particular role	2.0312646501273113
understanding applications	2.193056710825906
new algorithm	2.7808421025044696
semantic function	2.382846862041011
first part	3.2240881453450974
recall measure	2.184223228921038
language engineering	2.8321993042395746
2 words	2.5425569303229363
common language	2.3718500764739723
particular combination	2.3322409522263214
research issues	2.4405894669393287
best set	2.522851097270778
two operations	2.375954909324613
nl interface	2.204447158215016
leibler divergence	2.4336907963284773
specified feature	2.130459427106893
different model	2.365645084287615
word ordering	2.242792024766331
limited memory	2.000659750129984
results support	2.4251394527998116
empirical research	2.3777902240579687
tagged examples	2.2218970906312903
syntactic objects	2.0343808239087764
chinese sentence	2.7329175484301156
lexical cohesion	2.3153721239937797
johns hopkins	2.4168518426735073
short phrases	2.3464441074524007
atis task	2.3610670224699604
morphological knowledge	2.3052107327182076
generation tasks	2.2835191531602663
provide examples	2.318793079863193
cannot produce	2.3763414719464064
overall f	2.5125820457649146
given document	2.771101887765794
different events	2.185232752671164
standard practice	2.560964388468025
ratio test	2.0569504790471376
one wishes	2.29746681981663
automatic analysis	2.693433816631637
dictionary used	2.4716065331390826
one aspect	2.9924539320191625
two problems	3.1619059845133153
similar ones	2.122246315203648
extract word	2.1543888787937475
recognition engine	2.2348978035422116
local syntactic	2.6790463476394577
whose name	2.2930541601155934
null tem	2.1290482690107417
structures provide	2.022261218861711
general category	2.19110807291794
linguistic expression	2.6656164663468584
hand tagged	2.0923919824150863
morphological system	2.14851837479882
syntactic factors	2.0302811783853087
words including	2.319324296244544
express constraints	2.2062448606993037
parser processes	2.031264650127311
rewriting systems	2.3210818356646294
different processes	2.2429157999161107
input speech	2.4232116075588728
software used	2.1946340157728974
first interpretation	2.2074757805024428
corpus study	2.4321511947879046
take part	2.193626590719127
acl reviewers	2.1290482690107417
procedure similar	2.1290482690107417
good features	2.1210870929251424
query expansion	2.7547228776577413
rules based	2.8509098013575054
commercial advantage	2.6295252639927584
context features	2.6425731257622505
first uses	2.257186090032611
scientific texts	2.284516273330294
lexical resources	3.208675656262802
symbol x	2.292343489683976
lexical relationships	2.224348485612061
single person	2.1156428254927526
art accuracy	2.0995450879838162
front end	2.7902880181768324
like subject	2.289161918175575
nigam et	2.1689432010517136
tile source	2.031216633777997
6 describes	2.34443972705697
paper proposes	3.365426892812459
features must	2.469909103548966
case involves	2.053986137989304
processing software	2.2735815381098616
aquaint program	2.094727102798552
single reference	2.1685587523101804
necessary step	2.21826141526814
verbs occur	2.109437912434099
eurotra project	2.0073178378257412
type feature	2.260122055223635
nominal expressions	2.12780568052783
prior probability	2.869944321690626
feature type	2.1863412538399727
linear algebra	2.0262602507669065
describe objects	2.039007283873737
anecdotal evidence	2.0539861379893036
contain variables	2.282311649307406
cannot say	2.4850442033614906
analyses per	2.0743965972347684
different human	2.369115447338463
speaking rate	2.369315935807954
linguistic formalism	2.2323865842224184
empirical work	2.572956185397291
allow multiple	2.4489623523461015
grammars based	2.2380656204269957
dependency accuracy	2.154347535854356
two subsets	2.5813200365499944
special meaning	2.159746288077376
words x	2.1789148691025395
hmm training	2.1694718574205556
user groups	2.014014536653706
set provided	2.0074734400897967
special tags	2.146224553829869
exponential model	2.045079865815686
example 4	2.8092754210934245
speech rate	2.0171625906989608
constraints may	2.7389053642959613
information needed	3.1416287341177536
output representation	2.265853273580531
semantics interface	2.320529083473108
good set	2.045229961808099
representations produced	2.2328679513998626
surface generation	2.190594299963376
temporal ordering	2.3960682220648915
utterance interpretation	2.1245637044373713
describes several	2.067747107964575
interesting issue	2.2427935805269064
selection using	2.189546252614866
phenomena like	2.544538423836437
graph whose	2.172274429303222
iterative procedure	2.2880423038702316
text domain	2.0539861379893036
grammatical errors	2.473712840527347
phonological processes	2.0525618073674012
entire system	2.2118346396398993
last example	2.7010852234394753
statistical evidence	2.298286641553239
five languages	2.0534430737113434
best algorithm	2.1965725411856996
comparable results	2.684001745599942
different granularities	2.00238351760398
head word	3.1801087678590947
k e	2.308380113990724
used method	2.2169936022425736
second method	2.895261866319775
training example	2.7154423793684757
three features	2.8804407397565472
manual translation	2.1112053387580056
general type	2.537716292907896
basic concepts	2.5738555996284656
fall within	2.427131308580682
anaphoric relations	2.334281594055616
san francisco	2.040846138692384
user could	2.6978660105683288
particular node	2.408973091929181
combine two	2.6190789711591393
type discussed	2.022261218861711
grammatical form	2.235160170828582
parallel training	2.2003728402747718
upper bounds	2.5413352226115
new hypothesis	2.1380488015835484
normalized form	2.317086239777604
syntactic nature	2.014014536653706
two phases	3.1697925070652
appropriate category	2.1678854032223676
clause structure	2.5564626096485865
many approaches	2.5002982171432886
additional parameter	2.116591742190463
gaussian distribution	2.223453205129625
control strategies	2.43154475751745
surface sentence	2.278763242036071
varying levels	2.4475495170792905
1 contains	2.7447068900789136
another question	2.18446353633398
resulting models	2.165115471861699
auxiliary verbs	3.044037411411921
possible variants	2.070902079606615
particular interest	2.9619846557353178
simple surface	2.053986137989304
segmentation using	2.1816335351833818
american chapter	2.0656667908183506
careful selection	2.109437912434099
detailed analyses	2.2112071020073643
resulting model	2.700421521868201
english dictionary	2.9734118135664356
cases involving	2.018419702884334
first names	2.4511792980386775
trees produced	2.5227807276951992
time periods	2.0414659412751233
syntactic constructs	2.3388713664907446
probability density	2.2806959766308665
higher dimensional	2.061212299166531
28 aotrr	2.1023887620661283
per discourse	2.1353034784701483
evaluate performance	2.299686872045762
declarative specification	2.093294558436213
system component	2.238945374754718
using models	2.0923919824150863
features necessary	2.022261218861711
great extent	2.18446353633398
dialogue corpus	2.2323838087863166
published work	2.166102255087602
tagging experiments	2.071454311737682
using model	2.064139780413935
given sentences	2.1222463152036486
atomic values	2.486871972538326
computational complexity	3.1855503919754757
formal rules	2.122246315203648
isa hierarchy	2.0089365131039165
interesting problem	2.4779074915814485
v n	2.3712033839704256
uses three	2.368834809141683
general methods	2.11911577017412
language input	2.985770456587699
informative features	2.377282352832986
close relationship	2.0144910268560356
one solution	2.986662738054894
confidence scores	2.525248030470264
relation r	2.7904816415746763
certain information	2.041711055070633
essential properties	2.014014536653706
two utterances	2.4585083622877137
one language	3.475417318565276
positive result	2.108179066756418
proper use	2.045521226679158
connected graph	2.2069160185517696
language modeling	3.321991341564312
figures 7	2.1380488015835484
small test	2.5263775806495476
partial syntactic	2.3657358503229133
phonological information	2.228615365198183
related tasks	2.603665161750606
analysis must	2.2208458445818073
second possibility	2.267991541156711
recursive calls	2.1240946476680165
word ending	2.333950776258771
new constraints	2.3041921387464726
low confidence	2.3506228458883727
right place	2.1051956698941945
answer key	2.5922390340528985
results must	2.1290482690107417
still others	2.282311649307406
category set	2.0461724308427813
function associated	2.1275552563638707
particular relation	2.4602434646053126
hierarchical representation	2.2019050190787297
unknown word	3.1290831028669754
important tool	2.1023887620661283
efficient representation	2.120473489839857
prosodic cues	2.4037605141020757
little data	2.233997204181393
programming method	2.033972314067133
roles associated	2.172274429303222
participating systems	2.4776317124987517
taking advantage	2.921047474500442
25 words	2.347634011307485
standard n	2.2177044363196816
nlp systems	3.2675351121544467
papineni et	3.020908290080164
parsing stage	2.2512557276899194
equation 1	3.058841738030514
fixed value	2.0312646501273113
module would	2.089026915173973
functional structures	2.3005469950868456
modeling approach	2.390993657182815
different purposes	2.6414992614000585
procedure uses	2.1312839727750825
occur within	2.8002616952414208
brute force	2.3161212503528343
task performed	2.089026915173973
3 words	2.7152296878710196
general technique	2.3841838643071576
development effort	2.403717943407269
verify whether	2.3028207919454977
generalized lr	2.101066673745967
first sense	2.5931006931169645
various patterns	2.0817120252117016
grammatical descriptions	2.2062448606993037
exhibit different	2.1597096509760467
high performance	2.981150140799523
appropriate type	2.5997499619420785
data contained	2.546733973111389
n f	2.2086985117409923
following noun	2.332240952226321
systems differ	2.109437912434099
development set	3.2958730772789697
based ones	2.2250657850226014
events described	2.2335905533336406
semantic rule	2.5000693116544244
frequent nouns	2.0410184476447633
language recognition	2.098549595788317
issues regarding	2.109437912434099
grammar model	2.277764584882625
best candidates	2.3282428017805086
corpus contains	3.3932861146646114
parse time	2.5230943004573407
data structure	3.4882232696839353
large coverage	2.2654812855760516
different measures	2.5212210571604703
current clause	2.0245889944641733
evaluation based	2.3263148914896092
one action	2.3538885971421086
formal description	2.768070954533244
triphone models	2.2376057270974585
human translators	2.6241328198274494
second evaluation	2.4241657936680436
object position	2.797402063801829
directed arcs	2.1954890274836183
based dialogue	2.3002368953133274
knowledge required	2.7012216470899393
different forms	2.9986013666072266
subsequent work	2.403472566572053
learning component	2.239642313236425
show high	2.0074734400897967
japanese text	2.734695670325129
types using	2.0824927688889554
lesser degree	2.0866040331547397
n e	2.4170729192885183
nominal modifiers	2.2330652260757713
future tense	2.4089049470943342
aro grant	2.5302215052732078
preprocessing step	2.7283025780765726
prosodic analysis	2.0064971718505564
second approach	3.057796561101928
spanish sentence	2.1358664885908043
observations made	2.249257833574009
ditransitive verb	2.0730662534060538
output list	2.0808892584499605
tagged text	2.8969951761311545
may share	2.377838240407285
initial seed	2.210530136031284
section explains	2.3125352367853242
involves determining	2.1946340157728974
sentence generator	2.479250638892612
another difference	2.5343882187925413
tile head	2.012347776929124
highest scoring	2.785389886700772
tentative conclusions	2.34443972705697
node x	2.5135662694143455
specify constraints	2.022261218861711
empirical methods	2.950764034893925
8 sentences	2.0264528217305253
definition may	2.2112071020073643
also belong	2.0312646501273113
use various	2.472067708086249
two issues	2.602993423397017
sentences already	2.053986137989304
tree corresponds	2.305458956322113
rule format	2.3203118749100002
research goals	2.2685844072039183
three annotators	2.2828415378352105
summary contains	2.016336526582215
words tend	2.8212480443893613
sentence containing	3.1109819919586923
issues associated	2.053986137989304
different discourse	2.6601978680217564
simple techniques	2.329066040124741
illocutionary acts	2.032006967058394
temporal information	2.6713100321636953
main feature	2.3130951516759186
target language	3.6867471910682856
several authors	2.4359140509656703
est la	2.094273477246064
research projects	3.3778803664584562
la structure	2.054021540311003
par des	2.1552211276539577
system creates	2.2823116493074065
resulting translation	2.1543888787937475
possible reasons	2.487920588919004
main ideas	2.3681262299005272
data formats	2.014014536653706
new representation	2.246574089087019
word occurring	2.3841838643071576
two word	2.987376628836721
linguistics literature	2.5806028878812137
several experiments	2.797361294526122
sentence boundaries	3.0848627485692055
certain forms	2.0817120252117016
using translation	2.0555976865666903
phrase consisting	2.0709020796066153
long time	2.5921802818985844
alignment process	2.500577260908271
nothing else	2.4894440017958916
frequency f	2.1320750321761928
2 proc	2.4659128163621613
separate words	2.5454616944663857
parameter k	2.096756183704444
modular system	2.3728338410468788
act type	2.068727108924794
bies et	2.0792499645377673
node labelled	2.229319530332922
formal system	2.437508621736751
scoring function	2.748025858635253
particular sub	2.015795287891615
matching procedure	2.358288031914048
including speech	2.3263148914896092
sister nodes	2.361328650739999
position 1	2.1217054302872724
paper considers	2.30606794696972
morphosyntactic features	2.421490882658759
independent way	2.272505380879168
association measure	2.101460170138088
single case	2.1245637044373713
important features	2.8399765692978587
8 show	2.089026915173973
kingsbury et	2.0423259728447545
r 1992	2.899149806177168
design decision	2.093294558436213
parser first	2.1852404006823356
random sampling	2.1363579739420246
thank mark	2.183647914993236
words long	2.7724475410445724
finite clauses	2.005615436453831
good candidates	2.659901682175474
newswire articles	2.548082431822374
different features	3.091398115127431
phonetic features	2.076214973947902
retrieval systems	2.6980216646710318
based scoring	2.058162491400105
sense numbers	2.052002004683636
modal logic	2.3634552536643483
given situation	2.5759754126609633
equal length	2.167252082381521
significant way	2.1543888787937475
preceding np	2.0763533583416773
systems participating	2.464836609822279
agreement figures	2.023537111427965
systems perform	2.4943690648994177
system showed	2.026260250766906
relative reduction	2.3060853433103885
mechanisms described	2.067747107964575
two thresholds	2.043229651767234
bell labs	2.251824255316957
human information	2.021422743908272
different constructions	2.1240946476680165
truth value	2.78568098754917
third phase	2.052352800514945
initial results	2.6714519959342202
vocal tract	2.25772233891842
last position	2.222406898745732
language generation	3.576924454437455
work well	3.2810666733133047
space limitation	2.014014536653706
query language	2.8587919930196164
experiment described	2.5477576273960527
high weight	2.168967603804668
one role	2.1119430342456558
column 2	2.4029015502540476
relationships may	2.026260250766906
model component	2.013376363891808
first applies	2.067747107964575
computational efficiency	2.780449000616688
third kind	2.1290482690107417
world knowledge	3.458412527527356
partial parsing	2.811568025280463
linguistic elements	2.308101606493371
horn clauses	2.4107184454548642
first identifies	2.1029215145255775
adjoining grammar	3.063514472282727
frequency distribution	2.7738053068040784
tag grammar	2.3166534739637923
past tense	3.2456533018189684
linear representation	2.0947271027985526
nlp task	2.3918035520880734
another parameter	2.067747107964575
automatic recognition	2.6545782769584503
specific value	2.1597096509760467
little improvement	2.0656667908183506
internal semantic	2.000178283129414
provide access	2.285913086717917
previous discourse	2.7861468889467327
curly brackets	2.4351181598732587
system developer	2.227785770660102
probable sequence	2.1516988527908136
tile text	2.2394738813759885
specific tasks	2.6178833625544735
short documents	2.2016158367443897
montague semantics	2.0260500286285597
given length	2.222406898745732
lexicon containing	2.553692816164179
graph unification	2.1911677802799514
us something	2.014014536653706
method described	3.36856424270734
language sentence	2.8430127310066604
systems command	2.183647914993236
statistical models	3.2282597902434342
speech recognizer	3.3373796655152907
sense 1	2.6044311698572944
final alignment	2.070902079606615
major effort	2.1222463152036486
phonological features	2.016036842031037
two predicates	2.3956483501507027
journal corpus	3.015471398184458
adverbial phrases	2.741807187400179
annotation work	2.124741563612165
sentence whose	2.3949498305472474
complex interaction	2.3000015484793463
slot fillers	2.5800224419447706
theoretical considerations	2.014014536653706
linguistic assumptions	2.1312839727750825
based strategy	2.0918364857418936
like figure	2.313258134832421
merging process	2.258095050950521
role labels	2.3199013494155976
cases like	3.03983020457251
possible part	2.1569808507858435
trees would	2.0355401813680145
much room	2.2631802623080817
particular input	2.3906121063052925
assumption underlying	2.181370500368545
initial data	2.3412321988210874
consider sentence	2.1642359406653675
contract n00014	2.6143455184452526
defined set	2.4873437032542043
analysis consists	2.427896658773757
ordering constraints	2.548092239068278
artificial languages	2.0641397804139356
estimate parameters	2.122563170379007
rationale behind	2.566491334978754
ir system	2.57190452520725
summarization system	2.7465324224351204
major modules	2.014014536653706
applicability conditions	2.0068432256402065
correct pos	2.2742912111882294
work includes	2.9308987312847012
using discourse	2.2147431158325057
rules cannot	2.446115845819681
several classifiers	2.216382284267942
american english	2.8954149692587374
sun sparcstation	2.297574746882776
given source	2.6929603798679147
certain set	2.40919019044283
low performance	2.346889123261037
results achieved	2.3169101672270087
approach would	3.1562887704101383
scientific abstracts	2.2165983686619217
depends directly	2.0659571110450177
annotation would	2.116591742190463
figures 2	2.8475776454955355
original speech	2.061969254719088
database entries	2.1075606806289287
intermediate structure	2.0516706949643604
specific languages	2.089026915173973
currently contains	2.485049614866351
procedure works	2.2578698255858516
earlier experiments	2.3322618602644836
classi ers	2.176574357589444
sentence segmentation	2.4557063902020144
two grammar	2.0883166975146934
processes involved	2.4489623523461015
class label	2.704779420066
decoding process	2.542175741137359
different conditions	2.6190789711591393
generate text	2.3922788066689504
1 c	2.3689838315122618
current state	3.4948050975551443
segmentation model	2.179736313714674
generalize well	2.3031542996401577
model 3	2.361168484547866
users want	2.122246315203648
v l	2.0123114646746347
perfect match	2.11204510557289
functional descriptions	2.2795899904934487
many computational	2.1414753566437956
classification using	2.5036285688060933
useful way	2.4559838465843646
three constraints	2.2401342019912964
possible interpretation	2.4301190004329456
art speech	2.094727102798552
similarities among	2.0918364857418936
paper contains	2.183647914993236
management succession	2.1545713565359597
automatic semantic	2.556631357144834
tile process	2.1023887620661283
four models	2.301027839440682
functional application	2.5233687593151624
interpretation rules	2.5023721996666293
binding theory	2.605431227065617
contextual clues	2.416115729457519
filtering process	2.522318883739232
less training	2.446115845819681
abstract notion	2.11638795219105
using features	2.774624126835123
given term	2.3484137985934623
resolution using	2.012347776929124
configuration file	2.003456043157813
bikel et	2.653343951349568
sparse data	3.335414336793876
analysis techniques	2.675739693959226
logical inference	2.0633132861039947
extraneous words	2.00238351760398
compares favorably	2.6084209245278007
translation using	2.504517714047468
using maximum	2.7332257703206957
one complete	2.1479184330021646
new framework	2.3344467872150743
grows exponentially	2.446115845819681
5 percent	2.230577630180687
total frequency	2.49845193745055
various feature	2.352053963100165
first issue	2.0330146025299998
provide important	2.2578698255858516
similar way	3.3125371194568802
6 illustrates	2.420009569037414
english would	2.2532512925302806
following experiments	2.724008606566628
uniform probability	2.3712391303048754
special symbol	2.678781129264756
evaluation corpus	2.501639660805743
personal pronoun	2.7326329527137854
transition function	2.31185232206522
separate entries	2.1683471279633215
used sections	2.014014536653706
jing et	2.2119777154543563
retrieval task	2.566890339712939
natural choice	2.4784612405958746
nominal phrase	2.394277630442341
certain conditions	2.393406260817177
natural text	2.2512557276899194
two output	2.009868063569498
following phrase	2.1072480253375248
style grammars	2.075729255121515
second classifier	2.1315696211503674
information relative	2.0312646501273113
method could	2.6683454571266996
processing method	2.176180166955999
x r	2.265954289163924
good representation	2.0376102740271858
idiomatic phrases	2.265481285576051
difficult task	3.1024456815848254
basic problem	2.0295673425070127
processing mechanism	2.3338761996148443
recall results	2.3956433384568747
relevant items	2.0639751608073635
models using	2.980392774446833
syntactic relationships	2.610321212215252
two parse	2.1204734898398576
functions defined	2.166102255087602
section deals	2.0817120252117016
corpus containing	2.8933075633913616
unification operations	2.0524573025671087
historical reasons	2.045521226679158
scale systems	2.049069748176902
automatic sentence	2.269813783347405
quantifier scoping	2.2830308960808106
words spoken	2.1353034784701492
bilingual corpus	2.904698591747535
new domain	3.2340945149064875
average precision	2.9952044154935415
procedure using	2.183647914993236
statistical translation	2.7931280249741803
develop techniques	2.109437912434099
parsing could	2.0965741974051317
trec questions	2.0566213940037734
man loves	2.1494275104398577
statistical dependency	2.068430306576883
corpus c	2.2087686577275685
alternative solutions	2.0640655835943473
complete word	2.007038640285683
two patterns	2.501370118679795
expression may	2.4144226718804758
aligned word	2.1637030579857273
system configuration	2.2224068987457315
different formats	2.3271110475429713
computational linguist	2.440385624828699
first problem	2.8553094037716935
says something	2.067081229293324
two goals	2.4036930538896257
syntactic frame	2.271216308915548
final stage	2.8357805073665467
formal proof	2.185240400682336
j p	2.4003742274832387
news sources	2.618208562268448
entire documents	2.1023887620661283
supervised approach	2.3090146370482403
bias towards	2.6172588004480755
event may	2.255593583249476
magazine articles	2.233997204181393
boolean combinations	2.0474712140316846
three parts	3.2259528157869672
problem addressed	2.1984572101092845
complex expression	2.1984572101092845
detailed comparison	2.478461240595875
syntactic tree	2.9959655184993665
formed text	2.1432314806483994
best list	2.718495478994087
processing times	2.184223228921038
disambiguate word	2.3752638941834947
pattern matcher	2.557817759641343
minimum cost	2.2093029625031484
syntactic relation	2.7436252609924097
manner described	2.4917756927623538
templat e	2.261744351334622
markov random	2.135407690482631
methodology described	2.33582780054359
level representation	2.4894382053254107
corpus 2	2.1730383175438037
small scale	2.584325141004728
first criterion	2.1312839727750825
technical documents	2.4094074199220756
another group	2.3005523645853
information explicitly	2.122246315203648
previous algorithm	2.030143546304348
structures described	2.1413469986390323
sentence grammar	2.098379838901587
nodes connected	2.042325972844755
one corresponding	2.709727741353372
textual information	2.6638818513792955
1 word	2.230524125991308
evaluation experiments	2.4136548195732956
works best	2.260688804545853
ralph grishman	2.1845557267559226
contract n00039	2.1290482690107417
human effort	2.7917427911271675
high computational	2.309782368028973
rule system	2.5481578705670587
similarity threshold	2.3030874682306703
inference algorithms	2.051285638402277
two sides	2.0474725266722142
absolute performance	2.109437912434099
less time	2.713199861648787
lowest level	2.3424270994663905
state machine	2.7396043678320536
enough evidence	2.375583664014516
null based	2.183647914993236
given segment	2.1339258481610717
labor intensive	2.0757967043408576
student may	2.1253274598177834
anonymous reviewers	3.720767825801954
core knowledge	2.020363647132216
pp headed	2.0643104749903145
another rule	2.6295926808709433
validation procedure	2.0850741607377534
particular query	2.1413469986390323
semantic grounds	2.307382090261814
handle multiple	2.357947172648072
particular method	2.159746288077376
transfer phase	2.2824082037244118
automatic approaches	2.0435211751140128
form words	2.0414563206925194
examine two	2.1290482690107417
request information	2.0381169104851677
accuracy comparable	2.1119430342456558
complex morphology	2.1499148428335464
columns correspond	2.014014536653706
case marker	2.4666809189391987
semantic components	2.6878632151172277
experimental conditions	2.55637973425133
since x	2.060145782650642
facto standard	2.1852404006823356
systems also	2.4122791606266993
resulting structure	2.6421936814357827
contains syntactic	2.014014536653706
conjoined noun	2.1872962618521736
mapping algorithm	2.0452145060368503
best translations	2.106592193937516
system presents	2.2679915411567104
vocabulary used	2.424241819101846
specific problems	2.53512869780983
several classes	2.673509753814863
possible sense	2.3816070707806287
central idea	2.406719372930616
interpretation system	2.2402982620557466
better recall	2.428939489115404
pragmatic analysis	2.329938689142474
2005 association	2.9814223151299535
higher weights	2.1497135154557805
frame contains	2.14851837479882
main processing	2.022261218861711
target structures	2.0541824194288747
interesting features	2.1607388591823553
based language	2.962323684590447
shen et	2.011910686948191
system architecture	3.112680539775438
definite descriptions	2.3554923117139466
words wl	2.237670801565959
labeling problem	2.19110807291794
systems without	2.3005523645853
confidence level	2.809202749878182
parameter vector	2.361861893891029
vary depending	2.2006548464050617
statistical techniques	3.1861558417827713
future works	2.5182706918152777
one alternative	2.3692784870601207
represent various	2.0601457826506424
head feature	2.515380340617677
extraction based	2.2147431158325057
systems may	2.8260061172915374
event occurred	2.086134808534669
possible labels	2.264546408884895
module takes	2.3907410734021184
language developed	2.0376102740271858
local level	2.0853665562579624
john saw	2.4304640851048487
canonical form	2.871656200336997
measure using	2.2020202169645566
projects agency	3.2524229126069626
among sentences	2.18618848555381
markov chain	2.349671711528341
various topics	2.053986137989304
following actions	2.014014536653706
linguistic reasons	2.233997204181393
second run	2.064139780413935
primary purpose	2.357121432847829
van dijk	2.259035932852377
passive forms	2.216598368661922
5 minutes	2.514624768642089
wsj treebank	2.01632640812982
identify sentences	2.045521226679158
grained level	2.1390445946327175
columns show	2.528133582595804
tagger uses	2.3109116208105
systems requires	2.022261218861711
reasons discussed	2.1413469986390323
semantic rules	2.9003072197740005
n p	2.6909800991182435
related issues	2.160733364715673
state information	2.2275895034737268
sentence parse	2.118274250141516
right frontier	2.1133217156982185
extended domain	2.452787945487347
filtering step	2.0452143290890312
lexical approach	2.066512623606332
target strings	2.083963992592561
bootstrapping method	2.19969957725486
sentence without	2.7325787473058973
node raising	2.016666553235367
heuristics described	2.172274429303222
different constraints	2.4789486950782598
approach might	2.515272207357606
adequate representation	2.1499148428335464
speech annotation	2.1222270206314318
available text	2.031264650127311
many candidate	2.135844661161516
generation grammar	2.3114551145891866
canonical order	2.1796238958338456
official evaluation	2.140860171621032
pruning threshold	2.033741118405042
knowledge representation	3.4868308006356608
sentence patterns	2.5628815274020207
represent knowledge	2.4260324477975805
words correctly	2.4074914462519823
take account	2.263062192650032
satisfactory results	2.1615258659456145
imposes constraints	2.2224068987457315
various algorithms	2.2717132851229636
spurious ambiguities	2.1728922157102217
transfer dictionary	2.162757286566767
whole class	2.352331304842637
require additional	2.1281928639883363
numeric values	2.189546252614866
type may	2.4251394527998116
first model	2.5648350055481135
second goal	2.319324296244544
feature spaces	2.306111902316842
grammatical role	2.5954220838469744
space requirements	2.349235192224533
causal relations	2.153609386307652
intended use	2.1809543568234924
challenging problem	2.357121432847829
semantic markers	2.2344759837146446
large domain	2.233997204181393
simple words	2.254862809312444
score reports	2.0306952911479725
disambiguation techniques	2.1413469986390323
groups according	2.2532512925302806
based learning	3.219472151217757
lob corpus	2.362792537117965
improving precision	2.075729255121515
yarowsky et	2.1029215145255775
system comprises	2.089026915173973
function used	2.4658585645856936
clustering task	2.0806068342718453
word frequency	2.9853952545553653
known methods	2.014014536653706
also worth	2.458989459396439
semantic object	2.312053869702645
fixed word	2.279425300453922
yarowsky 1992	2.1538868574590295
relative word	2.104065875772636
good translations	2.237189563920543
similar systems	2.2427935805269064
chart edges	2.194944574322724
5 note	2.109437912434099
independence assumption	2.850607766958255
binary features	2.830805372519444
learning architecture	2.012347776929124
algorithm makes	2.7171915475564252
dekang lin	2.007315006862137
type ii	2.0053504773005244
trigram models	2.4734903356118654
representation could	2.022261218861711
two probability	2.3588495388442663
give rise	3.158640299730768
every sequence	2.0301435463043473
trigram model	2.944761570642068
little research	2.3841838643071576
information presented	2.4537453156582276
yu et	2.2207046594768247
pradhan et	2.4033890542768184
tree structure	3.4934241804967288
several parameters	2.0709020796066153
david yarowsky	2.0743965972347684
cannot apply	2.227249320716272
large grammars	2.5479192651636
word definitions	2.092856715072056
entity mentioned	2.045229961808099
indefinite np	2.1823661345799445
russell et	2.027838880322471
best ones	2.2339270481759206
fellbaum 1998	2.1638349431134714
training time	2.9035186272427693
great help	2.2823116493074065
core lexicon	2.1077360029617003
given text	3.1990930452685262
learning systems	2.7484612796308583
tile right	2.1312839727750825
functions described	2.0790198876097175
existing language	2.0381169104851677
likelihood estimates	2.6718404345702846
different analyses	2.3486087333327097
surface linguistic	2.2602360398815144
user cannot	2.135844661161516
grammar parser	2.5813779155779466
passive constructions	2.408982961146827
rows correspond	2.031264650127311
generative grammars	2.0545687534546078
concepts represented	2.1543888787937475
given pair	2.694843206421357
recognition based	2.063808915705952
sentences taken	2.474693770770925
words corresponding	2.2492578335740094
method based	3.2334002783899978
special care	2.109437912434099
graph shows	2.523192526452843
like tense	2.0965741974051317
similar analysis	2.417618427399807
basic information	2.590885508004248
empirical support	2.194634015772898
dry run	2.5380457400617815
network may	2.0965741974051317
next sections	2.7587721003626213
standard model	2.1481070588346927
user modelling	2.1569698111237328
right column	2.296234813805337
crucial part	2.352331304842637
complete parse	2.903173265230283
corresponding elements	2.0817120252117016
one input	2.488045314009769
task complexity	2.021974287579246
word count	2.5546269195353797
distortion model	2.0977378027498643
automatic procedure	2.622756498145506
experiments used	2.443147787685106
information appears	2.1290482690107417
sentences per	2.7872990090883363
supervised wsd	2.3107430852718926
roles played	2.0817120252117016
bayes classifier	2.6368099659514144
semantic value	2.385133558094342
derivation steps	2.0215688023747367
finer distinctions	2.282311649307406
one application	2.685384818316671
similar structures	2.25398049501041
speaker wishes	2.051405404013556
riezler et	2.342078023309222
language resources	2.7157674100906735
traduction automatique	2.1136165174266255
preliminary evaluation	2.6638190139871987
le cas	2.1132203767259847
john left	2.1793483932589273
feature must	2.1290482690107417
translation models	2.9624267001336833
tagging system	2.6774908893770073
eugenio et	2.022195114057918
adequate treatment	2.109437912434099
different representation	2.399252281275852
cannot expect	2.1765854705154744
reported results	2.6717518307208645
phonetic representation	2.14851837479882
translation model	3.123851914556968
concepts using	2.1479184330021646
positive evidence	2.1526220174421544
whose scores	2.081242005652375
method involves	2.332743262433283
basic vocabulary	2.1132889815075147
quickly becomes	2.116591742190463
four features	2.513319638697177
technical terminology	2.3229908045214485
kevin knight	2.1119430342456558
computational implementation	2.377282352832986
error prone	2.2838818466058597
first answer	2.079019887609717
third category	2.2020202169645566
sense tagging	2.399307057388265
human experts	2.6972410376745115
little work	2.7955359426260724
everyday language	2.1290482690107417
semantic functions	2.3596325887723424
description length	2.3972601548296035
b b	2.234602249558998
linguistic techniques	2.3548005898232187
method assumes	2.255349860587285
current techniques	2.109437912434099
english examples	2.1678854032223676
various strategies	2.3839874389711913
functional generative	2.169463326892629
sample sizes	2.0852239997314856
include syntactic	2.318793079863193
existing ontology	2.0445276499269367
different scenarios	2.116591742190463
following format	2.375180844916545
segmentation bakeoff	2.229434266472709
language technology	3.0813564181019517
unary predicate	2.0076675616835056
world texts	2.2567006233564806
new component	2.0423259728447545
linguistic interpretation	2.141447453937137
using constraints	2.042325972844755
two feature	2.8990385298444954
following classes	2.292453688514974
robust processing	2.199770381720386
parser finds	2.1946340157728974
use word	2.7146815047826367
log scale	2.067081229293324
single transducer	2.0364139754210724
speech processing	2.85335330475742
log n	2.279260087943023
speech waveform	2.0256307281812576
reduce ambiguity	2.2975747468827756
two ideas	2.0376102740271858
signi cant	2.8105760206757435
optimal parameters	2.010845255290387
incremental parser	2.0553852839257436
various processing	2.089026915173973
yorktown heights	2.28442369794204
assumption behind	2.3073820902618136
definition would	2.116591742190463
subtrees rooted	2.016336526582215
human generated	2.133165345977436
certain restrictions	2.272505380879168
shows two	2.9218308478609996
take care	2.645580479940295
underlying idea	2.2062448606993037
next steps	2.544746394422729
conditional likelihood	2.0679726666734286
phonological representation	2.1557077759553547
like part	2.0601457826506424
lexical chains	2.1004200912601774
highest similarity	2.446550455992202
scope ambiguity	2.2490077209959694
bayes classifiers	2.1649504455579036
best scoring	2.519622046131618
extremely time	2.067747107964575
standard deviations	2.640437911558811
trees derived	2.200183409315332
main aspects	2.1290482690107417
additional advantage	2.48504420336149
individual sentences	2.8289229305237225
rich morphology	2.2742912111882294
coreference chains	2.1824911091132915
syntactical information	2.158057566768644
tag trees	2.032117414479018
word class	3.006283862300571
one copy	2.0918364857418936
test corpus	3.53615678935616
language described	2.0743965972347684
infinitive marker	2.141475643340291
specific issues	2.306150715775418
best fits	2.1479184330021646
word entry	2.106592193937516
differ significantly	2.299296701238553
coordinate structures	2.375898878716617
features encoding	2.0376102740271858
highly context	2.060145782650642
possible words	2.6116024853092257
total score	2.5356828051672804
psycholinguistic evidence	2.1813446704867463
approach takes	2.307382090261814
hoc analysis	2.0262602507669065
predicate p	2.476626197409145
third type	2.7859456766417003
identi ed	2.635581146065151
small constant	2.272505380879168
9th conference	2.3398800045580104
01 iv	2.278461809175595
predicates may	2.1023887620661283
data comes	2.3133280918955017
similar size	2.0074734400897967
row represents	2.1023887620661283
data available	3.0203615978234044
user actions	2.114299018453181
specific word	2.9227433549034907
il ne	2.049653377978883
2 demonstrates	2.109437912434099
similar task	2.3567860333521553
treebank wall	2.3384628419127154
boundaries using	2.0965741974051317
target application	2.2093487881958334
based architecture	2.3484504851702557
basic forms	2.1798605341165187
example suggests	2.0817120252117016
words 1	2.039007283873737
features instead	2.045521226679158
retrieval performance	2.601880236158524
different treatment	2.2224068987457315
stochastic methods	2.154626642362153
conversational speech	2.421068841410884
hyphenated words	2.08301014520863
feature combinations	2.6203737020043567
separate set	2.5034484832534796
single class	2.3138050930290617
resource management	2.8787438121025044
associated press	2.493794714329463
also points	2.022261218861711
rules need	2.24583021875162
learning task	2.767173469031978
sense defined	2.2062448606993037
single entry	2.3088417794636857
based similarity	2.5043338883890094
simplest approach	2.257869825585851
pronunciation dictionary	2.3040004708074777
syntactic criteria	2.3762318217827874
document contains	2.341718162168719
decision function	2.043441408539511
discourse models	2.429862628987019
verbs take	2.2470811344797337
separate modules	2.3955781761503503
state set	2.0758920949213184
system supports	2.233997204181393
multiple occurrences	2.46648603533181
text analysis	3.160566333260824
surface expressions	2.1753774860867354
improve translation	2.3681262299005263
result reported	2.0376102740271858
sense assignment	2.0176429747071856
following parameters	2.332240952226321
correct attachment	2.3186350158649853
time information	2.2971141718756547
description may	2.3464201787543617
node corresponding	2.5188026057744635
two trees	2.9301589600951194
original rules	2.096756183704444
user preferences	2.146665527173802
broader class	2.1222463152036486
previous efforts	2.366082551445672
great majority	2.267991541156711
interannotator agreement	2.174330659408481
solved problem	2.0312646501273113
text structures	2.3371689462405847
incorporate information	2.296234813805337
analyses produced	2.3028207919454977
chunk tag	2.204444284292501
grishman et	2.4835486736008545
initial test	2.1479184330021646
matching mechanism	2.0743965972347684
phonemic representation	2.0906047175178246
become clearer	2.045521226679158
heuristic methods	2.2470811344797337
whose top	2.175400407836868
query interface	2.061969254719088
la r	2.0828351553974813
chapter 3	2.346889123261037
phonetic transcriptions	2.3558670356035636
next goal	2.0806068342718453
among alternatives	2.227249320716272
unification grammar	3.0371527222010015
processing steps	2.884092243505443
overall complexity	2.061212299166531
model score	2.295993176291664
classes using	2.368834809141683
following question	2.5380211325085984
little use	2.2875126027234267
word processors	2.1170202699997867
traditional word	2.1678854032223676
small proportion	2.3161212503528334
two papers	2.3644826080334638
two topics	2.122246315203648
users could	2.3955781761503503
syntactic environment	2.20581029094498
7 proc	2.24825378073324
representation based	2.417618427399807
structural configurations	2.0532484523230123
need access	2.1642359406653675
efficient way	2.702027284550044
translation purposes	2.035540181368014
use hand	2.338978613401074
language could	2.172274429303222
term goal	2.518557419164695
given test	2.448562772805704
search system	2.03427809694608
tagger using	2.099458264360342
clauses within	2.022261218861711
new definition	2.025603650533774
sentential complements	2.5500900540683187
many times	3.138516034970446
determine exactly	2.0074734400897967
lodhi et	2.03820918056356
relational databases	2.0103321539039403
using svms	2.1037925639037014
wide coverage	2.8506918840088225
null let	2.4211599391922816
string project	2.1702395039009192
one sentence	3.472380408439428
initial step	2.5429423198957037
different translations	2.5030728746431574
original context	2.0157594426817584
kernel functions	2.430098507699861
intelligent information	2.2046317742048496
system still	2.4012688714330688
semantic operations	2.1975706215825945
grammar representation	2.0965741974051317
8 shows	3.1391145196578893
following tables	2.109437912434099
direct translation	2.4392671950653337
classification techniques	2.311117526994872
9 times	2.122246315203648
document summary	2.0944180653276705
polysemous verbs	2.1185479858645677
two algorithms	2.90909515064837
verb occurs	2.2842975125096614
people tend	2.2458302187516197
see appendix	2.8147420596784256
figure la	2.0799644973902938
model size	2.253406277427487
module must	2.200616430908741
limited space	2.1290482690107417
tile meaning	2.0824927688889554
computational research	2.067081229293324
second np	2.0648406357378892
category assignments	2.1621373145487084
research results	2.480090296249826
information value	2.2534176846803433
case system	2.029928771701769
many verbs	2.596191215145221
experimental setup	2.8147537549950745
semantic problems	2.1023887620661283
different domain	2.796495787128569
basic set	2.3027646212687944
object control	2.016336526582215
finite subset	2.233997204181393
parallel corpora	3.0941529933894696
human understanding	2.1871636353649375
unified approach	2.1397692523882723
natural interaction	2.0514054040135554
say nothing	2.1728673576400466
linguistic structure	3.1486551301053036
ce qui	2.2680938726376714
fundamental role	2.0376102740271858
specific grammar	2.4682791781671014
misspelled words	2.27264175975403
pragmatic factors	2.5350154129391833
k c	2.0312646501273113
entities involved	2.216996932935226
another study	2.1413469986390323
made precise	2.109437912434099
one defines	2.022261218861711
directly address	2.202020216964557
compare three	2.305097752546449
combined score	2.150832327588077
following description	2.403472566572053
theoretical results	2.189972618711585
german words	2.399426259028305
semantic dependencies	2.47010956395644
conditional probabilities	3.200092067862923
random subset	2.00238351760398
work done	3.169942917785531
10 seconds	2.4196519231039106
baseline system	3.136274840723368
hierarchical relations	2.5230101023893345
algorithm produces	2.6156389421994195
different grammars	2.475593129161638
time available	2.053986137989304
carnegie mellon	2.881545953691855
two fragments	2.184223228921038
various aspects	2.8329180143540627
anaphoric pronouns	2.3748793628350033
conversational implicature	2.0351122267235264
relative improvement	2.5038190800982125
compound sentence	2.094727102798552
adds information	2.1023887620661283
lemma 4	2.0537205948354664
tags associated	2.218434485750926
system answers	2.236207490380668
inverted index	2.003573785155834
heuristic search	2.4271804245518185
state parser	2.0734372155711296
category x	2.542907266437924
phrase attachment	2.949170867319822
string edit	2.1539084822622216
matching approach	2.39137722096019
understanding model	2.1076034117323625
functional relationships	2.083963992592561
semantic classifications	2.010758130710284
reader might	2.200598690831077
model becomes	2.1023887620661283
three modules	2.665020298065938
simple heuristics	2.7444804059180865
new way	2.130393854774857
dynamic programming	3.4990557548421206
complex models	2.5360948406750348
main memory	2.075387885829387
candidate parses	2.083758513891132
another advantage	2.929633652086625
found evidence	2.022261218861711
right parser	2.153824252402984
valued function	2.026260250766906
different queries	2.075729255121515
example involving	2.109437912434099
information concerning	2.8761218098243475
words along	2.1072480253375243
various examples	2.0376102740271858
two subjects	2.3803040425985245
paper proposed	2.435600505453947
optimal parameter	2.350800143454687
appropriate surface	2.031216633777997
fruitful discussions	2.6242476210246806
standard maximum	2.0965741974051317
task data	2.256674691521405
speech may	2.2459839572002176
parser always	2.045521226679158
another object	2.453238475110625
pronoun may	2.1543888787937475
example given	2.9002621359675635
motion verbs	2.159532551070812
specifies whether	2.172274429303222
prepositional phrases	3.5638563406518005
computer processing	2.3005711211682884
nlu system	2.0225949040519957
one proposition	2.1569808507858435
linguistic coverage	2.601239527381541
language front	2.2532646035293618
target sentence	2.9756596444973633
tense verbs	2.0301435463043473
raw corpus	2.4034139134998123
recognition experiments	2.4026638775360936
also measure	2.1479184330021646
much interest	2.326012528775637
minnen et	2.1635173252589412
high degree	3.2459747124933536
different ones	2.183647914993236
contain two	2.4172567595503076
every case	2.709872906918389
concepts within	2.292453688514974
newspaper text	2.9220562619421195
among terms	2.1683088730154427
sample set	2.3235034497790936
terms like	2.5308936187941877
language allows	2.222406898745732
context includes	2.0817120252117016
traditional grammar	2.1030794070658043
crucial importance	2.3005523645853
single example	2.2578698255858516
simple left	2.022261218861711
quite straightforward	2.468819905483623
syntactic derivation	2.117976918624664
factoid questions	2.333371351913367
examples involving	2.2776740307447065
chinese corpus	2.672664314130947
linguistic entities	2.465866818670254
negative weight	2.0355401813680145
planner must	2.026306434023506
phrase contains	2.2827808432516172
closer look	2.8654549930129023
alignment using	2.161246853402207
000 training	2.376854768665281
scoring system	2.513972701036543
linguistic phenomenon	2.710330403257162
method cannot	2.453255888575513
algorithm uses	3.094776233040215
quality speech	2.1418309641168305
prototype systems	2.08469715677149
significant performance	2.6776831829771157
two strategies	2.7596617589064487
specific patterns	2.5492813917277766
word processor	2.452580983891515
becomes clear	2.7229150622263774
cue phrase	2.243732174247019
input texts	2.6684790657027295
different dialogue	2.313900874640595
paper also	2.3849278689843185
entire range	2.2112071020073643
independent training	2.2601420188029433
emnlp 2006	2.6383330595080268
basic algorithm	2.8141471176054265
build systems	2.286725248739412
minimum description	2.551363649358609
examples given	2.9215095909382756
copy without	2.850240182896208
specific goals	2.1543888787937475
best solution	2.3366800162559764
words according	2.7526048406631083
entropy principle	2.1239639918785804
infinitival complement	2.1379435795907584
input side	2.029677424295417
parsing experiments	2.2421869127123264
experimental framework	2.0329822559918163
increase recall	2.1946340157728974
shieber 1986	2.2384227085772213
file system	2.1052708565452414
occurrence data	2.4622835876836966
string w	2.7319291004044213
section 9	2.440243192925816
paper may	2.0355401813680145
highest confidence	2.318334366012213
morphological disambiguation	2.1830479746363824
task without	2.200598690831077
second function	2.0817120252117016
default case	2.285913086717917
paragraph structure	2.0900996013327275
speech production	2.38084302763885
score obtained	2.1774937848124125
simple one	2.722915062226378
items whose	2.049069748176902
corresponding part	2.25398049501041
task may	2.414320698244546
proper interpretation	2.0376102740271858
using domain	2.494551009800219
first algorithm	2.430622988482785
important advantages	2.154388878793747
enables one	2.2776740307447065
semantic nets	2.332067426744522
language expressions	2.81684396187453
telephone numbers	2.0947271027985517
active form	2.2713037750780782
r rules	2.0213043660700594
additional problems	2.2459839572002176
output structures	2.095889919400075
correct disambiguation	2.0019190514861265
processing unit	2.037490565157132
suggest possible	2.0312646501273113
system depends	2.222406898745732
task requires	2.6303629390379415
new class	2.054620936781632
without regard	2.7346700805094724
another agent	2.3479223064700623
robust parsers	2.114299018453181
nodes correspond	2.524359712641554
structure corresponding	2.615295408923962
polynomial kernel	2.48307696253503
new entry	2.095801344725564
word errors	2.2514978073951983
representation level	2.233997204181393
contextual rules	2.1842909289841694
grammar framework	2.512252162586371
chain rule	2.2773488517879152
large differences	2.2328679513998626
n sentences	2.49934710385296
preceding context	2.5249843288368377
similar semantic	2.30018490760141
best way	2.680820996253512
algorithms used	2.7234880988372834
full np	2.2298717609582903
simple discourse	2.0817120252117016
possible answer	2.4403483572687596
provide evidence	2.8212158560920866
user question	2.002101204408598
noisy input	2.329081436908228
systems would	2.6293888667104
second level	2.9235551235625596
recognition rates	2.341927992584661
best path	2.6907721283588475
larger window	2.0824927688889554
template fills	2.0157594426817584
previous ones	2.756930017472095
model still	2.060145782650642
first parser	2.111260367809465
testing set	2.7797342280047403
elements used	2.0918364857418936
system constructs	2.1499148428335464
word form	3.084627313575517
disambiguation problem	2.465656257223266
system performance	3.473477764808061
u l	2.09586186254122
noisy environments	2.0355401813680145
word used	2.260688804545853
boundaries within	2.172274429303222
performance among	2.257869825585851
system helps	2.0074734400897967
algorithm introduced	2.116591742190463
linguistics computational	3.112873336856607
trees must	2.1023887620661283
evaluation system	2.1332561820253773
two taggers	2.0091347967798665
overall results	2.866815933210036
reference word	2.0730662534060538
resources required	2.369841578143437
choice point	2.3104718387666985
constraints expressed	2.2506290832274614
user need	2.1954890274836183
previous dialogue	2.248968785663327
event type	2.438773522152052
constraints using	2.0817120252117016
linguistics volume	3.284471746961398
spoken utterances	2.65496393713884
based interface	2.4656981825450215
conclusions contained	2.7386684072391034
ones whose	2.060145782650642
length n	3.0177609208414955
set fills	2.1088928759945356
four parts	2.4619447088476862
system extracts	2.402081248484701
based application	2.1543888787937475
good solution	2.1789148691025395
random variables	2.711030193162932
representative sample	2.0703845377380894
appropriate interpretation	2.1295010830873577
generation algorithms	2.500395003129813
detailed analysis	3.166738651269351
parsed sentences	2.803852785889292
version 1	2.702381822688378
word order	3.6116514706486154
sentence would	2.881547096560375
standard algorithm	2.093379458516597
many authors	2.417618427399807
general concepts	2.3650499971239713
monte carlo	2.315939163198885
state transducer	2.7777427394373593
meaningful units	2.1263300838943215
words considered	2.067081229293324
verbmobil corpus	2.1077074249880274
score given	2.0824927688889554
fig 2	2.270820552349159
two methods	3.3893585837657323
module consists	2.1275552563638707
type would	2.0817120252117016
single form	2.060145782650642
better measure	2.0312646501273113
error propagation	2.064468417437918
sliding window	2.206751229082256
morphological complexity	2.019557844009582
textual representation	2.0376102740271858
similar sentence	2.2218827559617456
optimization problems	2.0919512504091182
also demonstrate	2.427896658773757
methods discussed	2.249257833574009
hierarchical structures	2.4190388834065324
current utterance	2.782195891166287
module produces	2.1358446611615154
central concept	2.1543888787937475
sentence fragment	2.556623982159797
information requested	2.1295694626022152
computational methods	2.407491446251982
form shown	2.4683935743557397
structural similarities	2.014014536653706
theorem prover	2.5805821334229164
examples provided	2.1390445946327175
corresponding nodes	2.4670470801198285
given pattern	2.045229961808099
testing conditions	2.0516186188527366
key phrases	2.079264951728469
increasing number	2.2970050342683517
similar performance	2.7359546972580224
smaller corpora	2.1222463152036486
specific kind	2.2679915411567104
singular values	2.198076956159719
500 sentences	2.6940625260632634
primary difference	2.022261218861711
maximum number	3.1727996574655597
algorithm fails	2.1773123985040854
text coherence	2.1727480547434777
semantic word	2.233997204181393
detailed example	2.189546252614866
based recognition	2.174323003243204
reliable estimate	2.0376102740271858
two dimensions	2.764652996284999
recognized words	2.1284212508475324
conversational participants	2.038623875559448
active area	2.200598690831077
important component	2.7347778602541566
text summarization	3.0610101914400296
good way	2.525410700315977
last name	2.5057561810065425
line lexical	2.075729255121515
next character	2.0382420403058585
simple set	2.5920008293886196
et le	2.3820509303473845
corpus collection	2.1397467029652075
linear interpolation	2.8475953774978184
discourse history	2.330374143299445
000 pairs	2.1356018195482047
general sense	2.134612489364615
first hypothesis	2.349906446668441
definite articles	2.178488819041351
general lexical	2.3605160818311552
level language	2.5490708135956193
expected number	2.3780451654078196
role label	2.0526958695128266
subcategorization frames	2.775080105589507
word position	2.5687189945956552
current problem	2.085074160737753
lexical types	2.1073621572332435
structures obtained	2.022261218861711
positive feedback	2.086091306651066
early versions	2.194634015772898
derived tree	2.449912155023795
crucial problem	2.24825378073324
error types	2.47299116491493
local context	3.2166175558458643
lexical look	2.0626199010020683
third clause	2.127040445265349
feature structure	3.2509279945046674
sur le	2.4670157012549296
generate different	2.380830363798011
contextual knowledge	2.274826775419323
corresponding concepts	2.178653004950429
romance languages	2.4392022794795754
statistical machine	3.2877296771135827
sentences without	2.7012216470899393
individual terms	2.283880613603289
falls within	2.156511645508365
improve coverage	2.015795287891615
low degree	2.14851837479882
special words	2.0445276499269367
node dominating	2.2610850680099106
experience gained	2.2776740307447065
verb semantic	2.0076680603651567
million tokens	2.2243484856120617
sentences may	2.930358051953692
possible forms	2.3362105897148213
variables occurring	2.0329822559918163
experimental comparison	2.0376102740271858
wsd using	2.012347776929124
salient information	2.036634004975291
improved performance	2.7334313200077758
lexical mapping	2.0103115008582284
studies suggest	2.022261218861711
step 3	3.2756209355378196
tests performed	2.053986137989304
longer n	2.170243850585469
use feature	2.3229739757638654
two novel	2.0665126236063314
similar approaches	2.282311649307406
source words	2.7371292710272166
example like	2.21826141526814
experiments indicate	2.6516949064001833
whose purpose	2.322894481658006
web site	2.7137344655434483
also differ	2.3005523645852994
communication channel	2.025073621873738
new analysis	2.0390072838737376
sentence type	2.4503865694572884
approach leads	2.431764288227561
construction may	2.075729255121515
access system	2.079964497390294
wordnet database	2.131569621150367
paper attempts	2.2020202169645566
lexical acquisition	2.8764714160311464
large class	2.701255263541598
shriberg et	2.378654102291726
system maintains	2.2275895034737263
finite number	3.0303718829073056
plain text	2.7495235938839184
right association	2.163268421009886
selection among	2.3553853167628196
human dialogue	2.4961781426515413
second group	2.6316242284079907
new questions	2.0872002222075654
features required	2.0376102740271858
parent node	2.9139699424112884
learning approaches	3.082896836192978
native speaker	2.998857755350979
template slots	2.409643708165759
algorithm works	3.036300167022251
sufficient information	2.8063230324217527
worth noticing	2.2147431158325057
possible discourse	2.0092767183683407
new structures	2.0743965972347684
expressions whose	2.2380656204269957
several documents	2.0460967409737787
f denotes	2.0376102740271858
system often	2.1290482690107417
restricted domain	2.6651814723603593
ideas presented	2.558787148135115
anderson et	2.0388643487004234
given meaning	2.1813446704867463
demonstrative pronouns	2.313592565634665
content selection	2.4801162649684705
markov model	3.3917884975833914
various versions	2.1413469986390323
structures involving	2.0817120252117016
examples like	2.953854446386294
general characteristics	2.0312646501273113
first draft	2.2717534996401523
significant drop	2.1290482690107417
symbolic approaches	2.007315006862137
two aspects	3.158896717121438
stochastic process	2.390611729622481
model depends	2.1479184330021646
statistical information	3.0997282978331393
semantic predicates	2.2164708079094826
ai systems	2.1926272599203376
given n	2.464532383816498
real number	2.5983758925109566
translated texts	2.0937331662816834
first letter	2.608366095666879
significance level	2.444858458160345
upper part	2.3723877389097376
automatic method	2.7708281721763406
discourse structure	3.3227876603977533
appropriate sense	2.6678159093357863
whole training	2.550105828369166
new example	2.0013487161900905
lexical categories	2.9959560893346757
also work	2.33582780054359
limited vocabulary	2.4682791781671014
translation hypothesis	2.0682414415090786
different levels	3.6327439893472016
great number	2.3867895757331636
dotted line	2.701926834291007
simple string	2.466925268566296
domain objects	2.379140427112616
expert knowledge	2.3420936710009004
first generates	2.1774937848124125
objects representing	2.026260250766906
wrong answer	2.175400407836868
highest performance	2.0709020796066153
combination method	2.245977705409868
simple algorithm	2.844135049761102
takes advantage	3.148503677426937
angle brackets	2.5607653198151468
english letters	2.060476496954844
two examples	3.362471329822602
overall design	2.4260324477975805
based generation	2.6031577499468805
il est	2.370316057611264
evaluation metrics	3.1078270185436296
model performance	2.240972355536064
scaling algorithm	2.1131099776164364
broad coverage	3.1127148458480582
phrase containing	2.5191893533933394
possible features	2.5276537243274797
two passes	2.2735815381098616
human users	2.6390442340458824
morphological segmentation	2.016283897970683
performance comparison	2.278461809175595
particular domains	2.3922788066689504
two pieces	2.607463689063491
speech ambiguity	2.0340875085360004
algorithm returns	2.326156671484873
alignment models	2.432361122402217
best systems	2.3661314342895565
g n	2.0854413137138312
expert system	2.7861181656827383
algorithm applies	2.1127786641962607
following rule	3.065231264998896
immediate dominance	2.6690214557307397
data entry	2.2679816335603733
proposed system	2.202190366943775
third line	2.3462454374844146
grammatical frameworks	2.098379838901587
correct word	2.927348662792899
algorithm identifies	2.368126229900527
optimal solutions	2.200661481121184
algorithms require	2.014014536653706
phrase boundaries	2.6917339816087984
training method	2.5291790619558343
given expression	2.1119430342456558
semantic grammar	2.6743085987232624
main result	2.5201146206561638
corresponding positions	2.022261218861711
additional word	2.151302016846157
theorem 3	2.4343869981374624
correct segmentation	2.4863951963424054
random samples	2.3831704925481247
thus require	2.022261218861711
including word	2.5765298321699723
freer word	2.0959736926527275
probability mass	2.8583589043815842
vectors representing	2.1519861904642914
level relations	2.0329822559918163
linguistic theories	3.1258610532910795
syntactic transformations	2.397479428988884
based sentence	2.1376478921795745
de ce	2.328592741714822
lynette hirschman	2.1395152299866322
gram lm	2.008312803416202
features shown	2.135844661161516
speech applications	2.282398402847935
also show	3.2695984556485476
related sentences	2.290213159846572
algorithm using	2.8562918528191794
makes explicit	2.6123916139998635
require knowledge	2.3005523645852994
accurately estimate	2.0390072838737376
lexicon lookup	2.2424226425547475
overall improvement	2.143411888144605
chosen based	2.286725248739412
various applications	2.6239319852280314
best analysis	2.3174394199662327
one described	3.055918753373987
total time	2.6456419718502944
tabular form	2.0965741974051317
edges represent	2.2458302187516193
constraint satisfaction	2.6068262581632298
conditions hold	2.5247277233047374
capture generalizations	2.264070934029513
bias toward	2.386958661868031
mt output	2.1445801202094423
new task	2.57326102323454
stored information	2.1275552563638707
also cases	2.2917594692280554
open class	2.710663011129561
set inclusion	2.088316697514692
concluding remarks	2.8810869673988777
machine translation	4.0976162776253435
entire word	2.1312839727750825
features include	2.8058164978186078
approach consists	2.384183864307157
gives information	2.267991541156711
sentence might	2.286725248739412
2 x	2.615504335457798
recent years	3.1509668187231967
surface syntactic	2.8643869589702393
discourse entities	2.69287654708477
figure 3	4.3941890248185995
also capture	2.053986137989304
collection contains	2.122563170379007
rules rather	2.0817120252117016
appropriate place	2.189546252614866
language corpus	2.4193544819247768
spelling rules	2.124133663933101
precision scores	2.7241327949582983
approach produces	2.109437912434099
strategy would	2.3840489253691346
indicate whether	2.6638688931804393
deterministic parser	2.3952472037931107
value may	2.3540470303697685
section 00	2.165420395236709
complex lexical	2.3777902240579687
main drawback	2.314007818435079
stochastic tagger	2.173021333840518
main difficulty	2.050145631954837
experienced users	2.0210388623219226
corpus show	2.172274429303222
good quality	2.3001654690461426
statistical measures	2.6971929301364757
square root	2.2055664310469827
important differences	2.6305677470027193
training session	2.0514054040135554
simple voting	2.0197018222327414
achieves state	2.120473489839857
lexical item	3.556411364089576
experience suggests	2.189546252614866
unsupervised manner	2.2074714886932463
rule contains	2.3634279675149354
a1 a2	2.0129016904138224
detailed descriptions	2.6058182593196633
dialogue participants	2.491614067132303
training corpora	3.108220287026254
original texts	2.447662308263296
automatic alignment	2.253114600967547
analysis modules	2.230524125991308
tag b	2.022410241232288
parameter l	2.20379976615122
maximal number	2.4304431782151887
tree set	2.003391327172082
results indicated	2.305458956322113
significant amount	2.514683649855485
automatic derivation	2.014014536653706
variable length	2.4192841623303925
automatic evaluation	2.734419863985713
hypotheses generated	2.0997204758643067
methods perform	2.260581860751368
l l	2.5579008619525903
conclusions drawn	2.0355401813680145
paper shows	2.9564552948209797
parsed data	2.3381640765732143
adapted version	2.1290482690107417
underlying semantics	2.314007818435079
400 sentences	2.2006614811211835
root form	2.7902680872600603
two parsers	2.36582681462721
three things	2.149914842833546
words w	2.9070676954485504
whose semantics	2.465705166234148
research shows	2.159746288077376
based approaches	3.558020021665746
nodes without	2.0817120252117016
automatic approach	2.363952599360679
another reason	2.5946315607860653
perfect tense	2.0809444184762627
phrase whose	2.33582780054359
best score	2.812772227145453
method achieves	2.6320044032339216
dependent word	2.209135229889813
000 items	2.119240389473862
automatic induction	2.1543888787937475
nodes may	2.5570824702860264
argument may	2.3841838643071576
also hope	2.3271110475429713
pay attention	2.356571202034841
base case	2.2665896422685985
arbitrary set	2.0965741974051317
frequency threshold	2.492658464598212
corpus processing	2.1566279942021183
5 proc	2.2776740307447065
per class	2.071454311737682
highest weight	2.2062448606993037
database queries	2.5245100775029443
stage 1	2.1064814171632493
complete specification	2.24825378073324
based frameworks	2.178914869102539
temporal adverbs	2.1124575461997575
system operates	2.5182706918152777
transition probabilities	2.990689476355479
contains two	3.2265358948123586
general words	2.147342467231957
unannotated corpora	2.023537111427965
called upon	2.3402017998642997
constraints within	2.067081229293324
long sequences	2.0965741974051317
paper makes	2.014014536653706
user provides	2.2389453747547177
prima facie	2.2062448606993037
complex structure	2.671285053439984
linguistic information	3.5561274322313237
n may	2.1543888787937475
current information	2.406719372930616
important facts	2.014014536653706
tts system	2.186142628900986
foreign names	2.2312871521912916
meaning postulates	2.345439310546962
e u	2.225202286931343
specific topics	2.022410241232288
important way	2.2112071020073643
oriented towards	2.3004387076891266
possible future	2.1207748533802038
type n	2.0460964401335864
spoken output	2.251642442858885
w occurs	2.008083705228559
worst results	2.1290482690107417
inductive logic	2.1963305057478624
certain language	2.351334739093333
word training	2.0636931540622787
english part	2.475520658821424
groups based	2.009868063569498
results shown	2.945499776082673
frequency data	2.5592441366300958
different way	3.0378188989027595
source model	2.0592462203794692
use large	2.200598690831077
upon work	2.764273484958835
planning process	2.6798194420597605
sri language	2.026260250766906
structure according	2.0539861379893036
good thing	2.031264650127311
nist evaluation	2.039007283873737
lexical gaps	2.2080910085338683
lower bounds	2.491450601759153
case frames	2.7276856537499916
lexical property	2.0592136405805275
tag pair	2.094557943574348
recognition rules	2.029285680107773
discourse constraints	2.026260250766906
unification categorial	2.040238153498075
various senses	2.30018490760141
two perspectives	2.1462245538298697
three systems	2.815079015561835
correlate well	2.2288911216551726
results comparable	2.4559838465843646
tags used	2.54286239146793
strong emphasis	2.1023887620661283
practical point	2.5356198281017184
new users	2.102921514525577
language phrase	2.0025135692567964
rule 7	2.114299018453181
different learning	2.6649796730064055
somewhat ad	2.045521226679158
r f	2.059969967618288
question could	2.172274429303222
speech database	2.366334147006171
language cannot	2.159746288077376
whose argument	2.0520370107650927
viterbi search	2.534542510200496
linear form	2.1117654159915116
interpretation module	2.065429826730168
surface representations	2.0619692547190884
japanese dependency	2.1007895988159273
report experiments	2.014014536653706
consonant cluster	2.0887615850234242
document may	2.362505415324706
description logic	2.020628357393833
information already	2.5327107950191152
c x	2.1117654159915116
single training	2.189546252614866
within documents	2.032982255991816
default reasoning	2.1865938445396056
application domain	3.1176830374458016
algorithm chooses	2.042325972844755
problem since	2.4972046802134193
features generated	2.097490679815661
probabilistic context	2.9272666175698765
extraction system	3.185864348432604
language modelling	2.6674122640106974
also results	2.200598690831077
dialog systems	2.572983560662866
yield better	2.6989048951226087
le plus	2.151475859170876
corresponding words	2.574343535202831
tutoring system	2.547473913151172
r e	2.246138989419734
entire grammar	2.0919612108326557
princeton university	2.1543888787937475
tokens per	2.0632925122102863
generalized markup	2.061212299166531
large size	2.609045923772392
sentences produced	2.2975747468827756
general problem	3.03693588239135
capture semantic	2.1774937848124125
correct output	2.335249710733848
better capture	2.1023887620661283
hundred sentences	2.0490697481769016
important factor	2.6490482221426985
linear order	2.9736126513651686
three questions	2.335895980419083
main part	2.542572978529364
like john	2.3129286601061936
central question	2.1023887620661283
structures like	2.643532998388592
documents selected	2.0073178378257417
computer programs	2.5516299230211485
accurate enough	2.3605881269336804
knowledge stored	2.045521226679158
appropriate words	2.238945374754718
appropriate places	2.0376102740271858
hotel reservation	2.2357673240413214
local dependencies	2.3805824362592096
constituent structure	3.155828924767164
reader must	2.146224553829869
level structures	2.37518838599954
next rule	2.3785805965501896
bayes rule	2.341549856487929
types occurring	2.026260250766906
also accounts	2.0817120252117016
open questions	2.45829838090292
speakers tend	2.0601457826506424
different relations	2.7363824778963153
properties associated	2.2485681115889076
identification number	2.1850304950140793
rare words	2.6626485938603315
correct parse	3.039238054051167
pronoun would	2.0355401813680145
particular instances	2.0965741974051317
union operation	2.028939001938722
important class	2.1543888787937475
syntactic unit	2.3589708974356802
count noun	2.026492103796458
social sciences	2.189546252614866
n terms	2.0817120252117016
another word	3.016785193332816
categories like	2.4978582270037895
alignment techniques	2.312149585164438
scientific papers	2.311280599537815
unsupervised approach	2.466285024494802
fold crossvalidation	2.135844661161516
source information	2.0593134444963326
using frequency	2.2112071020073643
different class	2.403472566572053
single root	2.1853232171907857
development sets	2.245558162117437
model 4	2.5724681832771408
finer grained	2.285913086717917
known techniques	2.0730662534060533
documents according	2.357121432847829
collocation information	2.1275552563638707
high scores	2.541926234035544
brief description	3.1043040049531245
label indicating	2.02585040092793
end state	2.148515580743208
context size	2.148459474082921
possible candidates	2.534358301488204
graph theory	2.3109116208105
automatic assignment	2.086551350552231
japanese thesaurus	2.034188985362456
performs best	2.6500927132708547
particular level	2.1946340157728974
intended interpretation	2.552479956477353
example 9	2.1982506992808917
approaches differ	2.0656667908183506
bayesian network	2.1539438492157394
every class	2.0743965972347684
information beyond	2.357947172648072
modal operators	2.213546394196763
sufficient conditions	2.3886619127043263
almost half	2.174158325108224
english speaker	2.1748379727149225
multiple parses	2.400923604221667
questions remain	2.24825378073324
many types	2.927144384741338
scientific research	2.818733492870756
greek letters	2.2011501968875664
two programs	2.070902079606615
every entry	2.264070934029513
input feature	2.2649705162624336
remaining word	2.0376102740271858
constraint requires	2.0376102740271858
viable alternative	2.41853693394125
simple representation	2.075729255121515
names may	2.194634015772898
higher number	2.4684806500055676
certain relations	2.21826141526814
two versions	2.969654508439556
entire parse	2.314007818435079
method needs	2.233997204181393
ascertain whether	2.109437912434099
last stage	2.4659566781235185
french words	2.5456593538741705
higher correlation	2.1588487997915733
provide adequate	2.079019887609717
subsumption relation	2.4231314611648838
review related	2.1774937848124125
conditional probability	3.484018745372364
tasks using	2.2717534996401523
training stage	2.150839857097719
specific category	2.077438554079712
paper would	2.1023887620661283
language comprehension	2.5856148948038182
current discourse	2.695915079164941
two hypotheses	2.4506455573323587
terminological knowledge	2.1740139076072804
alternative parse	2.0312646501273113
provide feedback	2.4113744956123186
network formalism	2.1295010830873577
major role	2.314592857858957
language semantics	2.375983397034176
many mapping	2.4059342579310696
resulting transducer	2.052121739635591
biomedical literature	2.340295440950751
somewhat simpler	2.022261218861711
bounded number	2.112778664196261
task would	2.4851459567760594
node v	2.114089549477496
inheritance mechanisms	2.1380488015835484
recognition system	3.4396789980238274
new objects	2.3507667525833957
hpsg grammar	2.453755157360731
selected verbs	2.007038640285683
domain ontologies	2.0448617371174302
longest matching	2.182991047900984
retrieval method	2.0902383172714174
darpa grant	2.611814665828829
problems remain	2.0362752451396027
different things	2.623495438528372
active role	2.1813446704867467
deep cases	2.185775841892095
human language	3.308558167853146
method outperforms	2.5345530535321426
word pairs	3.2114304077983835
special case	3.5704564930135367
exactly matches	2.061212299166531
everything else	2.230367634659114
relations holding	2.5300786089506877
note also	3.2238518111066643
specific processing	2.1023887620661283
dans un	2.2780761875113322
tree generated	2.356441653849472
semantic units	2.6384793976785486
way without	2.022261218861711
method combines	2.061212299166531
common noun	3.0692703204675045
knowledge structures	2.61230724948925
first round	2.4305753837911164
words could	2.658473813394275
candidates whose	2.1563079384871173
entity tagger	2.3980843659132924
tree associated	2.12319209177683
bob moore	2.045521226679158
context used	2.1127786641962607
basic ones	2.0157952878916157
methods proposed	2.4697040910441137
phonotactic constraints	2.1398544480981716
lisp program	2.1584071623262564
following lines	2.159746288077376
set 2	2.1952142668975227
small experiment	2.172274429303222
essential feature	2.145668076397672
sighan workshop	2.313328091895502
usage patterns	2.026896614175345
paper concludes	2.3271110475429713
different tests	2.0262602507669065
user enters	2.288085882671463
syntactic types	2.2475258420906217
becomes even	2.3567860333521553
little difficulty	2.089026915173973
senses per	2.3459495556057903
parsing system	3.0253628622907525
better results	3.4208108064952585
size increases	2.5496429368413596
performance differences	2.319063195109515
causal relationships	2.061969254719088
additional condition	2.053986137989304
data used	3.398156341386326
hmm based	2.1017707458424697
word combination	2.2024639441989313
wordnet synsets	2.695461510482095
english lexical	2.7534680569257115
larger vocabulary	2.2100577668648653
example 8	2.2933803017825145
several tasks	2.2965916473768724
following principle	2.0074734400897967
lexicon derived	2.086884416965048
many categories	2.0824927688889554
two applications	2.4741288466438416
final selection	2.075729255121515
whole tree	2.313258134832421
select information	2.109437912434099
main concepts	2.1290482690107417
verbal nouns	2.1845477060545786
chinese character	2.8433222844652892
whole relation	2.096954748857603
specific type	2.829131803778947
one target	2.6054615307890945
lexical rule	2.2086680420984255
different systems	3.0748101776001198
level 0	2.150854232608996
different similarity	2.433034406274223
phonetic similarity	2.063857309522051
wordnet sense	2.5049763240781155
manual analysis	2.4378112906511173
two syllables	2.1775082148381277
inference techniques	2.2243484856120617
sort hierarchy	2.088684141276191
readable version	2.313900874640595
corpora available	2.4250738008550288
phrase without	2.0880414073297957
restrictive relative	2.323113441955237
semantic feature	2.785670493043642
initial state	3.155694323512874
discourse theories	2.00238351760398
interesting differences	2.075729255121515
time interval	2.5753123699314213
grammar learning	2.22869336456261
ie tasks	2.2629472033502003
oov rate	2.2333994593951907
input layer	2.203479970045749
verb frame	2.1284698148385544
screen shot	2.031264650127311
deeper understanding	2.3692777127487306
phrase patterns	2.200661481121184
type checking	2.1089536884889815
another grammar	2.0312646501273113
prepositional object	2.3686809080355857
common evaluation	2.2781657771744412
current set	2.6831209090625503
subsequent words	2.099458264360342
using class	2.098317269161578
desired information	2.5722552722251932
significant increase	2.197728476343991
stochastic part	2.0214227439082717
spatial prepositions	2.0169497368248344
input document	2.1791019371996696
extracting relevant	2.0817120252117016
word matches	2.177114122531358
condition 2	2.0866142952191007
parsed text	2.438189314135232
whose parameters	2.285744341339744
causal relation	2.1072062894064474
common approach	2.7029940953165204
disambiguation algorithm	2.5322189613260067
different sets	3.220649014439775
possible analyses	2.862405140823326
difference lies	2.403472566572053
real system	2.3922788066689504
agreement rates	2.0799644973902938
explicit knowledge	2.21826141526814
document text	2.233065226075771
columns represent	2.3231354352745397
nearest neighbour	2.3078030607686673
news domain	2.348264336935353
analysis procedures	2.0539861379893036
figure 17	2.226384358522402
representation formalisms	2.11911577017412
different analysis	2.060145782650642
linguistic modules	2.1195868662417405
using statistics	2.2020202169645566
dependency parse	2.2889693316544975
feature information	2.524338387222941
determine appropriate	2.089026915173973
important questions	2.189546252614866
many tokens	2.014014536653706
third part	2.316121250352834
single finite	2.0577375124403856
mean average	2.1717648262598126
less sparse	2.0074734400897967
trivial problem	2.2776740307447065
2000 sentences	2.1816335351833818
several languages	2.9623810482026536
200 documents	2.0560728800261816
intended sense	2.1114916656312035
translation lexicons	2.1692556179750286
different structures	2.556634853199504
several iterations	2.2443687102109093
utterance u	2.488837743978718
syntactic categories	3.466328082104301
second advantage	2.067747107964575
simple list	2.3271110475429713
c language	2.245558162117437
sentence hypotheses	2.2556032421326218
particular properties	2.1946340157728974
data including	2.022261218861711
architecture described	2.255349860587285
helpful feedback	2.022261218861711
approach applies	2.089026915173973
human translator	2.5691898039364958
interesting way	2.242793580526907
recursive call	2.307091302036315
basic english	2.020363647132216
sample dialogue	2.2570842246566976
high success	2.0965741974051317
automatic syntactic	2.2073697906566263
long vowels	2.18723333687008
machine learner	2.090521538038587
highest priority	2.041867094220748
preceding paragraph	2.0601457826506424
one assumes	2.4711372947173462
exhaustive list	2.2532512925302806
general information	2.5143170265098114
document vector	2.097643329300209
main conclusions	2.135844661161516
many people	2.656700406388052
retrieval engine	2.5373429600176416
falls short	2.34443972705697
predicate arguments	2.0612190479046277
1 b	2.391803552088073
significant gain	2.1119430342456558
structural ambiguity	2.8616125797689764
new training	2.622802458720332
discriminative power	2.1534171575007686
american news	2.1117654159915116
sentences would	2.654634989696059
topic structure	2.106396951872571
low scores	2.329081436908228
network using	2.0376102740271858
always contain	2.045521226679158
possible connections	2.061969254719088
incremental learning	2.1135841251493312
internal representations	2.591398791091959
since wordnet	2.202020216964557
rare events	2.093894285709819
parsers use	2.022261218861711
exceptional cases	2.0403769008422494
every example	2.015759442681759
certain structures	2.075729255121515
observed word	2.208775653151079
previous literature	2.109437912434099
system allows	2.920927478068522
notational variant	2.022261218861711
two variants	2.5073017100902475
first select	2.045521226679158
sentence elements	2.1414474539371366
language like	2.8429636167702204
one noun	2.4728837265095613
trees t1	2.0676325559456012
annotators agreed	2.2834374391972916
language parsing	2.8906126228438227
larger discourse	2.0669832844535927
details see	2.6127947423729774
traditional methods	2.3955781761503503
linguistic string	2.4580151518116007
rule whose	2.4674059732124207
knowledge used	2.663924314891822
inner product	2.4735812480763943
related terms	2.5910619782611253
new approaches	2.357121432847829
current algorithm	2.3728830312074773
recommendations expressed	2.6653666701431624
original motivation	2.1413469986390323
phase ii	2.2384381685594192
previous analysis	2.2532512925302806
ner systems	2.054938735466706
learning program	2.404413153490621
use examples	2.1072480253375243
nous avons	2.2265718451267196
main characteristics	2.4917756927623538
constituent parts	2.3673672188851462
web data	2.1683165967514677
feature values	3.3047608741717185
two step	2.2742912111882294
alphabetical order	2.497757810643672
entire dictionary	2.00238351760398
basic strategy	2.3322409522263214
chinese characters	3.03030428935371
anything else	2.3510528260755685
following considerations	2.014014536653706
scores obtained	2.577151220460659
basic component	2.079019887609717
single node	2.8162445685193895
clustering method	2.6798472304104237
new root	2.079631979156151
input sentences	3.2116083639123936
important task	2.6776831829771157
independent method	2.022261218861711
developed techniques	2.0880414073297953
different size	2.227249320716272
best guess	2.307091302036315
additional feature	2.8077279216354722
representation system	2.6868718125300806
among researchers	2.2328679513998626
existing resources	2.6411747234957947
many phrases	2.049069748176902
best sentence	2.4275172295128393
tagging scheme	2.212265093892994
different task	2.4658585645856936
actual system	2.313328091895502
column shows	2.9468870181213784
experimental systems	2.0601457826506424
et al	4.683527872835454
particular event	2.3955781761503503
current parser	2.012983746580426
likely word	2.486460305236611
unlimited number	2.25398049501041
modular design	2.2487571437358924
generation process	3.327553972524688
syntactic clues	2.21643725514963
two error	2.135844661161516
specific aspects	2.512514922083054
word strings	2.731218220032675
specific term	2.4304431782151887
using latent	2.067081229293324
appropriate information	2.4602434646053126
existing dictionaries	2.3962349727697023
many characters	2.053986137989304
one alignment	2.242793580526907
many instances	2.8773983329132404
logical formulas	2.3106049668344175
first component	2.7126188945552463
discourse phenomena	2.6316868032072116
dictionary look	2.6216420354458805
specific subject	2.055118260693857
best performance	3.425053366593514
higher accuracy	3.021427401031784
compound noun	2.531489110464491
single frame	2.0763533583416773
2 presents	3.2214873421312924
constituent ordering	2.11638795219105
design decisions	2.6655056970721485
underlying knowledge	2.490586939987368
one approach	3.2016838890911576
1 l	2.579841029394192
external argument	2.1293804227047146
information units	2.122789967120956
cannot capture	2.5055533459995827
unseen word	2.2008910332408402
specific examples	2.5213477264853457
probability using	2.185240400682336
multiple representations	2.11911577017412
standard prolog	2.172274429303222
words based	2.9114969041833523
acres de	2.9811501366305864
mor e	2.1828884764914416
like structure	2.658319828001733
document classification	2.532652307305912
hidden variables	2.2371909552301057
electronic form	2.514624768642089
practical system	2.5886045470541506
two roles	2.446565811067697
information measure	2.25398049501041
single relation	2.2061294109490195
scale corpora	2.092320451219691
training samples	2.4808961032074057
underlying system	2.1362645948193832
significant degree	2.022261218861711
restricted class	2.0709020796066153
partial word	2.135844661161516
test documents	2.5621437038107118
simple ones	2.052037010765093
several problems	2.8633949609508376
english words	3.4435692064107046
pilot experiment	2.19146098952863
syntactic phrases	2.443264771919205
text file	2.5366368580942424
ambiguous word	2.8426619971164055
errors involving	2.1462245538298697
view point	2.2492578335740094
syntactic function	2.853697554317829
formal terms	2.2328679513998626
following decision	2.0236385391388776
function h	2.3023924577133164
things like	2.2632785081204534
related text	2.079019887609717
r p	2.1557946417682414
distinct senses	2.2917775396136193
language retrieval	2.0695878023520193
module receives	2.0074734400897967
penn wall	2.171178031468078
source string	2.3031059244887055
particular point	2.514458993502342
framework used	2.122246315203648
binary relations	2.848170626570995
particular argument	2.267991541156711
three fields	2.0490697481769016
sophisticated methods	2.3752638941834947
defense advanced	3.1508624044244398
im walde	2.1829249682847367
differs significantly	2.314007818435079
programming system	2.2070563180044633
logical structure	2.6865149958667476
general discourse	2.089026915173973
variable z	2.143898852826872
use another	2.414320698244546
directly correspond	2.067747107964575
consistent improvements	2.022261218861711
latter case	3.361732001234278
loss function	2.2390484623976015
system implements	2.1413469986390323
content determination	2.3618470148086335
sentence must	2.6259085953970995
background noise	2.418253921396954
lexical selection	2.458118278746291
probability space	2.0520370107650927
data increases	2.345768269478816
full context	2.1646123301199256
line 5	2.0329822559918163
broadcast news	2.8090096465049035
scale evaluation	2.395330969400053
language question	2.5322942443939533
state language	2.0291822979260266
control flow	2.2722558893814297
multiple classes	2.181633535183382
automatic parsing	2.2078449813130154
large training	2.983435829074412
two random	2.1569808507858435
commercial products	2.361034984088791
role assignments	2.2833676641866325
tile structure	2.067081229293324
always sufficient	2.014014536653706
experiments suggest	2.5361198428948413
level semantic	2.6517555611526777
another kind	2.684001745599942
empty word	2.167789635478137
structure described	2.267991541156711
john ate	2.2861364561587374
several improvements	2.0074734400897967
dictionary based	2.1377733122219267
relative clause	3.372354362710875
users need	2.233997204181393
additional layer	2.122246315203648
correct classification	2.412719788832708
language acquisition	2.90421754014618
full description	2.60254866758455
appropriate values	2.321839114388654
look back	2.0865513505522313
using giza	2.327403995660175
syntactic level	3.0006714493943174
complex information	2.3293373381455873
uses semantic	2.406719372930616
computational reasons	2.1414753566437956
possible translations	2.8833237337170985
evaluation tasks	2.151676491844686
acquired rules	2.030756751210168
discourse referents	2.5735660200034007
good performance	3.0401723657629147
rules discussed	2.108179066756418
original feature	2.081936758491265
different derivations	2.304008524296741
complete theory	2.061212299166531
dictionaries contain	2.1984572101092845
later section	2.5034484832534796
cannot recognize	2.089026915173973
also causes	2.075729255121515
positive polarity	2.021038862321923
form x	2.5631378708143018
pairs may	2.2328679513998626
output tree	2.0522602329453306
agent may	2.2541748566198656
one corpus	2.3759476483867115
np np	2.0667307264095895
entire collection	2.242793580526907
p e	2.3588463137300035
object types	2.4410187416006917
similar manner	2.5921348215522113
known words	2.7933529442127703
main objective	2.087771235464716
informational content	2.324871720725265
feature set	3.424419590064039
set p	2.4500118650579084
flexible approach	2.0074734400897967
wordnet synset	2.4559295809593715
karlsson et	2.3815493147331397
compact representation	2.6940521328414917
approaches may	2.159746288077376
writer may	2.014014536653706
sentence error	2.3998267181234105
first determine	2.0944332572144115
knowledge plays	2.0656667908183506
linguistic facts	2.448499087268562
edr corpus	2.2036223534436252
architecture allows	2.282780843251617
discuss later	2.1023887620661283
semantic category	2.9324463309221867
highest average	2.079019887609717
final system	2.607577074093963
system relies	2.580570794461365
performing system	2.3823009839180083
different application	2.5059141499101956
right way	2.2112071020073643
japanese words	2.639327472264943
many speech	2.0376102740271858
computer interaction	2.713702306106219
different position	2.1222463152036486
many years	2.4653372129168836
similar method	2.452321695174711
learning environment	2.2657830205233993
text annotated	2.135844661161516
semantic similarities	2.17375937451299
graph nodes	2.017570716396185
present approach	2.5575854359548984
incomplete knowledge	2.1569808507858435
better choice	2.1479184330021646
hierarchical relationships	2.0850741607377534
units called	2.2532512925302806
important area	2.278461809175595
commercial applications	2.1946340157728974
word candidate	2.0595415358998705
15 times	2.019557844009582
impose constraints	2.314007818435079
communicative acts	2.363325333148595
complex phenomena	2.175400407836868
requires knowledge	2.3130951516759186
appropriate discourse	2.2294384602439496
word 1	2.1718237813971424
high level	3.0595666929310035
frequency distributions	2.3814334911263666
general set	2.079019887609717
interesting questions	2.352331304842637
learning method	3.169543082074915
inference mechanisms	2.2889459954794247
full system	2.3513777083226852
semantic processing	3.121630598873538
representation provides	2.166102255087602
adjunct distinction	2.045422903193595
particular classes	2.1165917421904625
distinctive features	2.329820719430746
unique identifier	2.695164427125169
lack thereof	2.053986137989304
first definition	2.172274429303222
paper takes	2.194634015772898
crafted rules	2.5184782702533712
relations found	2.271753499640152
way classification	2.2743376934365527
basic language	2.1513738263092224
l j	2.2658748144574723
processing components	2.664911980816145
function defined	2.384227704698257
complex case	2.24825378073324
selecting appropriate	2.3435066063864065
situations described	2.1339262987939316
terminal nodes	3.1130284924143288
interaction among	2.5014005829538326
several utterances	2.0074734400897967
phonetic transcription	2.5034547067158326
unseen data	2.8775881752896377
error detection	2.495276054407819
et l	2.394067673183756
document would	2.0965741974051317
grammar size	2.370365867089665
related applications	2.1023887620661283
two part	2.0312646501273113
leaf nodes	3.0730645818194153
corresponding features	2.1543888787937475
many properties	2.022261218861711
system evaluation	2.741772790538177
daelemans et	2.8463747410196576
many helpful	2.6287060153335826
relevant properties	2.22672986885133
linear combination	3.1034578687492425
chapter 4	2.253310279048359
hierarchical tree	2.221537802998995
input length	2.0501330295709366
different styles	2.2606438459244673
section provides	2.57917382991629
interesting example	2.2917594692280554
turn may	2.296234813805337
magnitude faster	2.2635846387787795
structure could	2.4459112107066288
content word	3.0189644017778807
main problem	3.0250417725250593
another feature	2.8345050377367764
us government	2.233997204181393
sur l	2.2250559641441665
operational definition	2.0355401813680145
protein interactions	2.0309968124474223
complete discussion	2.086884416965048
still room	2.0817120252117016
likely parse	2.3429208778801653
possible ones	2.189546252614866
john went	2.186063228270615
one phrase	2.611095811873278
x occurs	2.2151089798860455
gender agreement	2.264546408884895
stress pattern	2.121787743320869
using brill	2.039007283873737
queries used	2.045229961808099
one cannot	3.124210517493357
processing rules	2.0057196349988926
noun pair	2.1784560929037036
section illustrates	2.1413469986390323
original document	2.7453423907363104
additional work	2.5361198428948417
different views	2.5864645700626294
vocabulary v	2.2443687102109093
target corpus	2.291698495929917
argument type	2.367613571731908
connectionist models	2.112222873986634
acquired knowledge	2.227256700460021
simple strategy	2.430601366722213
hoc manner	2.014014536653706
connectionist model	2.0491836984819196
first extract	2.159746288077376
ranked lists	2.224739992481587
translated sentences	2.351151121060626
different answers	2.075729255121515
inflected verb	2.0148243230043406
world text	2.075729255121515
entity names	2.113355460944316
basic notion	2.0965741974051317
object np	2.764635929670333
similar verbs	2.0362289675655436
corresponding set	2.4611017632787435
root node	3.442697785871522
best combination	2.656603676114912
readable dictionary	2.8092329490288948
following problems	2.5387687219528585
speech using	2.34443972705697
also test	2.159746288077376
different objects	2.5036285688060933
standard definition	2.152138618710441
integrate different	2.135844661161516
new node	2.8239953220179244
interpretation may	2.3097823680289724
readable text	2.012347776929124
time increases	2.0539861379893036
lexical rules	2.634610842090168
case complexity	2.43064228935014
two uses	2.1427556701350663
syntax rule	2.0023051547392217
single input	2.307382090261814
computational techniques	2.0726832585413097
number 3	2.766674944905212
grant number	2.747886493589054
cambridge university	2.2742878796306982
segmentation systems	2.126026753337463
word penalty	2.193957026760592
processing tools	2.5155134062059297
small values	2.293917267737899
input data	3.1850678183297325
appropriate set	2.6320798365707314
examples presented	2.4103259885026898
among elements	2.1023887620661283
one classifier	2.4738458550833906
syntactic properties	3.090151189280198
two matrices	2.0116819766087874
base systems	2.215936754328058
interpretation requires	2.159746288077376
constraint would	2.089026915173973
modus ponens	2.199650664335948
term weighting	2.6779251488100067
partial parse	2.4767785167460716
variables used	2.1312839727750825
tagging results	2.404513573759893
unification process	2.250108402045351
different factors	2.115481067246263
6 shows	3.546661068970332
general algorithm	2.5219739981168368
four systems	2.321839114388654
systems achieved	2.022261218861711
output file	2.073827057886965
action verbs	2.1638819787693686
production rule	2.580331540423018
similar approach	3.065550316253162
formedness conditions	2.4851950310062754
polysemous nouns	2.035711781460289
correct category	2.1127192053446717
document collections	2.697743398248267
associated features	2.1946340157728974
pour l	2.2068342474563343
best matches	2.3548005898232187
end positions	2.202640507036321
infinitive verb	2.016336526582215
valuable comments	3.0457067067135655
situational context	2.133165345977436
high reliability	2.291328123830457
used e	2.1290482690107417
tile original	2.067747107964575
new software	2.072290098419245
limited domain	2.7920313095776046
item may	2.3789674931888145
contexts may	2.135844661161516
document type	2.382987565701046
template contains	2.014014536653706
describe experiments	2.369115447338463
first compute	2.21826141526814
two nodes	3.2012592291696995
1 corresponds	2.164235940665368
morpheme boundaries	2.428487217690196
one consequence	2.15088526499282
logic grammar	2.1145684224360575
one concept	2.606695645503608
new tree	2.5017492401362924
current efforts	2.2384227085772217
minimum distance	2.2261270822018155
preceding verb	2.067081229293324
sentence consists	2.4174756698865894
use contextual	2.0539861379893036
research using	2.1023887620661283
new relation	2.2271997797956677
several word	2.4282856154916748
semantic specification	2.1139545725559947
related senses	2.4130192572336435
corpus must	2.278461809175595
adjective phrases	2.2195762285281164
rules correspond	2.0539861379893036
highest degree	2.082847793663254
bayesian networks	2.12974658166465
schabes et	2.256863208816413
participating sites	2.185216671905355
new speakers	2.032982255991816
additional processing	2.3107061751596207
appendix c	2.1889605088507667
probability models	2.5674621401263353
alternative would	2.1192140615400183
capture linguistic	2.119240389473862
word formation	2.671960988454735
great variety	2.4017401613941365
subject areas	2.339577733817647
scoring scheme	2.2484119335867057
new items	2.481752609007825
precise formulation	2.1462245538298697
independence assumptions	2.8506831040082687
target phrase	2.4437550049826813
supporting information	2.067747107964575
four questions	2.0390072838737376
input may	2.286317287646451
clearly show	2.259685434348394
also differs	2.3841838643071576
machine translations	2.184176654252197
subsequent discourse	2.2181630663035277
j n	2.070902079606615
indirect way	2.1563079384871173
latest version	2.3817590733968528
semantic network	3.1493294779922594
structure allows	2.257869825585851
two names	2.179209876010538
standard way	2.7786364936627352
way would	2.348409524038697
analysis gives	2.060145782650642
clear definition	2.067081229293324
based mechanism	2.0376102740271858
null since	2.5871936349478175
one event	2.499796377644391
first factor	2.1516988527908136
model relies	2.1023887620661283
alternative strategies	2.154388878793747
processing tasks	3.21559082546182
two concepts	2.8598257981133357
semantic web	2.4079924061443707
litman et	2.140614079500608
speaker could	2.012347776929124
slot filling	2.1581607745358693
original sentences	2.3841802834313377
contextual feature	2.089896763732044
use less	2.014014536653706
complex set	2.2776740307447065
human input	2.0312319370715612
three ways	3.0245741707208835
sentences extracted	2.628218162087504
different nature	2.4439311201945193
explicitly encode	2.0817120252117016
also compare	2.67239759197034
complement structures	2.0918364857418936
three pairs	2.2112071020073643
mixture model	2.3307436469594833
linear precedence	2.9052263455264447
process allows	2.089026915173973
first section	2.1419408209779665
first class	2.747031808101423
verbs tend	2.0074734400897967
uses wordnet	2.022261218861711
separate sets	2.067747107964575
following reason	2.060145782650642
world applications	2.737703654605833
parser performance	2.375421242208037
following equation	2.8159114694270118
preliminary study	2.501684903330372
low level	2.562620901649556
words obtained	2.0390072838737376
following utterances	2.1166196741856655
sentential complement	2.4482431373978777
particular context	2.948750995007736
6 senses	2.0817120252117016
understanding component	2.5124944362749546
contains 2	2.0312646501273113
avoid making	2.014014536653706
sentence interpretation	2.2517149257502638
level constituents	2.1992696891479233
two synsets	2.164235940665368
functions f	2.3633976741456193
perform semantic	2.1479184330021646
order may	2.109437912434099
rules whose	2.709943018873549
system failed	2.3814643032204326
text corpora	3.2596626794152237
lexical functions	2.2370735884512447
words cannot	2.512514922083054
becomes less	2.5126758453675735
regular grammar	2.305902690465104
extract words	2.102921514525577
consistent results	2.0965741974051317
semantic aspects	2.5424782356057807
example rules	2.0123477769291243
specific applications	2.5967692970440184
phrase would	2.3229739757638654
random selection	2.6834907635685155
transitive closure	2.9741394004463655
tables 2	2.9519100305873542
combine different	2.357947172648072
one daughter	2.208091008533869
methods would	2.2169936022425736
external knowledge	2.4909152358501094
english vocabulary	2.4118625260628157
third problem	2.352331304842637
word according	2.360588126933681
3 task	2.020515556632957
analogous way	2.166102255087602
actual texts	2.165115471861699
japanese machine	2.075816946920645
application systems	2.365567038782495
two notions	2.3605152005252172
rank order	2.2179188892448387
research issue	2.314007818435079
relative error	2.1797707896278893
information processing	2.8769769083303594
pos tagged	2.633828135737718
target concept	2.2474132845230286
question q	2.218798673569961
small percentage	2.1864631747925722
short term	2.202231567985755
europarl corpus	2.240492985381084
segmentation task	2.3517277131322563
every training	2.075729255121515
pattern used	2.1023887620661283
copyright notice	2.854765100656169
small set	3.653848241002352
applied linguistics	2.194634015772898
local feature	2.0620999672143876
punctuation mark	2.48570454019746
evidence suggests	2.0073941126162715
character n	2.1936057357775143
nodes n	2.233997204181393
insufficient data	2.061212299166531
human evaluations	2.042725064445334
nodes represent	2.890312113157997
text alignment	2.1246035547470328
way similar	2.6380055972263228
must share	2.135844661161516
majority voting	2.1420641426729405
incident type	2.14203372965276
evaluation methodologies	2.3272306281129014
c 1	2.515250309569853
whenever one	2.159746288077376
different approaches	3.3632222487747243
word clustering	2.436109120683434
person may	2.059213640580528
alignment information	2.1640141455486264
synonymous words	2.2454289611614024
basic architecture	2.1943253913209606
size 1	2.017570716396185
given structure	2.2062448606993037
template element	2.229647825003502
component also	2.1290482690107417
simple solution	2.380830363798011
large amount	3.3852540459121996
previous sentence	3.022154660949235
single query	2.238945374754718
marcus 1980	2.075729255121515
current focus	2.5835911216255156
simple language	2.4103259885026898
one goal	2.560964388468025
4 introduces	2.2328679513998626
inherent difficulty	2.00238351760398
based alignment	2.314254597654921
information extraction	3.771751082986308
est un	2.1825764762315654
based evaluation	2.837978939130569
speech tagged	2.6802979566254663
kernel methods	2.2790644721001394
experiments described	3.1287632420614897
elle est	2.0316144144734802
direct comparisons	2.282388132164508
segmentation errors	2.3916185746144976
frequent terms	2.086835260083649
robust statistical	2.160181161519838
daughter node	2.278942846375257
information even	2.031264650127311
component may	2.2224068987457324
alternative methods	2.296190680369272
search results	2.5106958049594894
based approach	3.7380660258512375
individual elements	2.1290482690107417
computational linguists	3.1521782037946044
input japanese	2.009868063569498
next iteration	2.6060400925819858
julia hirschberg	2.1018382138609883
accuracy increases	2.3992451970276365
noun must	2.2169936022425736
pruning techniques	2.0565667685928792
e used	2.536119842894841
domain corpora	2.1927290873508665
sense knowledge	2.5867922166695907
particular system	2.471137294717346
among others	3.408272282550797
function composition	2.047426859831795
current domain	2.321839114388654
sentences using	3.0810317431789986
new candidate	2.2421869127123264
event structure	2.5146257343936353
like machine	2.022261218861711
possible extensions	2.4284483103381946
technique described	2.7617453655389417
contains word	2.060145782650642
two characteristics	2.230524125991308
4 reports	2.368834809141683
conservative approach	2.1413469986390323
semantic ambiguities	2.554828548257736
algorithm generates	2.3566317351508
base form	2.945996249629758
complete interpretation	2.174323003243204
problem without	2.227249320716272
operate within	2.223453205129625
preference semantics	2.1762965319631853
word token	2.5851590702092144
constraints among	2.393832620296625
domain specific	3.1815142977637985
model pr	2.138071104528787
order terms	2.2411459399188014
le module	2.0276832217780947
phrase type	2.766138936817673
two templates	2.007321589507728
simple way	3.0724380333021433
following word	2.839860127352029
first briefly	2.368834809141683
summarization research	2.166102255087602
training instances	2.9724865303651002
parsing speed	2.38113960926794
model estimation	2.2458993968868173
optimal results	2.227249320716272
inference processes	2.0764551704567427
6 concludes	2.617053252298629
case studies	2.227386446378864
accurate description	2.089026915173973
another category	2.296234813805337
analysis reveals	2.3523313048426364
different weight	2.153552027758611
see figures	2.355695717315683
two edges	2.372069358669325
inflectional morphology	2.763866605921644
stochastic models	2.51747292560294
multiple possible	2.55239596401209
combinatorial explosion	2.931651005429022
feature representation	2.5427345120381877
stochastic model	2.4715644330440787
models make	2.172274429303222
steps towards	2.2992838842823984
previous approaches	3.297863097120019
approach first	2.0817120252117016
continuous density	2.0330676613993424
11 shows	2.440974907463567
feature extractor	2.002806506217434
first analysis	2.209192498501841
morphological tags	2.209648974005781
step procedure	2.264070934029513
methods use	2.544073863552885
semantic parsers	2.0832082124114963
cell contains	2.1543888787937475
building large	2.2492578335740094
baseline results	2.436369723844453
time spoken	2.009868063569498
single token	2.7194838716584
speech models	2.0074734400897967
every time	2.9369177643323097
franz lisp	2.0074734400897967
certain extent	2.660290650199456
using training	2.3546421252999927
ordering relation	2.368142392654711
first thing	2.534113160865616
abstract concept	2.158407162326257
lexical sample	2.5539678384590556
optimal threshold	2.0306796584289044
diverse sources	2.075729255121515
property p	2.261674856234268
semantic head	2.313931301625011
clark et	2.0234705961913417
unknown words	3.347304779283935
strings like	2.216914018697418
semantic terms	2.1508113980595063
algorithm iterates	2.067747107964575
corpus consists	3.2915107867706035
model defined	2.2751250090009747
ranked based	2.161246853402207
initial experiment	2.233997204181393
various models	2.4607232496576326
correct tags	2.238195109838169
input text	3.4820571620150225
several patterns	2.075729255121515
operation called	2.2512557276899194
preceding sentences	2.33790118533611
earlier results	2.1222463152036486
interpretation process	2.7994102368080553
important ways	2.462292105818408
art methods	2.154388878793747
data items	2.3242004114207075
identity relation	2.3151287238085265
em algorithm	2.9312123003033475
earlier studies	2.0824927688889554
word tagging	2.0345219001984995
similar concept	2.0376102740271858
general representation	2.0539861379893036
seconds per	2.5881313750468116
state 0	2.0268093295691045
additional examples	2.0806182014260814
elapsed time	2.1963874985108163
frame elements	2.2167186285899025
becomes necessary	2.15088526499282
second paragraph	2.051285638402277
application task	2.064139780413935
document content	2.4617051168326753
human sentence	2.2932023084855766
performance increases	2.2955083501313895
first example	3.1261470677025427
hmm parameters	2.0450576213811003
types used	2.3229739757638654
null ments	2.24825378073324
anaphoric np	2.105083838552665
meaning representations	2.840980159864995
current dialogue	2.438868260370345
ambiguity resolution	2.8411344166343167
formalism described	2.189546252614866
translation systems	3.4192502082821585
given predicate	2.4640832187374757
two respects	2.6239319852280314
method performs	2.746072680079757
syntactical analysis	2.2709576744960334
full texts	2.17499507388204
poor results	2.216666602714568
certain attributes	2.11911577017412
term containing	2.0376102740271858
information source	2.450317074597381
database interface	2.247796036035031
best candidate	2.6313470040479747
computational mechanisms	2.0388643487004234
large numbers	3.1761574359441886
also lead	2.057959227210873
rules proposed	2.026260250766906
three items	2.045229961808099
feature fi	2.2028095163090176
technique could	2.045521226679158
paper argues	2.3310204571264768
words sharing	2.1240946476680165
grammar specifies	2.089026915173973
finite automata	2.363926690689876
standard analysis	2.053986137989304
input vector	2.301679646325512
search may	2.1290482690107417
kleene star	2.4122227552400366
automatic speech	3.1556566163841238
de son	2.037402853550243
technical term	2.146514055105463
different combination	2.2112071020073643
unannotated corpus	2.2046457300936666
verb phrase	3.4399649513634643
discourse referent	2.5427097843842934
major constituents	2.1859857899642225
expected value	2.6214360435481785
corresponds directly	2.022261218861711
similar syntactic	2.4480432416516154
applications may	2.181370500368545
van deemter	2.416498536194898
incremental generation	2.01285466568744
grammar defines	2.161700268474605
extraction module	2.493816554533813
semantic tag	2.436355650593366
another use	2.108212199944729
retrieval based	2.1649596527500012
used hand	2.122563170379007
state changes	2.141506865293792
majority class	2.6020578648159924
system provided	2.075729255121515
2 defines	2.060145782650642
generates one	2.1597096509760467
analysis showed	2.3839874389711913
original source	2.355523553890346
efficient method	2.6167442200032394
state 1	2.045643082159504
constraint relaxation	2.0617768928801943
consistent improvement	2.060145782650642
results showed	2.731675559519745
test sample	2.372423246354356
definite np	2.389039217180314
close relation	2.11638795219105
resolving anaphoric	2.020363647132216
lexical dependencies	2.253597975517775
bilingual word	2.238015834121698
great advantage	2.3271110475429713
result also	2.296234813805337
many rules	2.55061890816116
relevant rules	2.060145782650642
original data	2.6770226565668427
lexical probabilities	2.207689304056973
target classes	2.1042904425091606
u r	2.1154129062412803
cosine similarity	2.83965398979955
second conjunct	2.291817380993432
rules also	2.3879279742060486
weak generative	2.3110339333836816
rules operate	2.307382090261814
per frame	2.044877827032594
paper examines	2.5577806922873396
also represent	2.2754268053105045
also improves	2.297574746882776
matching rules	2.4731369167273947
syntactic grounds	2.2891619181755756
rules without	2.4115483316123614
verbal complex	2.038024989271382
first task	2.8556432915856482
result may	2.505695738740391
appropriate representation	2.4250738008550288
cannot serve	2.1479184330021646
given sequence	2.4103082454717084
syntactic components	2.1557946417682414
algorithms developed	2.0709020796066153
rules may	2.979608760581791
main features	2.5546076730138516
limited scope	2.00238351760398
languages differ	2.2047194975469235
one arc	2.2658748144574723
include word	2.3483606540998085
approach include	2.014014536653706
based phrase	2.11911577017412
possible way	2.7605132463414224
language documents	2.2469344087913554
several arguments	2.00238351760398
following data	2.412081342249917
testing sets	2.433445006721607
4 times	2.2745920140449525
phrase headed	2.3938340305606505
one mapping	3.0015144438172956
one question	2.5395618126257675
general treatment	2.272505380879168
translation example	2.174208145881277
current english	2.286725248739412
larger domain	2.0817120252117016
partial trees	2.127336691309381
short text	2.314007818435079
information may	3.2041217238809843
prolog code	2.2514136313663715
based wsd	2.187328299544666
existing corpus	2.278461809175595
tile case	2.4459112107066288
tile left	2.1728673576400466
28 ao	2.8532920452020973
rule schema	2.173849256559028
system scores	2.2685844072039183
speech translation	2.8658644704881744
network structure	2.2739075676745735
space defined	2.2058102909449806
processing phases	2.0381169104851677
combines two	2.54695026759109
model given	2.116591742190463
special form	2.2165983686619217
natural human	2.260122055223635
answering questions	2.710229602010261
draw upon	2.2047194975469235
intransitive verbs	2.670794453107718
good candidate	2.649627866458325
relevant context	2.3332911335775464
coherence relations	2.1972685634682136
representative examples	2.21826141526814
overall precision	2.582857234767002
two key	2.3561901764841124
controlled language	2.187455400578445
system took	2.014014536653706
proper treatment	2.3639525993606796
additional requirement	2.067747107964575
medical texts	2.0591424124860733
additional benefit	2.1290482690107417
syntactic rules	3.183435439625052
semitic languages	2.300271047249476
randomly drawn	2.060145782650642
martin et	2.3167254621139604
per hour	2.3258587082214444
acceptable results	2.1222463152036486
sample size	2.7491125035437864
parser operates	2.3879279742060486
great value	2.272505380879168
larger grammars	2.08546976509845
hybrid model	2.1248251706002037
three sub	2.2207046594768247
research workshop	2.045521226679158
logical forms	3.0674718817653455
little information	2.6757724021355656
search terms	2.2903907963713634
based classifier	2.509192233892212
performs significantly	2.6248475564334957
extract information	3.021910204694232
word bigrams	2.3012052527274625
literal interpretation	2.150241035204294
theoretical reasons	2.067747107964575
new query	2.286552317547569
true positives	2.1770257914536884
words automatically	2.0023835176039797
next word	3.120488562045285
new method	2.930474043614457
action sequences	2.0554813829994836
system like	2.7347778602541566
node labeled	2.6540071031304056
n l	2.577363717821489
recognition module	2.5087785388539317
4 explains	2.200598690831077
5 x	2.1724093943740512
technique allows	2.045521226679158
4 evaluation	2.0619692547190884
collocational information	2.3341482104959947
positive class	2.0121142258873945
many situations	2.406719372930616
first checks	2.3806000578467827
level linguistic	2.3376993763582856
becomes part	2.177312398504085
various purposes	2.1222463152036486
syntactic markers	2.0381169104851677
third column	2.9884370135765295
following patterns	2.302820791945498
work described	3.2504439556617313
verb clusters	2.1339772389945644
tree learning	2.5031532157465586
100 texts	2.284966413176913
first order	3.063043575670819
passive sentence	2.270062617764849
clear example	2.0376102740271858
phrase chunking	2.4697561497410705
million word	3.099709347088765
partial match	2.346837723459159
possible relations	2.693229903583146
translation task	2.849417145178031
remains constant	2.3229739757638654
application area	2.500407695373407
le premier	2.0430031661685746
every node	2.9924533112844167
computational problem	2.082836083424519
based concept	2.0074734400897967
semantic contribution	2.4875533315884457
terms whose	2.269813783347405
summarization methods	2.0330639966084334
short overview	2.2169936022425736
uses linguistic	2.067747107964575
supervised word	2.060640253035934
many tasks	2.4331310213021236
ensemble des	2.091296296992124
analysis revealed	2.278461809175595
example illustrates	3.125630152287855
regular verb	2.014014536653706
crucial component	2.200598690831077
second row	2.768044844506168
correct tag	2.5480802397868456
technical terms	2.932793298220951
syntactic context	2.901695045168084
user satisfaction	2.33291449525709
many knowledge	2.135844661161516
uses four	2.067747107964575
relevant knowledge	2.3354892788655413
fourth column	2.5713098687748897
atn grammar	2.188356648285989
function based	2.233997204181393
preliminary experimental	2.2275895034737268
following theorem	2.3989263907949185
good basis	2.3841838643071576
parsing module	2.268054700012325
simple analysis	2.1290482690107417
using finite	2.5964571960559573
left daughter	2.0258906977733666
classification errors	2.2600957914555506
little bit	2.052450786945606
another term	2.0817120252117016
labeled data	2.7311658943384836
situation arises	2.135844661161516
possible class	2.0577375124403856
arg max	2.2367133824095684
hmm system	2.0894727222085043
statistical training	2.099458264360342
specific part	2.3139333113471174
brief summary	2.5270204646552323
paper consists	2.1946340157728974
information request	2.061969254719088
reasoning systems	2.161246853402207
entry contains	2.377639832514428
trees generated	2.525525252514147
one character	2.8169942831607027
certain contexts	2.674810042978926
possible referent	2.1358881772255405
briefly discussed	2.30606794696972
english parser	2.330318201869984
lexical co	2.360501651354225
multiple solutions	2.045521226679158
important consequences	2.3540470303697685
simple structures	2.0157952878916157
sentence initial	2.346889123261037
based classifiers	2.3020504751187163
standard form	2.24456493554562
task consists	2.5469502675910904
word w	3.46301920277909
context vectors	2.3247284957219216
syntax alone	2.0483937955874474
high dimensionality	2.1497135154557805
problem mentioned	2.200598690831077
translation errors	2.362503865303612
one difference	2.524067375092386
different relation	2.018214473388641
research tool	2.11911577017412
un ensemble	2.024812513410251
statistical language	3.1731596603045458
one needs	3.1576591619925223
second condition	2.498891506626836
also introduce	2.6695475660438333
generate one	2.115630796056495
sentences within	2.661955009718142
class items	2.0530862049338263
data annotated	2.0965741974051317
quantitative results	2.300319814079341
problems encountered	2.693118014467522
general domain	2.426160901687615
different speakers	2.6139035761592133
generation using	2.227386446378864
new corpora	2.0551182606938574
interpretation would	2.2959583628072817
overall result	2.3483606540998085
among concepts	2.311280599537815
discourse focus	2.284645403471984
overall error	2.566442259840419
challenging task	2.7144635782873006
new piece	2.172274429303222
different items	2.014014536653706
interactive system	2.6659471442804796
word occurrences	2.6581112366064112
separate semantic	2.1312839727750825
el al	2.8719050130727193
translated text	2.2382409020569716
word v	2.110333165700758
first application	2.497858227003789
every relation	2.1063710162749305
maximum distance	2.260688804545853
first position	2.7649339380546385
structure cannot	2.233997204181393
null c	2.2224068987457315
multiple documents	2.5121628242314236
strict sense	2.4520966145663854
lexicon using	2.285913086717917
c h	2.1765868883788446
various discourse	2.1290482690107417
con guration	2.0319227942036577
experiments based	2.1983168585210526
cannot resolve	2.014014536653706
branching trees	2.215684847877435
preference rules	2.146984805596909
future systems	2.105322206137931
frequent senses	2.2122922241265175
specific interpretation	2.026013456327906
items must	2.045521226679158
available machine	2.0539861379893036
syntactic knowledge	2.999926643667678
test utterances	2.1222527108831395
use n	2.4211599391922816
limited way	2.1774937848124125
bottom row	2.125470520333839
selection restriction	2.0557161824276884
either word	2.305458956322113
two entries	2.3974559319503426
formalism allows	2.4682938305116267
best method	2.4829696016728517
charles university	2.1739387095397857
le monde	2.0308108985702593
serves two	2.24825378073324
syntactic functions	2.797203756296626
new alignment	2.0730662534060538
narrative texts	2.1342024470118135
semantic theory	2.596928014794985
practical considerations	2.1413469986390323
relation could	2.075729255121515
transcribed text	2.049069748176902
acoustic models	2.675365582890584
user studies	2.1621793113023307
identical results	2.166102255087602
words assigned	2.032982255991816
fixed window	2.1165917421904625
names using	2.105195669894195
graphic representation	2.093294558436213
surrounding text	2.1828884764914416
describe various	2.061212299166531
refer either	2.0376102740271858
models used	2.9915545357024014
measure score	2.1240946476680165
noun n	2.474537668930563
discourse connectives	2.0421574305857826
text preprocessing	2.0435975245753424
decision trees	3.2266433824338696
les r	2.0009004403793877
nominal arguments	2.0450608250123605
words need	2.060145782650642
appropriate translations	2.0074734400897967
random sample	2.965918154340184
procedure used	2.6188294796613985
related works	2.7029940953165204
english counterparts	2.122227020631432
lexical word	2.144889285781033
various features	2.7849086146886854
separate sentence	2.0074734400897967
understanding systems	3.1435675271973533
last point	2.5063198783573477
technical text	2.1769309117994284
two models	3.2108051995237856
last argument	2.1240946476680165
produce translations	2.185240400682336
statistical learning	2.6532131838576096
authors wish	2.7345015644860258
entries contain	2.137726762436956
algorithms described	2.556189327465679
also deal	2.089026915173973
relative positions	2.57585633010138
highest scores	2.274287879630698
value must	2.127375476291898
narrow scope	2.364593568064148
also identifies	2.067747107964575
weakly supervised	2.0017484258644385
text samples	2.3568178908799604
shows examples	2.7995984076463674
high value	2.195128257658278
substring table	2.0382975066978917
v x	2.354987462946172
parameter setting	2.5620665521632104
representation contains	2.1774937848124125
performs worse	2.5922893931369995
general ones	2.3540470303697685
structure given	2.4459112107066288
model scores	2.4538392743442294
ways depending	2.2112071020073643
telephone number	2.1052717830968084
negative instances	2.162288603168996
software components	2.1943733144794164
parameter values	2.871035995007736
vander linden	2.0186290373446383
relations extracted	2.222442978488717
aligned words	2.2404676499729774
essential step	2.0824927688889554
appropriate domain	2.164235940665368
remaining ambiguities	2.094727102798552
one parameter	2.510371117379182
following kind	2.1852404006823356
valued feature	2.525749425607957
process involving	2.109437912434099
compact way	2.314007818435079
european union	2.5953541772498503
positive integers	2.305097752546449
language variation	2.022410241232288
one function	2.5412076060528688
empty category	2.2282385109358973
five words	2.607593269408653
minimal pairs	2.0855369629697718
similar types	2.1290482690107417
hypothesis would	2.085074160737753
search process	2.8918112208970257
cannot appear	2.3380561048888593
diverse information	2.0490697481769016
two processes	2.753538786877734
section contains	2.339880004558011
different sub	2.4175764205816344
disambiguate words	2.014014536653706
answer pairs	2.380499901992999
string e	2.2074714886932467
argument must	2.2112071020073643
three criteria	2.1133985699178823
specific concepts	2.6970479426993035
french translations	2.12319209177683
complete utterance	2.032982255991816
next page	2.545289389517901
wider class	2.067081229293324
frequency less	2.0532484523230123
using tree	2.309174972365528
relate two	2.022261218861711
original text	3.1747235048310123
verb like	2.7140153088477432
extract features	2.4043111027203574
possible analysis	2.0532135964597886
latter task	2.067747107964575
used measure	2.045521226679158
occur infrequently	2.135844661161516
second measure	2.159746288077376
semantic networks	2.89771906670743
generator may	2.026260250766906
objects referred	2.0082269995432465
semantic database	2.0648905011597334
inference algorithm	2.2040657503686156
flexible system	2.1774937848124125
strong assumption	2.116591742190463
1 words	2.616978297127306
large degree	2.1445259685017777
real problems	2.154388878793747
noun sense	2.2416452808873375
order would	2.109437912434099
r may	2.0928475837926213
increasing complexity	2.2917594692280554
word lists	2.909902189288306
syntactic construction	2.6564660440339547
word sequences	3.291292766443321
evidence supporting	2.2777975658644114
l r	2.162217330171991
positive example	2.47640311570308
large majority	2.3955781761503503
technical documentation	2.360632457317722
examples 2	2.0918364857418936
possible structures	2.5018496095078633
bar level	2.0116738715915847
semantic analyzer	2.33903849060587
2 v	2.2301766631730864
cutting et	2.5976169919083394
type person	2.1882216921535553
best alignment	2.43373175390597
main predicate	2.4459878653605953
k nearest	2.3441330607318784
parsing scheme	2.1750553422631933
free form	2.141447453937137
system involves	2.089026915173973
rules produced	2.1432314806483994
current language	2.0907374619642662
generation problem	2.5196566430631027
observation sequence	2.3471965335855884
huge amount	2.321618294173861
second module	2.230524125991308
data according	2.3605160818311552
subject area	2.4137121169100824
wsj corpus	2.82683985995489
syntactic lexicon	2.055267979223369
whose members	2.166801438022043
complex example	2.242793580526907
word trigram	2.1058537190199544
language since	2.089026915173973
retrieval experiments	2.2074714886932463
large corpora	3.482248475999355
top level	3.051835293999978
best approach	2.3906121063052925
measure used	2.642610937097339
several papers	2.183647914993236
used feature	2.00238351760398
second feature	2.3236155386271937
complete problem	2.248968785663327
optimization process	2.191114775755527
new template	2.0532484523230123
user may	3.237889214081164
word groups	2.4614595075793657
respond appropriately	2.1275552563638707
modification relation	2.069542139177546
first paper	2.316121250352834
experiment 2	2.430382313654072
approach seems	2.6864535948379773
gives rise	3.3017913767933784
length 5	2.10330156437999
term unification	2.4162477927772006
indirect objects	2.606054156445266
require much	2.24825378073324
june 2006	2.5794415416798384
systems take	2.0817120252117016
specific sense	2.3609902008768238
short introduction	2.109437912434099
elegant solution	2.242793580526907
based parsing	2.8685176164647945
basic sentence	2.083497359221134
derivational morphology	2.6848598648380735
document must	2.0817120252117016
parsing complexity	2.2582820894786675
prolog implementation	2.443329773984415
free order	2.1568439095006307
language skills	2.0721570056738
shows another	2.0817120252117016
term goals	2.084052376668106
algorithms could	2.045521226679158
syntactic constituent	2.8050830688444264
section 7	3.4759171905411645
general english	2.5640896516604634
sentence null	2.045521226679158
several research	2.3879279742060486
time recognition	2.179209876010537
nodes within	2.105322206137931
paper discusses	3.21843659875854
information access	2.52250553258402
set may	2.280451946807226
error rate	3.7181994061551498
semantic roles	3.09095216244292
among entities	2.1856471212645787
maximization algorithm	2.261411232688366
individual concepts	2.288241543657806
word could	2.526642750486463
relations expressed	2.178653004950429
several algorithms	2.2863172876464515
priori reason	2.067747107964575
head features	2.319594278416528
relations using	2.399252281275852
rule might	2.3161212503528334
target phrases	2.03001321995555
two translation	2.3861501869633983
sophisticated techniques	2.1946340157728974
four ways	2.264070934029513
initial values	2.4244007727274406
pattern matches	2.4762516989122743
component would	2.1739387095397857
pitch accent	2.2609819387065007
linguistic notion	2.122563170379007
result shows	2.9081145609879995
auxiliary trees	2.5467045708952316
isolated sentences	2.3725247035269392
mark steedman	2.374467719515322
least twice	2.163361218661979
expressions must	2.067747107964575
standard deviation	3.2338754443457947
parsing sentences	2.4784612405958746
question mark	2.757260178313357
structure sharing	2.378930286812391
preliminary version	2.525226595230403
ultimate goal	3.030296855411564
newswire texts	2.112373192775555
hoc retrieval	2.00499466256068
every input	2.532031966771681
infinite loop	2.0763533583416773
constraints associated	2.4663630969640167
also arise	2.1290482690107417
automatic detection	2.621998045608785
understanding task	2.1903098749527143
user asks	2.5057869148746397
therefore conclude	2.031264650127311
morphological variants	2.677456713728907
final decision	2.1402438041633807
given case	2.2685844072039183
8 proc	2.403331244885159
verb types	2.2788983594338115
distributional information	2.407127435138254
additional test	2.0074734400897967
syntactic operations	2.1097113655852784
length three	2.070902079606615
grammatical formalisms	2.624876697377694
though see	2.022261218861711
probabilistic chart	2.067747107964575
robust parser	2.3124924759126175
simple application	2.0601457826506424
precision recall	2.148839950876514
capture syntactic	2.0074734400897967
components must	2.045521226679158
problem would	2.2874806129846443
raising verbs	2.1982193087188198
less context	2.026260250766906
parameter set	2.4005209176072047
three non	2.0918364857418936
making inferences	2.083497359221134
dictionary system	2.0293651015647973
xml document	2.3064503888675176
average frequency	2.07817541032735
two purposes	2.4910639166060555
word unit	2.1128484790707267
specific characteristics	2.1992696891479238
one speech	2.0763533583416773
structure representations	2.406971685431841
traditional sense	2.0312646501273113
general strategy	2.427896658773757
languages without	2.3707749680500125
results showing	2.156307938487117
viable approach	2.0965741974051317
individual lexical	2.6684193472738813
text types	2.6037619910727434
less straightforward	2.0074734400897967
languages may	2.5588863143163523
input pattern	2.1985632813339864
list consisting	2.154388878793747
f c	2.3650580248015816
features together	2.1414753566437956
source language	3.56362534899496
system got	2.0074734400897967
pragmatic level	2.2404676499729774
features representing	2.386958661868031
useful clues	2.0659571110450177
world wide	2.803138222291461
three corpora	2.5581122607510056
tree corresponding	2.4525134064079723
sample sentence	2.7181641587242784
c v	2.3236685394278567
task structure	2.0003023828449846
dialogue fragment	2.0763533583416773
language tasks	2.476758248530734
computational purposes	2.166102255087602
attribute name	2.021237938105666
software system	2.404538137201497
dialogue strategies	2.311475181488279
hendrix et	2.122563170379007
large fraction	2.022261218861711
additional step	2.1479184330021646
2 lists	2.755429753258423
second layer	2.088757010266874
theoretical linguists	2.2784990018139752
syntactic dependencies	2.7248884539697333
grammatical description	2.4372508818684384
levin et	2.0327473091216577
words consisting	2.099458264360342
appropriate english	2.0743965972347684
algorithm determines	2.2224068987457315
manageable size	2.2631802623080817
personal pronouns	2.8364970218940266
extraction results	2.0013487161900905
deerwester et	2.5067819616860993
content extraction	2.378432024624069
linguistic relations	2.276133819373259
potential problems	2.3805199672575776
initial states	2.2547771753718466
improve readability	2.064139780413935
training parameters	2.0532484523230123
minimal amount	2.3417181621687195
possible exception	2.116591742190463
generation model	2.2767440939218035
disk space	2.2452898164543535
different branches	2.0601457826506424
core meaning	2.1047514556667677
deleted interpolation	2.525522364661984
superior performance	2.561146204943073
two forms	3.021959679274862
cognitive processing	2.0388643487004234
based retrieval	2.434275479601515
one group	2.4363717885503346
among words	2.9132121866729914
pairs extracted	2.4150111748359775
computational linguistics	4.119635586219963
written rules	2.414496040103572
state university	2.42601602003143
combination rules	2.1373473000548566
single unit	2.6786882720613177
tag features	2.168859806039811
one entity	2.7220216334063774
work based	2.1479184330021646
correct result	2.483813388204008
discriminatory power	2.1151232889064175
dialogue processing	2.3115482206313187
concepts used	2.4148184100839467
g u	2.033986867040896
automatic identification	2.7072157799670733
atis system	2.2969433223394633
sentence john	2.681730661341967
complete model	2.3997947207672894
lexicon model	2.2361136014339595
candidate word	2.4444940162627002
identify words	2.380830363798011
level alignment	2.369185065336008
words plus	2.0355401813680145
cue word	2.2917651170046334
probabilistic version	2.0983798389015873
type information	2.7392783324374426
intuition behind	2.8556173714341906
distribution using	2.0312646501273113
regression model	2.1539981363239527
preceding text	2.308730445842729
structure like	2.5278499898731828
one rule	3.096149072420628
rough estimate	2.2380656204269957
position p	2.026306434023506
multilingual text	2.376717136584321
several strategies	2.417271613249906
different criteria	2.5212015420754317
leaves open	2.2917594692280554
relevant portions	2.0376102740271858
improve performance	3.334676381294082
typing errors	2.13618257732392
level approach	2.2519925058249575
thus allows	2.1023887620661283
longman dictionary	2.7920701106663612
language contains	2.007473440089797
also account	2.166102255087602
two discourse	2.5426820103649668
increasing amounts	2.11911577017412
standard training	2.449495104805549
row shows	2.514882463295416
representation scheme	2.5981713251104948
progressive aspect	2.0655672944959265
web using	2.264070934029513
six features	2.139044594632718
predicate must	2.019557844009582
running example	2.307398993835071
dictionary construction	2.196914862326209
cannot represent	2.315547565900346
structures containing	2.366082551445672
high probabilities	2.200661481121184
one operation	2.075729255121515
search would	2.031264650127311
scale experiments	2.3633976741456193
part due	2.0053972830294953
coarse grained	2.12417212138905
computer programming	2.0577375124403856
inference procedures	2.2434846698300097
grammatical system	2.0423088260870266
performance drops	2.3000015484793472
dependency theory	2.0301202218354755
text based	2.184564169947356
model involves	2.1290482690107417
proposition 1	2.0945773305382933
normal distribution	2.5010841219670215
eugene charniak	2.089026915173973
hand annotated	2.1312839727750825
surface generator	2.1166369460196153
corpus tagged	2.267991541156711
annotation schemes	2.4931207690907233
similar function	2.089026915173973
mechanism allows	2.233997204181393
set b	2.356602859222334
rule compiler	2.057533426344042
semantic part	2.1527910912671047
binary tree	2.615424139217111
diverse set	2.161246853402207
conventional approaches	2.1930567108259056
possible derivations	2.52566418463804
binary relation	2.84386842734493
intermediate stages	2.2939172677378985
different manner	2.2224068987457315
semantic pattern	2.1459709921022743
two disjoint	2.369115447338463
words usually	2.285913086717917
pos sequence	2.164761441335337
initial set	3.009680683484581
complex descriptions	2.0301435463043473
parsing efficiency	2.583321876953035
using tf	2.015842285784369
punctuation marks	3.1625316033792545
semantic associations	2.0493961548232065
two features	3.131167092257149
recall values	2.576589148562164
sections discuss	2.0390072838737376
phrase must	2.514628276488317
message passing	2.2426911932854265
words marked	2.0966706680978793
simple semantic	2.406676464637198
language dependent	2.745086117553766
terminal label	2.080508366153162
upper limit	2.1899749832948974
word expressions	2.528835990291976
systems available	2.1023887620661283
group contains	2.1039380034279427
judges agreed	2.147741798365251
real word	2.0301435463043473
two constructions	2.073066253406054
program takes	2.1479184330021646
connected components	2.181510383691461
various constraints	2.3644803559732956
following schema	2.2380656204269957
also play	2.5523873992877064
many texts	2.086884416965048
pronoun refers	2.0262602507669065
relevant texts	2.0947767265031825
functional components	2.022261218861711
match words	2.045521226679158
recognition results	2.8354773402444793
value matrices	2.206735090086417
different method	2.4489623523461015
cases may	2.1413469986390323
rule takes	2.0880414073297957
system responses	2.484760689625446
represent partial	2.060145782650642
result would	2.3068844457724467
recall measures	2.5239977043766686
possible application	2.3483606540998085
objects like	2.089026915173973
method results	2.0074734400897967
woods 1970	2.017570716396185
second author	2.724961745321722
rules generate	2.109437912434099
also contributes	2.2328679513998626
chinese word	2.948087880111787
limited size	2.4584934740439652
produces better	2.2917594692280554
modi cation	2.024810778465194
work quite	2.0817120252117016
rules would	2.5873674731752976
actual sentences	2.1414753566437956
verbs according	2.2577963294869763
flight information	2.0364139754210724
complete information	2.345768269478816
novel features	2.143898852826872
linguistic cues	2.1744796914986493
oriented dialogues	2.5478771336736643
building process	2.3839874389711913
program written	2.053986137989304
constituent order	2.5626453140059575
unrestricted text	2.9640267935312683
task oriented	2.273911614223675
automatic knowledge	2.0880414073297957
speech sequence	2.0619463749107956
similarity values	2.5734920746927594
sentences contained	2.3537384159980346
pairs using	2.4458819725670438
specific words	2.7934474645315
parallel texts	2.7853523882850513
parsers using	2.070902079606615
high recall	3.0242059663335263
extraction patterns	2.417910678022719
keeps track	2.9780710937848114
information necessary	3.0456452184844967
recall r	2.1413469986390323
simple declarative	2.458989459396439
algorithms based	2.6324973499642152
english translations	3.0198359584562207
sentence forms	2.087315143807288
logical representation	2.523139766454079
sentence one	2.022261218861711
possible instantiations	2.1413469986390323
1000 sentences	2.2883593728604255
parsing results	2.748420183995792
high agreement	2.3588495388442663
dashed lines	2.567312282971793
approaches using	2.255593583249476
driven machine	2.2408519899111075
special role	2.067747107964575
j j	2.4779739731397648
enough training	2.5924038090315897
valid word	2.1750836290866196
similarity measures	2.9770068030199988
conceptual model	2.360992632158749
foreign broadcast	2.022261218861711
several methods	2.7724553139304797
grammatical construction	2.031641067721154
recursive structure	2.5384004663539406
simple sentence	2.995766983878317
word based	2.8179279523132874
performs much	2.5085220297816346
linguistic literature	2.6306827317107158
closed classes	2.219403039111911
nlp components	2.3498141702765167
detection system	2.151089808573441
cannot directly	2.3005523645852994
rule b	2.1007317628781736
multilingual applications	2.129784667216119
different times	2.5846392107069693
matching word	2.159746288077376
embedded verb	2.080218100597496
full document	2.1664602597817737
right contexts	2.654025936296516
second assumption	2.2207046594768247
phrase following	2.135844661161516
declarative way	2.07837559830591
corpus frequencies	2.2070563180044633
binomial distribution	2.298226068321352
structures built	2.409537891678971
asked questions	2.158071728322869
bigram probabilities	2.4841730226083443
partial matching	2.2140291983157585
current analysis	2.230524125991308
long texts	2.01625945747488
limited form	2.348359550546241
general solution	2.5593166380164063
forms used	2.086551350552231
general language	2.659133551260297
translation time	2.047815473751682
semantic meaning	2.456323471299222
grammar allows	2.54695026759109
et une	2.0516186188527366
method proposed	3.0504349031844686
every non	2.4211599391922816
development corpus	2.716978001098144
test split	2.1380488015835484
temporal expressions	2.6249775380207785
classes used	2.357121432847829
likely tag	2.323905564305961
problem might	2.2631802623080817
multiple languages	2.909778744737465
derivation trees	2.7673008439926083
several dimensions	2.4406532125653317
features selected	2.3324072652354535
lexical probability	2.2029431012610914
involving multiple	2.1638349431134714
words e	2.0460967409737787
desired effect	2.2996868720457626
formal account	2.24583021875162
2 reports	2.0817120252117016
measure performance	2.359836558671865
verb phrases	3.3175288543318677
rank 1	2.1833671063702296
correlation coefficient	2.676182214102799
equation 7	2.081441004544135
000 nouns	2.072005300633911
karttunen et	2.2428414733639057
different verbs	2.5810006438949067
two conditions	2.8226921677055525
start symbol	2.940259389745931
state technology	2.2506916503496783
possible answers	2.7345970765099135
successful analysis	2.049069748176902
white space	2.6159387138035037
computing research	2.2465740890870194
attractive alternative	2.045521226679158
tagger used	2.172274429303222
rewriting system	2.419810035632776
tables show	2.1678854032223676
performance increase	2.178914869102539
allows different	2.183647914993236
correct part	2.408982961146826
individual words	3.4409391788817096
specific problem	2.544073863552885
syntactic behavior	2.5194130903581002
every token	2.4364022959537746
depends upon	2.791761264162063
larger class	2.0880414073297953
trec qa	2.141052427710164
language problems	2.395330969400053
constituent x	2.032982255991816
4 indicates	2.122246315203648
similar vein	2.305458956322113
next turn	2.2087756531510787
including part	2.397957210424912
different alignment	2.049933746777792
functional description	2.209538173959108
lexical units	2.9017763299313355
canadian hansards	2.245988322727558
finite alphabet	2.3608514782456758
english queries	2.2532354804420445
two standard	2.326012528775637
enhanced version	2.099458264360342
systems capable	2.3097823680289724
different evaluation	2.610321212215252
black box	2.603733039889391
relations within	2.598081744672086
practical cases	2.015795287891615
single np	2.264070934029513
free language	2.536725201311857
highest level	2.4390519280729186
exist two	2.257869825585851
translation pairs	2.565928696019537
precision rate	2.6158172826070003
surface word	2.662456933274291
general way	2.5031832001835164
clear cut	2.2062448606993037
reduce parser	2.5444525629409918
many distinct	2.070902079606615
different approach	3.0016420673892625
candidate parse	2.2052043418475185
fundamental question	2.061212299166531
semantic expression	2.04755904944182
know anything	2.014014536653706
successive utterances	2.0215688023747367
score computed	2.1563079384871173
method without	2.1380488015835484
processing module	2.5654803223202602
also note	3.1147867333052734
multiple inheritance	2.594618681405723
general notion	2.3922788066689504
two annotators	2.7931662073041355
better models	2.296234813805337
projective dependency	2.1850666911282772
assuming independence	2.1275552563638707
first reading	2.330001604487663
require significant	2.159746288077376
local ambiguity	2.28546752226797
seven types	2.031264650127311
language communication	2.367178218047503
line learning	2.053248452323013
algorithm looks	2.257869825585851
english ones	2.221537802998995
local contextual	2.0269714420891014
sciences institute	2.204719497546924
scores reported	2.1939531715857408
propose using	2.369841578143437
start state	2.662102555624079
fit together	2.4384100760218654
includes word	2.067747107964575
interpretation using	2.039007283873737
correct interpretation	3.008875014967934
first tries	2.222406898745732
method requires	2.7150383897200143
low probabilities	2.255349860587285
user makes	2.079019887609717
inherent limitations	2.1773123985040854
four sub	2.0435211751140128
extraction phase	2.104857870958635
generative capacity	2.599524755727754
learning experiments	2.5327727906821913
starting position	2.040707995161948
node marked	2.012347776929124
noun entries	2.1353034784701492
main aim	2.002885596916659
system currently	2.823542254685
text level	2.24288063176538
linguistic expertise	2.0918364857418936
appropriate language	2.122563170379007
sentential structure	2.10637101627493
dimensionality reduction	2.377486613549685
similar properties	2.2231389704589786
views expressed	2.1290482690107417
perform better	3.2794887775664936
lexical feature	2.529393832804225
parsing may	2.1954890274836183
performance gain	2.510318958929169
english treebank	2.122659564933155
several aspects	2.8648647388676958
underlying word	2.1171504799210448
sur les	2.2287396680217015
telephone conversations	2.4634999256759924
semantic notions	2.014014536653706
european language	2.4285369926040326
explicit indication	2.042325972844755
supervised classification	2.0122843825452965
integrated approach	2.5000813685491567
top ten	2.4424862067710746
results provide	2.4282856154916748
actual situation	2.0595811562598767
statistical modeling	2.5259589229774253
conceptual dependency	2.5798874575550346
point de	2.0795209436238373
two principles	2.4995406926829333
existing system	2.326211376803704
promising approach	2.6843667913479385
much knowledge	2.0376102740271858
classifier using	2.530728686616713
full sentences	2.5978293179698886
first node	2.293773362494991
information loss	2.147741798365251
allows easy	2.2380656204269957
following type	2.3928273266625144
share common	2.33582780054359
testing procedure	2.053986137989304
represent different	2.730406791534569
language generator	2.7119961282913883
several examples	2.8922935644760326
system incorporating	2.067747107964575
another difficulty	2.022261218861711
using non	2.4734971618570185
use within	2.3005523645852994
argument labels	2.264449216350286
characteristic properties	2.1222463152036486
mellon university	2.798991061012813
quotation marks	2.7224295640113207
across sentence	2.1612468534022073
general relations	2.0539861379893036
first prototype	2.3073820902618136
work properly	2.075729255121515
transition networks	2.686754163178128
focus spaces	2.16769506022177
previous evaluation	2.0023835176039797
alignment errors	2.2238598895035904
wsd systems	2.49088609066336
additional nodes	2.0709020796066153
systems include	2.4328329620168185
phrase consists	2.264070934029513
art supervised	2.025073621873738
using corpora	2.297574746882776
language community	2.3805199672575776
appropriate way	2.2785429625031686
additional meaning	2.014014536653706
previous approach	2.1497135154557805
figures 6	2.509984657951576
actual results	2.045521226679158
ng et	2.0454750360581366
sentence shown	2.0947271027985517
presents results	2.5361198428948413
average score	2.5813034746061847
words rather	2.724620719270728
positive training	2.3227921467355777
precision p	2.2492578335740094
derived rules	2.017486143513637
darpa speech	2.356441653849472
two extremes	2.6182878861798575
following methods	2.24825378073324
using multiple	2.7082971756497507
low recall	3.0598953778334455
distribution within	2.082836083424519
input using	2.222406898745732
memory limitations	2.2137974252004256
argument positions	2.711345725819758
scoring methods	2.221805315818197
theory based	2.0965741974051317
five features	2.2618943472263284
immediate context	2.598303873891689
spanish word	2.1632280516362288
lr parsing	2.3466035734753756
data described	2.2917594692280554
roles like	2.022410241232288
good estimate	2.159746288077376
best scores	2.227249320716272
carroll et	2.5509038054744484
rules might	2.166102255087602
specific model	2.3229739757638654
language utterances	2.6101582290587517
immediate children	2.125470520333839
first tree	2.2193006070212666
nlp system	3.0401186864158922
number 4	2.6779935205348893
alone cannot	2.369115447338463
two candidates	2.4553263834344854
temporal modifiers	2.1411515379744985
particular verb	2.679517590861206
pour les	2.169246632570827
fundamental difference	2.2169936022425736
text mining	2.64909248218749
great flexibility	2.022261218861711
grammatical rule	2.0730662534060533
3 introduces	2.563567192522544
human interactions	2.0824927688889554
corpus linguistics	2.2884873213439025
two operators	2.0490697481769016
par le	2.353883561306927
based metrics	2.173588366094867
word level	3.320735896944503
tilburg university	2.0074734400897967
1 iff	2.034503831882627
level syntactic	2.5261839243266233
valid parse	2.1315696211503674
250 words	2.0520370107650927
complex language	2.3605881269336804
complete set	3.1382672384943993
10th conference	2.34443972705697
possible transitions	2.060145782650642
system trained	2.6139763375681673
language 2	2.0439237932091405
pos information	2.7501637222697703
tag sequence	2.9838422230409467
corpus collected	2.11638795219105
recognition phase	2.12365761472282
dimensional array	2.14851837479882
average size	2.449699911611846
2 times	2.005397283029495
uses syntactic	2.3715855296329234
graph representation	2.3022674218007158
new types	2.6195544201626793
better translation	2.5616539212631784
grammar writer	2.9113103952385595
several researchers	2.890176925165882
u u	2.0355669302402033
communicative act	2.3245717914960755
exploring ways	2.060145782650642
others may	2.1049705072743077
simple procedure	2.3483606540998085
decisions must	2.0817120252117016
main advantages	2.5685897531840975
new top	2.056714748913411
larger structures	2.4168518426735073
input space	2.23138000280802
actual data	2.1668743438134994
thompson 1988	2.07124123081296
3 reports	2.267991541156711
individual entries	2.1063710162749305
verb group	2.3916787466482057
possible approaches	2.3605881269336804
used data	2.1414753566437956
second data	2.1680556475717436
response time	2.7308272981350457
achieve similar	2.2380656204269957
sources used	2.3571838468625623
wrong answers	2.0520370107650927
professional translators	2.1754004078368676
many projects	2.060145782650642
darpa resource	2.421223276837969
based speech	2.5192277651754202
single link	2.2404676499729774
document structure	2.542689381591694
embedded sentences	2.3401286078640084
single level	2.2020202169645566
structure consisting	2.4282856154916748
parser failed	2.014014536653706
show two	2.3806000578467827
parsing phase	2.398762557235977
sentences drawn	2.11911577017412
different authors	2.2937733624949908
high rate	2.084699319720085
true distribution	2.1963385494642136
k words	2.26424969026563
hypothesis space	2.2981005682919564
correct sense	2.8673224124829795
comparison purposes	2.2166155371597625
two constraints	2.7087193112661474
new domains	3.160196444816431
many points	2.1023887620661283
ensure consistency	2.1380488015835484
user would	2.710526189640651
sense per	2.4724863714215237
funding agencies	2.045521226679158
f1 measure	2.289591391117587
traditional model	2.0225191486479366
second term	2.5531284731245467
taxonomic information	2.0131719474230962
sorted list	2.5138256726614925
english parallel	2.37529822342427
including proper	2.172274429303222
three lists	2.099458264360342
logic programs	2.0874136289258693
sicstus prolog	2.3448914281695803
information improves	2.159746288077376
evaluation methods	2.871716047187938
answer generation	2.116600547129541
increases significantly	2.1072480253375248
language whose	2.198316858521053
linguistic generalizations	2.682288914914013
context could	2.109437912434099
general architecture	2.670189396355812
another part	2.2828770320288334
empty elements	2.1172967366833184
estimation methods	2.32361490139756
provide users	2.2421869127123264
june 1990	2.109692903261103
constraints used	2.2207046594768247
contain less	2.1023887620661283
redundant information	2.700181492753436
underlying data	2.2028549369081576
linguistic input	2.1366263129684153
current input	2.627579133395433
probability estimates	2.997796086978567
feature sets	3.149484388423607
individual events	2.070902079606615
xue et	2.043597524575342
grammar approach	2.480570340527418
translation relation	2.152251233203918
computer screen	2.280088143077826
results suggest	3.3018550562825992
original meaning	2.2577963294869763
100 sentences	2.8325177433116973
information conveyed	2.8005301376797664
example taken	2.116591742190463
one study	2.020363647132216
equation 2	2.90919634979592
global information	2.3178441366080396
current application	2.240851989911108
grammar induction	2.478922230931862
increased performance	2.1023887620661283
integrate information	2.053986137989304
test collection	2.6982329576396227
representation may	2.6471259548652695
data could	2.307077104526734
level goal	2.27415343941206
priori probability	2.3197418369628675
relies heavily	2.3346193374696225
program must	2.0050370684111063
generation algorithm	2.8204376882427855
three groups	2.817655819915779
system presented	2.6939041734902753
example discussed	2.0817120252117016
rule must	2.7819601560376173
particular document	2.503435746284169
kupiec et	2.3802217666230057
model currently	2.022261218861711
based rule	2.1000868080586823
language word	2.683114390694619
different variables	2.109437912434099
class classifier	2.3240693892890647
combinatory categorial	2.789675855022228
remaining part	2.664512352396105
feature structures	3.2829302993789993
texts containing	2.153552027758611
contains examples	2.0390072838737376
dragon systems	2.1222530938767195
application program	2.1840440389732736
treebank tagset	2.088507986683518
morphological characteristics	2.1353034784701492
evaluation technique	2.0407044394535045
specific vocabulary	2.3605160818311552
automatic determination	2.0601457826506424
complete algorithm	2.0965741974051317
first sub	2.0376102740271858
retrieval system	3.181386347404531
successful application	2.4459112107066288
wsd task	2.308343582051611
sources including	2.022261218861711
final results	2.5006761681560943
semantic net	2.662746275175588
speaker believes	2.4175883537824365
estimation technique	2.1851123324453434
hypothesis testing	2.158766132283613
sense disambiguation	3.625750254020339
larger size	2.0376102740271858
numeric expressions	2.315211737950161
two analyses	2.50901383274263
one advantage	2.6809518940358483
german sentence	2.404795721515457
serious difficulties	2.022261218861711
news articles	3.1951867510631002
sentence generated	2.1353034784701492
recent results	2.5275580183983113
following examples	3.398413179068425
named entity	3.5367426226713925
semantic space	2.387497884184649
simple past	2.4578602523018462
edge labels	2.302639744133172
identify discourse	2.067081229293324
independent components	2.108179066756418
recent studies	2.688435371384082
rewriting rule	2.243404578024225
quality text	2.0577375124403856
class given	2.039007283873737
one object	2.7250782010547363
motivated features	2.2186466793560866
simple clauses	2.0763533583416773
contain important	2.172274429303222
thus cannot	2.468819905483623
given class	2.6742246497532682
sentence hypothesis	2.020551431000259
system appears	2.067747107964575
complex ways	2.1290482690107417
word cannot	2.296234813805337
system represents	2.135844661161516
speaker wants	2.407045118083926
system evaluates	2.070902079606615
different concept	2.086884416965048
section ii	2.058162491400105
techniques like	2.1414753566437956
languages would	2.1290482690107417
structures called	2.3097823680289724
possible antecedent	2.4412571305783177
first candidate	2.2869864929056836
functional dependencies	2.073633017513815
shows part	2.299686872045762
form described	2.0074734400897967
limited training	2.419651923103911
negative value	2.4485627728057033
significant challenge	2.053986137989304
empirical tests	2.045521226679158
segmentation method	2.21236520237362
incremental approach	2.379198295769141
cannot account	2.538352219881667
stopping criterion	2.2011686204719005
base query	2.147302533755746
factors like	2.045521226679158
decision process	2.4745233635282418
n words	3.0119239880188844
phrasal patterns	2.07032436716707
occurrence information	2.5715203318925894
method produced	2.109437912434099
rule corresponds	2.105195669894195
rules derived	2.1913414770139688
higher threshold	2.1023887620661283
information directly	2.440653212565332
distinct words	2.6597936326955116
structural constraints	2.584450764510852
event representation	2.116873767497815
2 corpus	2.106565125997501
different steps	2.289694481867339
language user	2.444532763075335
turn leads	2.159746288077376
two equations	2.031216633777997
type definitions	2.3537756970273485
match one	2.257869825585851
character sequence	2.5605114819851704
small change	2.1312839727750825
design features	2.0763533583416773
rules written	2.2856827436154
one value	2.6311076689956256
examples demonstrate	2.0918364857418936
patterns involving	2.009868063569498
comparative analysis	2.2294991808452247
corresponding sets	2.0435211751140128
feature engineering	2.2695637326580083
precision measure	2.1816335351833818
information extracted	3.0753275068586885
previous sections	3.176581320943687
different clusters	2.190615528797272
certain concept	2.0355401813680145
major steps	2.422302845401488
provide answers	2.161246853402207
possible combination	2.1895186541064584
system analyzes	2.1678854032223676
full data	2.0923919824150863
pragmatic rules	2.0577375124403856
overall model	2.1387069824765974
particular topic	2.478654190425757
remaining nodes	2.1290482690107417
concrete example	2.669547566043833
new item	2.352574130709022
3 times	2.802834834875581
incomplete data	2.0931526746305975
assign weights	2.075729255121515
boolean query	2.00238351760398
discourse relation	2.4634272049657753
predefined threshold	2.37421679203876
spoken words	2.1030446135144105
category symbols	2.146589754761379
analysis procedure	2.3819109483068512
present study	2.9241995267738092
syntactic subcategorization	2.181633535183382
existing rule	2.031264650127311
grammars written	2.345768269478816
appropriate analysis	2.031264650127311
disambiguation performance	2.0222472693763525
partial result	2.0355401813680145
supervised methods	2.5211603974910415
several versions	2.267991541156711
one agent	2.360290363088648
report results	2.8974646873196406
labeled examples	2.3074271335823906
used simple	2.067747107964575
system components	2.587764584676309
subsequent section	2.045521226679158
ir techniques	2.2731621966744497
use features	2.446115845819681
particular sentences	2.1165917421904625
empirical analysis	2.3763414719464064
training texts	2.5633419582353376
likelihood approach	2.020363647132216
framework described	2.3540470303697685
use maximum	2.24825378073324
efficient computation	2.211790418408607
treebank project	2.3057690484625177
conceptual structure	2.8055100304512894
number n00014	2.122246315203648
combine information	2.0805774075772683
another expression	2.067747107964575
user interaction	2.6907473246977927
software package	2.748846127501902
lower case	3.048334683505983
lower left	2.3375092699964686
specific discourse	2.2508232358709215
lexical processing	2.1112053387580056
search using	2.368834809141683
1 st	2.1225377264764296
words surrounding	2.5366883617989258
structure rule	2.701285637138922
ibm research	2.025960240740144
target object	2.05003429346983
natural dialogue	2.190309874952714
also yields	2.014014536653706
6 discusses	2.440974907463567
declarative form	2.0213043660700585
generator uses	2.357121432847829
20 times	2.1241181034113934
right manner	2.218434485750926
translation candidate	2.209722786844478
varies depending	2.3605881269336804
additional evidence	2.463691434571561
multiple word	2.5873640611794047
language grammar	2.5183717140104784
two partitions	2.00238351760398
head grammars	2.0423695928371037
theoretical linguistics	2.7658297405840524
words since	2.109437912434099
lower values	2.314007818435079
applications using	2.414320698244546
spelling variants	2.032982255991816
current generation	2.2093487881958334
frequency words	2.717042324433512
four corpora	2.0700434154811678
pragmatic context	2.019557844009582
features describing	2.30529007692608
inference component	2.0092767183683407
full parse	2.819011891896591
frame information	2.064527256012421
types according	2.3130951516759186
whose arguments	2.366420119698261
first name	2.539863568674372
proposed methods	2.6781598763676784
3 explains	2.305458956322113
new function	2.1612468534022073
preliminary work	2.649627866458325
since speech	2.0918364857418936
3 evaluation	2.318872612351411
previous steps	2.351334739093333
common problems	2.1290482690107417
distinct parts	2.1290482690107417
possible reading	2.020363647132216
different sequences	2.189546252614866
shallow linguistic	2.0661964899827088
someone else	2.2253186810808385
information represented	2.4791444527834123
text summarisation	2.039772569904624
running time	2.6021339686356333
one day	2.168822999053493
help explain	2.0074734400897967
generate english	2.1774937848124125
either use	2.1290482690107417
cannot occur	2.259472706460728
computational treatment	2.3420410239921114
new instances	2.404538137201497
general statement	2.045521226679158
collected corpus	2.009868063569498
correct words	2.441847436678823
travel planning	2.5531818635265626
models 1	2.271736857675024
relevant feature	2.184223228921038
specific features	2.9578183919232286
lexical organization	2.144036220355602
certain number	2.7354857253899736
current sentence	2.9578241225490443
result using	2.26470180877317
global features	2.303874133573191
sentence representation	2.0607512253343714
example sentences	3.1310876429717753
algorithm used	3.2036092681905526
model selection	2.2256647585164995
lexical lookup	2.5863085183974444
particular cases	2.515272207357606
introduces two	2.1413469986390323
model presented	2.7364904310180673
algorithm converges	2.0490697481769016
independent knowledge	2.3022561655547333
manual annotations	2.170243850585469
scale system	2.1222463152036486
also thank	2.909769494915435
global maximum	2.1659825136431445
mean value	2.2673299858792415
specific constructions	2.0763533583416773
language output	2.101425459690837
type restrictions	2.071454311737682
collection consists	2.257869825585851
single event	2.3551696634468424
null table	2.7071566472721313
system reported	2.1023887620661283
proposed technique	2.072697162060231
information expressed	2.4051035853758815
section 24	2.300498850224894
actual text	2.53361510235606
context based	2.102902855044873
final punctuation	2.2785266044515318
paper first	2.2169936022425736
linguistic constructs	2.2717132851229636
probability associated	2.0965741974051317
information using	2.7191137971900075
major goal	2.430311728785848
algorithm checks	2.0376102740271858
recognition research	2.233997204181393
syntactic choices	2.1728673576400466
three domains	2.126072372661181
independent system	2.3961631313818152
conceptual entities	2.0974923841702804
actual implementation	2.7258644430221723
program consists	2.061212299166531
second pair	2.273017465702769
also experiment	2.075729255121515
study described	2.067747107964575
wahlster et	2.265328272893196
pragmatic knowledge	2.659148009417642
high probability	3.103593436700583
rule consists	2.584325141004728
natural english	2.0850741607377534
possible relationships	2.1852404006823356
data based	2.433600690224864
different scores	2.0236385391388776
component must	2.406676464637198
verb appears	2.338164076573215
linear function	2.4960426728936507
shown ill	2.1852404006823356
third component	2.2685844072039183
similar rules	2.2380656204269957
language patterns	2.085074160737753
line 1	2.21267812847179
processing results	2.13638592708426
verbs could	2.0817120252117016
semantic constraint	2.33227677336801
adjoining grammars	2.967454006631764
given example	2.1452511774913
rayner et	2.064310064665028
formal structure	2.275125009000975
also argue	2.089026915173973
agreement results	2.009868063569498
author wishes	2.2427935805269064
model represents	2.4873413864592644
string c	2.0918364857418936
largest number	2.735105391377588
main differences	2.5777505121879893
lazy learning	2.039772569904624
atomic symbols	2.362544862488466
three runs	2.052037010765093
grammar given	2.200598690831077
exclamation mark	2.1572134025120344
words alone	2.1955791002020293
achieves high	2.1637030579857273
research effort	2.6770209839457553
tile second	2.5078204436821783
syntactic feature	2.653304555635315
tagged training	2.6408727251688466
following clause	2.116591742190463
john hit	2.1109583363687694
past participle	3.0438936216381
two translations	2.1871572067263907
one correct	2.627413296515105
parsing algorithms	3.1066760202140946
conditional entropy	2.1657618417864066
templates used	2.2974668198166297
separate evaluation	2.0312646501273113
practical implementation	2.1543888787937475
one constituent	2.710502706268558
substantial amount	2.0691028174434463
japanese language	2.8391716921776866
idiomatic expression	2.1782697579799617
semantic models	2.259015578128234
target languages	3.1850291894538447
answer keys	2.4759843509689516
unification algorithm	2.4868294854032618
small sets	2.2631802623080817
specification language	2.46145068748323
semantic model	2.6197489489350874
current approaches	2.750068651342006
simple linguistic	2.2112071020073643
tense verb	2.4372244434685313
4 let	2.116591742190463
basic mechanism	2.257869825585851
level categories	2.2683089476942175
linguistics community	2.3955781761503503
words preceding	2.314007818435079
english preposition	2.1112603678094644
possible tree	2.022410241232288
action rules	2.345806284480412
unsupervised word	2.151676491844686
last paragraph	2.185240400682336
end users	2.317997804384167
lexical specification	2.140633564328426
single point	2.33582780054359
set n	2.548593807244528
possible relation	2.019557844009582
structure rather	2.166102255087602
single argument	2.183647914993236
examples show	2.828040848090426
typical word	2.060145782650642
representation produced	2.182888476491442
subsequent analysis	2.2896944818673393
phrasal units	2.2199831911746366
official results	2.059638053083522
positive integer	2.1884104135847955
linguistic frameworks	2.060145782650642
two domain	2.021237938105666
system knowledge	2.107999890411095
first query	2.053986137989304
major classes	2.327403995660175
written grammar	2.0850741607377534
similar context	2.311303830728837
state approach	2.292559594587968
first method	2.840294882355865
complete representation	2.1414753566437956
annotated corpora	3.253367223487037
successful approaches	2.089026915173973
average performance	2.722795711995046
classifier achieved	2.0817120252117016
particular situation	2.4261542590040044
news reports	2.0981179139326436
phonetic symbols	2.2218955463682537
operating systems	2.458729310314128
specific conditions	2.1023887620661283
better coverage	2.435632598873795
source sentence	3.0216889881325257
defined classes	2.060145782650642
relevant document	2.55244008906675
naive user	2.1405120286798134
vector space	3.064029362283367
pieces together	2.045521226679158
application specific	2.2751250090009747
present day	2.161246853402207
kind described	2.109437912434099
sentence planner	2.373970941967425
different sites	2.364587837999639
later stage	2.640303853125822
desirable property	2.339880004558011
model describes	2.099458264360342
knowledge intensive	2.311280599537815
particular user	2.3971810911137896
given task	2.614572318009971
drawing inferences	2.012347776929124
certain classes	2.7238929950144457
make inferences	2.483169438504678
language called	2.332240952226321
alshawi et	2.600497728926099
conversational interaction	2.020363647132216
two grammars	2.56590404901308
also generates	2.422302845401488
rule base	2.494021486269932
several variants	2.3639525993606796
nonterminal symbol	2.618504162932983
dif cult	2.602639883996133
method would	2.484754518219956
disambiguation procedure	2.026921650032393
evaluation metric	3.0021138103509486
useful knowledge	2.014014536653706
helpful suggestions	2.49449202328214
2 summarizes	2.800697415048748
two phenomena	2.20020844517238
discourse modeling	2.0210388623219226
individual modules	2.230524125991308
query languages	2.0955232592434623
short vowel	2.049039082891178
principles underlying	2.242793580526907
long distance	2.559138290629111
analysis described	2.183647914993236
one iteration	2.4373652052207966
correct prediction	2.0433576362240267
similar situations	2.1543888787937475
x denotes	2.2062448606993037
provides support	2.1030830829470486
patterns like	2.321839114388654
sigmoid function	2.2625698361927253
reference translation	2.524751431945411
syntax trees	2.504222816076605
results described	2.369115447338463
deep structure	2.5993946568396393
cluster c	2.012767258614205
time expressions	2.6352513306868586
weighting methods	2.049183698481921
recent attempts	2.0656667908183506
clause level	2.7123856990954796
first possibility	2.0262602507669065
l x	2.2336962458510286
extraction methods	2.419026179375131
first non	2.083679193991461
trivial task	2.851425817664841
qui est	2.2627779964468626
see sections	2.2639833969689853
plausible explanation	2.0880414073297953
similar tasks	2.2112071020073643
attachment preferences	2.0376102740271858
typical user	2.223453205129625
attachment point	2.1015369330656686
final choice	2.022261218861711
word appears	2.9656103096114004
becomes evident	2.089026915173973
final text	2.3035256947230445
patterns automatically	2.185240400682336
practical nlp	2.237516467984816
overall discourse	2.1813446704867463
standard method	2.458985057585732
partial interpretation	2.2216805907274333
resulting scores	2.022261218861711
random sentences	2.045229961808099
good compromise	2.045521226679158
ranked documents	2.4246864991220614
phrases like	2.9934704408572053
processing capability	2.049069748176902
knowledge based	2.6362312779693906
given form	2.007038640285683
representation makes	2.272505380879168
processing may	2.267991541156711
models provide	2.3513347390933323
logical language	2.395157654967046
corresponding position	2.0157952878916148
section shows	2.4829696016728517
argument information	2.0923919824150863
connect two	2.172274429303222
inflectional information	2.1127786641962603
following elements	2.2112071020073643
position k	2.0643104749903145
sequence must	2.0312646501273113
matching algorithms	2.2577963294869763
linear regression	2.466423513997234
translation method	2.4335277909483306
different types	3.907136103589995
variable bindings	2.3034730760216977
binary classifier	2.50598738498118
values using	2.1222463152036486
full text	2.7976777148150873
6 words	2.468831387492778
thank aravind	2.045521226679158
medical terms	2.015418629607659
assign scores	2.1023887620661283
frequency events	2.059827773997295
term matching	2.0423259728447545
400 words	2.2158264170851822
extraction technique	2.1859857899642225
former case	2.942535530871418
contains less	2.014014536653706
left context	2.896426895194535
weights based	2.11911577017412
lafferty et	2.8189983640117786
three readings	2.1191668933749868
possible sub	2.257869825585851
particular data	2.350128731396749
translation directions	2.1293995430967367
learning system	2.9605288228936777
error may	2.035540181368014
given order	2.2974668198166297
semantic translation	2.362522975407847
often use	2.7267999291464706
sufficient amount	2.166102255087602
semantic dependency	2.028031751829162
intended referent	2.406576603120635
method similar	2.318793079863193
questions like	2.7329841475940966
identity function	2.1231920917768297
many kinds	2.8677751323554035
limited amount	2.833239920205076
first evaluation	2.556189327465679
refer back	2.56735003485172
function f	2.99930797958052
shared information	2.2165983686619226
verb object	2.1414753566437956
involves selecting	2.0074734400897967
system lexicon	2.061212299166531
linguistic constituents	2.0001782831294137
different domains	3.305301270335243
categorial unification	2.0919512504091182
words taken	2.1312839727750825
common word	2.7328186243505597
speaker adaptation	2.2840813696533404
comparative evaluations	2.060145782650642
types defined	2.0157952878916148
morphological processes	2.4348076984708085
rules must	2.6510969431468485
verbal arguments	2.189546252614866
many sentences	2.9962384712492462
business news	2.2658532735805306
relevant sentences	2.4263739720369424
apply machine	2.2384438317931403
work focuses	2.4943690648994172
writing style	2.489932277147994
one containing	2.7052822355494777
contains approximately	2.650962107947459
low probability	2.744695878888188
certain types	3.3351318891238737
two advantages	2.6262393868602247
average time	2.582748806729284
provide sufficient	2.422455833148834
c denotes	2.2062448606993037
answers questions	2.082836083424519
selectional preference	2.083200249665076
syntactic annotation	2.5305715564496607
words per	3.215486397801139
another issue	2.2308948252190905
probabilistic methods	2.3803748847608253
one variant	2.014014536653706
two variations	2.159746288077376
document information	2.130459427106893
complete list	2.557514256446332
general semantic	2.7878804400318353
possible candidate	2.4028451845347893
inside probabilities	2.0970315672229902
second test	2.489932277147994
lexical ambiguities	2.6338301562553474
accurate results	2.4829696016728517
one c	2.116591742190463
different layers	2.2685844072039183
analysis could	2.309782368028973
based tools	2.2165983686619217
declarative nature	2.042325972844754
parallel sentences	2.295812779772474
computational advantages	2.338888780287266
one reason	2.981696551985947
complementary distribution	2.327403995660175
term memory	2.407066348737842
grammars must	2.045229961808099
simard et	2.0896124775616745
particular parse	2.1413469986390323
work needs	2.3955781761503503
recognition methods	2.0720053006339114
candidates using	2.0041357124283135
html document	2.0433576362240267
several reasons	3.0676412291543476
last row	2.7629337439192923
syntactic complexity	2.3678853357497887
class categories	2.0329822559918163
improving performance	2.462292105818408
highest value	2.404182443801024
component consists	2.2532512925302806
example data	2.012347776929124
test examples	2.5741121622147234
derived word	2.2654812855760516
optional elements	2.24599615797519
allen et	2.539632422504158
show later	2.2427935805269072
sont pas	2.006287103426194
identifying word	2.1479184330021646
different categories	2.9470143908219075
improves performance	2.8291455456595402
whole system	2.966637039111659
reverse direction	2.4993686801135198
frequency information	2.904491326406325
important practical	2.1479184330021646
c q	2.120551687520131
performance could	2.4211599391922816
research areas	2.263348544014791
cannot distinguish	2.5044970440603693
standard evaluation	2.5489971548972274
evaluating systems	2.012347776929124
english analysis	2.1816570181891475
grammar like	2.089026915173973
framework based	2.267329985879241
learn patterns	2.0236385391388776
distinct values	2.026260250766906
morphological information	3.164121962176151
constraints specified	2.1816335351833818
node contains	2.372833841046879
machine interfaces	2.1023887620661283
second step	3.4559248320198823
alignment quality	2.1643910305772485
scheme allows	2.1290482690107417
recall errors	2.0643620962409948
transfer component	2.5059253962514543
length increases	2.0947271027985517
trees corresponding	2.237092130323064
major categories	2.5996476625250438
derivation process	2.4034665305497596
parsing methods	2.777198457724295
another problem	3.092739510255093
ascending order	2.2561487647668486
sense may	2.1413469986390323
present experimental	2.5526054360700314
many similarities	2.0965741974051317
problem consists	2.1984572101092845
human parsing	2.0367940828486537
possible segmentations	2.5768714240352457
r l	2.465956678123518
many cases	3.716392984082443
system provides	2.949893849343937
contains 10	2.109437912434099
given x	2.21826141526814
likelihood estimate	2.8046125120559227
word identification	2.305720807465189
long sentences	2.942670039129562
lexical head	2.863352024436548
correct answer	3.1448484060822657
correct information	2.3333329581503177
dependent models	2.4186476484632973
natural solution	2.1290482690107417
national library	2.5669508978825397
grammar symbols	2.116324022777621
natural language	4.532380315514245
desirable properties	2.420009569037414
closely matches	2.060145782650642
logical relations	2.076658594427388
dependent model	2.031216633777997
case letters	2.457927442453385
model outperforms	2.659153481883283
contextual evidence	2.0281097831836057
general nature	2.043521175114013
different character	2.293054160115594
specific constraints	2.626808342489313
lexieal entry	2.0312646501273113
typical patterns	2.0074734400897967
arc used	2.0817120252117016
therefore require	2.060145782650642
standard parseval	2.019557844009582
example also	2.3644803559732956
global constraints	2.127087696778326
already contains	2.257869825585851
since g	2.1029215145255775
hard constraint	2.2006164309087404
course also	2.166102255087602
certain categories	2.2631802623080817
data mining	2.3547261535000836
information within	2.9188596583015776
programming algorithms	2.044877827032594
scores computed	2.0423259728447545
genia corpus	2.2211051268331135
single question	2.0074734400897967
remaining problem	2.089026915173973
translation engine	2.0690197926120764
idf scores	2.17375937451299
selecting sentences	2.0539861379893036
referential information	2.045521226679158
set using	2.8924363401379733
simple mechanism	2.267991541156711
representation would	2.5886045470541506
newswire data	2.075729255121515
n r	2.1671647356522157
paper demonstrates	2.4835486736008545
parsing technique	2.5406645937389905
example may	2.3567860333521553
precedence relations	2.4127197888327077
hash table	2.3658957357771633
biased towards	2.5446847166031232
focus stack	2.2267056989976073
text system	2.1076034117323625
resolve ambiguity	2.3435066063864065
mary likes	2.174190173079227
word context	2.606234448198367
fall outside	2.4898583050393785
one side	2.8011584046730316
vector model	2.0452070232690627
rule templates	2.1880330722199277
major difference	2.8336922883862323
analyse de	2.077438554079712
place holder	2.1432314806484
annotation task	2.6414621647728906
process proceeds	2.2062448606993037
labeled precision	2.4506925387562015
segment may	2.086134808534669
linguistic contexts	2.330001604487663
two component	2.2068642602077393
mental state	2.2942631754919516
google search	2.167252082381521
readers may	2.049069748176902
first stage	3.1165476532174
ongoing project	2.1290482690107417
following result	2.264070934029513
represent lexical	2.3398800045580104
different people	2.7041887163968075
better prediction	2.0376102740271858
federal ministry	2.2183357382334514
linguistic resources	3.0491883166427183
frequency values	2.075729255121515
important clues	2.227249320716272
answer could	2.0074734400897967
sentence aligned	2.180117288262722
incremental parsing	2.101013273356319
every noun	2.6217762071382875
semantic relationship	2.7206834368200004
knowledge management	2.1337829151903054
incorrect segmentation	2.017570716396185
data obtained	2.584808357508425
semantic features	3.4582764296299238
success rates	2.428008732003227
current computational	2.0376102740271858
sample corpus	2.1769022112262286
system utilizes	2.014014536653706
corresponding sentence	2.4166887309446423
entire vocabulary	2.1678854032223676
simple noun	2.671823347978251
substitution operations	2.0817120252117016
function calls	2.1651154718617
conditional maximum	2.051405404013556
thank john	2.089026915173973
dependency model	2.206292498696807
semantic interpretations	2.8274902748595414
symbol c	2.060145782650642
list containing	2.5034484832534796
general approaches	2.122246315203648
one word	3.759663752068059
plural forms	2.577827006622913
compares favourably	2.0074734400897967
value set	2.0082269995432465
single context	2.075729255121515
way around	2.651694906400183
choose among	2.8532095449000536
processing text	2.0539861379893036
precision results	2.524359712641555
third section	2.4910639166060555
final test	2.2580361359046064
spoken discourse	2.255587703589782
every man	2.496945046892699
error metric	2.0105227577523688
terms found	2.1042904425091606
rule applies	2.8251192835652974
reasonable way	2.2328679513998626
current technology	2.26470180877317
words containing	2.3790843298205955
algorithms exist	2.060145782650642
propositional content	2.8774129702630433
external world	2.0539861379893036
probability score	2.1297846672161187
ungrammatical input	2.2434019358046564
candidate term	2.019476094569706
different readings	2.506738716489239
domain knowledge	3.251174898812228
backward algorithm	2.749328807624764
column 5	2.00238351760398
english task	2.283974078526839
speech understanding	2.9152347400826275
translation project	2.6367009708630222
fourth section	2.0817120252117016
full nps	2.165138635671425
lexical choice	2.7887958790799616
three experiments	2.575088166129905
morphological forms	2.3861511774041837
current tree	2.0055165239033714
string may	2.1637030579857273
make parsing	2.045521226679158
worst case	3.1616493697150743
classification procedure	2.0539861379893036
trees representing	2.222406898745732
potential antecedents	2.244669732189628
input structure	2.2701300988393234
feature weight	2.030101409193854
one version	2.3992522812758517
broader context	2.264070934029513
specific representation	2.1774937848124125
says nothing	2.296234813805337
character set	2.4951887137681443
utterances may	2.232062756869182
considerable attention	2.509365157065348
improvement comes	2.014014536653706
another model	2.1543888787937475
resolution procedure	2.065185753618689
future directions	3.037477909191375
processing programs	2.070902079606615
plan inference	2.0948086811392566
ill l	2.2328679513998626
one annotator	2.4677620002979004
different phrases	2.3028207919454977
narrative text	2.081715542841045
important property	2.7377030776457265
phrasal category	2.13540769048263
4 describes	3.161504989569203
bottom right	2.1480601086854554
recognizer output	2.4854309622308164
queries based	2.060145782650642
automatic feature	2.2658748144574723
et la	2.4465261688201836
unsolved problems	2.079019887609717
four groups	2.491165732965941
october 2005	2.2776740307447065
expansion techniques	2.0592191196139593
pereira et	2.6518297076023765
errors made	3.0919108330161023
two levels	3.375394411701891
following reasons	2.869252827320689
relations among	3.3992601299129923
compound word	2.503237534743716
sentences generated	2.487258520884037
simple approach	2.801834510824848
different instances	2.259284473295974
inheritance hierarchy	2.6963667742283843
available training	2.6998131898696984
translations using	2.2394738813759894
level rule	2.3463160073852665
target side	2.286760762549587
french sentence	2.466573834354465
particular grammatical	2.3289945710953415
evaluation result	2.2973038319991463
user chooses	2.2147431158325057
sur la	2.45101348383304
common patterns	2.278461809175595
one user	2.3981332997989453
resolving ambiguity	2.253310279048359
search techniques	2.329494167932685
testing corpus	2.5188902125098624
incorrect results	2.1290482690107417
type x	2.1822759631740007
tasks require	2.0074734400897967
basic concept	2.107877432185496
regular expression	3.0869668473964795
system 2	2.110676283984369
several nodes	2.0312646501273113
given slot	2.1119430342456558
language research	2.6080671532264947
main task	2.6772258481030375
information flow	2.398152022096352
single system	2.589011629618206
dependency information	2.333749024188485
specific reference	2.1859857899642225
different level	2.357121432847829
6 proc	2.435600505453947
derivation step	2.1012631079430957
nl processing	2.188519966016826
end time	2.1852210035985085
transitive verbs	2.953170922366847
certain phrases	2.1774937848124125
particular text	2.6611507396474163
initial training	2.3425632368038216
generated texts	2.316939865126237
models developed	2.1984572101092845
human activity	2.0817120252117016
complex sentence	2.586109911184963
whose type	2.2421869127123264
target word	3.2921204909076756
alternative forms	2.1774937848124125
semantic label	2.2299935841769343
natural spoken	2.108179066756418
gaussian mixture	2.011906274726142
document set	2.731331400665233
model provides	2.9064275771818897
particular implementation	2.2924536885149736
values within	2.0817120252117016
every level	2.153110240658155
r must	2.1029028550448725
cohen et	2.31790214095777
related phenomena	2.2024639441989313
robust systems	2.0376102740271858
unique name	2.20379976615122
one encounters	2.075729255121515
practical use	2.90458880116778
statistics based	2.1390445946327175
z e	2.501397366149239
5 lists	2.1479184330021646
entire discourse	2.122227020631432
words whose	3.0723578444056874
way one	2.1222463152036486
occurrence restrictions	2.3832784123824156
source software	2.0379203068673784
lexical access	2.3402960135336137
given training	2.5797359845644343
maximal length	2.053248452323013
various systems	2.53840046635394
several concepts	2.045521226679158
processing applications	2.9828846204017596
additional features	3.126135260864553
semantic filtering	2.004131288758728
performance based	2.0236385391388776
systems require	2.592295963423138
predicates used	2.200598690831077
structural transfer	2.285237063251877
approach still	2.022261218861711
parallel data	2.2424184454001033
v v	2.026694875138565
one grammar	2.2381772962997206
structure contains	2.6039953903403683
first steps	2.4532558885755127
anaphoric definite	2.0143433317033805
unseen words	2.5734699483822325
acquiring information	2.0965741974051317
many aspects	2.8553740913037746
function g	2.1341601797129837
par l	2.123037939396218
user must	2.884112314689289
last element	2.5383485690862857
three nouns	2.067081229293324
added advantage	2.200598690831077
general procedure	2.2664627381026032
phonological level	2.020121462798538
one hand	3.7233460518872654
improved recall	2.026260250766906
3 indicates	2.089026915173973
word lexicon	2.5631290250564924
possible non	2.0965741974051317
role assignment	2.3373585695487
model probabilities	2.530891173014599
system built	2.3263148914896092
intuitive interpretation	2.1165917421904625
parsing decisions	2.3129958467238954
null l	2.109437912434099
posterior probability	2.830290624638334
method takes	2.3879279742060486
single parameter	2.3109116208105
next step	3.5419274944646895
two requirements	2.114299018453181
basili et	2.0721406102054596
briefly introduce	2.5055533459995827
phenomena within	2.0157952878916157
sparse training	2.154626642362153
example text	2.3689267664870304
present version	2.4587293103141286
feature weights	2.657014671711023
uniquely identifies	2.2380656204269957
restricted domains	2.532268205068072
correct set	2.0865513505522313
following part	2.0157952878916148
mccarthy et	2.1099076479327437
diagonal matrix	2.27202920771712
two differences	2.1290482690107417
subsequent sentence	2.0837225057868056
system behavior	2.307382090261814
highest f	2.3899312265039985
joint distribution	2.480673537661454
mapping function	2.2727287493904864
finding word	2.029677424295417
thes e	2.045521226679158
expected counts	2.1185277309604222
plausible interpretation	2.2425989341683077
node containing	2.0460967409737787
w appears	2.0947271027985517
users would	2.2661576011805113
constraint set	2.0983605839860155
detailed description	3.505233324342681
several thousands	2.109437912434099
syntactic data	2.1749635955627893
realization process	2.000415158520729
modal verbs	2.36664240375256
second uses	2.022261218861711
technique presented	2.1222463152036486
data point	2.2291521893733495
gave mary	2.3077297484954804
nonterminal nodes	2.345347533507049
generation component	2.960165543373327
pruning strategy	2.0301287778202664
reduce action	2.0163727664540283
sentence 3	2.380435925314334
one output	2.3971810911137896
standard dictionaries	2.1992696891479233
larger amount	2.0722788862565436
incorrect parse	2.0790198876097175
node c	2.2569758280309005
one page	2.061212299166531
units within	2.085074160737753
strong evidence	2.7663608196056684
communicative function	2.1794099336673627
missing elements	2.0082269995432465
semantic coverage	2.09369246530338
easy access	2.509365157065348
different speech	2.277797565864411
training document	2.182403755777452
non terminal	2.070902079606615
figure 8	3.5215189468525856
3 types	2.012347776929124
two definitions	2.4260324477975805
oriented programming	2.3639525993606796
see tables	2.184223228921038
rules involved	2.022261218861711
magnitude larger	2.2147431158325057
two baseline	2.355654078039955
different sorts	2.384227704698257
possible alignment	2.2840983571629154
systems tend	2.331780823064821
xml tags	2.232062756869182
new rules	2.4691370136321233
represent words	2.319324296244544
entropy model	3.010323436884807
possible derivation	2.0918364857418936
acceptable translation	2.1219736983354656
programming technique	2.292453688514974
frame representation	2.412880268334785
uses two	2.9651448161428196
consuming task	2.611029671005839
john took	2.0665038522145966
using explicit	2.067747107964575
added value	2.012347776929124
separating hyperplane	2.152246546462954
semantic entities	2.313868601599509
prepositional complement	2.1554471791596175
highest recall	2.251642442858886
correspond directly	2.4850442033614906
hidden units	2.17639814422538
synonym dictionary	2.0200567914199983
following interpretation	2.0435211751140128
unification grammars	2.636711987941408
sense ambiguity	2.6912733064205763
ungrammatical sentence	2.126823141441437
computer vision	2.0862895726458337
entity class	2.0946381087062913
base de	2.0518871493107156
dependent features	2.298286641553239
approximate solution	2.1290482690107417
null elements	2.029627688007821
newswire text	2.5942173687262824
judged relevant	2.0438889489034233
average numbers	2.022261218861711
detailed knowledge	2.523192526452843
frequent words	3.1907087972585897
research questions	2.202640507036321
000 lines	2.105195669894195
algorithm tries	2.3523313048426364
prior experience	2.183647914993236
surface subject	2.4037011269866393
accuracy level	2.1240946476680165
automatic annotation	2.4753463443986368
mathematical properties	2.2613965909547256
em training	2.2729485341573215
intended application	2.332743262433283
question may	2.507355080104039
correct label	2.2620459985504136
input sequence	2.7321467943248114
language processing	4.348495374211332
pattern based	2.0674392339874954
similar behavior	2.1774937848124125
mandarin chinese	2.4202548824738255
important properties	2.621776207138288
given time	2.6689759789517287
target grammar	2.2599968223739237
different entity	2.137726762436956
form given	2.3117828295131293
used n	2.1023887620661283
function p	2.4028240943187362
input utterance	2.679710844334973
japanese input	2.01625945747488
novel way	2.3955781761503503
database retrieval	2.0956712066720753
different properties	2.5713098687748897
null symbol	2.0475590494418205
system already	2.0880414073297953
selectional restrictions	3.1929540750011567
syllable boundaries	2.0477041790474435
tags assigned	2.525846321293602
categories may	2.3953272468316316
tagged input	2.075729255121515
classes according	2.468819905483623
easiest way	2.24825378073324
research focus	2.109437912434099
specific domain	3.018897259782883
object denoted	2.009868063569498
either direction	2.0443681060234233
audio data	2.005994243847885
important resource	2.067747107964575
complement types	2.00238351760398
new context	2.434488508055094
based generator	2.0660571893865733
provide input	2.172274429303222
coordinating conjunctions	2.400056038279295
selection based	2.265726339495774
contains n	2.1739387095397857
processing step	2.984541953897638
official test	2.3504232386355843
semantic predicate	2.3648378845609135
e et	2.104773188796117
recent progress	2.1918638189878417
tree approach	2.0138712013117335
arguments may	2.2578698255858516
acquisition methods	2.2369016741321595
tree induction	2.2097165974427684
data format	2.316121250352834
line resources	2.2896944818673393
lexicon includes	2.222406898745732
formalism presented	2.014014536653706
features defined	2.345768269478816
techniques presented	2.267991541156711
determines whether	2.987969710500878
figures 3	2.8065548616745266
algorithm correctly	2.206916018551769
many terms	2.372833841046879
avoid generating	2.0763533583416773
relevant terms	2.2348692452839063
morphological parser	2.0245278187040876
node whose	2.357121432847829
shallow syntactic	2.6878619112412623
case labels	2.067072652732122
lexical database	3.0701499620293977
dependency features	2.0758169469206447
source file	2.017570716396185
structural differences	2.3920948169591325
ne sont	2.053174792484266
speech acts	3.032261490742871
usual case	2.067747107964575
wordnet hierarchy	2.64864478982872
meaningful comparison	2.089026915173973
evaluation techniques	2.3510054483476335
observed frequencies	2.2594356772821076
system encounters	2.1414753566437956
pages 1	2.1479184330021646
example shows	3.221988170705398
segmentation based	2.232062756869182
intuitive sense	2.149914842833546
subcat feature	2.122227020631432
word f	2.2187646447089193
top row	2.0643104749903154
matching terms	2.085074160737753
many simple	2.200598690831077
models described	2.6007101367055183
gram language	3.044232477446761
nonterminal labels	2.158157609776741
basic parsing	2.1872962618521736
approach depends	2.1642359406653684
five levels	2.1166196741856655
semantic constraints	3.2582518653178223
lexical analyzer	2.0565600779224518
truth values	2.6010521446723422
language use	3.135050445750084
entire string	2.045229961808099
discourse contexts	2.193056710825906
left side	3.0173018827273568
differences may	2.2062448606993037
speech errors	2.1234143692948715
also determines	2.3567860333521553
nouns like	2.403472566572053
procedure described	2.9716398586764394
one condition	2.1597096509760467
representation formalism	2.5147161357347616
include part	2.1023887620661283
utterances produced	2.102921514525577
training sentences	3.048186717607396
case structure	2.1004580205182664
various lengths	2.1852404006823356
new knowledge	2.8145329509982964
common feature	2.544352186920131
mutual information	3.3691642727649547
random choice	2.4211577250508514
word e	2.4413721192005475
disambiguation method	2.445732609260673
performance level	2.448934337375546
left implicit	2.339875472386836
interesting property	2.200598690831077
three columns	2.2384438317931403
component words	2.5435312371416696
open classes	2.099458264360342
whose presence	2.109437912434099
system performed	2.5833816248818815
guiding principle	2.2578698255858516
different nouns	2.093294558436213
model built	2.3706473607714544
information contained	3.3833112094461044
graph structures	2.1527910912671047
linguistic expressions	2.899477685150656
shows one	2.6190789711591393
time point	2.2410420483197213
previous rule	2.0918364857418936
syntactic head	2.6829866932543545
new system	2.2457000619668457
german verbs	2.1720899366581143
br example	2.089026915173973
syntactic terms	2.053986137989304
performance due	2.1413469986390323
word distribution	2.397066130365608
several respects	2.547172281111051
see fig	3.104433205180067
retrieval module	2.0150046109079685
leave aside	2.0376102740271858
following formulas	2.0965741974051317
information structure	2.6891787779487513
different actions	2.1163879521910505
unsupervised algorithms	2.0564340940627437
appropriate contexts	2.0376102740271858
conll shared	2.2133187583704625
many attempts	2.116591742190463
english sentence	3.2965427651127013
whose content	2.361034984088791
every event	2.159746288077376
best overall	2.694876341575682
second pass	2.7655901707368
following sense	2.272505380879168
tile input	2.4549958602644195
unseen events	2.19118644637554
one kind	2.673432682273815
ranked element	2.0991618454791166
chosen randomly	2.4387305739872613
automatic processing	2.739083250808437
reasonable size	2.422302845401488
smaller data	2.2532512925302806
user inputs	2.330467908155574
structured objects	2.0826089265393186
words without	2.7884416051050347
mother node	2.648380664268087
words immediately	2.270820552349159
dictionary data	2.1715331461640934
semantic analyses	2.7023685854089097
multimodal interaction	2.0166324504379958
remaining features	2.3997947207672894
wilcoxon signed	2.025073621873738
hidden layer	2.3169066049161104
computer systems	2.3474751579189155
two things	2.9408153115190316
translation could	2.172274429303222
better estimates	2.0376102740271858
simple syntactic	2.5570824702860264
research interests	2.0532484523230123
research done	2.2427935805269064
squared error	2.1497135154557805
natural numbers	2.4893292604150363
many text	2.348360654099808
statistical approach	3.1790587728280713
methods include	2.1563079384871173
relevant text	2.4367701772223143
learning rate	2.238211951962939
complete solution	2.1557946417682414
treebank corpus	2.700106828439213
cannot take	2.3151413747592446
application domains	2.748385097568233
output probabilities	2.332762129584898
types based	2.122246315203648
following conclusions	2.318793079863193
first concerns	2.089026915173973
air travel	2.8391296750330577
users might	2.233997204181393
corresponding tree	2.212862020185457
two tests	2.379555846271788
additional experiment	2.012347776929124
basic types	2.674341046495968
parser trained	2.1528072904936053
important point	2.8652465241902463
simple fact	2.183647914993236
2nd person	2.008083705228559
use three	2.7267999291464706
brief explanation	2.014014536653706
language might	2.045521226679158
automatic extraction	3.0166150462614247
human speech	2.583005085251154
initial node	2.0343808239087764
features given	2.172274429303222
structure containing	2.5383522198816664
parallel text	2.703589177707109
summaries using	2.0577375124403856
models could	2.431764288227561
one moves	2.00238351760398
graph corresponding	2.0376102740271858
several words	2.876879547207339
manual editing	2.052037010765093
argument representation	2.007473440089797
based word	2.660341473599098
translation equivalents	2.399813173174182
best value	2.0102301716085935
parallel implementation	2.0577375124403856
single pos	2.014014536653706
atis data	2.152229534312333
separate tokens	2.0965741974051317
procedure could	2.2492578335740094
mass nouns	2.0282778255421867
strong constraint	2.045229961808099
one unique	2.110676283984369
satisfactory solution	2.2380656204269957
sigdial workshop	2.403472566572053
intermediate representations	2.2887502153978536
ordering rules	2.1222508564894698
translation unit	2.1497012856048303
overall framework	2.159746288077376
three parameters	2.407396607448329
two systems	3.17937969975403
feature could	2.0312646501273113
preprocessing stage	2.441686762172832
make possible	2.4561333536596983
every point	2.190982236626934
sense would	2.014014536653706
algorithm builds	2.230590646944645
particular category	2.3715855296329234
np may	2.189546252614866
normal text	2.014014536653706
given node	2.722060242402397
results obtained	3.6374408785551435
following characteristics	2.611814665828829
first look	2.0621467153115782
dictionary may	2.230577630180687
one measure	2.0053972830294953
word wj	2.1590976696462425
action performed	2.014014536653706
4 illustrates	2.8265964957780594
additional constraints	2.890082699102504
analysis cannot	2.2380656204269957
generative description	2.1820320949886263
suggest using	2.1222463152036486
treebank parses	2.105663339238975
assumptions underlying	2.2074757805024428
common ancestor	2.604189324602435
perform actions	2.00238351760398
accuracy rates	2.440748491232637
translated words	2.082836083424519
paper gives	2.4659566781235185
system made	2.0973890129090687
inflectional paradigm	2.175409328526317
typed input	2.1873013317936936
new lexicon	2.118274250141516
japanese sentence	2.8517204810597145
tree obtained	2.2233541795025014
imperative sentences	2.2523049448112693
words occurred	2.0074734400897967
less data	2.4328329620168185
events occur	2.089838197048368
annotation guidelines	2.4167490974858197
likely translation	2.1240946476680165
specific terminology	2.215156728075037
condition holds	2.339554560148772
time parsing	2.215156728075037
longer strings	2.1222463152036486
semantic interpreter	2.650446109052364
considerable effort	2.5002982171432886
word rather	2.060145782650642
draw inferences	2.09844301409668
shallow parsing	3.0050450235010073
one location	2.1739387095397857
look like	3.0422723698298655
form like	2.233997204181393
atn formalism	2.065945938341496
dependency parser	2.945753737949791
open question	2.8547413572670277
problem arises	2.9924539320191625
coreference chain	2.0696936250971283
demonstration sessions	2.045521226679158
example would	2.6161488971406976
native speakers	3.180372443111661
two questions	2.951128279614778
similar goals	2.159746288077376
derived trees	2.4980164664726283
one feature	3.124235732595076
general linguistic	2.732682946267567
system called	3.033825880476932
large knowledge	2.3997947207672894
document representation	2.3596337751006335
computational analysis	2.167994537676787
exceptional case	2.0157952878916157
two structures	2.8387779678819944
matching process	2.8925285511573318
user selects	2.5191906435653086
assumption made	2.1946340157728974
basic categories	2.448694234164285
problem caused	2.200598690831077
thorough discussion	2.109437912434099
labels used	2.329081436908228
entity task	2.5920087472033417
equivalence class	2.9105946245405896
preliminary investigations	2.014014536653706
distributional similarity	2.3718199416065877
also extract	2.267991541156711
left hand	3.103039991690742
parser presented	2.075729255121515
structures found	2.223354179502502
method must	2.2917594692280554
given relation	2.24929853809406
modelling techniques	2.070902079606615
document understanding	2.415637084675171
multiple dimensions	2.093294558436213
case words	2.116591742190463
research area	2.5504302040366493
nsf grant	3.187256583591089
base management	2.158057566768644
communicative actions	2.122613052618691
event structures	2.267668311412048
tree classifier	2.4338193038776987
large system	2.135844661161516
main approaches	2.440974907463567
classes defined	2.303241766231916
documents containing	2.8678997350585824
person pronoun	2.287753372532491
power set	2.225741513567855
particular type	3.1124053107465177
resources like	2.2947344061038626
analysis given	2.022261218861711
since c	2.014014536653706
selected sentences	2.6401667560772584
two example	2.2924536885149736
grammar currently	2.014014536653706
one occurrence	2.784908614688686
tagged corpora	2.893718834771277
semantic theories	2.3111175269948716
length 1	2.6153987872398896
many applications	3.275151984416013
represent information	2.415062027105976
issues concerning	2.4943690648994172
l 1	2.7060097835492685
results given	2.5570824702860273
lexical pattern	2.091189163076878
interesting properties	2.4303117287858482
structure whose	2.33582780054359
weighted combination	2.366082551445672
good knowledge	2.022261218861711
training results	2.0355401813680145
multilingual lexical	2.261158953773757
lexical associations	2.151583723734582
measure scores	2.280005513470255
actions performed	2.0965741974051317
intonational phrase	2.026991071124429
model performed	2.2140741258816408
performance relative	2.0817120252117016
existing word	2.291328123830457
tree given	2.3537384159980346
description language	2.7011230225399965
graph representing	2.1222463152036486
verbs like	3.0247481125307742
annotation procedure	2.026260250766906
corpus frequency	2.5408437105109756
production process	2.1475326128984564
local optimum	2.2318569333019083
features provided	2.031264650127311
connections among	2.0965741974051317
current evaluation	2.245558162117437
null note	2.5270204646552323
grammatical patterns	2.0451823506947937
probabilities given	2.0918364857418936
formed input	2.4604436990568197
linguistic approach	2.34836653334773
brief discussion	2.5713098687748897
current approach	2.53224698243103
lexicon entry	2.756619611521524
development tools	2.0013487161900905
rule r	2.6751455682994263
first np	2.6112288918209248
object may	2.4543205823915613
recent works	2.4251394527998116
language production	2.443765233228402
various fields	2.236207490380668
dialog context	2.0428332311555266
sentences used	2.701345696828706
one conclusion	2.022261218861711
syntactic ambiguity	2.8363881539986013
xml elements	2.0038732867133815
simple parsing	2.053986137989304
early stages	2.5293371134975215
parse trees	3.6121128011351575
feature allows	2.070902079606615
subcategorization information	2.701482666603779
naval research	3.0791523375858714
examine whether	2.583615886270918
based grammar	3.1412835087831605
sentence similarity	2.130724074221839
english wordnet	2.0046381103259057
output texts	2.1160084991347903
head position	2.141635865407804
equal numbers	2.0817120252117016
order information	2.4312338027118967
relations associated	2.2062448606993037
john wants	2.208981645077106
cognitive models	2.02585040092793
successful approach	2.1023887620661283
tree classifiers	2.071387681096856
cognitive model	2.2366146781564646
typical use	2.053986137989304
extract answers	2.008083705228559
significant reduction	2.240331865611403
parsing result	2.221598000689612
c e	2.4249411527169733
language parsers	2.2685844072039183
automatic term	2.1781669520461753
parsing error	2.1543888787937475
dialogue model	2.365643374071497
part contains	2.1168737674978155
semantic framework	2.171232004243547
one layer	2.008226999543247
training data	4.139223512907558
labelled data	2.0314598122182854
model p	2.770697942701959
ten percent	2.0743965972347684
languages involved	2.509868305081658
well understood	2.9618897364005154
f must	2.070902079606615
3 years	2.0865513505522313
sentence analysis	2.789783553096848
following words	2.5822297337370914
valuable feedback	2.305458956322113
entry must	2.022261218861711
deterministic parsing	2.324499520944781
words related	2.426273716557479
original utterance	2.0532484523230123
ordered set	3.015499337466294
multiword expressions	2.4228266928697577
become possible	2.267991541156711
orthographic features	2.1844599860674014
possible sequences	2.598791904624916
experiments discussed	2.11638795219105
seem promising	2.0376102740271858
fundamental issues	2.045521226679158
human operator	2.149311594676423
encode information	2.500962469073721
large lexicon	2.638609132548608
state transducers	3.017736092278959
translation based	2.338978613401074
lexical nodes	2.0687637372443772
bleu metric	2.3640086599266508
three slots	2.034188985362456
cognitive processes	2.558423328073597
sentence structures	2.8520607546267827
contemporary english	2.9385071398824185
potential applications	2.596983776795545
another experiment	2.7150383897200143
lexical meaning	2.6643499566340654
multiple passes	2.0965741974051317
unannotated data	2.0792889102476986
way distinction	2.142755670135066
third sentence	2.545920452966007
generative lexicon	2.3441632378587403
documents whose	2.014014536653706
different components	2.7495256726064956
intervening words	2.193555637917294
automatic means	2.3689838315122613
also section	2.309782368028973
first item	2.5230221380219713
statistical classifier	2.071428330752876
intelligent tutoring	2.4111322642949293
generation architecture	2.0844862373503052
effort would	2.089026915173973
remaining candidates	2.3312450102132822
parser proceeds	2.064890501159733
cannot possibly	2.238945374754718
induction process	2.0042427325657095
order phenomena	2.0721570056738003
system keeps	2.24825378073324
many senses	2.3634279675149354
whose elements	2.6325497540930827
following utterance	2.3986106010450645
one cluster	2.6443619590776932
high school	2.501675707497506
false positives	2.833168000030338
paying attention	2.2427935805269064
resolution mechanism	2.273911614223675
lower bound	3.078612688874539
analysis provided	2.1946340157728974
training material	3.0799997789510836
features like	2.8110531162965327
current corpus	2.2965916473768724
dialog system	2.5039137156466618
since word	2.4659128163621613
arbitrary number	2.921386650445058
generation systems	3.162857787428131
sections describe	2.684723926233512
based unification	2.207234477629015
results provided	2.122246315203648
models based	2.96736929644677
four experiments	2.057207058709099
important question	2.7915888905072834
3 let	2.1499148428335464
set contained	2.590766014468006
introduce three	2.0312646501273113
precision would	2.00238351760398
speci ed	2.313008750559102
procedure based	2.4115483316123623
formal definitions	2.469909103548966
actual state	2.029677424295417
frequency list	2.243422462687156
proof tree	2.029580794876373
data consisting	2.27225588938143
words produced	2.031264650127311
different rules	2.77358570113113
semantic grammars	2.494670401499851
empirical basis	2.1739387095397857
extraction techniques	2.6972629455490407
meaningful way	2.297574746882776
existing grammar	2.1977105156918295
training requires	2.0376102740271858
identification process	2.3037244708679805
model called	2.193056710825906
insufficient information	2.064310474990315
sentential form	2.201208242009483
semantic phenomena	2.4095378916789705
following problem	2.1023887620661283
stochastic approaches	2.1882216921535553
every question	2.2530740722450693
1 presents	3.0151647089197504
primary importance	2.0376102740271858
simple linear	2.502802217719413
difficult cases	2.6308631155129554
simple mapping	2.0656667908183506
larger corpora	2.799764113311746
tool allows	2.125470520333839
local structure	2.0971119680543455
system configurations	2.0774165797489923
ambiguous interpretations	2.0967561837044446
roughly correspond	2.544073863552885
obvious difference	2.0376102740271858
sequence model	2.162640848718925
underlying theory	2.014014536653706
comparable performance	2.5806028878812137
default parameters	2.222124734973158
interesting point	2.3567860333521553
corpus statistics	2.7405559044725094
core set	2.098972949558809
english news	2.3487811004799837
complete analysis	2.7355404257596074
semantic case	2.355347701629528
improvements could	2.1023887620661283
structure grammar	3.307963321996463
10 times	2.7006463978095483
involves computing	2.0376102740271858
g 1	2.047247764865797
two derivations	2.0846971567714894
key elements	2.278461809175595
miller et	3.2084177697347678
useful feature	2.462292105818408
representation must	2.5624041347169113
grammar presented	2.0435975245753424
discriminative model	2.1702891151816877
writing system	2.5549699717015115
various approaches	2.764246188408105
specific requirements	2.33582780054359
given input	3.2060541705316075
one author	2.0659571110450177
inverse document	2.974667403946374
best f	2.5120747860790003
cannot go	2.2112071020073643
dependency representation	2.407333234593554
much data	2.3955781761503503
sense tags	2.471821230869434
svm classifier	2.2982480304685335
common use	2.462292105818408
dialogue translation	2.0604383990463697
different context	2.713828842543993
phonetic context	2.0204872169209556
rule shown	2.178653004950429
phonological rules	2.59696011009446
method consists	2.6010996188684152
function c	2.2100577668648653
subject noun	2.5268039725276688
verb subcategorization	2.481613385672584
system prompts	2.172274429303222
atomic value	2.3622845340613816
specialized domains	2.207020916205373
two descriptions	2.3564987098729633
significant improvements	3.0383638139399896
first level	2.945462005025918
information present	2.7820824566595306
major advantage	2.5327107950191152
ne recognition	2.152933792704732
quality translation	2.498794770907674
resources needed	2.1268231414414376
sufficient number	2.611029671005839
occurs without	2.064139780413935
similar feature	2.1479184330021646
method produces	2.517058807274161
knowledge concerning	2.1946340157728974
context provides	2.109437912434099
bracketed corpus	2.047704087123119
tagger described	2.4260324477975805
acquisition process	2.6746379860669367
syntactic analyzer	2.673979029541558
sequential order	2.28193063938493
entire list	2.109437912434099
th e	2.8240320792971247
every context	2.2660680568573235
general mechanism	2.4251394527998116
classification approach	2.237189563920544
identify relations	2.022261218861711
correct translations	2.6027448127399335
current phrase	2.080418360093704
typical examples	2.68775118838988
infrequent words	2.378684863875976
final testing	2.0577375124403856
intended reading	2.0262602507669065
fair comparison	2.766360819605668
chapter 2	2.3283294780005988
second constraint	2.2658748144574723
embedded clause	2.701679618131002
reasonable performance	2.4005597282692768
ten minutes	2.0866317927056155
level constituent	2.085366556257962
new texts	2.478876998805241
specific communicative	2.022261218861711
model assigns	2.743893477113073
strong relationship	2.012347776929124
separate rule	2.154388878793747
cannot solve	2.0376102740271858
adding words	2.089026915173973
formed sentence	2.3529597784478353
also briefly	2.2328679513998626
commercial systems	2.37897281735096
kipper et	2.078804598561622
absolute improvement	2.2068642602077393
best recall	2.102902855044873
validation process	2.061212299166531
empty set	2.9574639815751826
appropriate target	2.2230472273691797
highest probability	3.203306026464448
system follows	2.045521226679158
therefore provide	2.075729255121515
characteristic feature	2.026260250766906
question would	2.2776740307447065
measure based	2.5567795844238517
important difference	2.9710210314423744
different situations	2.5641954020490867
input set	2.1438550596377937
vocabulary word	2.1356018195482047
specific way	2.515272207357606
given name	2.138678866207396
action verb	2.022353176207058
sentence becomes	2.171178031468078
utterance level	2.148257584208509
main objectives	2.1946340157728974
3 depicts	2.067747107964575
languages share	2.022261218861711
growing body	2.089026915173973
one category	2.931597307775241
inference procedure	2.0850901436644964
estimation problem	2.147283650605927
important knowledge	2.1275552563638707
giving rise	2.7260912977350804
following entries	2.1290482690107417
recognition techniques	2.351204572393724
algorithm relies	2.369115447338463
tree transformation	2.1062224723821283
speech time	2.120161421615255
systematic differences	2.184223228921038
cognitive linguistics	2.037402853550243
breiman et	2.3371330543417326
structure unification	2.0186794915086903
better understanding	2.872260715904297
transformation rules	2.7916494186052185
labeled arcs	2.039007283873737
different knowledge	2.836387698346952
svm classifiers	2.166621205959369
present algorithms	2.00238351760398
training phase	3.0193545434014117
syntactic roles	2.773850828286359
preferred antecedent	2.047333841424571
associations among	2.031264650127311
000 utterances	2.2959583628072817
english speakers	2.8049289581684183
w 2	2.4210241141436493
oxford advanced	2.2594937395335144
morphological analyser	2.622727977825292
individual language	2.146224553829869
different functions	2.543359598838406
null lexical	2.089026915173973
evaluation criterion	2.412952565303204
terms using	2.420621948345764
000 sentences	3.258640492652571
language access	2.1961457982046735
alternative translations	2.1569399962260416
using rules	2.743927565986507
ask questions	2.7589911266503933
e q	2.406472420519478
fewer parameters	2.1606544481704093
contextual constraints	2.3910560646174783
information among	2.116591742190463
major tasks	2.1499148428335464
generation task	2.6488832568142304
create two	2.0545531255831855
property holds	2.1312839727750825
model significantly	2.305097752546449
later point	2.012347776929124
final state	3.054869487741707
several purposes	2.1023887620661283
lexicon size	2.2196155476028725
parsing techniques	2.9957302008334117
retrieval results	2.4794434770784926
main interest	2.2062448606993037
case information	2.4350502254508246
certain situations	2.380830363798011
also notice	2.3723877389097376
du texte	2.08440087075994
two views	2.286777020069459
grant gr	2.458989459396439
facilitate comparison	2.264070934029513
possible types	2.603292282323356
many alignments	2.067494599873277
function whose	2.1051956698941954
words ill	2.0355401813680145
cannot tell	2.332240952226321
use statistics	2.194634015772898
entry consists	2.2823116493074065
based probability	2.143959761128314
noun class	2.0913343837069087
also satisfy	2.075729255121515
type 1	2.3828978564589995
determining word	2.020363647132216
every system	2.135844661161516
identical words	2.258441565292678
language applications	2.8808035410418027
corpus analysis	2.8887135841128737
taxonomic relations	2.2517149257502638
interpolation weights	2.0140062535812753
5 hours	2.5092369869337006
larger project	2.122563170379007
textual documents	2.146224553829869
travel information	2.8496195324399536
certain tasks	2.2776740307447065
simple matter	2.4379491266117213
higher precision	3.002172564321858
automatic text	2.9586008220866233
perform best	2.348359550546241
two objects	2.8602047226557743
system compares	2.1774937848124125
human judgements	2.4563255417361622
complex expressions	2.40097441318663
large portions	2.2112071020073643
irregular forms	2.3125352367853242
related questions	2.1110509596334177
ranked output	2.00238351760398
present work	3.118157148624715
subtree rooted	2.593429861385642
specific relations	2.505914149910196
construct new	2.1479184330021646
words across	2.1499148428335464
structural descriptions	2.713806893125259
accuracy reported	2.0376102740271858
several forms	2.289283904606315
correct ones	2.531813990962308
good recall	2.286725248739412
processing using	2.3097823680289724
text plan	2.3315556598807765
source phrase	2.305720161581171
maximal projection	2.467346962856608
corpus comprises	2.227249320716272
basic properties	2.2427935805269072
sentences tend	2.464240811309989
hybrid approaches	2.1884816114309817
using context	2.8028172854206432
user utterance	2.390252430035124
first experiments	2.5787635525752974
general approach	2.947644290857179
sentence including	2.0275161791419185
empty string	3.0981861208427466
partial parser	2.531344850913896
manual construction	2.371459599282743
initial query	2.298645289712497
major languages	2.0157952878916148
question type	2.3395702116319503
recall figures	2.4753388398307625
word preceding	2.0641397804139356
basic rule	2.2470811344797332
column gives	2.5269400084372706
structure called	2.7249023909513155
generated sentences	2.4696709782394217
specific class	2.3503564505425754
sentence position	2.265864660320843
200 words	2.5936978446613277
training text	2.539301876760249
grammar rules	3.651828948352398
hpsg framework	2.0577375124403856
based measure	2.4230287607367145
phrase translations	2.1179161213946784
one use	2.109437912434099
first group	2.622968171677119
rewrite rule	2.555756637749393
language theory	2.397752482359649
experience shows	2.272505380879168
lexicon used	2.5983758925109557
likely candidates	2.1165917421904625
pronoun resolution	2.7279064331451943
data distribution	2.0215688023747362
reasonable time	2.0236965031377614
leaves us	2.403331244885159
n x	2.6065421646419895
initial sentence	2.2656130929029503
applicable rules	2.24583021875162
check whether	3.2444281864660947
statistical parsing	2.9306560307482585
practical reasons	2.6167442200032394
factual knowledge	2.002072703690581
verbal elements	2.007315006862137
given query	2.5006521609674444
subordinating conjunctions	2.3687797100955486
basic operation	2.3841838643071576
process called	2.018638941929385
corresponding verbs	2.0817120252117016
information captured	2.070902079606615
deep understanding	2.3536762498822874
across categories	2.02585040092793
domain contains	2.045521226679158
semantic component	2.9120812330090775
indirect speech	2.4762594792640726
ordinary text	2.1413469986390323
charniak et	2.4658833515026464
possible categories	2.3770832809855094
produce better	2.51418325787583
expression generation	2.2798246023440516
word contains	2.1946340157728974
rhetorical structure	2.9147941615081714
multiple hypotheses	2.1474148213817243
preferred reading	2.4904935835881434
provides information	2.973685881182054
large texts	2.052352800514945
optimal values	2.344490863391834
names used	2.2020202169645566
available corpora	2.0993881729123993
words would	2.9503702285776314
simple function	2.075729255121515
5 summarizes	2.4282856154916748
sophisticated language	2.1739387095397857
nodes labelled	2.021237938105666
based feature	2.2831175450848797
following information	2.9796335487471586
variables ranging	2.175400407836868
control verbs	2.1851123324453434
comparative evaluation	2.6284213358439152
john likes	2.4229064763635857
clarke et	2.124987174230446
automatic process	2.427081039666146
stuart shieber	2.408973091929181
full discussion	2.3725247035269392
assigning weights	2.022261218861711
produce multiple	2.2823116493074065
representing word	2.24825378073324
using knowledge	2.6324866769978845
briefly discusses	2.1290482690107417
string fill	2.0020426825322555
representation consists	2.122246315203648
several features	2.7785086036735076
inflectional paradigms	2.181633535183382
greatly improves	2.1353034784701492
linguistic point	2.852131945080887
proper subset	2.5681740594952633
whole document	2.205486942333967
examples extracted	2.0460967409737787
many uses	2.089026915173973
ner system	2.258018874508938
users must	2.070902079606615
resulting system	2.977601455719871
dialogue history	2.4255825366092614
provide detailed	2.2147431158325057
case one	2.357121432847829
higher error	2.300001548479347
interesting question	2.7653114932472214
linguistic perspective	2.397957210424912
corpus size	2.9888062983372787
constituent coordination	2.0770550990770102
language must	2.536474754841504
translation experiments	2.1263807586508445
complex interactions	2.291628745249088
empirical evaluation	2.656254072735911
false starts	2.5416735968351523
hpsg grammars	2.2031712330839595
notable exceptions	2.24825378073324
general corpus	2.108503555035412
selectional restriction	2.488508011259575
consistent set	2.242915799916111
syntactic pattern	2.6061173265114466
also form	2.089026915173973
empirical approach	2.110676283984369
seminal work	2.2224068987457315
selected terms	2.0327016490429406
semantics would	2.045521226679158
branching structure	2.4969736956849884
current goal	2.1316128875171234
systems provide	2.3448347819209543
questions concerning	2.18446353633398
one network	2.061212299166531
level 2	2.481661393990733
constraints described	2.172274429303222
major constituent	2.026260250766906
language database	2.3064504123801735
information comes	2.2169936022425736
experimental system	2.6751873752381727
information available	3.1936327243660747
linguistic background	2.102921514525577
complex noun	2.6425660228313275
similar sentences	2.5227301992966065
french verbs	2.0435975245753424
rules r	2.022353176207058
dictionary would	2.1774937848124125
dependency grammars	2.511339312074749
context rather	2.022261218861711
system developers	2.688585095004731
algorithm developed	2.286725248739412
fixed size	2.389326705744299
weighted average	2.3446050692113616
likely sequence	2.293744369358863
query processing	2.0870053874756866
relative size	2.0123477769291243
lexical functional	2.823226194175978
variable x	2.8222433376944314
inheritance mechanism	2.293762623454198
given token	2.1499148428335464
deictic expressions	2.1035510003090367
two training	2.32313543527454
verb complements	2.098924548004499
paper presents	3.9208404935766907
leacock et	2.173933977683106
linguistic representations	2.4626132892880266
statistical parameters	2.018692905410081
text genres	2.405091006633171
subject phrase	2.055118260693857
different operations	2.015795287891615
parser provides	2.2380656204269957
acts like	2.3028207919454977
another domain	2.499540692682933
approach allows	3.007668141926738
constituents may	2.560565026996404
using ibm	2.1816335351833818
words similar	2.020363647132216
occurs frequently	2.5018832127250556
den bosch	2.3475431000249927
probability 1	2.5236658589636742
local maxima	2.352972537169612
models cannot	2.1774937848124125
extent possible	2.2917594692280554
straightforward way	3.0744415603306523
valued features	2.6000836585706146
providing feedback	2.109437912434099
text document	2.476538404917824
various relationships	2.022261218861711
studies show	2.189546252614866
user requests	2.485575882936182
framework provides	2.3005523645852994
ne tagging	2.164430978285169
algorithms discussed	2.1222463152036486
7 illustrates	2.1290482690107417
outside world	2.116591742190463
million entries	2.0074734400897967
limited success	2.2776740307447065
example using	2.318793079863193
two approaches	3.4016624787335967
limited domains	2.558539780893027
clustering results	2.2458516469755923
3 describes	3.387070035647535
morphological component	2.4334450067216076
information states	2.012705537697423
performance difference	2.400016663991315
many names	2.0301435463043473
one term	2.562258291188689
processing technologies	2.2380656204269957
operational semantics	2.030757955920695
following measures	2.045521226679158
mitre corporation	2.233997204181393
bayes model	2.2186527380505887
single verb	2.3810775377823585
one subject	2.4561716453314375
class probabilities	2.008226999543247
word per	2.1774937848124125
neighboring words	2.6407970447173215
memory size	2.21826141526814
major problems	2.505382274560832
computational approaches	2.6608502102657656
possible readings	2.6374260183914835
nominal forms	2.120473489839857
significant results	2.482418251441487
binary rules	2.3878196695557117
des langues	2.07782422651081
following forms	2.462292105818408
specific questions	2.282398402847935
two baselines	2.1390445946327175
compare two	2.9594495484068943
arabic word	2.0156949878082413
constraints imposed	3.028298926798809
similar cases	2.2512557276899194
mean length	2.158071728322869
test materials	2.073066253406054
computer time	2.015795287891615
function k	2.1972748577926797
resulting lexicon	2.0376102740271858
2 contains	2.744652545727583
different experimental	2.0965741974051317
two paragraphs	2.182580240463577
fig 3	2.013792686191998
dependency treebank	2.3787290072919247
extracting sentences	2.178653004950429
current parsing	2.116591742190463
examples using	2.099458264360342
order differences	2.1356018195482047
positive instances	2.2476853234174765
tests whether	2.3589213672090974
human languages	2.518890562903288
attachment site	2.27118595461877
word distance	2.1081259352536437
propbank corpus	2.1806129413887136
behave differently	2.55061890816116
tools like	2.070902079606615
theoretic approach	2.1105162663880916
system thus	2.2062448606993037
whole sentences	2.4039418597775994
cannot find	2.539353767281149
certain problems	2.194634015772898
systems today	2.267991541156711
alphabet e	2.0846971567714903
first argument	3.061887522177209
scale data	2.022261218861711
whose interpretation	2.292453688514974
semantic definition	2.2577963294869763
variable bound	2.0212379381056667
next subsection	2.6868767354008463
idf value	2.177679272627987
four parameters	2.085074160737753
model complexity	2.010935381492455
ith word	2.4104689468985963
research interest	2.089026915173973
sentence depends	2.0312646501273113
optimal set	2.3173820666020832
possible phrases	2.026260250766906
motivation behind	2.4497889724409743
adding information	2.348359550546241
partial analyses	2.4026585363181683
term frequencies	2.3049681357986938
people may	2.1205916259847135
following discourse	2.3051128498496225
relations used	2.63977890112046
strings may	2.045521226679158
classes based	2.5495540016704914
steps involved	2.1588487997915733
specific example	2.2777975658644114
translation equivalent	2.167042679101845
fixed order	2.5260231106536652
tag pairs	2.324820693869714
table contains	2.439868148716185
strong constraints	2.1816335351833818
multilingual information	2.3707970651046377
programming techniques	2.49867050250504
uses word	2.357121432847829
unification succeeds	2.11911577017412
methods applied	2.126823141441437
error correction	2.547817475535316
theoretical analysis	2.3861642926352005
constraints could	2.200598690831077
pentium 4	2.122246315203648
node would	2.093294558436213
tree adjoining	3.1189268907141874
experiment using	2.9336086408084423
pentium iii	2.314007818435079
significant source	2.0312646501273113
social context	2.0355401813680145
function may	2.2062448606993037
alternative strategy	2.285913086717917
tagger based	2.314625473863075
evaluation purposes	2.7263389031324476
average scores	2.3365869267409933
word disambiguation	2.207419624920388
single element	2.532268205068072
noun occurs	2.135844661161516
lisp code	2.067081229293324
add features	2.233997204181393
sentences spoken	2.0520370107650927
please note	2.4029015502540476
based algorithms	2.6176272211958302
grammatical category	2.8408121034341125
contains features	2.1413469986390323
quantitative information	2.0763533583416773
semantic differences	2.3589213672090974
four rules	2.348359550546241
anonymous acl	2.022261218861711
usually contains	2.1119430342456553
partial representation	2.022261218861711
previous iteration	2.4196519231039106
step 2	3.467866126000866
also express	2.2328679513998626
write grammars	2.075729255121515
terms within	2.4594105655609226
different phases	2.049069748176902
new edge	2.5190259773112103
complete semantic	2.4406532125653317
whose nodes	2.7483137516741123
system results	2.4041807501397283
entropy framework	2.3407666244002954
errors found	2.164235940665368
tables 6	2.168967603804668
following features	3.185981260563085
every rule	2.6202209067624005
reasonable amount	2.542316220970792
l w	2.014014536653706
characters per	2.3008906990628013
appelt et	2.233389600634541
experiments also	2.3271110475429713
simple rule	2.7864586592991416
automated system	2.3703165397459225
access information	2.3886619127043263
likelihood value	2.0048870107466215
original set	2.5202660926327476
initial stage	2.473704113933601
particular state	2.2057998196224617
head node	2.1603831677519683
phrasal nodes	2.030143546304348
central importance	2.1163879521910505
general concept	2.496054346478372
semantic definitions	2.0312646501273113
mean reciprocal	2.3583993570021478
ordered list	3.1017376342694205
information useful	2.3005523645852994
relative position	2.96728890494727
one segment	2.5191748617496708
words might	2.4459101490553152
single document	2.752386201071243
generated data	2.060145782650642
involves several	2.2328679513998626
right child	2.0720053006339114
l v	2.0361161293026124
similar situation	2.3763414719464064
three distinct	2.842192818156045
rules shown	2.085074160737753
specific task	2.9075105011741442
rough measure	2.070902079606615
html tags	2.3355058981341865
based work	2.2246290241237316
two pairs	2.748744950179395
extraction problem	2.2518949585195713
useful results	2.3271110475429713
syntactic formalism	2.096756183704444
agents may	2.124563704437371
nonterminal label	2.1617301394828186
underlying concepts	2.11911577017412
class 2	2.157354466172502
small subset	2.9405062316334987
final interpretation	2.176180166955999
conversion rules	2.094403954558685
text genre	2.1271397098359177
noun form	2.190982236626934
score using	2.305458956322113
processing within	2.0880414073297953
equal weights	2.0083588580543497
desired number	2.0973890129090687
following set	2.958145752936213
actual lexical	2.060145782650642
structure trees	2.8665467805786276
default rule	2.4843222368377953
c contains	2.022261218861711
one head	2.311448961688866
lexical model	2.0718908975344856
parses using	2.0074734400897967
henceforth referred	2.0817120252117016
corpora like	2.067747107964575
special symbols	2.690531559432191
intransitive verb	2.809293035269535
yields two	2.109437912434099
complete sentence	2.968114204195066
rules include	2.1638349431134714
abstract specification	2.014014536653706
language consists	2.2577963294869763
dashed line	2.5147059451889393
present examples	2.0817120252117016
information transfer	2.022410241232288
form two	2.0817120252117016
using probabilities	2.1414753566437956
great importance	2.4659128163621613
section discusses	2.822116550974692
avoid overfitting	2.3953272468316316
future experiments	2.552818450979276
main criteria	2.045521226679158
occurring texts	2.1752955788618014
monolingual corpora	2.251124644217322
similar results	3.1492370352579426
system proposed	2.397957210424912
general syntactic	2.6305673755421757
term translation	2.1275191846384596
subject argument	2.085074160737753
great deal	3.363589659489473
corpora automatically	2.014014536653706
practical utility	2.105195669894195
offers several	2.21826141526814
extensive training	2.089026915173973
par une	2.140750409429046
candidate words	2.5437087015987707
name list	2.223363030600837
overall architecture	2.81592773686255
let p	2.8568925279970134
several candidates	2.178914869102539
overall quality	2.4672805896862267
approach relies	2.638005597226323
initial attempt	2.0824927688889554
terminal node	2.9159914586610682
also reveals	2.183647914993236
figure 2a	2.305097418252439
words w1	2.0880414073297953
japanese expressions	2.0676325559456012
space complexity	2.3697972212010594
popular method	2.167885403222368
grained information	2.166102255087602
query term	2.5391123357112972
learning curve	2.7247040256397517
word c	2.2812468165198494
2 outlines	2.1479184330021646
entities like	2.0965741974051317
grammar provides	2.3956433384568747
small corpus	2.99043838279948
two finite	2.297574746882776
system accepts	2.305097752546449
baseline algorithm	2.1520038502811802
learning results	2.1798605341165187
theorem 2	2.58701194738702
phrases containing	2.5239297192242653
two extreme	2.166102255087602
conceptual hierarchy	2.318266515743697
level grammar	2.0872227910691654
existing algorithms	2.384690226067526
corresponding concept	2.2156845351889993
isolated words	2.3216422601919873
many discussions	2.067747107964575
functional uncertainty	2.011773012573135
systems often	2.497204680213419
representation structure	2.496094250291359
dorr et	2.0133469012571874
discourse semantics	2.089080486128487
quantitative measures	2.026260250766906
3 system	2.11734457737815
concatenation operator	2.012347776929124
governing category	2.154849769725224
different texts	2.509984657951576
electronic documents	2.067747107964575
information includes	2.305097752546449
output layer	2.202121455206453
attachment disambiguation	2.288203432120161
three lines	2.2073697906566263
e system	2.514628276488317
richer information	2.1513738263092224
word sentences	2.1752039892016404
frame structures	2.185216671905355
alignment based	2.243556336294379
driven generation	2.083315385486103
system also	3.160256770857976
require human	2.026260250766906
levenshtein distance	2.2186592203958067
segmentation system	2.4818812607129623
unit may	2.0376102740271858
also increases	2.145308387826936
support vectors	2.143659059576684
numeric value	2.103044613514411
following conditions	3.126545826307822
acoustic cues	2.078214031867156
related fields	2.200598690831077
application rules	2.039679083202998
available resources	2.2874806129846443
probability according	2.0965741974051317
de souza	2.1191668933749863
correct solution	2.338056104888859
empirical observation	2.159746288077376
higher nodes	2.1432314806484003
state automata	3.005345333306003
kappa coefficient	2.28815249741435
base system	2.480345903665896
also influence	2.089026915173973
basic components	2.149325824574177
query may	2.233997204181393
one tag	2.6918319927864105
based processing	2.5461486967024816
argument relations	2.711687835447479
linguistic categories	2.4062583298462226
r r	2.2092750740069844
system may	3.3470970676795324
single topic	2.3161212503528343
larger unit	2.026260250766906
constraint language	2.204358512002031
language interpretation	2.436589070143762
systematic errors	2.070902079606615
technique called	2.5412933136480826
context surrounding	2.530028728978662
text segmentation	2.517156380624426
figure 2	4.519936129162525
percent correct	2.346400474299738
semantic class	3.1664668680413492
see later	2.243872796595318
speech input	2.9340727781988982
following standard	2.2776740307447065
different behavior	2.1413469986390323
system response	2.6115720059598253
model thus	2.060145782650642
text extraction	2.2284015242645268
nlp research	2.689812643496151
active chart	2.401931365448987
feature representations	2.2549007478117185
first describes	2.109437912434099
human communication	2.7218598711241047
another component	2.292453688514974
system requires	2.8677199262863944
many respects	2.367987153949678
first child	2.085074160737753
existing work	2.5360948406750348
called word	2.4005597282692768
algorithms perform	2.075729255121515
spontaneous speech	3.057867579568086
significant improvement	3.1119530111932767
single sentences	2.787226802845304
central issue	2.2532512925302806
find examples	2.2640709340295135
confidence values	2.202222511509073
following algorithm	2.8332523836811925
paper explores	2.6951250945660497
scoring algorithm	2.169900017089878
unary rule	2.0203719243732428
document indexing	2.1511473879150715
righthand side	2.1103331657007582
general goal	2.072290098419245
include features	2.4251394527998116
time period	2.3668810736095653
step towards	2.847644422821076
information regarding	2.836105701873675
english verbs	2.7755738060812027
pairs used	2.026260250766906
passive construction	2.3390242091811286
new module	2.072005300633911
basic rules	2.2648628582651726
data since	2.1290482690107417
avoid redundancy	2.020363647132216
one pair	2.684029077726777
show examples	2.3483606540998085
application context	2.031264650127311
current character	2.0204096769896003
hence cannot	2.183647914993236
work aims	2.227249320716272
np complement	2.1696551173307155
last type	2.0601457826506424
using pos	2.317933477151697
confidence measures	2.208964802119865
discourse marker	2.246031934268692
psycholinguistic experiments	2.291251212917696
complexity results	2.069542139177546
extracted sentences	2.528990729545344
new verb	2.15314724978459
least number	2.2459622058213182
parsing problem	2.7031390258727512
approaches used	2.286725248739412
alternative solution	2.1023887620661283
specific application	2.7905464670877365
empty list	2.474699001344257
shows three	2.3448347819209543
level patterns	2.099387746278729
based taggers	2.085074160737753
algorithm could	2.7361786370688224
relevance judgments	2.4422826286115615
word whose	2.6094405876052553
therefore define	2.014014536653706
structures using	2.2427935805269072
system asks	2.4013350794450403
linear kernel	2.3784764101297533
lexical translation	2.346155323341468
rules generated	2.284277466590818
level goals	2.0860645961696633
ten words	2.4197547133270083
model results	2.0512856384022773
mentioned entities	2.060145782650642
certain word	2.719098293806916
combining multiple	2.5219739981168368
appropriate number	2.45078770424615
problem seems	2.166102255087602
following principles	2.411548331612362
one correspondence	3.011815430318466
using co	2.4249248189826718
phrases whose	2.3254539875119606
system detects	2.1588487997915733
string must	2.0074734400897967
enough detail	2.067747107964575
constraints based	2.0833196719432348
central problem	2.568611116983589
missing information	2.8484943732558783
extracting data	2.079019887609717
popular approach	2.1290482690107417
categories using	2.0601457826506424
information associated	3.059113374031458
three points	2.065291326064264
available online	2.4089730919291816
computational resources	2.5964571960559564
words derived	2.221882755961746
japanese noun	2.1947507319480275
whole data	2.3503564505425754
female speakers	2.249038330800002
utterances must	2.1023887620661283
one found	2.0157952878916148
training methods	2.500143640672319
2 c	2.3633976741456193
broad classes	2.285913086717917
satisfaction problem	2.3138839702959846
describe different	2.0376102740271858
tile target	2.2368067747493123
full lexical	2.042481049268214
sentence 4	2.3322618602644836
syntactic evidence	2.0490697481769016
word segmentation	3.0906705642709795
internal nodes	2.7900962753879877
query terms	2.751205768646323
scale text	2.1119430342456558
que les	2.347077643354848
text length	2.249405247176906
subsequent sections	2.584325141004728
main verbs	2.722004824368006
output sentences	2.4359041476270598
correlation coefficients	2.1529961686684675
formalism called	2.2577963294869763
important consideration	2.3567860333521553
alternative way	2.431764288227561
second instance	2.108212199944729
search procedures	2.1245510841937274
action may	2.135844661161516
previous word	2.859460335894981
mt systems	3.036008258684268
like np	2.0296774242954174
table containing	2.282311649307406
grammatical subject	2.140856954368349
dans le	2.485940034574008
relationship exists	2.1023887620661283
clause contains	2.2577963294869763
cannot simply	2.5648350055481135
different predicates	2.089026915173973
achieve comparable	2.0817120252117016
two tree	2.2243484856120617
second experiment	3.136962171340626
new element	2.318932106792177
one process	2.0390072838737376
detect whether	2.014014536653706
conditional independence	2.422528920046843
complex event	2.1518980086114476
mechanical translation	2.0562181805799282
relative performance	2.704789714535448
general parsing	2.34559161373679
noun hierarchy	2.1132908618613198
final structure	2.280293839842197
comprehensive set	2.238945374754718
estimation algorithm	2.3460644979329164
related language	2.082492768888955
two speakers	2.4283991429102727
global coherence	2.0641800801970285
classification accuracy	2.9586574640807894
researchers working	2.272505380879168
provide clues	2.242793580526907
feature indicates	2.159968128908506
introduce errors	2.022261218861711
straightforward task	2.0790198876097175
two items	2.7832743690258246
going work	2.178653004950429
theoretic semantics	2.426241537079238
definition allows	2.159746288077376
model proposed	2.796453051694816
inference mechanism	2.446805733312611
various criteria	2.0709020796066153
data alone	2.0074734400897967
work supported	2.8566134954362665
retrieval research	2.1042904425091606
hereafter referred	2.0918364857418936
case letter	2.118274250141516
abstracts away	2.3841838643071576
possible actions	2.355695717315683
tags per	2.2477960360350306
situation described	2.4204557247244143
optimality theory	2.0721429359686754
value structures	2.0161455458532545
specific objects	2.0362289675655436
basic tasks	2.022261218861711
bold font	2.045521226679158
actual words	2.7004337407531995
address two	2.2147431158325057
correct referent	2.0717717317588495
text data	2.8415736465086843
best feature	2.167538546921908
new document	2.5219785041784415
certain predicates	2.026260250766906
character sets	2.3307879207089446
event described	2.3746893983053257
one chunk	2.059313444496332
across documents	2.4334154285347256
cannot always	2.690653142973085
basic functions	2.079019887609717
user clicks	2.3332911335775464
subsequent steps	2.1222463152036486
instance x	2.2184312776515194
crucial property	2.045521226679158
trees obtained	2.1184726740880686
3 illustrates	3.041480199515029
model also	2.9169075039432903
4 sentences	2.026260250766906
centering theory	2.5934608468508995
makes sense	2.9342594894242597
context contains	2.2328679513998626
generated text	2.7622633899101565
active voice	2.763992833901043
includes words	2.067081229293324
distinct sets	2.0376102740271858
small clusters	2.017382421719911
speech generation	2.1150266819949994
expected frequencies	2.10700704470221
system attempts	2.5816299942389747
descriptions like	2.0763533583416773
12 shows	2.318793079863193
model without	2.598732913405275
argument structures	3.038174398434992
standard context	2.3435066063864056
sentences 1	2.1659825136431445
segment boundary	2.437326281616204
second utterance	2.381205263972201
existing methods	2.745609524435023
figures 4	2.690737937308404
based search	2.696332079117691
actions may	2.257281395364781
probability pr	2.481506298390026
following constraints	2.6650202980659388
model may	2.8634201299121482
verb may	2.6838751372688634
data contains	2.731529403721768
document similarity	2.142391964006801
third method	2.2060951327739398
filling task	2.1273754762918986
whose words	2.0539861379893036
reduced form	2.1223332960158245
scale nlp	2.042325972844755
capital letters	2.9129696532027554
single user	2.174963595562789
wer e	2.1910254601003247
parser output	2.75295921839013
text material	2.106698693209063
tile noun	2.0074734400897967
good measure	2.050145631954837
schulte im	2.1829249682847367
speaker intended	2.008226999543247
first system	2.4041807501397283
rule expansion	2.0355401813680145
model requires	2.632544970993565
human judgment	2.4856458731359212
c c	2.6574738652776455
better solution	2.472067708086249
better language	2.1535520277586104
support verbs	2.113962251503441
th element	2.134777754440562
speaker would	2.476869153688929
text without	2.7238929950144453
new scheme	2.0376102740271858
general types	2.3097823680289724
learning phase	2.3640769921710527
small number	3.8604163709652797
2 word	2.1413469986390323
lexical features	3.127443532317913
logic grammars	2.092390212119108
human behavior	2.2859679116760914
formal language	3.01209339485614
text may	2.842473005134121
per minute	2.3120029095786143
character strings	2.7725180888447603
document sets	2.29599180186404
short example	2.0312646501273113
new annotation	2.15407145247256
e f	2.685240256179226
probabilistic information	2.312928660106193
quality machine	2.0577375124403856
user said	2.010311500858228
heuristic method	2.231505672118325
cannot decide	2.314007818435079
frequent ones	2.2470811344797337
document analysis	2.1419507693322526
two sub	2.9040755984503996
input form	2.2081915007441992
sample tasks	2.1410534925341533
give examples	2.671035066389726
filled templates	2.0499885992847338
systems described	2.642081940411366
accurate translation	2.260688804545853
statistical distribution	2.0763533583416773
domain data	2.192023405214173
algorithms proposed	2.053986137989304
following procedures	2.089026915173973
say anything	2.55061890816116
text requires	2.0965741974051317
structural relation	2.052542639930625
allan et	2.239186747028712
pages 9	2.045521226679158
equations 1	2.1117654159915116
4 cases	2.1774937848124125
label sequence	2.2618797694789623
operational system	2.2172698987610535
words found	2.8569447076591534
good feature	2.1413469986390323
relative weight	2.399822042602986
high proportion	2.0711789464590007
powerful mechanism	2.159746288077376
figure lb	2.1275552563638707
intuitive understanding	2.1499148428335464
nlp methods	2.278968473400518
query vector	2.056940462445696
japanese corpus	2.3269936357438006
final experiment	2.227249320716272
different goals	2.114299018453181
third possibility	2.0539861379893036
candidate answer	2.224136422426099
potential problem	2.222211947202819
3 e	2.1741194313061794
nodes corresponding	2.560659941618245
level units	2.0709020796066153
expected answer	2.3298898791667186
specific nature	2.1943253913209606
e e	2.753876976666271
english phrase	2.4699314986877585
free parameters	2.5231493252988955
polynomial time	2.841694746981668
uniform prior	2.1379566368671767
implementation language	2.067747107964575
new topic	2.5348005186563958
supervised algorithm	2.1379566368671767
new kinds	2.154388878793747
another way	3.2095830471499083
messag e	2.0123477769291243
effective approach	2.3763414719464064
experiments carried	2.4150620271059764
algorithm runs	2.525808069723155
source phrases	2.0050075131793266
personal communication	2.919519878573186
important points	2.0755911374995546
key words	2.721264956831655
retrieved documents	2.7802982149478157
approaches require	2.417618427399807
speech corpora	2.442158508354121
monolingual dictionaries	2.1839550525047655
special characters	2.5263284910311548
practical problem	2.3161212503528343
pipeline architecture	2.377811827991964
background knowledge	2.794891342254467
research directions	2.408973091929181
free productions	2.049069748176902
translation knowledge	2.1856251726963523
major goals	2.0376102740271858
morphological analysis	3.5774324993968567
bootstrapping techniques	2.022410241232288
different points	2.6723975919703404
two candidate	2.4013570437461724
always correspond	2.3644803559732956
human annotator	2.6062518530587875
parsing information	2.1048578709586345
set includes	2.630935380649072
computational linguistic	2.8280201818028323
various corpora	2.2380656204269957
new sentences	2.587492941365563
incorporate various	2.014014536653706
type described	2.2823116493074065
accuracy rate	2.6364039426111185
question must	2.227249320716272
many models	2.0817120252117016
position j	2.5549865746827387
value 0	2.485737441968742
algorithm based	3.1836300743557957
single data	2.102921514525577
simple rules	2.9349475564174226
problem domain	2.3016763138447183
heavy use	2.3594778333373685
system treats	2.0539861379893036
dependency relations	3.1120568681363925
inter alia	2.756710345494716
act like	2.2823116493074065
improve system	2.3489571078839537
tagging errors	2.779326529083991
equivalence relation	2.7069972853156385
one direction	2.392203246999948
correct analysis	2.854824292804803
one step	3.0106062658926267
certain patterns	2.189546252614866
translation system	3.6754072253705594
pilot experiments	2.194237407681644
following representation	2.200661481121184
null first	2.438730573987262
structure might	2.1290482690107417
using bleu	2.0641397804139356
becomes easier	2.1290482690107417
general categories	2.4106910592380197
second person	2.6098814236875194
indicator function	2.4212861489765967
human user	2.6421637537838425
5 illustrates	2.6402601319532195
pos tagging	3.1431384018716066
entries containing	2.093294558436213
contain three	2.0376102740271858
word without	2.458989459396439
domain may	2.4103259885026898
natural science	2.2685844072039183
database containing	2.573822910996244
programming problem	2.075729255121515
acoustic modeling	2.4663505321788475
without word	2.2520605189855187
current speech	2.476530621448099
many others	3.035084962965931
proposed algorithm	2.637481312571971
analysis presented	2.369115447338463
correct structures	2.045229961808099
hierarchical organization	2.583770939354418
word may	3.2296130813612867
extracted data	2.1135215448445868
word patterns	2.498968221430167
hold across	2.085074160737753
large document	2.3362105897148213
algorithm involves	2.109437912434099
visual inspection	2.061212299166531
semantic errors	2.0774165797489923
results presented	3.1624936078043167
ones used	2.6215773021747437
one branch	2.278461809175595
following equations	2.332240952226321
nlg community	2.010845255290386
third approach	2.4920505435324705
weight vectors	2.1005272030344457
also examine	2.417618427399807
syntactic constraint	2.3530353849903065
one result	2.320083567690979
price et	2.0023835176039797
individual sentence	2.2427935805269072
xinhua news	2.420231466512533
relative pronouns	2.8214600060406596
semantic form	2.4226911848755153
wider variety	2.54956269079704
discourse context	3.091154057317499
another factor	2.1670955959087337
great care	2.2020202169645575
functional tags	2.133682513579842
complete tree	2.349235192224533
based parsers	2.664040512514886
many combinations	2.0376102740271858
word belongs	2.7179009042789932
independent test	2.404497833338496
single parse	2.5457482169722416
larger phrases	2.016336526582215
answering system	3.0158199028724857
reasonable degree	2.022261218861711
starting point	3.5399646952429085
systems could	2.686629074867625
null set	2.1023887620661283
speech tagging	3.5880090204848623
system shows	2.485841069839246
alternative sets	2.0521277112405505
new cases	2.042308826087027
syntactic models	2.0224102412322886
journal reference	2.305458956322113
better ways	2.0376102740271858
problems connected	2.014014536653706
hybrid method	2.1237553293864644
information systems	2.3247181131461776
01 level	2.1399461455651663
basic operations	2.3125485940591535
alternative analysis	2.2112071020073643
people would	2.028632806380823
enough knowledge	2.318793079863193
compares two	2.045521226679158
structures within	2.3100369696113856
input x	2.268817221010057
parent category	2.0076190003176295
among constituents	2.122246315203648
explicit information	2.501400582953832
early days	2.189546252614866
next experiment	2.159746288077376
similar process	2.2776740307447065
soon et	2.155661376885809
another class	2.6584018814021215
problem remains	2.3567860333521553
procedures used	2.314007818435079
whole text	2.7405092906015875
cant improvement	2.114299018453181
lexical structure	2.465714231538704
time spent	2.8163572422828063
jones et	2.1512812587396084
two vectors	2.7698405306703675
linguistic rule	2.082492768888955
immediately precede	2.049069748176902
different subcategorization	2.122246315203648
current rule	2.175409328526317
function would	2.1222463152036486
gender information	2.016732165904002
emission probabilities	2.0399939430120826
retrieval methods	2.335904293904669
alternative structures	2.0376102740271858
system makes	2.9772230592835456
basic problems	2.1290482690107417
using p	2.2511263496415284
useful applications	2.039007283873737
strong preference	2.4847053064828524
rule 3	2.5827715183613593
whose entries	2.019557844009582
particular sense	2.65008916131196
theoretical approaches	2.0388643487004234
categories within	2.059213640580528
garden path	2.3900816084965104
extracted patterns	2.015759442681759
per verb	2.2188080218191453
sentences selected	2.55109392182541
chinese translation	2.4151366479124494
detailed results	2.4432280019602066
english syntactic	2.199800839873624
one drawback	2.1222463152036486
one difficulty	2.227249320716272
rules governing	2.152793734451773
word occurred	2.166102255087602
mental representation	2.1226162390904557
operators used	2.009868063569498
two graphs	2.29720043864321
translation lexicon	2.3593122094672756
atomic units	2.067081229293324
discourse knowledge	2.033925507981963
traditional approaches	2.6336230406055834
backoff model	2.048990331461348
know exactly	2.2062448606993037
art systems	2.542639595744184
pronominal references	2.084250583899339
used machine	2.1946340157728974
news stories	2.9845943121241776
process may	2.794800753550539
verb pair	2.1527910912671047
study reported	2.185240400682336
underlying model	2.418153424291798
linguistic realization	2.514769791841908
semantic descriptions	2.474824617093869
noun pairs	2.450064558199527
acquisition problem	2.067747107964575
different probability	2.116591742190463
constraints cannot	2.172274429303222
elaboration relation	2.128390575123076
sentences according	2.6151579974838857
many translation	2.031264650127311
phrase analysis	2.0844428284589274
feature function	2.627700380140897
operations used	2.109437912434099
proposition p	2.4705375893856103
words must	2.6868743934430332
6 presents	2.7131709555326893
lexical coverage	2.4319353477982286
approach lies	2.166102255087602
language usage	2.4353865657576463
given target	2.4984531311683433
column 1	2.108179066756418
two criteria	2.7930933503169006
high frequencies	2.0483937955874474
certain kind	2.582526756791085
p r	2.141182714575442
speaker might	2.2775956773316364
errors introduced	2.534553053532142
group words	2.082836083424519
travel domain	2.1768318073628796
relational database	2.9251510713203035
analysis proceeds	2.3162726447741377
phrase structures	2.5730159803644317
possible rules	2.398762557235977
null string	2.20379976615122
following list	2.617053252298629
sparseness problems	2.3031542996401573
evaluation efforts	2.039007283873737
brevity penalty	2.146942921963902
processing mechanisms	2.178653004950429
newspaper articles	3.2730005931298822
languages generated	2.1966142039329544
existing parsers	2.0074734400897967
method first	2.257869825585851
generate summaries	2.248968785663327
syntactic surface	2.045521226679158
last clause	2.0074734400897967
observed frequency	2.3003244757708927
particular characteristics	2.1023887620661283
simpler model	2.0355401813680145
dependency graph	2.470561881127077
systems rely	2.6262393868602247
three subsets	2.099458264360342
also permits	2.272505380879168
help determine	2.408530211693359
texts using	2.6966792985775427
computer manuals	2.4116509519807785
objects may	2.504497044060369
context c	2.5522988195221368
better quality	2.027513691901601
dependency analysis	2.5743588652244056
used information	2.200598690831077
trigram probabilities	2.151205793921028
comparable corpus	2.0146635347221
special rule	2.1513738263092224
certain probability	2.075729255121515
different choices	2.4511780301711426
modular architecture	2.463844871864012
string concatenation	2.12319209177683
assisted translation	2.2697646091331602
conceptual relation	2.1949866785080125
measures based	2.453169315282728
every day	2.1941988162218147
100 words	2.8225682754835133
research project	3.058037481437329
nlp researchers	2.4507877042461494
search errors	2.189406820852234
necessary knowledge	2.0539861379893036
sentence basis	2.321839114388654
hybrid approach	2.7225591704045833
reasoning system	2.408982961146827
known fact	2.3666950530785096
physical object	2.7646082271117858
head h	2.128149293488139
dependency structures	2.9499854273917077
revised version	2.3279228643392806
language technologies	2.5899131591461395
several advantages	2.98527075357173
potential answer	2.1569808507858435
nearest neighbors	2.6057355843854135
character recognition	2.6406857930207597
de cette	2.2268204074080966
new results	2.233997204181393
extraction algorithm	2.6231746559202196
topic shift	2.3716600086774657
structure derived	2.11911577017412
overall performance	3.40986623924451
includes information	2.5922893931369995
syntactic dependency	2.8046325119098605
performance slightly	2.022261218861711
limiting factor	2.3676013596092336
per document	2.4127813119664063
possible context	2.014014536653706
right side	2.9824550609445697
common names	2.0460967409737787
free parameter	2.0423259728447545
different feature	3.042972704384602
one terminal	2.3689838315122618
anaphoric expression	2.590901251565044
different verb	2.54102799002033
analysis tools	2.5694973757931927
one end	2.3449410028607693
separate word	2.223453205129625
text produced	2.3239633389437593
words following	2.3661314342895565
time consuming	3.2295565390910217
commercial mt	2.06468203219145
paper concentrates	2.2776740307447065
effort involved	2.4358964112583816
performing model	2.147342467231958
word dictionary	2.5304835018042473
system differs	2.089026915173973
one part	2.901659807553581
integrating speech	2.0390072838737376
oriented dialogue	2.4426463606496753
core language	2.638775697339262
grammatical theory	2.3736634724511023
efficient means	2.0965741974051317
results reported	3.2254663646240704
science research	2.042325972844755
major feature	2.0312646501273113
c k	2.1220415161024233
restrictions may	2.0817120252117016
four distinct	2.3879279742060486
different configurations	2.370774968050012
14 shows	2.089026915173973
john smith	2.3615838784631444
annotated texts	2.4013535198919955
english bilingual	2.218456451693668
semantic hierarchy	2.5627762854667036
right word	2.251094232024288
logical consequence	2.06468203219145
phrase occurs	2.232060530757816
new source	2.140860171621032
size k	2.2103499632097234
computational mechanism	2.172274429303222
e g	2.5180883135202707
novel method	2.6845253293269034
context sensitivity	2.2147431158325057
singular value	2.674864982704357
lexical constraints	2.493551776383244
work carried	2.272505380879168
model shows	2.3028207919454977
words occur	2.8224690426302446
probable word	2.0329822559918163
mcdonald et	2.2321355136014205
three sentences	2.9010812094648704
perform significantly	2.402171276846661
second case	3.254846647471482
large vocabularies	2.4175764205816344
given noun	2.4331804291413484
test bed	2.427222503957905
new data	3.1386166141116734
experiments use	2.3435066063864065
default inheritance	2.0651981239592354
semantic fields	2.259441695304971
actual values	2.3741226029687725
tag derivation	2.0215520649144736
certain feature	2.327403995660175
likely path	2.0919612108326557
animate subject	2.137726762436956
earlier ones	2.1222463152036486
parameter estimation	3.112850760868342
iterative algorithm	2.6101582290587513
second column	3.1459266309102145
develop tools	2.031264650127311
real life	2.5188916201883385
agent role	2.278289831547166
joint research	2.0312646501273113
using decision	2.4545095672192527
last line	2.6303674549577587
rule learning	2.45662704973303
performs well	2.9570511563724797
another example	3.3824825979714515
particular instance	2.6629007994909957
nodes along	2.0539861379893036
english equivalents	2.2208966233478122
uniquely determine	2.2730174657027686
present tense	3.02351227767026
ambiguity problems	2.3332911335775464
filled pauses	2.4333318698326494
understanding system	3.3359273616981047
single text	2.4618418329489877
model makes	2.755551005033357
two non	2.790578923046004
evaluation framework	2.230820055126314
coverage parser	2.4160892086765857
xml files	2.0312319370715612
information used	3.02789698806927
transfer approach	2.270163221027329
string positions	2.109598840473198
still contains	2.2776740307447065
complex system	2.3644803559732956
dominance relations	2.368613374391658
subcategorisation frames	2.0335269376009846
processin g	2.045229961808099
english spelling	2.0206210212150983
key information	2.178914869102539
retrieve information	2.221537802998995
solving process	2.0013487161900905
indefinite article	2.505931930245143
based applications	2.4619447088476862
rapid prototyping	2.269227013456659
matches one	2.2020202169645566
two corpora	3.0297654233688704
practical purposes	2.38378820738436
features except	2.1240946476680165
word meanings	2.8640701875012504
applications including	2.2776740307447065
every element	2.563534049420018
whole test	2.11911577017412
value depends	2.015795287891615
input language	2.493605844942819
analysis tasks	2.0918364857418936
improve recall	2.4089730919291816
summarization systems	2.676701388822208
human evaluators	2.3093427512052402
accuracy significantly	2.016336526582215
original form	2.591076797546006
language models	3.5296043610832015
variable name	2.1739387095397857
best results	3.5633001349804463
node must	2.5350552017109504
choices made	2.7156890376184353
language model	3.7049449061491115
small difference	2.2328679513998626
two verbs	2.7429207728288234
pronominal anaphora	2.4509849957592116
new material	2.417618427399807
basic issues	2.0709020796066153
whose values	2.6980760280114633
correct transcription	2.0281097831836057
documents written	2.2188080218191453
make clear	2.3264567201926063
following ones	2.159746288077376
accuracy measure	2.171178031468078
short paper	2.1499148428335464
block diagram	2.444782663571849
source documents	2.387910049248233
rules applied	2.560565026996404
three cases	3.042845222930462
across languages	3.1304043530364747
system similar	2.166102255087602
correct recognition	2.266068056857324
speech technology	2.245060397055863
low entropy	2.020121462798538
particular words	2.6939253609787817
significant progress	2.0236965031377614
sentences often	2.022261218861711
relative ease	2.296234813805337
probabilistic parser	2.249467431079962
school students	2.019253224347238
planning stage	2.1677847630386546
considerable interest	2.4755429606947192
bottom left	2.108179066756418
wide variation	2.1485183747988197
different document	2.357121432847829
idea underlying	2.240851989911108
unique identifiers	2.1331653459774356
basic word	2.396505054175619
compositional semantics	2.626746873967036
one meaning	2.523671262016431
journal treebank	2.2430705160770392
following dialogue	2.3438378306650227
original order	2.183647914993236
deep analysis	2.5045474050024823
every iteration	2.114299018453181
parsing algorithm	3.4283783070510205
level processing	2.4977894579821367
added benefit	2.109437912434099
algorithm cannot	2.512402919153197
based theory	2.356443881178913
another task	2.183647914993236
semantic nature	2.3005523645852994
full information	2.0074734400897967
tile number	2.5299279475474306
shortest path	2.6204822138031925
figure 16	2.3874173854389507
nouns denoting	2.084842367669155
low frequencies	2.4469092116851296
one speaker	2.5942538932366697
incomplete information	2.2492578335740094
e p	2.507608411778873
extreme case	2.612534244730822
order language	2.8013607152285926
search strategy	2.944117902818935
verb senses	2.650124953645893
deep structures	2.2040987217101766
output text	2.6292230384019404
shows example	2.0376102740271858
based tagger	2.5264221687989967
information including	2.286725248739412
wider context	2.5073843372902678
particular utterance	2.3922788066689504
requires information	2.1413469986390323
results concerning	2.376717136584321
correct answers	2.9292281465508716
available corpus	2.3483606540998085
definition given	2.406719372930616
syntactic relationship	2.4703908722255754
scores using	2.2294991808452247
function tags	2.0323461096456525
distance information	2.152105323847251
manual annotation	3.0308966515005196
computational approach	2.4602434646053126
normalizing factor	2.1764275761209526
top n	3.000321343835553
limited context	2.3734276798593146
form c	2.2186044279222794
single variable	2.108179066756418
errors using	2.060145782650642
1 reports	2.109437912434099
general properties	2.485049614866351
number agreement	2.83707162670596
tags may	2.2169936022425736
association scores	2.058964472578879
state grammar	2.465454427663069
surface features	2.1490370721731393
means algorithm	2.337869311627852
specific relation	2.27225588938143
competing hypotheses	2.1557946417682414
following advantages	2.500962469073721
technical details	2.201992232705235
automatic morphological	2.072290098419245
american journal	2.3978008142431166
node corresponds	2.3605881269336804
major topics	2.0376102740271858
tables 5	2.54206261776359
sections 2	3.130021210897373
several extensions	2.2328679513998626
300 sentences	2.3147147578651737
general rule	2.923561849079413
original english	2.5935882737216653
recursive procedure	2.060145782650642
specific items	2.0824927688889554
figure 3a	2.0710504248557267
initial corpus	2.144048274836234
disambiguation rules	2.2531790163012326
participants could	2.0721570056738003
right hand	3.241494018593621
different annotation	2.3027611307787756
word classification	2.2512106881734706
another form	2.314007818435079
good results	3.2178207421825937
presents three	2.049069748176902
virtual machine	2.1540753847252425
several levels	2.873683079735653
parsed sentence	2.4179897018482635
wall street	3.258617954966184
particular problem	2.57276412044517
performance significantly	2.4793959105551036
final column	2.172274429303222
empirical investigation	2.341549856487929
network based	2.2226693635644947
multiple entries	2.1676408442429524
shows several	2.222406898745732
minimal attachment	2.115665326889551
exponential growth	2.11911577017412
novice users	2.117280330401933
number 2	2.7762071296964184
system interaction	2.112778664196261
perform worse	2.2685844072039183
body parts	2.2566978107196425
national institute	2.7987596811530864
large data	2.8782995532629325
declarative semantics	2.1110237106446705
edinburgh eh8	2.045521226679158
toutes les	2.145776874326602
active research	2.5300278128180302
devices used	2.067747107964575
model differs	2.1678854032223676
tag assignment	2.016662123980433
second set	3.167821634669326
inflected form	2.666567455154727
contains three	2.8455409581393942
corpus annotated	2.711034568539442
problems discussed	2.2112071020073643
possible correct	2.00238351760398
one sense	3.2668000579945344
paper described	2.3161212503528343
notice also	2.41853693394125
approach enables	2.1290482690107417
select documents	2.015795287891615
phase 1	2.2375017996661226
model would	3.084093238124428
sampling rate	2.0743965972347684
collegiate dictionary	2.1859857899642225
language vocabulary	2.021237938105666
research described	2.749525672606496
different kinds	3.6679738681795118
new terms	2.4971064668058984
target nodes	2.056054358920443
limited knowledge	2.0388643487004234
reference time	2.2898578348461127
linguistic variation	2.312523573666309
better fit	2.1290482690107417
free rule	2.6729617680462003
means something	2.109437912434099
approach involves	2.4784612405958746
spatial information	2.0146750195814107
ordered sets	2.072005300633911
acquisition tool	2.0923172520507163
generation method	2.309657223566233
insufficient training	2.3129587245528533
therefore cannot	2.3398800045580104
task described	2.166102255087602
applications require	2.3841838643071576
important sentences	2.3338358562287125
news texts	2.2669663123191315
text segments	2.708441520600402
expression corpus	2.053986137989304
main types	2.84094265517939
multiple models	2.257281395364781
discuss three	2.200598690831077
primary aim	2.067747107964575
good source	2.184223228921038
values used	2.3005523645852994
different sentences	2.802016952223147
medical information	2.0429058304291106
tree 7	2.067146347873961
slight differences	2.045521226679158
efficient implementation	2.7129937099782295
gale et	2.7437602138095385
strategy allows	2.045521226679158
system therefore	2.014014536653706
information plays	2.0817120252117016
gold standards	2.2857644333587315
summarization task	2.373325464043298
alignment accuracy	2.1331546425009478
implemented system	2.735248642133936
text would	2.684314596187818
natural number	2.164438641692602
system uses	3.5826997181170923
rule systems	2.258856542999384
sentence alignment	2.611615258677232
parallel structure	2.0450608250123605
confidence interval	2.3750926815708384
technical fields	2.061969254719088
probable parse	2.258618900520454
full forms	2.182244746097634
conceptual graphs	2.25905665041004
correct sentence	2.5611462049430735
local focus	2.122647145945626
spearman rank	2.0721183276801627
texts within	2.0656667908183506
distance measures	2.3156000097637186
training error	2.0541244052629724
appropriate rule	2.075729255121515
formed substring	2.1085346067300628
knowledge extraction	2.252242210619881
textual entailment	2.018869755092379
close examination	2.089026915173973
increasing availability	2.0074734400897967
structure theory	2.948342842924241
best performing	3.0004453128161597
something else	2.6203250789168173
initial experiments	2.9904360993243864
set union	2.2989767282602602
entity referred	2.2020202169645566
fine distinctions	2.2062448606993037
features f	2.3898090448393847
infinite number	2.8387933252036333
become clear	2.4928909871794263
final states	2.8392309405876865
gram approach	2.0864276890759808
mail address	2.00238351760398
case 3	2.2490494746704233
similar pattern	2.030942917306881
humphreys et	2.147342467231957
scientific articles	2.0566642355902185
linguistic study	2.1774937848124125
current study	2.336567322611504
word distributions	2.215740686320565
function takes	2.1946340157728974
rule sets	2.5461132341015813
process makes	2.200598690831077
specific sub	2.172274429303222
using shallow	2.060145782650642
words involved	2.579518879915923
binary decision	2.3866860126598572
bipartite graph	2.1767945149898447
upper level	2.1353034784701492
make choices	2.0817120252117016
syntactic positions	2.3858730811247
syntactic structure	3.872708660545777
builds upon	2.30606794696972
probability based	2.075729255121515
user queries	2.779918303503906
disambiguation tasks	2.4631653324704343
4 figure	2.0577375124403856
balanced corpus	2.601791973814089
conference poster	2.889561746555762
research groups	2.42570197061165
context also	2.0376102740271858
another area	2.2794066389673104
short time	2.3580915261665507
model similar	2.200598690831077
problem solving	2.8797476660170394
linguistic tools	2.3271845436418466
different cases	2.649502094360685
branching factor	2.4305476991852277
al 1998	2.1524701299532136
verb tense	2.3887640080005768
three data	2.403132787857741
additional training	2.6402228776712717
five times	2.4247612268442116
right sides	2.0297445546210566
tim system	2.089026915173973
semantic distinctions	2.6813510666202465
voting scheme	2.1935158823147516
relevant entities	2.218434485750926
aligned corpora	2.3423238600564327
major category	2.124033237154508
system improves	2.0376102740271858
false negatives	2.412696582688935
context may	2.7388523969489236
new dictionary	2.2218136924065175
data also	2.2166155371597625
reason may	2.1584071623262564
verbs whose	2.319855212465236
essential role	2.2818214018700496
previous work	3.92497939048584
new text	3.059810138181352
vast number	2.2328679513998626
linguistic object	2.331704723121005
de nition	2.3284763998324065
framenet project	2.2552558851981885
et un	2.1399609555095465
national language	2.102921514525577
existing database	2.089026915173973
agreement information	2.061574528285961
domain would	2.2062448606993037
structure must	2.776458922976461
data without	2.56332068098776
text collections	2.6042963072530494
new implementation	2.0918364857418936
best matching	2.2571931005566306
task could	2.275426805310505
large quantity	2.3926299907916917
word would	2.5327107950191152
format used	2.282311649307406
common usage	2.21826141526814
par la	2.2520444227245218
classi er	2.3536362602899876
certain category	2.0355401813680145
markup language	2.47830874901926
sample grammar	2.1898146905603255
sur un	2.0644684174379186
cluster words	2.1462245538298697
end point	2.3341430061167734
partial analysis	2.421133567144045
base contains	2.4860848643261964
augmented phrase	2.3887841950835043
final set	2.4049856125836695
contains 3	2.089026915173973
encouraging result	2.109437912434099
complete derivation	2.063858651895712
finite sequence	2.20581029094498
techniques may	2.4853086398786797
feature indicating	2.233181205507262
4 shows	3.932290356214159
system obtains	2.116591742190463
briefly reviewed	2.109437912434099
also avoids	2.0965741974051317
using equation	2.601004349755704
syntactic coverage	2.4546015195303097
reasoning capabilities	2.0553852839257445
different extraction	2.014014536653706
automatic translation	2.849046752753726
two sections	3.1313692017397217
resulting sentences	2.045521226679158
architecture consists	2.049069748176902
partial structures	2.2766033048248273
binary feature	2.8184307371586095
much discussion	2.1290482690107417
theoretical linguistic	2.1809543568234924
density function	2.3802882430650913
relation holding	2.39913920272528
large improvement	2.116591742190463
negative evidence	2.158416122581743
electronic version	2.108212199944729
multiple translation	2.0379203068673784
common problem	2.462292105818408
already part	2.039007283873737
similar meaning	2.507668226348355
oriented language	2.1403073737178895
feature selection	3.1030879876355626
simple word	2.819172108720133
terms occurring	2.3088417794636853
using sentences	2.24825378073324
natural classes	2.016732165904002
categorial information	2.0600984404566907
2 compares	2.3879279742060486
position information	2.290776255600169
set used	3.1340687933468185
noise introduced	2.369115447338463
full set	3.0909252262853553
method seems	2.227249320716272
focus articulation	2.161179228391738
japanese character	2.0730662534060538
three patterns	2.1000868080586823
empty sets	2.159746288077376
performance improvements	2.4824561477057
passive verb	2.2999424597608575
syntactic phenomenon	2.014014536653706
smadja et	2.2323699042331526
optimal solution	2.7828746788310696
input node	2.00518896099143
working memory	2.3560585612469476
step toward	2.5312582224880584
programming algorithm	2.9673952251480493
also discuss	2.709143902053129
abstract concepts	2.3485059345603894
linguistic tasks	2.1809543568234924
textual units	2.0236675596759115
polysemous words	2.777701816525786
making process	2.2880858826714636
hidden markov	3.5395420673837372
function ph	2.007038640285683
knowledge source	3.1039972668837006
language features	2.0832082124114963
small portion	2.345783690225046
alignment system	2.110952746409473
alternative approaches	2.6576461343749926
sample output	2.269813783347405
system fails	2.341285221188147
looking center	2.423681187514939
potential benefits	2.227249320716272
results reveal	2.0601457826506424
system responds	2.190982236626934
structure corresponds	2.045521226679158
many places	2.049069748176902
language constructions	2.0601457826506424
single category	2.3626292492350114
conduct experiments	2.3229739757638654
english phrases	2.443115044965803
predefined set	2.369994328044643
store information	2.2275895034737268
functions include	2.022261218861711
two dictionaries	2.429523988539337
illustrative example	2.338978613401074
high coverage	2.4614125967030085
particular information	2.329081436908228
new variable	2.1628917274538195
feature represents	2.131569621150367
systems cannot	2.480454307363556
given event	2.1227137463544357
note however	2.607249053146254
original training	2.4668661132541203
unsupervised fashion	2.2207046594768247
various types	3.4981430172866137
art performance	2.8335813110311268
incorrect words	2.012347776929124
equal number	2.048753475238681
paper introduces	2.866447796130439
wide range	3.6534776960555115
protein names	2.219989246152392
seed words	2.1900135077309058
formal semantic	2.2001164854726465
best choice	2.2075709959932643
first row	2.848653970478026
two steps	3.575173541352155
basic travel	2.1023887620661283
van halteren	2.3106562186805304
syntactic constituency	2.0970030313409285
tree containing	2.312149585164438
optimal way	2.2169936022425736
word type	2.4651608005964576
another paper	2.286725248739412
fast access	2.172274429303222
let b	2.335904293904669
weighting schemes	2.426062424028854
every user	2.049069748176902
corresponds exactly	2.227249320716272
search time	2.349398009379017
files containing	2.2532512925302806
model uses	2.964009774614721
based part	2.349377837282069
following definition	2.827999504739868
optimal path	2.051530319550983
different results	2.7939731664806717
sense information	2.4418062467489587
discontinuous constituents	2.434175139309607
smart system	2.006544232850177
algorithms use	2.1662438838961897
put forward	2.638958950433215
original language	2.242915799916111
tag sequences	2.74610481782493
closed test	2.221903805850325
word alignments	2.8494773343790496
different length	2.4043111027203574
possible co	2.11911577017412
input strings	2.665181304217737
unigram language	2.0297093966243978
plural pronouns	2.074778937253678
orthographic form	2.2324767069435336
theoretical basis	2.5846392107069693
based pattern	2.0374028535502435
thus use	2.2112071020073643
set including	2.0490697481769016
contain several	2.458989459396439
chronological order	2.463014016297776
tile context	2.318222716608852
5 contains	2.331780823064821
vary greatly	2.435632598873795
various expressions	2.040238153498075
annual meeting	2.7721476596872536
question words	2.1391339475483653
boosting algorithm	2.077350809117807
history h	2.127040445265349
described briefly	2.183647914993236
paper reports	3.1323834364629097
fragments may	2.022261218861711
various domains	2.549554001670492
words share	2.161246853402207
statistical disambiguation	2.168967603804668
human subjects	3.0181534631769824
intuitive notion	2.2256816819452245
5 iterations	2.025073621873738
speaker must	2.3217865221509157
system works	2.8396166232628133
occurring data	2.255593583249476
given number	2.4972046802134193
one issue	2.2486830981490296
work focused	2.045521226679158
material presented	2.0817120252117016
equation 6	2.165601985147279
second baseline	2.227249320716272
knowledge may	2.6378074479286577
using hidden	2.109437912434099
large dictionary	2.2760129163642313
communication science	2.067747107964575
special value	2.031264650127311
inconsistent information	2.017570716396185
single type	2.4311396099514315
large text	3.0352992753435433
correct constituents	2.0712470623364108
important source	2.435600505453947
varying degrees	2.774161684397842
three arguments	2.67239759197034
one key	2.200551097810321
2 show	2.9763551456190247
separate feature	2.222406898745732
system decides	2.1859857899642225
sentences could	2.563001484099255
comparable accuracy	2.007473440089797
different source	2.3928273266625144
shallow parse	2.202640507036322
semantic relationships	3.046051882398648
grammar fragment	2.281296781734758
set x	2.407448603838093
acquisition algorithm	2.0206605557124795
given point	2.7920132358118233
verb meaning	2.3198557910951427
three simple	2.0601457826506424
concepts rather	2.0452299618080985
distance algorithm	2.1380488015835484
relevant parts	2.4339025560556338
argument relationships	2.105195669894195
mit press	2.1851123324453434
different data	2.9463911216136998
interactive problem	2.035883689985079
identify noun	2.067747107964575
several things	2.4103259885026898
functional dependency	2.1976753808592973
model improves	2.2492578335740094
decision making	2.458297802630018
contain errors	2.28164599557574
whole expression	2.086551350552231
particular values	2.022261218861711
n v	2.371464947422226
compound words	2.829082321003072
case relations	2.158661932613817
simple grammar	2.6184099812082744
japanese characters	2.123414369294872
natural question	2.1946340157728974
larger context	2.7315294037217686
algorithm achieves	2.4126307796665474
dialogue systems	3.1363530832262096
discourse participants	2.295647294906258
component analysis	2.264457242840555
full parser	2.3433746358911502
bill clinton	2.3272982598964638
lfg f	2.1126886768322324
step may	2.1479184330021646
complete picture	2.060145782650642
recall rates	2.525598953067748
semantic frame	2.469229730014554
list using	2.1543888787937475
two sequences	2.505931930245142
across word	2.043794278158
parameters must	2.0490697481769016
good choice	2.3589213672090974
candidate list	2.4426142350811544
text meaning	2.280123681212183
framework proposed	2.1597096509760467
one problem	3.020866113737981
composition rule	2.071840152448747
sentence given	2.5053321500399353
present evidence	2.161246853402207
verb complement	2.1859857899642225
following way	3.504117874616479
deep semantic	2.590832547411854
agreement features	2.557152481092262
human processing	2.2368067747493123
multiple features	2.072123615273788
system identifies	2.5673100826218893
two exceptions	2.2631802623080817
user experience	2.0164282614915257
semantic tree	2.0546758779032634
language string	2.2831710170609325
semantic relation	3.1083099612608693
computational perspective	2.3000015484793463
positional information	2.4469899066325116
new research	2.024177603112759
retrieval result	2.031216633777997
domain expert	2.5297181307828445
palo alto	2.10362708484383
tile system	2.63099728822201
tree whose	2.784782614000735
dempster et	2.6699815476375597
query using	2.2404676499729774
new test	2.7059969532469745
two samples	2.1390445946327175
category b	2.2711027448243906
parser also	2.5523873992877064
two groups	2.889512165777535
partial order	2.780149700407938
basic units	2.647694853947539
rule application	2.9884976843691224
use non	2.2827808432516177
composition operation	2.2609807468680594
useful discussions	2.5794415416798384
source languages	2.296936574120876
briefly summarize	2.1023887620661283
system using	3.474909990966068
speaking style	2.118958284566388
newspaper corpus	2.626466002158584
case role	2.2147015928392433
based n	2.1209973948922216
node labels	2.775952546427342
xml representation	2.2017393269780796
english speech	2.1816335351833818
coverage grammars	2.5138256726614925
several systems	2.666209214311636
second factor	2.28182140187005
ct al	2.4902237579293494
algorithm starts	2.6968616101077494
humans use	2.067747107964575
help users	2.4070500872348157
class may	2.4211599391922816
information specified	2.1543888787937475
different pieces	2.1312839727750825
inflected language	2.127375476291898
parser may	2.6448556435754917
candidate translations	2.3738741854187877
acoustic observations	2.008226999543247
4 table	2.0376102740271858
preposition phrase	2.039007283873737
medline abstracts	2.5456711372179996
particular discourse	2.3658325421580413
different means	2.1222463152036486
speakers use	2.2577963294869763
context vector	2.263301211672161
language parser	2.527343091007038
classification tasks	2.892641828420856
every parse	2.031264650127311
boundary information	2.3405169074963794
general rules	2.947491781856646
consists mainly	2.075729255121515
interesting cases	2.103044613514411
certain aspects	2.555236560596062
phrases might	2.089026915173973
score among	2.061212299166531
ie systems	2.5510905913609934
accuracy results	2.6962767449157305
three text	2.0817120252117016
two scores	2.3263148914896092
surface representation	2.118620686661748
use multiple	2.4148184100839467
wsd system	2.494636968581562
form suitable	2.183647914993236
model fails	2.1245637044373717
existing techniques	2.3000015484793463
processing modules	2.71883424456731
600 sentences	2.182606963890602
spelling errors	2.5692132823695357
july 2002	3.048047075335186
verb morphology	2.080606834271845
timit database	2.042557557719475
open text	2.0363711465169505
arc labeled	2.127375476291898
significant proportion	2.1773123985040854
arbitrary word	2.1678854032223676
language learner	2.2510017117667465
experimental results	3.87941256474763
formal characterization	2.122563170379007
partial descriptions	2.3174356696231593
direct object	3.4771763186080653
human use	2.089026915173973
basic feature	2.305097418252439
tjong kim	2.566787336281031
incremental processing	2.278731935533561
main purpose	2.6494568948431527
transformational component	2.1546727971308623
experimental studies	2.019557844009582
following relations	2.3483606540998085
studies using	2.194634015772898
several questions	2.0219007873285415
text content	2.1063788781365194
grammar covers	2.1119430342456558
linguistic problems	2.444624958019281
discriminate among	2.3766279418997804
passive sentences	2.3679405969548153
thematic relations	2.134589368049598
constituents within	2.045521226679158
many theories	2.116591742190463
model directly	2.172274429303222
basic ideas	2.182457843140897
alternative formulations	2.022261218861711
normalization process	2.0763533583416773
web pages	2.9837481188078288
word tags	2.1475618361129225
efficiency considerations	2.060145782650642
find evidence	2.1597096509760467
journal section	2.0163365265822155
appear within	2.5751286128797615
low coverage	2.436369723844453
data produced	2.233997204181393
using web	2.179603819530149
first module	2.161700268474605
following section	3.527013957502472
function must	2.2380656204269957
english resource	2.1735708575635737
resulting data	2.4697040910441137
specific corpora	2.1745307144851918
underlying representation	2.558450853523188
traditional method	2.075729255121515
3 sentences	2.331245010213282
first feature	2.526642750486463
another time	2.1023887620661283
entity types	2.791837756100904
patterns may	2.554939735313317
quantified noun	2.178163068881081
first verb	2.2842645210265413
landauer et	2.0027497200481923
declarative knowledge	2.3479223064700623
also attempt	2.116591742190463
using data	2.847430974683483
running texts	2.0074734400897967
previous examples	2.453255888575513
feature value	3.0434969690736815
specific verbs	2.0275761683433906
nlp techniques	2.7922967298288137
let z	2.2578698255858516
mechanism also	2.060145782650642
german texts	2.171672995767222
tile lexicon	2.1023887620661283
corresponding rules	2.2020202169645566
official airline	2.008226999543247
example 5	2.535036139161009
problem occurs	2.417618427399807
many machine	2.406719372930616
two options	2.226678425750301
process uses	2.2427935805269072
one model	2.669831073230326
simplest case	2.7971757412829725
experimental data	2.6592604125885013
per text	2.0389084030512175
model works	2.1413469986390323
standard measure	2.1275552563638707
dictionary words	2.060765272950003
particular set	2.9549045935280818
chinese translations	2.2062423305763392
four sentences	2.369115447338463
using k	2.202529704553986
appropriate rules	2.135844661161516
posteriori probability	2.129716618758807
lexical transfer	2.3865127106194564
important terms	2.0002999060530913
correct tree	2.0005657906413616
di erent	2.513620949538349
feature functions	2.794877723941282
20 sentences	2.36351372380466
column lists	2.161700268474605
feature weighting	2.1890945406733104
different head	2.008226999543247
structure nodes	2.059691082649662
called head	2.067747107964575
main properties	2.0817120252117016
recognition errors	2.98592370161059
different terms	2.3064111847632556
five steps	2.267991541156711
computing time	2.1906155287972715
syntactic theories	2.419144864745955
relations defined	2.5725674597517894
complete syntactic	2.5900748667882683
performance results	2.734354082164003
resulting parse	2.5871936349478175
data shows	2.1004669767659387
intelligence research	2.060145782650642
data collection	3.061468705165877
best individual	2.1264544099151683
dialogue acts	2.4992142600875233
six types	2.2404676499729774
dans ce	2.137785748955035
input vectors	2.1515227752483668
judge whether	2.3603902435629145
words instead	2.331780823064821
oepen et	2.0931038629978387
descriptive texts	2.093019652531571
analysis grammar	2.0262566660283414
statistical word	2.3816160784197873
using log	2.014014536653706
appropriateness conditions	2.029927880067893
second half	2.1990773324944293
questions may	2.3263148914896092
one vowel	2.032982255991816
algorithm outperforms	2.0918364857418936
corresponding context	2.2020202169645566
larger lexicon	2.061969254719088
initial question	2.043521175114013
already knows	2.576542756506379
system chooses	2.309782368028973
third level	2.545329231458188
ikehara et	2.3007327689766113
language constraints	2.038024989271382
chooses one	2.227249320716272
translation module	2.1274632558685678
entire tree	2.255593583249476
accuracy may	2.183647914993236
coverage english	2.1023887620661283
interesting results	2.659313278797579
linguistic community	2.2147431158325057
development process	2.4836904265358797
problem becomes	2.584325141004728
systems like	2.774624126835123
tree grammar	2.0149348970135685
term frequency	2.923834404952305
computational overhead	2.228891121655173
sentences based	2.565684494322664
unmarked case	2.0923172520507167
head noun	3.405330755689146
relies solely	2.1023887620661283
occur without	2.135844661161516
perl scripts	2.0376102740271858
example set	2.092385356205602
two types	3.9963811729155165
cosine measure	2.6184110856397473
four cases	2.592435279111417
hold true	2.0965741974051317
end system	2.277178903392817
larger constituent	2.045521226679158
given question	2.306296769095789
ie task	2.198269099980138
incorrect information	2.0577375124403856
theoretical approach	2.2142371090381427
problems may	2.4489623523461015
new type	2.9098577468657343
results compared	2.166102255087602
16 khz	2.122246315203648
containing words	2.425593477889689
generate possible	2.067747107964575
simplifying assumption	2.7938500306669423
ten thousand	2.2093487881958334
performance data	2.210188457862065
speech corpus	2.595993092252508
difficult part	2.1290482690107417
extraction component	2.06117738616853
numerical expressions	2.0653223662083464
english documents	2.202881552633111
classifiers trained	2.2582820894786684
first document	2.0312646501273113
model using	3.269652095406402
cannot provide	2.4972046802134193
good test	2.045521226679158
average f	2.316888511944801
partial semantic	2.4213605893711114
referring expression	2.8042097434327062
different grammatical	2.6485507089546116
human subject	2.3408795766109813
one promising	2.022261218861711
one answer	2.4619447088476862
precision values	2.529357358926174
users find	2.0539861379893036
sentence like	3.1142349874220323
use exactly	2.1479184330021646
describe events	2.082836083424519
corresponding value	2.0817120252117016
probabilities computed	2.105195669894195
stage consists	2.060145782650642
segmented words	2.3056023533198706
proposed solution	2.0724157728539585
electronic dictionary	2.5250993342630017
grammatical words	2.127033094558831
particular string	2.014014536653706
treebank data	2.5817332671162463
two domains	2.7683684834962166
selecting among	2.369115447338463
share many	2.50352395005998
paper addresses	2.9788035203821623
particular algorithm	2.1413469986390323
values indicate	2.0539861379893036
retrieval techniques	2.6313691502350776
text planning	2.781871632481346
full understanding	2.3097823680289724
bottom half	2.0817120252117016
following sets	2.1380488015835484
generate appropriate	2.1852404006823356
appropriate value	2.409790394279902
many phenomena	2.1413469986390323
grammar may	2.6765788511577067
clause would	2.1290482690107417
ney et	2.1079357422934684
like e	2.2492578335740094
three papers	2.127375476291898
experimental design	2.4728227970652483
precision measures	2.5173477598646987
text using	3.036754842963639
way described	2.3644803559732956
following analysis	2.5469502675910904
variables may	2.1774937848124125
syntactic information	3.750579484386419
new methods	2.766963938925566
sql database	2.009868063569498
constraint imposed	2.086884416965048
different text	2.7513821300048638
paper suggests	2.2224068987457315
number feature	2.1358881772255405
undirected graph	2.2602474684506704
mexico state	2.5051296017851037
rule specifies	2.4917756927623538
second example	3.046998819326308
present model	2.178653004950429
multiple candidates	2.051285638402277
feature matrix	2.0441733702810536
science laboratories	2.067747107964575
processing requirements	2.0074734400897967
update rule	2.1595078836896286
rules corresponding	2.1380488015835484
single pass	2.539561812625768
tense information	2.0255574068965725
synonym set	2.231621703114163
f u	2.021237938105666
one translation	2.701400306287439
definition 8	2.109437912434099
average recall	2.500280002531735
dependency relationships	2.4855956416639025
given context	3.3083268419756777
annotated sentences	2.5834589514307296
differences among	2.941985871418339
following structures	2.109437912434099
also define	2.838566573545308
free languages	2.6850304374726144
become increasingly	2.7677335294722547
experiments comparing	2.267991541156711
classification problems	2.6246892639642905
inference rule	2.6688077399503882
common features	2.5419263403504315
null ing	3.1106969781868123
det n	2.1938356091169036
structure directly	2.067747107964575
information relevant	2.858273518656914
approach presented	2.9388797497166514
average agreement	2.085074160737753
shows us	2.060145782650642
performance without	2.272505380879168
models discussed	2.2470811344797337
reliable estimates	2.2459839572002176
robust speech	2.0281677197784704
accuracy improvement	2.231521639559652
step process	2.768876435681459
one bit	2.045229961808099
probabilities p	3.0629587243154233
functional structure	2.4068790451937305
attachment ambiguities	2.623397007241784
using terms	2.0490697481769016
gain insight	2.406719372930616
performance may	2.524067375092386
line 7	2.029744554621057
open research	2.3546421252999923
simple bottom	2.1222463152036486
simple measure	2.0965741974051317
prevents us	2.3644803559732956
treebank parse	2.172645333882902
new pattern	2.224348485612061
international chinese	2.232654874327043
depth 1	2.228891121655173
bootstrapping algorithm	2.137398980623712
lexical differences	2.1078516753501777
compute precision	2.286725248739412
using simple	3.0321376566066287
broad range	2.7372967436302034
various sorts	2.429850424615452
future development	2.1408692551629067
also lists	2.116591742190463
input query	2.2608599931331956
tile rule	2.085074160737753
training instance	2.4434084575698494
tagged texts	2.2203032704502665
models outperform	2.0223531762070586
next section	4.138797128346624
results vary	2.067747107964575
previous tag	2.2390450307028704
correct candidate	2.062777311020461
german grammar	2.191987901035769
miller 1990	2.0490697481769016
another node	2.538428950098558
system models	2.060145782650642
right processing	2.189546252614866
dependency tree	3.0699026084293837
linguistic issues	2.34636229888775
two judges	2.398832009714301
attachment decisions	2.403685013139803
syntactic phenomena	2.7818030940672522
3 results	2.1563079384871173
aspectual properties	2.0477789160236304
approach appears	2.1413469986390323
either way	2.1694283564854024
various experiments	2.075729255121515
two np	2.1882216921535553
main point	2.37495597934697
european languages	3.0915660239171117
dcg rules	2.0878157719930983
paragraph boundaries	2.3394505579860696
computing similarity	2.0601457826506424
current implementations	2.000178283129414
segmentation results	2.2840121296852023
linguistic behavior	2.4562189938979357
initial system	2.237602385670798
underlying domain	2.1984572101092845
input sentence	3.6642301180134154
e cient	2.028939001938722
effort required	2.854609705904853
whose domain	2.37858059655019
word stems	2.425933574843233
system within	2.3879279742060486
simple feature	2.3612865440695137
current word	3.120910286961423
lexical definitions	2.1222463152036486
syntactic case	2.099458264360342
unix operating	2.2429157999161107
generates two	2.2823116493074065
internal representation	2.9940701168431914
semantic criteria	2.5214564348285977
development test	2.6451360453516704
sentence types	2.5773021430737577
given knowledge	2.075729255121515
contains several	2.6927179621980586
important contribution	2.314007818435079
data would	2.7853041642672323
treebank ii	2.348674983792269
york university	2.1327272314127903
antecedent must	2.0514054040135554
strong influence	2.183647914993236
shieber et	2.6356176027771787
works better	2.510371117379182
previous text	2.2512557276899194
first sentences	2.1339258481610717
p x	2.0965098604425414
various properties	2.108179066756418
last words	2.0539861379893036
sophisticated approach	2.067747107964575
final list	2.1749635955627893
complex tasks	2.3953309694000535
clause type	2.232476706943533
prosodic structure	2.3083996867345284
parsing strategy	2.9586445024928825
text elements	2.2698137833474052
process works	2.3005523645853
language independent	3.112324530558855
grained distinctions	2.5825267567910846
many variations	2.30606794696972
concept c	2.3438963481743844
natural speech	2.5131047008393255
algorithm provides	2.468819905483623
given system	2.3922788066689504
large collections	2.6182085622684474
automatically construct	2.344490863391834
whose aim	2.0656667908183506
v c	2.311303830728837
parsing component	2.194928951085213
abstract model	2.020121462798538
normal forms	2.0549417809472406
order according	2.0843177461209628
different nlp	2.129501083087358
another tree	2.3897410375158987
encode semantic	2.1413469986390323
different contextual	2.0355401813680145
overall evaluation	2.061212299166531
k times	2.2046317742048496
single clause	2.3003560485426724
preference information	2.171178031468078
another relation	2.089026915173973
see examples	2.5328967085832406
specified context	2.075729255121515
word l	2.014014536653706
approximately half	2.4213605893711114
every variable	2.016336526582215
target translation	2.2167233619308075
overall approach	2.3922788066689504
full list	2.2251427952863683
linguistic relevance	2.2689486128575904
two people	2.3277832752933834
theorem proving	2.501875949010686
possible binary	2.059213640580528
resulting text	2.4328329620168185
performance improvement	2.9098081996790364
different applications	2.880323052409307
one discourse	2.4615620085705237
translation equivalence	2.0249804489746546
last utterance	2.3252411269236317
using bigram	2.1569808507858435
central component	2.267991541156711
words around	2.58158636762688
global variables	2.0577375124403856
second form	2.089026915173973
product names	2.144125256365731
tense forms	2.2259060221403466
state methods	2.226071138062105
experiment 3	2.1338681923974465
using text	2.492379298758605
two quite	2.0817120252117016
input token	2.322302553401289
multiple analyses	2.3384628419127154
first assumption	2.1245637044373713
new tags	2.026260250766906
pattern may	2.2878329869964507
special thanks	2.843938844891117
recognized word	2.3109116208104994
basic processing	2.0577375124403856
questions related	2.0074734400897967
small value	2.2427935805269064
transitive verb	3.1477451873948192
phrases found	2.081242005652375
contextual cues	2.2295768300727024
15 minutes	2.0539179006326544
high values	2.33334372677606
reverse order	2.7415234208272174
e k	2.1971086751727307
parent nodes	2.312149585164438
source side	2.2092110163963365
weight function	2.1843158161464165
quantified variables	2.079719379540538
2 b	2.1407430488088703
preference heuristics	2.0245278187040876
structure using	2.5713098687748897
evaluation score	2.1097113655852784
starting points	2.5761240497955167
ran experiments	2.440653212565332
parser makes	2.3322409522263206
two readings	2.724006196871631
requesting information	2.0355401813680145
second order	2.6048585469040644
ill section	2.014014536653706
syntactic variations	2.2842187306527597
data collections	2.081242005652375
never occurs	2.2020202169645566
database search	2.0516186188527366
1 indicates	2.614565131436681
work must	2.159746288077376
special word	2.2275895034737268
one relationship	2.055633163459631
human memory	2.2578794258062236
certain applications	2.2427935805269064
x b	2.1117964579724733
applied rules	2.014014536653706
several objects	2.049069748176902
problems related	2.5299279475474306
relevant set	2.0743142002902806
feminine singular	2.0315460095350906
describe one	2.357121432847829
long tradition	2.183647914993236
tile two	2.5036285688060933
specific dictionaries	2.102921514525577
data type	2.498627425509633
process using	2.5806168169900157
query translation	2.0621832677652234
present purposes	2.702437735712341
people involved	2.1774937848124125
redundancy rules	2.1031054608854465
source word	2.89572084671143
predictions made	2.0811827917300074
implementation described	2.2062448606993037
human reader	2.4917454619462753
clustering techniques	2.677094346605143
lexical class	2.1182938871924133
e may	2.031264650127311
french lexicon	2.012347776929124
two phrases	2.753100766970297
depend upon	2.6833283054879593
similar fashion	2.8465898613809566
primitive actions	2.0915287201397943
dictionary information	2.414242055670307
five types	2.569492862919813
et des	2.0339173165516224
dialog manager	2.152409128498042
possible choices	2.3402017998642997
simpler ones	2.1954890274836183
lexical entry	3.442274586271584
existing words	2.036577606014304
supervised systems	2.0368959417859647
transformation process	2.0352110047855154
mapping process	2.355695717315683
seed set	2.095075553667107
several factors	2.3421752523694894
occur frequently	3.0311899283696966
state network	2.079032025733796
functional words	2.7138358119263857
evaluation task	2.528158386901093
preferred center	2.010414490139241
training criterion	2.033996129918062
practical advantages	2.3605881269336804
lexical statistics	2.032652405401766
two instances	2.8544189941836784
important research	2.4282856154916748
output using	2.064139780413935
actual english	2.1023887620661283
features per	2.017570716396185
hypothesis h	2.2000513402573114
sur des	2.041033826771926
relations could	2.2380656204269957
textual material	2.157206459959845
section gives	2.596962029673703
current grammar	2.5155682891895257
sufficient condition	2.414375282541592
brown corpus	3.2490291683722683
example demonstrates	2.345768269478816
type hierarchies	2.177282624245386
additional criteria	2.0817120252117016
knowledge would	2.357121432847829
annotation tools	2.366426152845424
technique uses	2.166102255087602
interface provides	2.031264650127311
new content	2.045229961808099
different test	2.584090077565748
original input	2.723290726864296
entries may	2.272505380879168
subsequent iterations	2.172274429303222
traditional chinese	2.105665658062761
new formalism	2.129721222536923
multiple reference	2.0509508493014383
system achieved	2.73601625939483
n times	2.627427404167823
modal verb	2.3055965927534343
possible feature	2.4358964112583816
rule 4	2.487312589693387
two level	2.239473881375989
string generated	2.264070934029513
strong effect	2.089026915173973
w n	2.355843937367338
different senses	3.1545772143400717
la traduction	2.001802603755931
using nlp	2.278461809175595
operating system	2.7627843768140687
based account	2.10508814843703
generate two	2.3271110475429713
probabilistic language	2.4905869399873684
noun sequences	2.0063395002308586
common phenomenon	2.109437912434099
feedback loop	2.153414224966464
generation based	2.211790418408607
individual concept	2.1535700503923474
method showed	2.0532484523230123
rule matches	2.3706473607714544
ontological knowledge	2.2368067747493128
reasons given	2.1225631703790064
next example	2.6250353994555695
words one	2.204719497546924
documents using	2.6408538515145277
priority queue	2.0388385064976084
best n	2.021237938105666
single symbol	2.176234939134731
encoding scheme	2.277667048397515
traditional dictionaries	2.1119430342456558
van der	2.245420740175626
three methods	2.9074164636526074
cartesian product	2.42851266010876
help us	2.9531424791064254
poisson distribution	2.0322911346551
type system	2.5726557323924646
particular representation	2.0435211751140128
lexicon based	2.346889123261037
paper summarizes	2.0074734400897967
model within	2.1543888787937475
main components	3.0270539461587043
speaker knows	2.0435211751140128
vectors using	2.133165345977436
containing information	2.765917947584889
useful tool	2.654437714338689
models perform	2.4105022712590496
constraint based	2.0721570056738003
via unification	2.1245637044373717
location names	2.408836879527973
determine whether	3.569290405325265
standard arabic	2.2373775951712918
less information	2.6760156204978185
five categories	2.305187720620721
noun groups	2.6179022101304157
x axis	2.172274429303222
line 2	2.341285221188147
table l	2.213318758370463
regular language	2.424352668192542
second strategy	2.380830363798011
require extensive	2.2631802623080817
weighting factor	2.230664245176579
annotated data	3.250435880984285
commercial product	2.145857202115015
smaller values	2.0074734400897967
sense distinction	2.2722337532861054
main challenge	2.1413469986390323
individual classifiers	2.2442755651488584
another dimension	2.222406898745732
discourse topic	2.2194281411511985
sentences cannot	2.1290482690107417
000 concepts	2.053986137989304
language grammars	2.617877365358052
probability assigned	2.501182870472855
grammatical information	3.031403140474656
one label	2.3146254738630745
street journal	3.640003424951915
selected words	2.4872933375121544
van noord	2.7712975811697684
expected frequency	2.1219953701410867
first results	2.3725556044144516
replacing words	2.0601457826506424
types may	2.384366089237462
three dimensions	2.527046319127731
text within	2.0376102740271858
simple transitive	2.0312646501273113
different segments	2.181370500368545
representing concepts	2.067747107964575
large scale	3.1763164611312584
subsection 2	2.154210989920513
based models	3.158260030933856
use automatic	2.154388878793747
time processing	2.324871720725265
semantic tagging	2.6256887317926885
primary task	2.110676283984369
shallow processing	2.3505502049116567
based model	3.1155593563687916
a0 a1	2.1587653614039524
domain must	2.1499148428335464
linguistic patterns	2.6033014898685556
meaningful unit	2.022261218861711
another program	2.060145782650642
elements must	2.183647914993236
sgml markup	2.101222560614464
rhetorical relations	2.8138280863014953
data might	2.11638795219105
clauses containing	2.0221951140579186
feature whose	2.123414369294872
class would	2.0763533583416773
single state	2.3312450102132822
decides whether	2.2316085928388243
language queries	2.434606570585916
let n	2.8129647102333304
word must	2.7071617172136513
bilingual lexicography	2.1204734898398576
partial matches	2.4969193744149756
modi ed	2.470671811204313
automatic machine	2.178653004950429
find one	2.5610975188069385
clear cases	2.162826512399144
city names	2.1560002057003844
small sample	2.8716924738404885
promising direction	2.045521226679158
translated sentence	2.072675410681331
complex problems	2.1543888787937475
rules provide	2.082492768888955
dalrymple et	2.226827643437173
head nouns	2.75547946709995
forces us	2.1497902212099103
approaches described	2.0376102740271858
next task	2.2578698255858516
source grammar	2.160058884520012
information along	2.200598690831077
lower number	2.166102255087602
different roles	2.630325663022697
parameter optimization	2.302639744133171
peking university	2.3436402109491725
figure 10	3.1677706513127166
participle form	2.1752927122246914
european community	2.004219106832245
null tive	2.1413469986390323
similarity function	2.5473022021721965
figure 15	2.438905680521371
viterbi decoding	2.3792525955386923
keep track	3.1429022234674453
alternative method	2.632486676997884
theoretical perspective	2.183647914993236
application may	2.2685844072039183
utterance length	2.0332844037073405
sentence final	2.1005097306825258
typically use	2.3879279742060486
two sorts	2.33582780054359
worth mentioning	2.7432043307283336
superior results	2.045521226679158
mt project	2.2030887260567305
low values	2.3429208778801653
considered correct	2.708442038030921
basic aspects	2.00238351760398
previous step	2.892307708029677
sentences given	2.2224068987457315
several approaches	2.9567782811568963
second aspect	2.2112071020073643
english tasks	2.061212299166531
authors also	2.014014536653706
7 concludes	2.2631802623080817
technical manuals	2.3551584362026343
recognition time	2.0488604259408207
every category	2.268708705875723
application requires	2.0965741974051317
discriminating among	2.0490697481769016
general issues	2.172274429303222
web server	2.1832050380381904
capital letter	2.7045942330077524
two experiments	3.0457905183534146
grammar makes	2.135844661161516
sentence level	3.4545034681309423
incremental process	2.114299018453181
word error	3.2676836013356745
c l	2.5729823649414887
annotations produced	2.00238351760398
multiple translations	2.345591613736791
deep linguistic	2.3874214827243625
expensive operation	2.1119430342456558
new clause	2.158407162326257
learning techniques	3.4479659048095135
multimodal dialogue	2.2502012659123816
results demonstrate	2.813377078363564
length two	2.1411515379744985
italian language	2.016609770262217
process natural	2.1852404006823356
fundamental problem	2.1378070163030096
chang et	2.1046177407264204
existing entries	2.0221951140579186
improve results	2.139388926106576
major drawback	2.316121250352834
major difficulty	2.3567860333521553
general word	2.3178448553704096
information according	2.060145782650642
word corresponds	2.2234532051296254
translation results	2.6438151686399896
object case	2.1686767192898797
random baseline	2.3969684931882522
given e	2.060145782650642
null tween	2.166102255087602
given rule	2.386828305171403
x z	2.1366713893936176
several paragraphs	2.265874814457472
following factors	2.2532512925302806
many areas	2.241842428302255
shallow methods	2.0215688023747367
optimal performance	2.587041240252946
without access	2.21826141526814
level nodes	2.2369721410571994
two properties	2.320337643917314
preliminary tests	2.01761133185089
anonymous ftp	2.082836083424519
section 3	4.423691709810573
research grants	2.1245637044373713
correct alignment	2.2461868248931554
discriminative training	2.3998641594943546
three tags	2.178653004950429
german verb	2.349434930861479
greatest number	2.2024639441989313
applying machine	2.102921514525577
nouns may	2.292453688514974
takes precedence	2.1678854032223676
sequences like	2.0423259728447545
detailed explanation	2.1731109929399572
line version	2.049069748176902
person pronouns	2.3782843937613407
computational properties	2.5604192124699376
grammatical forms	2.022304899603323
existing model	2.000415158520729
text translation	2.168910793140592
range 0	2.3633976741456193
see figure	3.8470443591057806
naval warfare	2.109437912434099
g e	2.180924489434667
attribute type	2.0074734400897967
model achieved	2.2616778639909243
category n	2.2513354247482975
inference rules	3.004658761148745
standard data	2.645296056998217
discourse segmentation	2.4743194767316288
incorporates two	2.045521226679158
constructions involving	2.2380656204269957
value obtained	2.020363647132216
full details	2.145892423428313
reliable statistics	2.022261218861711
free word	3.0426690110878365
original sentence	2.810733783991776
vocabulary words	2.8761190525209352
produce texts	2.0539861379893036
suf cient	2.3872244579144395
every type	2.4484990872685626
approaches could	2.0817120252117016
node r	2.0326357214545703
parsed corpus	2.771285034300124
smaller set	2.7010852234394753
based translation	2.942977966706889
partial tree	2.1838306250883694
test questions	2.3198296068909716
many definitions	2.026260250766906
changes made	2.00220806117178
category names	2.2256816819452245
speci c	2.8768205175693793
similarity metrics	2.4416644243707353
terms used	2.7756672331475727
structures associated	2.4967981217917146
one hypothesis	2.297574746882776
relations based	2.159746288077376
xu et	2.1884068345583314
longer texts	2.1543888787937475
five characters	2.1245637044373717
one predicate	2.3155740676527605
relevant facts	2.1939531715857408
structure within	2.453255888575513
accuracy would	2.292453688514974
edges e	2.0355401813680145
preliminary analysis	2.549554001670492
disambiguation module	2.1309533457993064
space reasons	2.11638795219105
ney smoothing	2.1561678215797735
document summaries	2.0928623970669147
words include	2.1413469986390323
tree algorithms	2.0074734400897967
following sentence	3.3744515745554895
various components	2.780880903405426
corresponds roughly	2.2380656204269957
coreference task	2.3514707517428146
possible sentences	2.236207490380668
various parameters	2.2631802623080817
appropriate word	2.4784354496000445
categorial grammars	2.66479292330742
alternative approach	3.0762728800424144
en la	2.007038640285683
structured knowledge	2.0646820321914494
utterance may	2.514624768642089
categories associated	2.00238351760398
syntactic differences	2.327403995660175
difficult problem	3.024347818077277
test example	2.276264016384062
detailed presentation	2.159746288077376
representation language	3.163759990684813
arabic morphology	2.095786932304709
user input	2.9139916176123504
terms representing	2.012347776929124
features capture	2.1161528435313297
cornell university	2.000178283129414
john carroll	2.060481905665516
user interface	3.3759419148042977
human intuition	2.3000001696688788
based tool	2.014014536653706
substantial work	2.0074734400897967
software modules	2.1231057695388458
anaphoric reference	2.849095226705676
actual language	2.007315006862137
last case	2.515272207357606
rare word	2.072697162060231
creates two	2.022261218861711
process within	2.022261218861711
second solution	2.167885403222368
word boundary	2.844075279530768
w using	2.014014536653706
contain relevant	2.045521226679158
french texts	2.1039380034279427
particular domain	3.164520806351225
used language	2.022261218861711
second language	2.844764957179811
graphical representations	2.213318758370463
two nouns	2.75572986898843
problem encountered	2.014014536653706
equation 5	2.518378504571159
probable tag	2.322778367371739
figures 8	2.0880414073297953
logical object	2.1358664885908043
new elements	2.342324324567648
theory must	2.1156428254927526
grammatical features	2.753086656626907
improvement due	2.1479184330021646
b e	2.7823675856897183
last column	2.960864720858691
easily understood	2.1032032578825586
interesting research	2.3523313048426364
paragraph breaks	2.011237136247196
data derived	2.3097823680289724
k clusters	2.04755904944182
different pattern	2.089026915173973
system structure	2.049069748176902
performance across	2.480570340527418
specific types	2.7943276996160606
basic terms	2.020363647132216
level morphology	2.555630561413788
absolute values	2.366082551445672
semantic categorization	2.171178031468078
grammatical constructions	2.6068373546113115
another method	2.6968616101077494
two actions	2.327075730122159
translation without	2.1413469986390323
value given	2.022261218861711
approach provides	2.7834775250201687
major part	2.1378941928231434
linguistic units	2.7963007214898212
less effort	2.5076682263483554
root symbol	2.0868352600836486
human error	2.2078449813130154
tree fragments	2.4939969569987586
goldstein et	2.0520370107650927
12 words	2.0956483125987315
constraint violations	2.0891511348792857
translation may	2.272505380879168
unification based	2.683720148910221
desired output	2.487258520884037
new symbol	2.007038640285683
functional information	2.31962741375229
coordinate constructions	2.0846839150790553
lexical form	2.6527151373778968
statistical classification	2.0219742875792464
either order	2.0709020796066153
fifth sighan	2.3005523645853
language component	2.544845789541892
cky algorithm	2.2837755959660377
condition 1	2.1411867993382545
function words	3.436823975153161
practical solution	2.1254705203338395
constituent boundaries	2.4946903016549555
general grammar	2.369787405360392
anaphoric pronoun	2.2010503919436886
turing machines	2.02717722838181
one sequence	2.318793079863193
enough data	2.771264948093652
thus consists	2.1479184330021646
overall goal	2.727913714675366
phrase alignment	2.2211700834871344
unlabeled examples	2.0479437416278454
technique using	2.109437912434099
translation requires	2.020363647132216
two phonemes	2.0256307281812576
work involved	2.0965741974051317
heuristic rules	3.0757570451682277
stochastic context	2.5066040539535726
question whether	2.5471177226057558
easy task	2.65635595830894
specific domains	2.7963703895206504
word consists	2.403331244885159
database access	2.463129860956629
3 shows	4.105548804823647
relation names	2.041476093116467
possible cases	2.346889123261037
much information	3.0351778613826736
objects within	2.1222463152036486
certain part	2.278461809175595
parser uses	3.0227380129693358
deutsche forschungsgemeinschaft	2.257869825585851
6 times	2.3130951516759186
without fee	2.8592494356475466
different uses	2.358255776160617
models without	2.2062448606993037
current location	2.0157573240154925
special kind	2.7036950231278056
higher levels	2.6100100162942685
approaches rely	2.200598690831077
likelihood ratios	2.0532255206686756
high frequency	3.2105788997634543
testing examples	2.129954971091929
detailed discussion	3.10552835058796
future version	2.026260250766906
graph represents	2.1222463152036486
large bodies	2.296234813805337
values according	2.045521226679158
research question	2.469909103548966
possible antecedents	2.6933910693551253
based discourse	2.1647581039515353
results show	3.803509547037843
cky parser	2.0021324668667613
first phase	2.8160211395398362
documents within	2.3028207919454977
search procedure	2.7448539444370863
performance would	2.054295478719192
word correspondences	2.05224394935537
programming language	3.229054611334319
katz 1987	2.008226999543247
annotated text	2.772161336750534
another possibility	2.8974100407146866
target predicate	2.033273034910213
linguistic feature	2.2888729275082835
current feature	2.108179066756418
complex word	2.346931582586499
two argument	2.189546252614866
information supplied	2.2421869127123264
c must	2.2224068987457315
similar structure	2.453255888575513
good deal	2.315930051494693
support system	2.115398787992162
feature unification	2.112453766509824
extensive experiments	2.332240952226321
crucial feature	2.1023887620661283
every student	2.084275265325128
new state	2.5719459729038214
powerful tools	2.0074734400897967
following paragraphs	2.3310204571264768
separate process	2.0817120252117016
detailed error	2.2112071020073643
tree fragment	2.2540911074877856
two weeks	2.1287628657654403
000 words	3.6419595352370613
temporal logic	2.0607202793422417
length principle	2.0402381534980756
second time	2.612026322609136
40 words	2.480943185627197
clustering words	2.1704795969713517
almost perfect	2.222669363564495
specific categories	2.131569621150367
two conjuncts	2.181355330160005
indirect request	2.0738985247364026
fixed phrases	2.3244150737863607
tell whether	2.1830679458060054
independent words	2.102736033565815
possible segmentation	2.1462245538298697
multilingual generation	2.218957106579605
take arguments	2.045229961808099
edit distance	2.858427699009128
extra information	2.799039345552719
using methods	2.271753499640152
five years	2.2169080830348027
conceptual level	2.5907266633764383
words generated	2.082836083424519
together form	2.318793079863193
simple search	2.116591742190463
different meaning	2.6902426722536297
f1 score	2.2770689171868757
lower accuracy	2.520512299819756
achieve performance	2.122246315203648
6 evaluation	2.0621571607676303
h l	2.566442259840419
lexical descriptions	2.2472521072790617
application programs	2.0514054040135554
based features	2.617377524152227
0 indicates	2.331780823064821
selection algorithms	2.067081229293324
avoid ambiguity	2.014014536653706
measure proposed	2.053986137989304
improvement obtained	2.0965741974051317
different n	2.022261218861711
trees containing	2.109437912434099
canned text	2.1276821286650733
classes c	2.1513738263092224
test used	2.007473440089797
first attempts	2.2169936022425736
simple term	2.031264650127311
exhaustive search	2.753182534905945
one year	2.1710841765814624
node label	2.494185256177411
class names	2.2194030391119117
good evidence	2.2577963294869763
compare different	2.6239319852280314
preceding sentence	2.7067095998454467
header information	2.079019887609717
nouns appearing	2.105195669894195
common framework	2.1984572101092845
28 aout	2.060145782650642
candidates according	2.240851989911108
syntactic restrictions	2.3301285965227896
functional linguistics	2.079019887609717
different names	2.00565885731747
final solution	2.0947271027985526
partir de	2.071624015659564
n denotes	2.1413469986390323
various knowledge	2.484641189426206
input could	2.061212299166531
final language	2.193056710825906
flat list	2.1312839727750825
techniques using	2.30606794696972
lexicalized tree	2.746561087058133
atomic formulas	2.0366374744353792
bilingual training	2.0826398427153876
simple examples	2.55705955157426
researchers use	2.026260250766906
using pattern	2.135844661161516
base nps	2.010254445146706
nodes labeled	2.3951683502608727
first text	2.154626642362153
sets described	2.109437912434099
german sentences	2.2742878796306982
unstructured text	2.3044835637674232
knowledge extracted	2.00238351760398
expressions involving	2.2275895034737263
represent linguistic	2.177312398504085
certain nodes	2.0824927688889554
es de	2.245977705409868
3 senses	2.075729255121515
segmentation algorithms	2.3120859661529627
speech system	2.808201528328803
ranking algorithm	2.173735553102911
need two	2.1774937848124125
bonnie webber	2.3728830312074773
following attributes	2.140860171621032
results even	2.172274429303222
modular approach	2.144331852103044
algorithm considers	2.462949053409004
first pair	2.3099619353525487
ideas behind	2.2112071020073643
processing strategy	2.2915678590386976
positive results	2.045902079164808
like verb	2.0817120252117016
research focuses	2.067747107964575
first implementation	2.5447463944227295
answer would	2.1499148428335464
corresponding output	2.053986137989304
whereas others	2.3953272468316316
new constraint	2.3282069724983367
rules consist	2.0074734400897967
systems become	2.1946340157728974
known problems	2.3388887802872667
last set	2.200598690831077
shallow parsers	2.1068067231610383
rule allows	2.433738217595396
noun modifiers	2.4136548195732956
systems trained	2.29706382359031
without difficulty	2.1479184330021646
six months	2.054381152197297
whole structure	2.1984572101092845
example dialogue	2.2756755572061147
entity detection	2.3527614963784598
one reference	2.4742957091193407
pairs based	2.159746288077376
level using	2.3723877389097376
labels assigned	2.3303417736661065
training set	3.9120684231885616
prior distribution	2.2629906157546325
example used	2.089026915173973
parsers based	2.2618943472263284
ambiguous input	2.2458216818148236
distinguished symbol	2.146224553829869
large proportion	2.375739731742398
existing discourse	2.022136909889968
another view	2.022261218861711
small amounts	2.1635080400217968
document clusters	2.00944705989149
entropy method	2.0735937644612115
approaches based	2.6470486187383586
french grammar	2.2857443413397442
constructions like	2.6167678594633594
important content	2.035905703361854
slavic languages	2.0148243230043397
important characteristic	2.3920948169591325
token sequence	2.105020264746285
dans une	2.0860558776801588
two columns	2.631815733039671
1 let	2.4658668186702544
using svm	2.236207490380668
clear improvement	2.067747107964575
little knowledge	2.014014536653706
john loves	2.2010733849167567
equation 3	2.8115505049581344
three versions	2.2227571870018084
column labeled	2.1336234409631984
edit distances	2.150476107451325
parser fails	2.2863172876464515
sample text	2.6587987785986957
semantic composition	2.4283868396651704
following procedure	2.850022073533449
also cannot	2.116591742190463
der beek	2.0355401813680145
model structure	2.217363696646407
two months	2.021063001257434
scoring procedure	2.1527910912671047
previous n	2.184223228921038
domain concepts	2.4833119974334528
examples used	2.3483606540998085
closed vocabulary	2.0684303065768828
standard dictionary	2.141475643340291
identify possible	2.2776740307447065
cannot rely	2.0887626916208926
15 words	2.4571494633554742
translate english	2.045521226679158
similar experiment	2.267991541156711
noun phrases	3.8762658702264003
dictionary definitions	2.832258539080286
standard text	2.313997171686319
syllable structure	2.319257080336805
complex grammar	2.089026915173973
reported speech	2.160497378890954
meaning representation	3.048947544281548
et ai	2.851162434456198
become important	2.2532512925302806
arbitrary depth	2.0817120252117016
word j	2.2364407446696575
document number	2.0423259728447545
relies upon	2.380830363798011
des r	2.004238053548262
individual tokens	2.30606794696972
data elements	2.1090188943970896
knowledge represented	2.4460318546403155
features also	2.264070934029513
first process	2.21826141526814
lexical hierarchy	2.135575509265734
translation dictionary	2.230145923014924
e would	2.133165345977436
user specifies	2.2074757805024428
average distance	2.106506287794641
cfg rule	2.097185429160314
training conditions	2.040707995161948
text structure	2.824134070806195
many questions	2.311507707094213
system generates	2.9720408110972576
possible source	2.218434485750926
another project	2.0074734400897967
trees built	2.113344143752296
experiment used	2.1612468534022073
semantic annotations	2.327414726568135
statistical processing	2.1380488015835484
processes may	2.2421869127123264
psycholinguistic studies	2.264070934029513
system exploits	2.022261218861711
extraction application	2.1000868080586823
immediate left	2.257879425806224
algorithm development	2.0435975245753424
entity identification	2.1163937317345756
entities mentioned	2.6153987872398896
explicitly state	2.1222463152036486
detection algorithm	2.2789684734005182
features may	3.0144791865271405
summarization evaluation	2.3080026708713843
nl understanding	2.1877257454107046
tag assigned	2.2251427952863687
probability distribution	3.522799905210988
structural representations	2.071454311737682
production systems	2.092049087357028
f score	2.1964224362917255
natural languages	3.4412461879788334
systems presented	2.0074734400897967
overall accuracy	3.1000622751645017
specific phrases	2.0277178367412123
model yields	2.316121250352834
logic program	2.105195669894195
grammar uses	2.126823141441437
section presents	2.9841784749551805
algorithm assigns	2.1946340157728974
perfect agreement	2.1245637044373717
answer type	2.288489168328619
2 uses	2.3005523645852994
supervised training	2.801329200885694
parameter value	2.28010507945228
central task	2.070902079606615
accuracy improves	2.099458264360342
transfer rules	2.6497219243677415
simplest form	2.4241657936680436
low frequency	3.1049020814675603
multiple levels	2.5851662054727305
syntax analysis	2.2868881080684305
variable number	2.3091749723655277
arbitrary length	2.4849003933319374
three elements	2.5567795844238517
clause must	2.155589295660503
wise mutual	2.222267530886849
cognitive load	2.28725953349893
standard vector	2.060145782650642
simple terms	2.1023887620661283
000 entries	3.0330613434703713
3 table	2.1275552563638707
analysis might	2.045521226679158
ocean systems	2.015759442681759
retrieval effectiveness	2.213525488657558
word sense	3.7103404538239078
average document	2.0355401813680145
text passage	2.0748550852359298
linguistic sophistication	2.020363647132216
relation expressed	2.267991541156711
logical operations	2.139044594632718
training algorithm	2.895087357585241
news text	2.3962271819939667
specific instances	2.2207046594768247
parser works	2.3365869267409933
medical language	2.3637227566136794
improves significantly	2.089026915173973
small grammar	2.2658748144574723
ary predicate	2.0172047205465917
automated methods	2.0843177461209628
without loss	3.0699392510593277
weight assigned	2.4800274354647276
two components	3.165864785176485
much effort	2.7732460415612255
two parameters	2.849619532439954
sentence selection	2.370481567746602
small hand	2.067747107964575
key role	2.245080114581675
long list	2.2685844072039183
universal set	2.007315006862137
semantic unit	2.438894041499663
null tion	3.1248985319544134
tool called	2.24583021875162
one member	2.357214513223689
supervised approaches	2.141105015900133
total words	2.1432314806484
evaluation procedure	2.628025624807374
best models	2.1222270206314318
statistical taggers	2.043801523656219
sphinx system	2.005606918154524
evaluation exercise	2.1712320042435467
annotated test	2.047815473751682
everyday life	2.116591742190463
1994 association	2.022261218861711
default settings	2.4005083165968557
source document	2.431668774011629
two modifications	2.0965741974051317
vector w	2.1304376757279937
global level	2.275862962051041
value indicates	2.1678854032223676
daniel marcu	2.045521226679158
complex patterns	2.322778367371739
linear combinations	2.1563079384871173
relations cannot	2.1413469986390323
english side	2.675192644691238
systematic approach	2.067747107964575
planning system	2.276051141065744
entropy approach	2.392191586565283
linguistic work	2.3398754723868356
current work	3.3358615466607455
1 depicts	2.5412076060528683
representation structures	2.551494361469825
patterns extracted	2.2207046594768243
final configuration	2.11638795219105
research council	2.937779297983112
concepts may	2.368834809141683
wiebe et	2.375778737379104
single sense	2.5414052312275053
whose similarity	2.0223531762070586
grows linearly	2.0376102740271858
finite form	2.0452143290890312
direct implementation	2.1029028550448734
location within	2.200598690831077
shed light	2.5055533459995827
subordinate conjunctions	2.051443724448745
proper probability	2.049069748176902
surface structures	2.5534160410959355
definition 7	2.166102255087602
2 n	2.517805594523519
word positions	2.4692820935818824
new dependency	2.072697162060231
statistical methods	3.4196765928696005
two rules	3.2053223320573374
certain characteristics	2.0817120252117016
time axis	2.0018213545663768
particular aspect	2.235522740985398
several sub	2.3922788066689504
parser returns	2.3177155258598034
sentence appears	2.3125352367853242
probabilistic modeling	2.1097950308192193
complementary information	2.0722900984192454
become necessary	2.031264650127311
tagged sentences	2.43325908340919
precision errors	2.143902638500865
context must	2.2207046594768247
one domain	2.918555715335519
demonstrative pronoun	2.2505373407091294
state corresponds	2.020487216920956
exist among	2.122246315203648
parser identifies	2.022261218861711
abstract representation	2.533873479772075
rules described	2.6030934787416564
development foundation	2.0376102740271858
verb predicate	2.1300968175171144
selection methods	2.2610399437278126
next time	2.403941859777599
whole words	2.126326666154899
active edges	2.3530635448091313
x n	2.6826774063622882
nlp modules	2.083963992592561
system running	2.109437912434099
language change	2.050757568018618
grover et	2.308202774842574
major importance	2.045521226679158
individual characters	2.365645084287615
data must	2.3605881269336804
two formalisms	2.0101902550589283
following notation	2.4611017632787435
p 1	2.108179066756418
elementary discourse	2.1171926832729326
approach towards	2.172274429303222
information described	2.145668076397672
parser using	2.753136223329459
wide use	2.0312646501273113
della pietra	2.354750360202228
every clause	2.1275552563638707
selected set	2.3548005898232187
performance improves	2.470467437983557
processing language	2.0355401813680145
string x	2.422480160714459
discourse units	2.5906657181670987
lexicalized pcfg	2.0571052519152087
underlying structures	2.0490424470581194
previous works	2.9291983985825376
class membership	2.469987959574842
statistical technique	2.275125009000975
different agents	2.0722900984192454
random order	2.460177056879256
lisp machine	2.3789913216204757
forward pass	2.0156831546491727
text sources	2.2996667813525304
null mation	2.089026915173973
sense 3	2.03980194380808
first test	2.2799475881571714
several candidate	2.031264650127311
computational tools	2.3494271666348956
based decoder	2.1124063020200383
chunking task	2.225294544611655
two context	2.2965916473768724
time line	2.1446399594323617
negative values	2.2827808432516172
independent human	2.0355401813680145
whole utterance	2.27225588938143
phone number	2.1904523038789647
many errors	2.7422169287368465
initial value	2.4453115007638466
general feature	2.1245637044373713
named entities	3.410290880582156
define three	2.3130951516759186
6 hours	2.070902079606615
two children	2.4080561147698702
added features	2.015795287891615
also point	2.159746288077376
following tree	2.230524125991308
spatial relations	2.291421227689426
different degrees	2.8990535467573273
koehn et	2.520980050396671
expressions like	2.9072745953265073
flexible framework	2.067081229293324
following relation	2.053986137989304
conference registration	2.0031504296467904
journal articles	2.8833667122669877
semantic specifications	2.105195669894195
empirical probability	2.0965798710124774
many possibilities	2.431764288227561
linguistic processing	3.0351219807189898
side effect	2.947674224280928
argument types	2.2434342168090753
adverbial clauses	2.0743965972347684
solution could	2.1413469986390323
incorrect ones	2.369841578143437
four methods	2.2857443413397442
domain entities	2.1490267116423087
time applications	2.020363647132216
grammar formalism	3.3231783558816863
5 concludes	2.6383330595080268
remaining sections	2.2776740307447065
canonical forms	2.3504965343002957
noun group	2.582213686254386
limited data	2.1976485766852405
specific speech	2.102921514525577
known word	2.448499258423991
performing systems	2.419965457381231
generation conference	2.045521226679158
independent information	2.0919512504091182
existing parser	2.00238351760398
algorithm discussed	2.022261218861711
computer system	3.0140648093415474
new techniques	2.1786566600374835
sentential context	2.33213580110774
contain non	2.1946340157728974
higher quality	2.3298840765841042
sparck jones	2.6366972807113993
high score	2.599647662525044
interactive dialogue	2.310596103170427
present framework	2.0599211086933935
authors would	3.1619453264737474
semantic expressions	2.0409232428624824
precise characterization	2.0376102740271858
question remains	2.331780823064821
semantic domains	2.2169246295592355
significant information	2.189546252614866
patterns using	2.3432734531227437
rule induction	2.3191119792241137
japanese word	2.632919998764939
best translation	2.682238380791955
adjectival phrase	2.3109116208104994
second alternative	2.406676464637198
higher similarity	2.2112071020073643
correct spelling	2.231505672118325
whose corresponding	2.0967561837044446
recent literature	2.357121432847829
works quite	2.159746288077376
real application	2.4404450794090975
target domain	2.3583917576839264
pragmatic information	2.742541085271363
standard test	2.6314733709397857
simple probabilistic	2.001705936914745
correctly predicts	2.358255776160617
two knowledge	2.2975747468827756
classification algorithms	2.3429208778801653
methodology used	2.4420550512076584
advanced research	3.329021061674891
syntactic errors	2.2577729352457645
rich source	2.5577806922873396
candidate answers	2.305600257511024
tree traversal	2.1285486098719364
result could	2.0376102740271858
greater number	2.629380342691895
output structure	2.339554560148772
words included	2.396803858151075
level models	2.092263424163747
boundary tones	2.2272356707644656
contain multiple	2.455983846584365
computing resources	2.089026915173973
outside algorithm	2.5580404058070103
phrases using	2.2838818466058592
specific entity	2.233927048175921
many users	2.45078770424615
structures used	2.754571605823417
level model	2.540973396733756
prediction accuracy	2.357965256465166
simple process	2.1543888787937475
vice versa	3.673571940564071
corresponding feature	2.4476623082632964
ones based	2.1023887620661283
significantly outperforms	2.6677899579337736
translation research	2.272505380879168
processing time	3.2293585611913094
choice depends	2.014014536653706
particular language	3.044173696393068
specific properties	2.0866040331547397
corresponding verb	2.2243484856120617
one limitation	2.1774937848124125
resolution algorithms	2.284323867320806
whose size	2.2601598698812575
set defined	2.079019887609717
generating sentences	2.3097823680289724
syntactic form	2.892701710631915
always selects	2.0376102740271858
document types	2.1716729957672225
6 summarizes	2.331780823064821
features appearing	2.0790198876097175
sentences consisting	2.022261218861711
words ending	2.4249002327303764
evaluation software	2.02394593055395
partial derivation	2.013798055147169
information could	2.7849668903292004
document retrieval	3.1088310123974847
appropriate feature	2.3161212503528343
information scores	2.0123114646746347
model instead	2.067747107964575
surface form	3.2765156898768644
arcs represent	2.269813783347405
last letter	2.109711365585278
categories used	2.1037328099486374
control verb	2.0381169104851677
growing number	2.10212398879197
figure 11	3.0407717624987693
system searches	2.3605881269336804
free variable	2.371219022850079
assign different	2.2328679513998626
tagged version	2.1222270206314313
section 0	2.0176429747071856
features contribute	2.2421869127123264
language processors	2.1110509596334177
constraint propagation	2.2453060862861562
adequate account	2.0817120252117016
linguistic methods	2.108179066756418
null however	2.5765298321699723
english grammar	3.0387510356801033
qui se	2.0351580383855916
subject nps	2.09570925880563
conceptual representations	2.326376326198785
training procedures	2.2827808432516172
times greater	2.0843109059317095
possible world	2.2832432278260026
three letters	2.27225588938143
questions using	2.2188286573520086
special processing	2.183647914993236
described method	2.0023835176039797
language systems	3.3083350214858527
single instance	2.278461809175595
distinct categories	2.122246315203648
generation components	2.481700234601778
multiple relations	2.0551182606938574
greedy approach	2.012347776929124
different assumptions	2.2147431158325057
word initial	2.0643620962409943
one phoneme	2.1954890274836183
parallel english	2.1462245538298697
coded rules	2.288240590275326
modern standard	2.1615612190715634
surface order	2.5455534388772207
better estimate	2.0074734400897967
transfer lexicon	2.0288331617982958
sentence context	2.3575505004625135
author thanks	2.089026915173973
particular way	2.6167442200032394
steps 1	2.5263904435364712
following sub	2.075729255121515
variation within	2.0539861379893036
bad ones	2.0918364857418936
intuitive idea	2.2169936022425736
right part	2.5252265952304027
extraction approaches	2.02585040092793
structural description	2.743822454632862
particular action	2.086551350552231
manual correction	2.2168941177783568
good translation	2.5234341705757926
arrival time	2.141440835038482
using l	2.1290482690107417
many details	2.3879279742060486
darpa contract	2.3644803559732956
preprocessing phase	2.2959583628072817
problems using	2.2234532051296254
three reasons	2.680109832137205
diverse range	2.012347776929124
form must	2.060145782650642
hierarchical structure	3.34146623025448
10 sentences	2.3520539631001647
language analysis	3.198694717963979
several criteria	2.329081436908228
13 shows	2.2224068987457315
state devices	2.0659571110450177
automatic labeling	2.0923172520507167
heuristics used	2.3362105897148213
major problem	2.980767804262034
concepts related	2.1984572101092845
speci es	2.0299278800678926
english output	2.1192403894738625
possible pair	2.3000015484793472
bleu score	2.7422250489942153
higher score	2.734066029926233
formal point	2.2532512925302806
chapter 6	2.1245637044373713
small probability	2.0376102740271858
special type	2.5221715616788662
contains 1	2.3922788066689504
generation rules	2.0995149736624272
primary interest	2.24174354648276
time step	2.24629413262962
identification task	2.5624390731436226
different state	2.0157952878916148
recall value	2.039007283873737
provide results	2.183647914993236
entropy classifier	2.430074131186293
syntactic descriptions	2.08219941271214
next node	2.184223228921038
language questions	2.3500286266584824
two individuals	2.159746288077376
unsupervised way	2.0530842980529727
single words	3.274596481123723
1 describes	2.5412076060528683
method depends	2.2275895034737268
phrase like	2.57130986877489
language structures	2.0847374545525685
symbols used	2.4459112107066288
linear relationship	2.0160320742930855
potential users	2.4949866697330707
random fields	2.8347936174590953
tile word	2.4923592291587133
tables 1	3.040862648982013
signi cantly	2.712314644766585
inactive edges	2.207185960699194
translation would	2.34443972705697
context consists	2.0376102740271858
disambiguation may	2.012347776929124
eac h	2.159746288077376
methods like	2.222406898745732
adjacent word	2.250669973561125
grammar checking	2.2009713577604533
declarative sentences	2.7200463250080626
conjugate gradient	2.059894410367275
vector machines	3.25126214371912
uniform distribution	2.933933438105527
every state	2.274257737378864
gives examples	2.366082551445672
maximum performance	2.00238351760398
actual application	2.1413469986390323
test time	2.0103115008582284
grammar using	2.5689804890257024
definition 2	2.8887029004685845
structure constraints	2.1285451397149435
intermediate step	2.3922788066689504
set w	2.176234939134731
definite article	2.8446997384873227
japanese grammar	2.3818432810222685
extracted words	2.2011686204719005
often contain	2.774446948918292
deep syntactic	2.6548229154462404
data consists	2.853133705281302
distinguishing features	2.3258667452149284
notational conventions	2.169237981741658
average user	2.026260250766906
qualia structure	2.1874530798748184
adjectival modifier	2.116591742190463
similarity value	2.619399428548095
symbols representing	2.0376102740271858
formalisms like	2.0123477769291243
single utterance	2.598826418613489
word problem	2.31680497220451
linear programming	2.0660172131101415
statistical parsers	2.7241886676481064
different morphological	2.313328091895502
briefly discuss	2.7962070042875036
level translation	2.0178014047825186
subject would	2.1023887620661283
morphological categories	2.3130394093215143
potential application	2.230524125991308
dependency parsing	2.6152059956083855
set v	2.417618427399807
models would	2.2380656204269957
approach called	2.109437912434099
possible meanings	2.552422512448631
translation table	2.1917381925409414
quadratic time	2.031231937071561
segmentation ambiguities	2.1477639802462143
internal structures	2.5357127958402033
figures reported	2.1275552563638707
simple test	2.089026915173973
verbs must	2.067747107964575
string language	2.0397786916969833
models achieve	2.2112071020073643
ground truth	2.2711430025210397
extension would	2.172274429303222
language competence	2.014014536653706
one component	2.8965492299796853
copestake et	2.4262454853058815
tagged data	2.710575891669398
constraint solver	2.2525253259766354
average results	2.305097752546449
method extracts	2.146224553829869
local contexts	2.357939535974931
000 features	2.0432296517672346
personal names	2.493284440018879
intermediate result	2.099458264360342
ge et	2.0527835904394376
acquisition task	2.026260250766906
prolog program	2.3397389946891236
parsing performance	2.6445675453782833
sense distinctions	2.7014538078651618
practical application	2.729478643984572
atis domain	2.7271799853694842
grammatical framework	2.3032141113275078
probabilistic approaches	2.1067733244316074
linguistic aspects	2.4793959105551036
parameter c	2.0490697481769016
precision increases	2.135844661161516
formed utterances	2.0435211751140128
underlying application	2.093143489625991
questions arise	2.166102255087602
l c	2.303952688869935
marker passing	2.002417726544942
id number	2.046843490766307
input corpus	2.074066354066776
low score	2.452321695174711
generate word	2.0656667908183506
contain exactly	2.2421869127123264
unification failure	2.175872542799265
mathematical models	2.1708197166165597
technical domains	2.41747566988659
performs better	3.2927592667268666
tile user	2.2736914730037947
similar phenomena	2.172274429303222
new structure	2.1360979043114887
see sec	2.119240389473862
linguistic examples	2.0073178378257417
international conference	3.0207818114495177
search based	2.020363647132216
dialogue corpora	2.168943201051714
structure analyses	2.022261218861711
words aligned	2.00238351760398
lists used	2.116591742190463
analysis process	2.803715539907626
semantic property	2.0865513505522317
following recursive	2.2492578335740094
different models	2.9853815640666723
phonetic models	2.3348101979457025
obvious reasons	2.4012688714330688
process generates	2.014014536653706
extracting information	2.8070036225868864
first pattern	2.031264650127311
extensive knowledge	2.0817120252117016
combines several	2.045521226679158
directed edges	2.1499148428335464
competing analyses	2.2427935805269064
word prediction	2.0341629068233575
tile results	2.183647914993236
l 2	2.248313594213265
problems arise	2.81818012341906
also need	3.0318670111714034
phrase types	2.6162523052866278
independent set	2.164235940665368
possible applications	2.2922536187453257
cross references	2.0051584301940206
alignments using	2.171178031468078
entre les	2.2427420687189534
like data	2.164039709875629
parsing approaches	2.2237394420595993
significant effort	2.200598690831077
strung together	2.067747107964575
several non	2.086884416965048
shares many	2.2328679513998626
several times	2.7953731870349103
model human	2.4364022959537746
search mechanism	2.042148066313454
data base	3.0812600246104513
discourse segments	2.6326493911549838
significant percentage	2.1222463152036486
slight decrease	2.166102255087602
set described	2.3792362054750646
extraction procedure	2.335389569203854
resulting state	2.2033628606828244
main points	2.323740184366363
classification methods	2.471039640877453
complete knowledge	2.227249320716272
n characters	2.0659571110450177
independent modules	2.329688915787785
ran two	2.122563170379007
structure representation	2.619786496335776
project funded	2.135844661161516
three techniques	2.0157952878916157
u p	2.001031596754819
semantic level	3.010046443140059
individual documents	2.2721169897388886
national science	3.4466805979211066
potential answers	2.0856220814628434
adequate translation	2.079019887609717
evaluation program	2.044877827032594
several corpora	2.2062448606993037
word entries	2.5768737795168595
returned documents	2.067081229293324
related question	2.108179066756418
development purposes	2.067747107964575
computational problems	2.2492578335740094
sentence construction	2.049069748176902
one basic	2.397957210424912
future research	3.6075198858661124
rule given	2.3922788066689504
generation system	3.344373860611553
person singular	2.910781057830144
running text	2.8193147258319486
dialogue act	2.5206420366720947
resolution methods	2.0721570056738
common way	2.3422763733583603
weakly equivalent	2.481366153498846
corresponding sentences	2.341285221188147
two probabilities	2.319324296244544
model information	2.2231389704589795
integrated model	2.1653190899653616
translation methods	2.247574109847509
german text	2.3058736679435454
different combinations	2.888302986490586
natural extension	2.5673100826218893
level discourse	2.1763122466007108
attachment problem	2.179420927296495
information based	2.5489971548972266
figure 13	2.6206723424699168
since muc	2.099545087983816
common subsequence	2.179351126231439
tile data	2.014014536653706
language used	2.8859310265176337
knowledge consists	2.164235940665368
greater accuracy	2.122246315203648
new entities	2.0190188659123502
input phrase	2.1078516753501777
related formalisms	2.2965916473768724
models use	2.425593477889689
following criteria	2.711053946253264
wrong choice	2.116591742190463
slots may	2.045521226679158
produce sentences	2.108179066756418
small collection	2.1543888787937475
based framework	2.6394602941993703
n possible	2.031264650127311
difficulties encountered	2.0074734400897967
several evaluation	2.0790198876097175
similarity matrix	2.2999917376093846
higher confidence	2.1462245538298697
text information	2.3553259035798133
current segment	2.090986735518801
understanding research	2.0601457826506424
complex nps	2.219404517741477
patterns within	2.1543888787937475
sentence constituent	2.0074734400897967
previous models	2.460663626535683
many training	2.1290482690107417
second generation	2.004280538585712
relation holds	2.813591135185031
paper presented	2.594827371013212
lr parser	2.379635720494263
longer sentences	2.8894006914633668
system actually	2.227249320716272
whic h	2.2855868712078773
lexical relation	2.1411463286379018
frequency function	2.0021324668667617
system capable	2.709727741353372
source material	2.127375476291898
coreference relation	2.106013032782654
direct use	2.272967390283797
different surface	2.5591742079035504
sciences research	2.109437912434099
lexical strings	2.0085691891255983
level features	2.62069603336983
five iterations	2.0299278800678926
deep parsing	2.0196100575469704
main motivation	2.357121432847829
first n	2.611253230043792
many language	2.734777860254156
lexical items	3.8672593917348603
short summaries	2.151302016846157
phrases within	2.5552022266142815
better retrieval	2.049069748176902
using constraint	2.1222463152036486
new instance	2.4753493949047822
anonymous reviewer	2.5922893931369995
fold cross	3.4041617944486346
particular analysis	2.022261218861711
given c	2.014014536653706
developing systems	2.116591742190463
left column	2.331780823064821
uses features	2.2427935805269072
following manner	2.8706558399912976
actual sentence	2.099458264360342
given topic	2.4949936044935948
assumptions concerning	2.014014536653706
system modules	2.269929398602674
larger data	2.658401881402122
j e	2.365567038782496
french sentences	2.4828425292721814
second sentence	3.183870160099325
expression containing	2.1023887620661283
possible senses	2.689740088139312
better accuracy	2.72946391259611
previous subsection	2.4712936923738704
paper extends	2.289526504269894
general use	2.3000015484793472
attentional state	2.227767734411367
highest values	2.0376102740271858
recognition procedure	2.305097418252439
form part	2.247643627139545
documents returned	2.4433527492450597
way people	2.1414753566437956
set could	2.200598690831077
null computational	2.4871944242259443
different translation	2.563802257828121
lexicon construction	2.222669363564495
nist score	2.1774561814065905
dialogues collected	2.185488549432976
different dependency	2.089026915173973
pair grammar	2.4984699872881198
paper represents	2.1023887620661283
ann arbor	2.904550886185094
later versions	2.172274429303222
data compression	2.0888683166324267
sproat et	2.228166051160129
language descriptions	2.289161918175575
following method	2.272505380879168
vast amounts	2.0598815590975614
past research	2.352331304842637
rules defining	2.178653004950429
e l	2.830625470583142
language interface	3.055641829499304
pp complements	2.1854885494329763
jelinek et	2.187328299544666
psycholinguistic research	2.1982506992808917
based grammars	2.9685368427916736
sentences rather	2.138048801583549
g c	2.2903625028082466
feature specification	2.4826785490372516
database using	2.267991541156711
language application	2.198250699280892
entities may	2.406719372930616
group together	2.6873989323418663
scoring mechanism	2.3123777978931637
epsrc grant	2.1479184330021646
structures contain	2.045521226679158
depends crucially	2.339880004558011
conceptual knowledge	2.559233601711133
complement clause	2.274643121291983
root words	2.273448091398798
knowledge base	3.6831812762917533
alignment probability	2.153880737727453
dimensional matrix	2.0312646501273113
two pos	2.0432296517672346
male speakers	2.0720531824241535
results confirm	2.4943690648994172
approach taken	3.172657947245391
evaluation criteria	2.752814497583264
optimal alignment	2.2135586619570162
domain information	2.60845367466309
new words	3.17561594083634
oriented approach	2.404915492293644
basic notions	2.3261044832209703
parsing procedure	2.435174862293796
system determines	2.490398246754468
next input	2.4242224416322045
word information	2.413882421208396
use rules	2.0965741974051317
end user	2.398167577343168
additional level	2.22672986885133
relative merits	2.3483606540998085
kernel function	2.642981335265519
tree must	2.4691898293704266
gave rise	2.2449674076758663
current tag	2.099406982915628
list includes	2.1023887620661283
systems designed	2.257869825585851
latter type	2.5777505121879893
paris et	2.0262602507669065
process stops	2.172274429303222
past participles	2.5746964493917193
make assumptions	2.1543888787937475
complete constituent	2.052352800514945
two co	2.1943253913209606
boundary tone	2.145005793049042
english verb	2.9527540227336284
specific components	2.1275552563638707
unification formalisms	2.114299018453181
semantic properties	3.141397897340993
test items	2.0619969764780968
whose performance	2.172274429303222
extraction technology	2.3650371565042465
stop words	3.067625982932222
order given	2.2955083501313895
text available	2.164235940665368
represent syntactic	2.233997204181393
words 3	2.022261218861711
new utterance	2.3944805527455664
table entries	2.174323003243204
languages could	2.0074734400897967
written form	2.5141506489911167
one way	3.5692446876810724
grammar models	2.084921955391791
disambiguation process	2.7269033674859133
paper describes	4.019882560174236
total set	2.204084453601128
baseline approach	2.1900356812428674
two solutions	2.337335047919672
object descriptions	2.069366669785133
complication arises	2.022261218861711
one path	2.1786566600374835
sentence f	2.318597163658117
every part	2.0965741974051317
working hypothesis	2.481877068750121
bell laboratories	2.561335578878418
complex category	2.2010503919436886
analysis result	2.1556917788222236
national corpus	3.1631931627386507
system includes	2.8363018634567565
texts used	2.477857563055056
experiments aimed	2.200598690831077
contains multiple	2.166102255087602
took place	2.633076912917867
local optima	2.0355669302402037
initial word	2.2712164204775496
appropriate use	2.269227013456659
text spans	2.419727388998351
typed unification	2.070918963738103
two texts	2.7048905501857567
problems occur	2.1023887620661283
switchboard corpus	2.459529667357538
various points	2.4910639166060555
use context	2.380830363798011
simple pattern	2.651727780196863
best hypothesis	2.5299144174330226
subsequent sentences	2.299112274004667
correspond exactly	2.330090425640618
initial segmentation	2.051446067377007
synthetic speech	2.365342890471688
existing domain	2.0074734400897967
welch algorithm	2.269561582225272
markov process	2.3456396806838673
position 0	2.236207490380668
values based	2.109437912434099
specific rules	2.962203632455774
speaker independent	2.4976898588743333
first pass	2.935802754180117
long way	2.1991762390258556
correct responses	2.179899779088691
algorithm proposed	2.732353124587178
lexicon provides	2.248968785663327
evaluation method	2.754205270132977
full semantic	2.400669279072792
category label	2.366560268225353
synonym sets	2.4142989859528057
using surface	2.156307938487117
extracted information	2.6820222464529175
longer version	2.060145782650642
third row	2.43808337586551
7 seconds	2.00238351760398
complex processing	2.014014536653706
estimation method	2.5206373641980373
three positions	2.1275552563638707
many ways	3.122259719956319
best paths	2.146975132180912
form n	2.0743965972347684
criterion used	2.190982236626934
four elements	2.116591742190463
likelihood criterion	2.2608962684880867
intermediate nodes	2.333950776258771
given word	3.516828959671899
object x	2.1952365236796383
work required	2.2020202169645566
interactive mode	2.365645084287615
relative likelihood	2.1672520823815216
step 6	2.4946540539243713
domains like	2.014014536653706
uses standard	2.089026915173973
martin kay	2.245970743500627
many pairs	2.296234813805337
good coverage	2.721580573637278
called discourse	2.1678854032223676
initial version	2.4502898241243782
geared towards	2.500962469073721
idiomatic expressions	2.8231607129789413
30 sentences	2.122246315203648
whole set	2.931986119902334
number n	2.494192577464091
information management	2.111216500526359
main verb	3.4763413876766234
significant differences	3.0401170357881253
cannot infer	2.0817120252117016
results compare	2.189546252614866
fourth row	2.1023887620661283
one leaf	2.070902079606615
linear sequence	2.5830476202418393
following derivation	2.064139780413935
multiple derivations	2.1117654159915116
future use	2.4682938305116267
sensitive language	2.082216408469818
final sentence	2.521311760953824
discusses related	2.2169936022425736
computational issues	2.049069748176902
matching pairs	2.012347776929124
research since	2.022261218861711
negative class	2.0113555126477998
provide significant	2.172274429303222
ranked list	2.965231404643545
rule like	2.507355080104039
small fragment	2.189546252614866
following approach	2.1413469986390323
acl workshop	2.4459112107066288
acyclic graph	2.8152675086525365
general term	2.474426416055006
example showing	2.045521226679158
plan recognition	2.5692246890512065
application using	2.0656667908183506
task since	2.3567860333521553
using dependency	2.302335064150728
one factor	2.052412024541947
algorithm also	2.709727741353372
third condition	2.0376102740271858
mani et	2.4992763016293598
state space	2.4296069539311613
type c	2.2622038590288636
produce results	2.257869825585851
empirical results	3.014430511486031
increasing amount	2.2917594692280554
results comparing	2.014014536653706
specific context	2.763425432953482
structure grammars	2.9170096899295417
length k	2.255986194915539
model estimates	2.275125009000975
algorithm without	2.0965741974051317
complex type	2.020034028846938
among languages	2.236207490380668
global context	2.435129693652727
use knowledge	2.3725247035269392
seldom used	2.022261218861711
role names	2.124245509180353
word forms	3.2745474946330213
additional cost	2.21826141526814
like text	2.189546252614866
full sentence	2.766328303606984
algorithm may	2.7470403741612865
possible worlds	2.4076974888249607
space available	2.022261218861711
solve problems	2.458989459396439
two copies	2.209997368053908
phrase level	2.8835823091231534
reference point	2.1799712463526015
recognition technology	2.6037705266207003
english document	2.061938987147525
one class	2.902072261733129
candidates based	2.301524832421798
atomic formula	2.2228184445481354
corresponding source	2.222669363564495
best sequence	2.3939653872793425
sentence 2	2.604639534542665
certain discourse	2.3079188680593132
additional experiments	2.481088896783804
annotation scheme	2.975094307322272
flexible way	2.233997204181393
one entry	2.7442427789507375
morphological properties	2.330318201869984
making reference	2.1023887620661283
different interpretations	2.8103541295465835
already exists	2.7503638953868674
algorithm requires	2.7897213779650762
subsequent processing	2.7843306973398603
existing ones	2.7346374033155896
one referent	2.1117654159915116
adverbial modifiers	2.2443687102109093
possible alignments	2.6640501698086037
higher performance	2.4336178509515736
development team	2.0355401813680145
token frequency	2.0279777623912025
type 2	2.3933863316468136
simplifying assumptions	2.4420550512076584
accuracy achieved	2.5683622307986207
ambiguities may	2.159746288077376
encodes information	2.0074734400897967
morphological rule	2.1326294699100985
v b	2.050881579198281
major features	2.0577375124403856
direct connection	2.2062448606993037
used wordnet	2.327403995660175
important concepts	2.4058702767287663
source text	3.1392701439583646
p v	2.0210388623219226
algorithm begins	2.1946340157728974
dot product	2.3272998004835683
systems whose	2.25398049501041
search engines	2.9695017967569552
knowledge contained	2.314625473863075
nouns using	2.2658748144574723
parsing technology	2.4301042377497284
original corpus	2.624006256846942
active features	2.040955821278623
together constitute	2.116591742190463
4 proc	2.5126758453675735
briefly described	2.7274039540779866
potential antecedent	2.1440806054233144
much wider	2.6426656928107324
linguistic applications	2.479436170212358
word lattices	2.2531554116933723
bilingual corpora	2.985713877500347
available data	2.790458104254212
ordinary english	2.159746288077376
corresponding term	2.007315006862137
knowledge structure	2.388389479891021
data sources	2.7404076208406205
two frames	2.018553812054714
published results	2.5784238874274066
smaller training	2.4843078272685934
syntactic role	2.577883621924482
human translation	2.65258253264966
certain features	2.8074029935371727
symbolics lisp	2.1954890274836183
occurs twice	2.172274429303222
specific knowledge	3.2463552884294318
cluster analysis	2.2542056077090464
shorter sentences	2.269297164568262
bigram language	2.6352782563593826
words remain	2.109437912434099
20 percent	2.007315006862137
n documents	2.0487425691756993
another property	2.014014536653706
incremental fashion	2.159746288077376
input format	2.291062489830228
generally use	2.222406898745732
preceding noun	2.3692777127487306
semantic preferences	2.2538376671359166
semantic point	2.3001849076014094
lexical databases	2.603879982568254
work include	2.200598690831077
semantic argument	2.457669772575816
certain constructions	2.292453688514974
classes would	2.060145782650642
generating text	2.2823116493074065
corpus provides	2.3725247035269392
using techniques	2.6242476210246806
automatic summarization	2.6547634952244668
information exchange	2.169463326892629
second stage	2.9484236706023137
context words	2.622506965200399
single discourse	2.0433739587762374
several dictionaries	2.0656667908183506
name types	2.017570716396185
set based	2.3091749723655277
support verb	2.0733669969735997
also address	2.24825378073324
constituent type	2.0919612108326557
result obtained	2.4755429606947192
detailed information	2.450248365071464
please see	2.3088417794636857
specific system	2.286777020069459
rule types	2.3163754437675057
system implemented	2.3271110475429713
language constructs	2.1312839727750825
rules obtained	2.007473440089797
creation process	2.0435211751140128
tagger would	2.060145782650642
given threshold	2.8270516537488226
particular application	3.097081606764979
three sources	2.4573912381563754
supervised method	2.2143303423358462
great interest	2.3725247035269392
class lexical	2.1809349226649872
de smedt	2.2318502971474357
contains exactly	2.5816553490817546
several steps	2.694934244379777
terminology used	2.357121432847829
another level	2.272505380879168
include multiple	2.1119430342456553
translation memory	2.1910604079443594
figure 6	3.881266985773071
independent representation	2.1773123985040854
linguistic theory	3.3449810729210965
decision lists	2.29681166832371
value indicating	2.0376102740271858
nearest neighbours	2.1996372224745313
information obtained	2.799991731595414
information theoretic	2.6570253975415214
great amount	2.1774937848124125
possibly non	2.067747107964575
interactive systems	2.351072211477366
average ambiguity	2.132118852147872
example 1	3.1852095387576123
language training	2.0516186188527366
user interactions	2.2324767069435336
space required	2.085074160737753
larger text	2.2062448606993037
embedded sentence	2.514275838542639
target class	2.070902079606615
gram word	2.009868063569498
several clusters	2.0709020796066153
information cannot	2.414320698244546
term clustering	2.098989875376178
given data	2.5204388695552393
model includes	2.425466838565806
every sense	2.21267812847179
extraction program	2.130459427106893
appropriate response	2.292688813088482
system computes	2.084486237350305
groups together	2.025549925076683
semantic representations	3.392457428116431
sequence given	2.412081342249917
word set	2.4696747940764503
common task	2.3313268720025637
performance achieved	2.4656893604540913
general terms	2.7851998383281966
different conceptual	2.0355401813680145
system looks	2.2857443413397442
object slot	2.2254338489148493
syntactic cues	2.3190106321216226
features represent	2.3000015484793472
two lines	2.378583471654162
based transfer	2.0305015763690903
alignment algorithms	2.3822972818873094
english sentences	3.393313534341982
three words	3.152540725196571
computational grammar	2.1681058923377785
different modes	2.471374838741637
possible uses	2.0435211751140128
acquire knowledge	2.2651322166113443
known algorithms	2.0595811562598767
use existing	2.2371302480222286
examples 1	2.3402017998642997
parse structures	2.2960712059304837
top half	2.1222463152036486
500 documents	2.0388643487004234
example input	2.2207046594768247
distributed processing	2.021237938105666
par un	2.2059386657599975
adjectival phrases	2.223453205129625
syntactic similarity	2.1546266423621536
rules involving	2.313258134832421
different positions	2.691171423380021
different structure	2.271753499640152
dans la	2.3933975610187614
quantitative evaluation	2.628045600029716
higher weight	2.4503922558383535
network whose	2.0490697481769016
clusters using	2.075729255121515
cubic time	2.2561038865543948
different annotators	2.286777020069459
psychological reality	2.11911577017412
one analysis	2.6102270244661887
inheritance network	2.0275671199063554
rules given	2.604082280013142
analysis uses	2.1774937848124125
sample space	2.1661065261527055
problem lies	2.018638941929385
single set	2.3632072556921497
comprehensive evaluation	2.060145782650642
basic assumptions	2.352331304842637
given parse	2.1637030579857273
interface design	2.2640242571042535
mildly context	2.40503026145317
cluster size	2.0543841980260447
one thing	2.632871198198877
seem like	2.2917594692280554
data problem	2.897727251833213
3 minutes	2.0965741974051317
negative example	2.3410112315031366
syntactic node	2.06468203219145
muc evaluations	2.0912728465707264
common form	2.2074757805024428
main topic	2.0268820788573385
ambiguous expressions	2.145668076397672
new information	3.314529369432865
complex examples	2.1774937848124125
estimate based	2.0965741974051317
algorithm attempts	2.2427935805269064
specific lexicon	2.234561366831871
possible meaning	2.0918364857418936
measures may	2.1023887620661283
first symbol	2.333059379978482
specified threshold	2.1774937848124125
language accepted	2.0820460914892704
linguistic intuition	2.4143100728666482
processing stages	2.520061924452455
knowledge sources	3.4826367007519443
individual languages	2.1234799066986305
crucial step	2.2631802623080817
classification process	2.5078027325453753
module may	2.236207490380668
ou de	2.0691747079076768
negative score	2.0433576362240267
rule np	2.2024639441989313
separate models	2.171178031468078
place predicates	2.2378720342358767
grant n66001	2.109437912434099
form using	2.3398800045580104
conversational participant	2.01625945747488
many relevant	2.1859857899642225
rapid development	2.6135299880658014
ways ambiguous	2.1268981124614226
syntax tree	2.421987001938355
following sections	3.657634250439538
semantically coherent	2.2652831475366395
expression patterns	2.056372148317519
matrix verb	2.329360397486651
one example	3.3841919246809886
simplified example	2.4560115027140723
two subtasks	2.3079188680593132
input question	2.0281097831836057
20 minutes	2.0430031125203487
2 features	2.060145782650642
state models	2.4178566445478604
unclear cases	2.022261218861711
state model	2.3368743877740505
word string	2.695745553904804
open problem	2.5512930878268945
users often	2.3322409522263214
precision drops	2.070902079606615
version 3	2.4801487310502157
tree nodes	2.8364634023143425
multilingual corpus	2.2214984034530496
particular semantic	2.818509743944346
dif culty	2.0517691345478033
two points	2.6977319643914632
different entities	2.532281852686494
highest frequency	2.4622827064456487
automatic training	2.357947172648072
single label	2.0388643487004234
pairs consisting	2.3100369696113856
optimal number	2.4154478605449703
per utterance	2.346245108543197
direct correspondence	2.3681262299005272
beth sundheim	2.06468203219145
project began	2.022261218861711
following observation	2.14622455382987
procedural semantics	2.13638592708426
text generated	2.2024639441989313
segments within	2.042308826087027
statistical part	2.275875400236362
adverbial phrase	2.58765397216344
two varieties	2.0376102740271858
last week	2.1335723901840655
sparseness problem	2.9311804162530652
features extracted	2.8643978463567255
speech disambiguation	2.018882355267409
automatic language	2.313088586931655
parsing program	2.131764459103371
randomly split	2.3886619127043263
different mechanisms	2.0435211751140128
news agency	2.016153872932039
proposed approach	2.8544825897381134
empty categories	2.2751895212019257
representation described	2.0355401813680145
single string	2.0439237932091396
300 words	2.19110807291794
estimated probabilities	2.2863172876464515
verbs using	2.230524125991308
secondary stress	2.072290098419245
steps described	2.060145782650642
cluster together	2.1499148428335464
c r	2.1474010711525744
texts contain	2.2532512925302806
discourse planning	2.0718060207312874
learned model	2.1454256200339774
use f	2.045521226679158
mark liberman	2.351334739093333
10 shows	2.764192017679775
following steps	3.3813321739549553
joshi et	2.4466603983386537
occurrence matrix	2.238303757498221
approach results	2.0376102740271858
one task	2.557360032065763
appropriate tag	2.122246315203648
entity recognizer	2.6462608371492693
contain enough	2.0592346389623892
initial phase	2.2492578335740094
tagging accuracy	2.7703532196627956
paper outlines	2.515272207357606
particular sentence	2.8086961426759713
b may	2.300001548479347
domain experts	2.491246572048403
base consists	2.2631802623080817
right fashion	2.3398800045580104
query systems	2.3609902008768238
occurring words	2.7269005711440117
every instance	2.5047205746788146
head verb	2.5281136227962784
root forms	2.62854208914076
method allows	2.6538988880551524
james allen	2.102921514525577
corresponding text	2.2074757805024428
general overview	2.2492578335740094
direct access	2.2745534754687053
morpheme sequences	2.042148066313454
different paths	2.4800274354647276
related documents	2.396217994014947
problem concerns	2.172274429303222
use e	2.067747107964575
grammatical relations	3.033957318299903
help people	2.020363647132216
machine interaction	2.510895764806523
data model	2.3180071868276277
computation time	2.691945097695815
selection techniques	2.2608962684880867
various subsets	2.116591742190463
past perfect	2.0948054753229224
nominal modifier	2.174323003243204
learning paradigm	2.348262650117465
different circumstances	2.0539861379893036
whole story	2.2062448606993037
equal probability	2.3436055949502137
weighting method	2.0318632598084605
basic model	2.3909514440386763
nl interfaces	2.0462485577187906
appropriate choice	2.411548331612362
constraints would	2.3605160818311552
based reasoning	2.498027697103176
first constraint	2.075729255121515
netherlands organization	2.2328679513998626
several sites	2.1954890274836183
important words	2.605190997714439
next stage	2.864818802890323
one exception	2.8155342430486088
right answer	2.5751065995974187
based baseline	2.016336526582215
ordinary language	2.020363647132216
early work	2.853159916761876
7 words	2.4559545453941283
user interacts	2.1023887620661283
produce text	2.2273864463788637
obvious problem	2.109437912434099
child node	2.5168322703025297
system translates	2.3997947207672894
different places	2.5300278128180307
recent advances	2.7543647561629605
object pairs	2.0700606182667944
larger test	2.357121432847829
word candidates	2.4500315976678513
theoretical problems	2.022410241232288
one form	2.5143260253864335
value decomposition	2.719766208630203
present example	2.22672986885133
simplified form	2.5610975188069385
general inference	2.043521175114013
manually sense	2.09861484173999
nouns within	2.0601457826506424
complete grammar	2.3537384159980346
representations may	2.145668076397672
graceful degradation	2.0074734400897967
basic approach	2.6004265526628854
adjacency pairs	2.044263377304508
human assessors	2.1221507378069426
approach proposed	2.6576461343749926
algorithm shows	2.014014536653706
draw conclusions	2.124567185122352
higher accuracies	2.0435211751140128
contiguous sequence	2.0157952878916157
precision figures	2.095397478447775
sense discrimination	2.1963907224456127
hearer must	2.230204708991294
speech community	2.1209219117797584
automatic indexing	2.4421903190896033
example sentence	3.208446697081634
use pos	2.0837225057868056
3 rd	2.0644684174379186
test corpora	2.934877550421164
alignment methods	2.2150832432339898
input system	2.0219742875792464
regular expressions	3.2105461536930235
mckeown et	2.567558581700766
made explicit	3.058759863940325
adjacent segments	2.0301435463043473
author would	2.7386684072391034
data sizes	2.032117414479018
spanning tree	2.1943686007690166
object role	2.060476496954844
relations rather	2.067747107964575
maximal projections	2.1947527626271213
ce que	2.2357404040060582
oriented parsing	2.393837378955854
np node	2.57405079310287
semantic context	2.4941770830184065
human annotators	2.939367454050719
best case	2.4995359117406406
unsupervised training	2.552868367055541
features using	2.603759680312581
xml file	2.197108675172731
ambiguity problem	2.4537013121142235
initial implementation	2.4768691536889285
parser generates	2.260890098783518
2006 association	3.7156655409667603
addresses two	2.0965741974051317
previous experiment	2.583065627984518
case like	2.0074734400897967
speech utterances	2.13892830891086
certain constraints	2.6579040141847816
translation must	2.026260250766906
various kinds	3.356845598811231
french text	2.2841177956957606
retrieval component	2.1118092223400327
verb class	2.6010440720076535
best values	2.0074734400897967
phonological constraints	2.05559768656669
bilingual dictionaries	2.8409297654340717
essential difference	2.296234813805337
every utterance	2.4405142273932974
context around	2.2512557276899194
three processes	2.049069748176902
label sequences	2.143891530151241
context information	3.035201000447088
english source	2.0621571607676303
iterative method	2.0355401813680145
semantic description	2.6743051805762024
verb entry	2.1718237813971424
transfer module	2.169246632570827
partial function	2.5078980085078997
approach also	2.8893464323366977
actual linguistic	2.007473440089797
absolute time	2.0965741974051317
vector representations	2.252405131816029
several types	3.20830280565844
easier task	2.1479184330021646
rules like	2.7514767657282175
reference system	2.240917123756148
decision based	2.049069748176902
chapter 5	2.2577963294869763
varying lengths	2.049069748176902
unsupervised approaches	2.1621373145487084
template filling	2.4542109634369904
ambiguity arises	2.299686872045762
ordering information	2.2246039886507187
equation 4	2.5740880131167407
main results	2.4358964112583816
general theory	2.6369791594818848
spelling mistakes	2.010123674909037
short description	2.435632598873795
present task	2.0817120252117016
design issues	2.020363647132216
answers may	2.0918364857418936
problem solver	2.1475757795694115
two members	2.176180166955999
subject must	2.19110807291794
contain additional	2.1774937848124125
approach may	2.910501404996695
among word	2.238945374754718
nonterminal symbols	2.738974779576361
future versions	2.447153044449877
classification error	2.3390744152867033
greatly reduces	2.59583142385398
models must	2.2823116493074065
important parts	2.2635846387787795
rules automatically	2.332743262433283
discriminant analysis	2.2177044363196816
relative strengths	2.026260250766906
probability threshold	2.1088465510966525
without modification	2.5275580183983113
identify word	2.286725248739412
minimum number	2.6569075994585227
additional parameters	2.257869825585851
family name	2.076316773188175
section four	2.022261218861711
grammar coverage	2.3846337804702675
different parts	3.039717139958351
another point	2.6287681394582973
semantic difference	2.0656667908183506
procedure must	2.189546252614866
another argument	2.286725248739412
approach requires	2.845540958139394
methods presented	2.357121432847829
two choices	2.3162726447741377
research topic	2.5805707944613645
new set	2.814479985426441
automatically identifies	2.0376102740271858
great potential	2.0056819512048896
several domains	2.3901049403272046
one position	2.5818268846111305
text understanding	3.011661574649677
simplest method	2.1852404006823356
large values	2.238945374754718
important criterion	2.0074734400897967
par exemple	2.2347738684699703
particle constructions	2.2040130552459476
potential candidate	2.060145782650642
process continues	2.727403954077986
morphological rules	2.8039842407467237
linguistic devices	2.1056636401214264
collaborative effort	2.102298845295786
semantic knowledge	3.222499740523929
onaizan et	2.295367896821569
indefinite noun	2.32960511477607
processing stage	2.5918904080996743
words task	2.1644314823725495
top scoring	2.195602150857384
discourse theory	2.165544524420019
adequately capture	2.067747107964575
mt system	3.2071615860774836
incremental algorithm	2.0787965488112174
extraction approach	2.0720053006339114
variation among	2.18618848555381
new modules	2.0646820321914494
embedded clauses	2.6345538640063975
exact word	2.053986137989304
constant factor	2.417618427399807
one area	2.288195916652983
e j	2.193056710825906
textual features	2.1571163999832637
syntactic environments	2.2020202169645566
text span	2.215379218286385
main source	2.2054126223674833
extract sentences	2.2658748144574723
additional rules	2.635895065183466
similar example	2.398088383227839
several layers	2.159746288077376
performs poorly	2.3752638941834947
semantic system	2.163055186177158
problem involves	2.161246853402207
higher priority	2.261882928725516
language interfaces	2.9016015200759764
present form	2.090167258120509
correct identification	2.278461809175595
internal node	2.7076406127499313
using words	2.649627866458325
two verb	2.2339270481759206
adaptation techniques	2.0004002263534426
parsing mechanism	2.6072172251255923
many potential	2.339880004558011
english may	2.159746288077376
employ two	2.1479184330021646
desired result	2.4887102679866837
tile type	2.102921514525577
efficient parser	2.275125009000975
analysis using	2.727403954077987
uses domain	2.0539861379893036
1 p	2.4388398028290212
coordinating conjunction	2.4543205823915617
several categories	2.4611017632787435
search starts	2.00238351760398
grammatical analysis	2.722439300135808
shared tasks	2.2280813991508994
first note	2.2169936022425736
results using	3.3924924717263965
first translation	2.0224102412322886
specific texts	2.1543888787937475
single letter	2.242422642554747
system always	2.1119430342456558
5 seconds	2.6780179951966994
temporal constraints	2.082897528544233
effective use	2.633298663839541
morphological decomposition	2.002821947387001
list used	2.0965741974051317
web documents	2.524647312519259
false alarm	2.3871462162524377
individual utterances	2.4555048416544336
naive bayes	2.9774080812046306
tree using	2.5445384238364364
fundamental differences	2.039007283873737
method relies	2.496488433113517
tile use	2.3271110475429713
grammatical structures	2.677517830509563
inflectional languages	2.1443968511685063
system errors	2.139437309227249
probabilities using	2.5327107950191152
systemic functional	2.156443582921459
fewer words	2.3645023113703294
representation allows	2.4005597282692768
fundamental frequency	2.1480225285359524
relation among	2.5297206260555933
structured data	2.6513970295485163
al 1992	2.11638795219105
section introduces	2.406719372930616
wilson et	2.054099651238877
software engineering	2.631494922645325
additional context	2.4631653324704343
footnote 1	2.0376102740271858
space restrictions	2.0965741974051317
temporal sequence	2.1683797150463713
including context	2.022261218861711
gram probability	2.0974923841702804
good approximation	2.5836694510592557
parsing times	2.3178053897035023
known examples	2.159746288077376
much time	2.3926691030748515
name entity	2.3044835637674232
rules arc	2.0376102740271858
linguistic meaning	2.0720645395124793
stage 3	2.0794977954812808
matrix whose	2.1380488015835484
word association	2.483273126872101
signi cance	2.0431051606586204
system used	3.091473501943134
evaluation scores	2.159272706081664
earley algorithm	2.164281909213404
assumptions made	2.0937854677401884
initial trees	2.595465553880053
words together	2.554828548257736
detecting errors	2.0107581307102835
means clustering	2.3059026904651043
performs quite	2.233997204181393
algorithm would	2.982891690074189
grammar constraints	2.0439237932091396
extracts information	2.2020202169645566
clause constructions	2.0157594426817584
functional relations	2.1037232711357667
probabilistic approach	2.539561812625768
ambiguous parses	2.1119430342456558
three values	2.531391211068655
current speaker	2.130459427106893
value matrix	2.2339270481759206
target position	2.1805074352334235
baseline accuracy	2.460709710406723
reasonable assumption	2.1023887620661283
data files	2.5713125844814697
different theories	2.24825378073324
null tures	2.194634015772898
section reports	2.200598690831077
system checks	2.2074757805024428
modified form	2.166102255087602
object relation	2.3901628362899903
semantic construction	2.0710874108027175
column contains	2.3765899324978887
single representation	2.3953272468316316
surrounding context	2.981934457766059
based systems	3.4696959022251623
high dimensional	2.5838322323105896
sls system	2.032982255991816
key idea	2.806172239471904
linguistic concepts	2.266217954911036
three measures	2.566345278541599
chance agreement	2.252376240506741
one second	2.1816335351833818
correct value	2.135844661161516
domain independent	3.0335679875635155
appropriate senses	2.122227020631432
xia et	2.182175163144094
possible explanation	2.7741689613218163
mark johnson	2.2663126820033765
theoretical issues	2.2140874168437605
semantic lexicon	2.557598820461051
every model	2.0074734400897967
information need	2.743778774515446
three forms	2.2608962684880867
simple models	2.2458302187516197
using xml	2.3448347819209543
new parser	2.106085689780873
simple model	2.6791775094092682
efficiency reasons	2.4459101490553152
lexicon development	2.23891528385263
similarity measure	3.274451626419819
original question	2.4083822308940053
english terms	2.276383970594768
biomedical domain	2.5097987416200898
gold standard	3.307231474527595
sentences containing	3.3900185636845834
finite verbs	2.544706855829143
primary goal	2.819780621761457
bootstrapping procedure	2.141447453937137
particular attribute	2.042479300477095
systems mentioned	2.070902079606615
preliminary stage	2.108212199944729
additional knowledge	2.8168230977152926
functional role	2.437544291347619
following types	3.007315360084258
matching method	2.406213352047865
allow non	2.1023887620661283
grammar generates	2.3540470303697685
large percentage	2.5516896619380334
different machine	2.5454616944663853
substitution errors	2.1677847630386546
phrase rules	2.060145782650642
act theory	2.2721169897388895
system gives	2.357121432847829
parsing approach	2.6959967751912277
similar form	2.014014536653706
large set	3.4129573794064325
formation rules	2.44583715232697
based analysis	2.6880200556308127
training sets	3.2236013081754344
processing technology	2.513825672661492
set q	2.2368067747493128
4 demonstrates	2.022261218861711
highest score	3.1818361357041978
monolingual dictionary	2.115819577522987
approach attempts	2.1023887620661283
rules describe	2.335904293904669
different sizes	2.890932940552799
phrase identification	2.249587707538431
overall recall	2.424165793668044
system including	2.286725248739412
la recherche	2.18437537712784
parentheses indicate	2.1023887620661283
domain independence	2.2831123461395806
valuable resource	2.2384438317931403
spurious ambiguity	2.2982670777494145
classified based	2.0376102740271858
using cosine	2.014014536653706
earlier stages	2.1029215145255775
latter result	2.022261218861711
individual word	2.8795907411815094
parallel sentence	2.1483087008901
three states	2.065429826730168
ten times	2.6746153610960977
complement structure	2.085366556257962
rules needed	2.183647914993236
linguistic sense	2.1678854032223676
effective search	2.0743965972347684
feature logic	2.089298816914331
corpus made	2.022261218861711
basic features	2.5911388632748573
external resources	2.1263858477526414
algorithm similar	2.3955781761503503
evaluation indicates	2.0376102740271858
coding scheme	2.5329246917367887
lexicon contains	3.0133231165798007
5 discusses	2.6719027109268394
sufficient evidence	2.4017401613941365
many non	2.0536262716772744
sentences 2	2.1222463152036486
possible state	2.160181161519838
analysis problem	2.0074734400897967
also performs	2.472067708086249
data required	2.4602434646053126
multiple speakers	2.019557844009582
morphosyntactic information	2.080606834271845
one slot	2.375972104603757
strong tendency	2.616978297127306
initiative dialogue	2.2338764643494686
context set	2.0154988496185773
gather information	2.2074757805024428
dialogue management	2.6437840667954737
logical subject	2.3849898877370492
important information	3.175884287143556
first selects	2.014014536653706
class information	2.6239173438305485
forms found	2.067747107964575
negative training	2.1285388721219967
document matrix	2.177720348401146
categories according	2.11911577017412
standard techniques	2.4619447088476862
different information	2.7909751612815983
b respectively	2.0539861379893036
sentence per	2.2024639441989313
oriented system	2.064139780413935
recent developments	2.218532566759176
paper aims	2.272505380879168
initial work	2.5063198783573477
scores indicate	2.1597096509760467
procedure consists	2.22672986885133
particular contexts	2.3900117065358337
clearly outperforms	2.11911577017412
word list	3.02499244042886
word stem	2.307668852386669
data set	3.706866678565027
speech data	2.9303870599014155
binary predicate	2.114299018453181
several instances	2.3540470303697685
case slots	2.0826646927418695
information needs	2.7335630632488006
long sentence	2.3486760576901946
work remains	2.4658585645856936
knowledge available	2.389326705744299
following categories	2.676015620497819
six sentences	2.022261218861711
translation techniques	2.1497135154557805
existing systems	2.835496200397677
various data	2.2459839572002176
portability across	2.022410241232288
design goals	2.055118260693857
testing purposes	2.5300278128180307
mother category	2.250963652997815
concept nodes	2.1268670444345736
also assigns	2.012347776929124
generation module	2.7983017866400144
dependent knowledge	2.4839437591743128
one training	2.1992696891479233
language program	2.1789148691025395
features within	2.361034984088791
english text	3.297083916947716
disambiguation using	2.5361655161788748
individual user	2.072254863988064
trained models	2.424857289027241
practical method	2.085366556257962
two kinds	3.7391363602933683
different heuristics	2.202020216964557
procedural knowledge	2.1855480139400463
sentences whose	2.7316321138180912
trained model	2.5551928553476424
linguistic characteristics	2.418153424291798
introduce two	2.7746006998088104
speech output	2.5493787142556115
cross language	2.1040611891484984
information system	2.8388341102411587
use p	2.1852404006823356
dimensional feature	2.5729585766873364
detection task	2.2818740385056686
detail elsewhere	2.2917594692280554
contains enough	2.045521226679158
time one	2.1023887620661283
j c	2.2093487881958334
structural disambiguation	2.3670301623174037
reasonable results	2.3841838643071576
node representing	2.549865224143249
programming environment	2.1448782419896597
british english	2.6376651739886348
word translations	2.592550375014302
entities introduced	2.257949048734074
briefly outline	2.109437912434099
clarification question	2.0535400337694356
two sentences	3.579341732563401
second noun	2.0235769044870318
de ces	2.343927751949222
different phenomena	2.285913086717917
dashed arrows	2.0790198876097175
also contains	3.159671607939006
asr system	2.160291201528139
simple translation	2.0865513505522313
concept names	2.220303270450266
remaining rules	2.053986137989304
form one	2.4459112107066288
ellipsis resolution	2.2210594837365623
annotation process	2.707638993203213
certain domain	2.3645129836869865
closed class	2.668039366445184
k 2	2.244063402215322
considerable variation	2.0656667908183506
verb form	2.91708006503662
three rows	2.1397467029652075
data shown	2.1882216921535553
larger scale	2.470467437983557
computational work	2.3926703312537505
string matches	2.0355401813680145
nl generation	2.1640306219778167
present paper	3.376191556224612
model used	3.1710305591399623
larger n	2.0388643487004234
using machine	2.8076388679476314
values assigned	2.3961508254242636
original system	2.471978595592088
type classification	2.0352110047855154
clue words	2.2366162775068856
different case	2.1810793546234883
project described	2.0824927688889554
structure analysis	2.5322407111337455
possible models	2.161246853402207
speech patterns	2.1087830059691863
distribution p	3.0051047829147426
another context	2.357121432847829
basic building	2.4103259885026898
general pattern	2.597753886289268
brief outline	2.1414753566437956
one root	2.064139780413935
entity extraction	2.6340739699493274
produce similar	2.116591742190463
constraints derived	2.1275552563638707
negative effect	2.1745898703581625
words selected	2.2275895034737263
different values	3.1977968496017737
expression would	2.022261218861711
various means	2.089026915173973
similar strategy	2.305458956322113
grant iri	2.5552022266142815
find answers	2.3031542996401573
given category	2.61195094249304
attribute values	2.7499185952948584
also plan	3.0003312754960714
text form	2.1678854032223676
main dictionary	2.0053762946230105
constituent must	2.067747107964575
art system	2.0992185857391816
form larger	2.049069748176902
overall word	2.056538221975785
text used	2.2777975658644114
words observed	2.00238351760398
factual information	2.3494158778977794
therefore use	2.2479606727184787
classifier trained	2.4571509714566635
certain phenomena	2.109437912434099
experiments showed	2.9349545950832665
write rules	2.422302845401488
abstract objects	2.240529114598758
three conditions	2.6449809454224313
learning strategy	2.0623363861225004
modi ers	2.1895635911586195
graphical user	2.8514512428152026
step 5	2.759659804166274
related languages	2.53975879112965
algorithm might	2.1816335351833818
phonological phenomena	2.0843109059317095
basic structure	2.3218138389165275
approach shows	2.2112071020073643
four words	2.701030431086519
ways similar	2.1946340157728974
20 words	2.8127084348573845
sections 6	2.189546252614866
conditional distribution	2.6268347322352166
two cases	3.430841124217005
logical terms	2.0883166975146934
probabilistic classifier	2.023298439509033
possible links	2.053986137989304
single domain	2.26730482181908
represent documents	2.0262602507669065
input documents	2.3102340691415835
occur either	2.2112071020073643
tree models	2.2299747157303518
test run	2.315664817623035
analysis within	2.0817120252117016
using information	3.017858643719951
model gives	2.5155411940846393
represent two	2.3567860333521553
experiment involving	2.1543888787937475
input two	2.014014536653706
system gets	2.109437912434099
performance varies	2.122563170379007
np pp	2.235461513463383
tree model	2.4023700838258586
possible combinations	3.2836005002554725
de l	2.6677352021083736
without reference	2.853060621535404
multiple candidate	2.0074734400897967
syntactic tagging	2.1411515379744985
approach makes	2.7081548291340884
special characteristics	2.0435211751140128
selection problem	2.2078449813130154
structured text	2.1051854525062543
one time	2.6408696496378274
least part	2.1290482690107417
languages like	3.089594995860071
zero probabilities	2.329081436908228
reader may	2.840934889583141
systems work	2.089026915173973
wordnet taxonomy	2.120339273513936
time required	2.862498531740243
several years	2.5199573147329555
manual inspection	2.4260324477975805
goal would	2.0817120252117016
n c	2.2909796341167166
default values	2.7863643345209743
comprises three	2.227249320716272
linguistic evidence	2.554020172979625
user types	2.3645553633450387
first glance	3.0199705883491004
computational processing	2.070902079606615
phrasal verbs	2.546139245012662
optical character	2.6513096869926227
discourse interpretation	2.4251696967210457
building block	2.342059777556324
worth noting	3.2958270098086957
performance decreases	2.166102255087602
speech synthesis	3.0551328842534082
appropriate part	2.022261218861711
different states	2.278461809175595
coverage syntactic	2.045521226679158
practical approach	2.1157313950935546
basic principles	2.1579776742216157
previous case	2.3567860333521553
wsj section	2.141400964822603
given type	2.7305243730202156
detailed treatment	2.014014536653706
level 3	2.2486840762944107
selection criterion	2.3390623500712313
another system	2.5593566798986345
strong correlation	2.47254633740149
feature names	2.363763832577824
6 contains	2.067747107964575
name appears	2.0312646501273113
make reference	2.59198384472979
using heuristics	2.435632598873795
b would	2.135844661161516
nominal phrases	2.536091305983448
segmentation problem	2.313933622733871
sentence fragments	2.8072080499363947
taken care	2.2466515047378937
first place	3.0181457797826283
forms must	2.053986137989304
major advantages	2.183647914993236
following function	2.185240400682336
one antecedent	2.0516186188527366
general meaning	2.1637030579857273
slot filler	2.2534271694597585
test samples	2.2294607513265086
lexical unit	2.604276980803335
acquisition method	2.077438554079712
possible surface	2.289694481867339
encoding initiative	2.177312398504085
phrases occurring	2.1312839727750825
cannot know	2.2112071020073643
qualitative analysis	2.329593112668035
useful suggestions	2.166102255087602
relation exists	2.403472566572053
following property	2.404182443801024
document containing	2.459352254476232
sag 1987	2.311927210286985
main contribution	2.350853380920612
state constraints	2.032982255991816
systematic study	2.110676283984369
several properties	2.368834809141683
human translations	2.293854964426066
new paradigm	2.260688804545853
manual effort	2.203969756328628
programming approach	2.2816522294196586
occurrence patterns	2.40176020165699
hand column	2.1672520823815216
authors thank	2.2776740307447065
words contain	2.135844661161516
ap news	2.1172218617251373
theorem 5	2.1075782252344206
set accuracy	2.0483526264702476
certain advantages	2.089026915173973
also denote	2.019557844009582
without recourse	2.5055533459995827
validation method	2.0376102740271858
noun compound	2.408505570865426
system training	2.147741798365251
software manuals	2.021568802374736
using case	2.008145918650696
lingual text	2.1976485766852405
formal languages	2.5211662137606043
higher degree	2.725249783651715
upper right	2.4340294815930408
yang et	2.366097699160298
simpler approach	2.089026915173973
5 words	2.7763202012873327
prior research	2.3571838468625623
actual corpus	2.007473440089797
possible translation	2.634902566155413
transcribed speech	2.3653859473220695
test scores	2.123657614722819
possible dependency	2.14851837479882
verbmobil project	2.4251830389411433
provide support	2.325823278732151
second corpus	2.355695717315683
indefinite number	2.049069748176902
expressions containing	2.1353034784701492
significant effect	2.480646012891988
data generated	2.081242005652375
simple features	2.433738217595396
autre part	2.0743309378242767
full noun	2.402991037946962
implicit knowledge	2.0031504296467904
concrete examples	2.2823116493074065
state sequence	2.610910844505188
verbal expressions	2.11638795219105
language query	2.440616305401278
anaphoric relation	2.0217565400350086
take place	3.1236622954296647
structures defined	2.2107864417857894
word aligned	2.3920565733739343
syntactic constituents	3.0070353243270382
proper names	3.4674825549031816
short texts	2.2582820894786684
nodes whose	2.3866967846580778
york city	2.199631911635379
assign values	2.070902079606615
experimental setting	2.4741288466438416
different set	3.082682585317102
estimated probability	2.50901383274263
helpful discussions	2.850240182896208
ordered rules	2.024241291878242
corpora using	2.71022590628043
one extreme	2.3806000578467827
significant time	2.089026915173973
disagreement among	2.1029028550448734
mean time	2.143898852826872
accuracy measures	2.0539861379893036
becomes smaller	2.0376102740271858
elementary structures	2.254991485671437
rule describes	2.053986137989304
figure 5	4.038809770529547
null tional	2.022261218861711
statistical model	3.2710329050616496
three areas	2.1363502970776813
per article	2.026260250766906
knowledge includes	2.0880414073297957
english test	2.394074031068053
final evaluation	2.64887549010175
broad sense	2.5062832412560216
structure used	2.640853851514528
country names	2.27458041023388
every set	2.27225588938143
different modules	2.6343403784238224
two results	2.531813990962308
1 compares	2.3763414719464064
support vector	3.3545410817382115
prepositional phrase	3.6622571552747742
multilingual system	2.019557844009582
character word	2.2553329612434467
example could	2.257869825585851
one attribute	2.262847864979297
two studies	2.1918696243569338
appropriate meaning	2.1275552563638707
present time	2.251337708024017
sentence processing	2.6459908328942174
evaluation function	2.371893152544226
first instance	2.2866069796043997
assign one	2.280088143077826
various attempts	2.045521226679158
interesting information	2.0312646501273113
constraint c	2.2644530822270106
sibling nodes	2.108179066756418
time algorithm	2.4028016943312016
similar training	2.0376102740271858
first experiment	3.2132031580596427
linguistic problem	2.1543888787937475
july 2006	3.6518727763561722
characters used	2.0539861379893036
prior work	2.773762273223266
retrieved information	2.044173370281054
n word	2.008984129158511
approach differs	2.8559716689887678
decision point	2.039007283873737
complete sentences	2.848718832983094
distinct word	2.4041807501397283
process used	2.4704674379835576
sgall et	2.30775488925178
aligned sentences	2.6476628779224964
scoring metrics	2.082836083424519
tile l	2.109437912434099
p denotes	2.1072480253375248
analysis tool	2.2742912111882294
structure representing	2.6539787810122157
based summarization	2.2689953162046326
terms based	2.233997204181393
significant ways	2.067747107964575
edge e	2.1663702764747326
wordnet senses	2.7234196077416986
hearer knows	2.2356355591831027
parse forests	2.2024214826294903
alarm rate	2.112825162077592
one reading	2.6436877621556287
user needs	2.6702350202792777
many chinese	2.1413469986390323
algorithm takes	2.8406829933111197
focus primarily	2.0918364857418936
similar language	2.1023887620661283
various text	2.159746288077376
major component	2.3644803559732956
synonymous terms	2.0817120252117016
inference process	2.511858047151154
entity may	2.2458302187516197
acoustic data	2.235236972021812
many contexts	2.272505380879168
grammatical sentence	2.5064715461073708
speech signals	2.178653004950429
five sentences	2.235674940581098
module uses	2.5785699156201485
also incorporates	2.3920948169591325
systems use	3.2022541207442337
language corpora	2.482269175708103
many occurrences	2.022261218861711
general conclusions	2.1499148428335464
presents experiments	2.022261218861711
word x	2.594615190291047
numerical data	2.0259892300993396
consuming process	2.34443972705697
techniques employed	2.222406898745732
lexical expressions	2.0093760756009043
sentence extraction	2.6725617730134776
name identification	2.150473286151822
one tries	2.2800881430778257
sparse network	2.00315042964679
single english	2.020363647132216
words often	2.406719372930616
shallow parser	2.6233341240598755
words listed	2.320501023225559
small part	2.407043985000019
richardson et	2.0947271027985517
second phrase	2.009868063569498
speci cally	2.5603283508290025
technical writers	2.061969254719088
set difference	2.10508814843703
generic information	2.0361161293026124
parameter settings	3.0241768536035245
theoretic interpretation	2.0880414073297953
main research	2.1739387095397857
first words	2.1023887620661283
feature templates	2.169829709635037
language specific	2.944560548516615
identifying relevant	2.2571860900326115
two data	2.882739403278083
special issue	2.0740667449590844
using tile	2.3725247035269392
leads us	3.1061294824431473
significant role	2.1207748533802038
parseval measures	2.0665126236063323
berger et	2.978597168456844
probability distributions	3.2189088542363065
baker et	2.680084909113892
information status	2.151986909460371
semantic frames	2.3867541071505953
coverage parsing	2.251642442858885
message understanding	3.155435946242689
processing based	2.135844661161516
simple modification	2.2917594692280554
markov models	3.235379031085013
one node	3.1882391456387444
rules learned	2.4460002801516922
longest word	2.0376102740271858
stop list	2.3637843227153668
project started	2.067747107964575
kyoto university	2.3966090793143273
language evaluation	2.053986137989304
normalization factor	2.700312006278004
class includes	2.008083705228559
automatic generation	2.8946667503292156
spoken dialog	2.419770588424445
data consisted	2.573562255840437
performance measures	2.7620738140296264
two alternatives	2.68037728221371
interesting case	2.34443972705697
horizontal lines	2.0965741974051317
given examples	2.022261218861711
large body	2.9115029594084447
first utterance	2.694477913901123
documents used	2.21826141526814
verbal noun	2.056712619644514
lower probability	2.292343489683976
considered part	2.0073046923332467
conflicting information	2.114299018453181
parse tree	3.6869312068202067
brants et	2.329799254074887
original rule	2.1975129671979836
every string	2.3352874750950967
grant n00014	2.59583142385398
corpus annotation	2.797471551115692
various meanings	2.022261218861711
parsed version	2.0577375124403856
top node	2.723906338443704
planning phase	2.031216633777997
syntactic contexts	2.6851831181223003
frequency word	2.169037068499984
knowledge associated	2.1882216921535553
termination condition	2.1562488978950904
primary function	2.1597096509760467
context representation	2.1754093285263165
simple structure	2.380830363798011
similar effect	2.272505380879168
nominal group	2.074835836913821
lemma 1	2.4240409765200743
precise description	2.310036969611386
3 proc	2.200598690831077
five cases	2.22672986885133
computational aspects	2.27225588938143
resulting set	2.736262696903211
specific feature	2.3415459481544776
language engine	2.608158338558529
specific verb	2.193486839445044
idf values	2.1057630111004197
working systems	2.060145782650642
occurrence probability	2.1213217085531824
100 times	2.286553889303477
perform well	2.992379333575209
channel approach	2.1222530938767195
concepts involved	2.2275895034737268
approach similar	2.3605160818311552
parameter b	2.112778664196261
aho et	2.174360990865262
1 show	2.568980489025703
texts according	2.1543888787937475
following properties	2.916936055662595
storage requirements	2.1984572101092845
nlp application	2.555381187702874
document relevance	2.008226999543247
one frame	2.292559594587968
one third	2.3941684597466026
representation given	2.014014536653706
user wants	2.7611359725912186
german translation	2.1009584054622508
probabilistic parsing	2.5638822656892537
l b	2.1290482690107417
see http	2.575106599597419
sections 02	2.444664471033472
performance degradation	2.5194176259753824
data representation	2.4706767091840023
possible interpretations	3.0382749437285765
closed form	2.2243484856120617
nsf itr	2.161246853402207
reasonable choice	2.0817120252117016
weights associated	2.3061507157754177
alignment error	2.4388531034875713
full corpus	2.3068035011098407
decisions concerning	2.022261218861711
automatic clustering	2.174925461565186
one tree	2.6847999418128525
second observation	2.075729255121515
four human	2.147342467231957
sentence length	3.317573589304191
e information	2.0551182606938574
words encountered	2.1543888787937475
algorithm terminates	2.2689486128575904
experimental set	2.2074757805024428
complex concepts	2.0764551704567427
model showed	2.00238351760398
one relation	2.5344045957954626
vocabulary size	2.9207808390149004
model training	2.6264242779470166
particular target	2.3005523645852994
words consist	2.0376102740271858
various nlp	2.5980817446720854
negative integer	2.040238153498075
systems might	2.159746288077376
planner uses	2.11638795219105
order variation	2.2807813900426543
arise due	2.2328679513998626
stress patterns	2.0222152695145756
linguistic models	2.536693933874847
current models	2.2965916473768724
therefore consider	2.022261218861711
kaplan 1982	2.02585040092793
last sentence	2.691863473697185
enormous number	2.3322409522263214
linguistic model	2.5914592252517323
current model	2.78921468117141
corpus sentences	2.2371931519418524
deterministic algorithm	2.0551182606938574
technical texts	2.568219188007077
scope ambiguities	2.5028632691753456
initial segment	2.0651359134372345
training sentence	2.405318197610027
model language	2.230590646944645
handle non	2.283111486169489
forms part	2.1236004800428185
also perform	2.406719372930616
multiple times	2.772040428845776
test runs	2.455139935068992
compilation process	2.4428483773652587
context independent	2.1646103030256825
next level	2.742880945853188
constraints must	2.4741288466438416
partial description	2.2237924935289506
similar ways	2.1774937848124125
one relevant	2.1331653459774356
igure 2	2.089026915173973
kim sang	2.5704023875775586
c u	2.0764121855548385
problem appears	2.022261218861711
one hour	2.17237046043026
make available	2.028632806380823
anonymous referees	2.658744056768151
underlying concept	2.1543888787937475
feature space	3.0856724325030083
full parses	2.061938987147525
set consisting	2.7520678037241733
time constraints	2.7919621594816597
target node	2.2424641811508828
individual feature	2.4128972951129395
value p	2.026260250766906
linguistic processes	2.259916269702888
arbitrary features	2.2256816819452245
binary values	2.272505380879168
parameters used	2.380830363798011
different areas	2.4459101490553152
alternative word	2.239473881375989
microsoft research	2.561176962768558
description would	2.022410241232288
subjects used	2.0123114646746343
salient discourse	2.0880414073297957
reference points	2.1202585043971305
summary sentences	2.2715901866407124
extrinsic evaluation	2.149029776420778
subordinating conjunction	2.4464634150917655
clausal complements	2.187296261852174
different requirements	2.060145782650642
software developed	2.067747107964575
recent experiments	2.166102255087602
information rather	2.1290482690107417
many groups	2.009868063569498
present participle	2.5323784231789457
using standard	3.0904248410281228
resulting string	2.166102255087602
n k	2.1574755962048053
large list	2.1413469986390323
classification problem	3.1643284086326893
new ones	2.789820836673456
first person	2.6555649446233827
similar role	2.0817120252117016
resulting algorithm	2.1984572101092845
best example	2.022261218861711
satisfy certain	2.242793580526907
exact match	2.9368861730035563
complicated cases	2.060145782650642
new errors	2.1275552563638707
keyword search	2.0413423598350944
notions like	2.061212299166531
using string	2.0490697481769016
unbounded dependencies	2.5131743166032483
untagged text	2.1295694626022152
subcategorisation information	2.0051363694186355
actual number	2.55061890816116
length l	2.3132912443312206
partial knowledge	2.031264650127311
lavie et	2.069542139177546
de facto	2.0088732254526573
resolution problem	2.083722505786806
different frequency	2.1339262987939316
original documents	2.4708190994603236
language defined	2.2455588337315464
new dialogue	2.191341477013969
analyses may	2.014014536653706
translation problem	2.464574893525888
obtaining information	2.0355401813680145
significant contributions	2.1946340157728974
system tries	2.7278913958646056
partial functions	2.1833983952733873
average error	2.300473636895357
unsupervised method	2.605200974611654
major differences	2.349936098063694
extends beyond	2.183647914993236
two terms	3.0307528417626446
class definitions	2.0532484523230123
simple experiment	2.089026915173973
questions asked	2.070021412749346
example based	2.156163587274852
pp complement	2.140856954368349
overall probability	2.327403995660175
retrieval application	2.0301435463043473
sentence also	2.109437912434099
extracting relations	2.080218100597496
different communicative	2.108179066756418
varies greatly	2.0817120252117016
presented results	2.24825378073324
two arguments	3.037870226313253
salient features	2.2742912111882294
speech research	2.288005735345077
phase 2	2.3571554589560257
different definitions	2.3645023113703294
second point	2.544073863552885
hansard corpus	2.4181382508324156
resulting parser	2.1275552563638707
operations performed	2.1816335351833818
second argument	3.062813444857616
stolcke et	2.288412102604172
two simple	2.8234249145563957
acyclic graphs	2.5076682263483545
key concept	2.034503831882627
automatic tagging	2.3334845246461358
particular noun	2.33582780054359
computational terms	2.045521226679158
various research	2.0074734400897967
four values	2.060145782650642
approach uses	2.9985591864008994
structural patterns	2.1211266904888584
one sub	2.207020916205373
immediately precedes	2.4148184100839467
sentence may	3.1055107327745093
possible positions	2.00238351760398
sentence means	2.031264650127311
grammatical constituents	2.1023887620661283
related word	2.484408641090203
patterns associated	2.32313543527454
unsupervised clustering	2.2271374227955074
recognition accuracy	2.9623930552609288
whose number	2.095973692652728
standard statistical	2.4829696016728517
general issue	2.1023887620661283
e r	2.368533626438758
particular piece	2.0376102740271858
cpu time	2.642461280133264
1 results	2.167885403222368
base forms	2.6633488910077205
four measures	2.0355401813680145
system displays	2.2571619331229797
pedersen et	2.0133930501284154
research papers	2.298126842431937
tile result	2.1275552563638707
driven methods	2.1275552563638707
small window	2.3310204571264768
research topics	2.180405842604579
others like	2.1290482690107417
helpful discussion	2.1774937848124125
questions regarding	2.0312646501273113
constraint grammar	2.4546011444762614
structures could	2.061212299166531
10 percent	2.145901909722843
text indexing	2.042325972844755
successive steps	2.116591742190463
system correctly	2.4565309208694748
particular verbs	2.1413469986390323
ratnaparkhi et	2.1142253992251336
one file	2.1563079384871173
minimal effort	2.1414753566437956
test item	2.0163776326726
rst relations	2.2217704927927855
strategies may	2.181370500368545
clause grammars	2.563942987524557
third step	2.816474726231347
computer implementation	2.3564416538494717
grammar includes	2.1597096509760467
analysis algorithm	2.1181218804951003
constituent labels	2.0434410961719496
empirical evidence	2.9865951165329605
syntactic annotations	2.1540507767674004
one candidate	2.7009618390426744
performance comparable	2.280088143077826
generalized quantifiers	2.1809543568234915
minimal set	2.533723914903159
parsing accuracy	2.899849716849126
paper compares	2.060145782650642
form representation	2.252700852797891
good example	2.9195970402314635
taken place	2.4902131311795985
tile following	2.8840984358454604
verb pairs	2.222847619614501
data included	2.0355401813680145
lexical level	2.9513717414335776
combined effect	2.2380656204269957
restricted set	2.6795890260838418
several components	2.4755429606947192
certain way	2.500962469073721
xml format	2.778518786118838
input contains	2.109437912434099
fourth step	2.0355401813680145
naive approach	2.335899628236332
data driven	2.4271563990788296
one dimension	2.59194602718549
chunk tags	2.056393399106074
level analysis	2.4956892080075765
tree algorithm	2.368667561100132
entities within	2.3483606540998085
intermediate representation	2.675803333436483
whose goal	2.5709437502869203
active learning	2.2268782771685522
approach improves	2.0376102740271858
single language	2.595142805345255
particular approach	2.2169936022425736
darpa spoken	2.119240389473862
semantic considerations	2.2147431158325057
specific topic	2.3926299907916913
length less	2.2955083501313895
specific kinds	2.022261218861711
domain ontology	2.2064051144794514
pattern set	2.130992948259358
representation format	2.1458761792786256
current purposes	2.3332911335775464
briefly describe	3.2190855501289763
consecutive words	2.4532372107181857
tagging tasks	2.1786195769868253
existing approaches	2.5810977625862233
e first	2.0656667908183506
classification models	2.1097814319754438
two text	2.626166445068135
lexicon also	2.116591742190463
classification model	2.4621166477871075
first member	2.176180166955999
substitution node	2.257935313795853
speech would	2.0817120252117016
muc evaluation	2.116591742190463
constant time	2.4894784816458135
weight vector	2.4017988216808988
english glosses	2.2020202169645575
set f	2.5193920459637322
become available	2.6781475127178638
important distinction	2.414320698244546
selected sentence	2.1297846672161187
several rules	2.6051982776453304
problem faced	2.0965741974051317
full syntactic	2.907138415153442
better word	2.060145782650642
actual parsing	2.109437912434099
lexical sign	2.011365348134488
flexible control	2.0490697481769016
verbal phrase	2.190309874952714
positive instance	2.023617009900904
transfer process	2.185934392241661
characters long	2.1994115355731094
key point	2.353009372849565
dialogue context	2.510581330045299
lexicon may	2.3213583873662316
behave like	2.5485474250234113
generation requires	2.0376102740271858
test instances	2.3769888925542757
final number	2.026260250766906
rows show	2.2818214018700496
wordnet glosses	2.155032034683227
given l	2.045521226679158
corpus studies	2.355695717315683
speech category	2.3179334771516977
problems associated	2.7226158006126915
order languages	2.738600001401691
describe linguistic	2.0824927688889554
linguistic forms	2.4936277500698543
research work	2.4658585645856936
technique used	2.8045443334951736
particular subset	2.227249320716272
set e	2.406154746539851
common method	2.2169936022425736
encyclopedic knowledge	2.07971315532368
tree learner	2.0770112638679543
bootstrapping approach	2.360290363088648
function application	2.4341155510077477
work presents	2.1023887620661283
standard machine	2.2503147118445974
weighting function	2.2519003752975277
single path	2.2680300640223705
algorithm must	2.6780005804409055
score based	2.6478811612593103
implementation issues	2.2492578335740094
training procedure	2.917673922638095
single action	2.0460967409737787
final point	2.3605881269336804
object referred	2.1353034784701492
extraneous information	2.00238351760398
gram counts	2.044338077217371
better understood	2.050145631954837
summer workshop	2.0423259728447545
find relevant	2.3523313048426364
lines suggested	2.0376102740271858
investigate two	2.1739387095397853
theoretical work	2.4157348968103562
temporal reasoning	2.2309110157396286
open domain	2.3209423653630132
dictionaries used	2.1946340157728974
fixed length	2.5637622884459255
possible sources	2.222406898745732
latter problem	2.438730573987262
slight difference	2.1290482690107417
learning curves	2.526641413701291
output language	2.189264125376979
phrase grammar	2.0297445546210575
approach fails	2.11911577017412
two meanings	2.417893911657522
rule probabilities	2.4852029033915395
1 means	2.3163999610718347
recursion semantics	2.3216129193964985
certain class	2.7305610292511604
one name	2.2047194975469235
summary length	2.233548627449669
entity classification	2.2176962355843073
word relations	2.0810858459877655
large quantities	2.256215378802819
pos taggers	2.627586337827177
specific cases	2.397957210424912
matched words	2.084112999678453
possible paths	2.5738945783457705
indexing system	2.0584774440832465
final goal	2.2405117043237706
since training	2.045521226679158
particular corpus	2.5103711173791825
indexed grammars	2.127271987719242
many domains	2.6462798099411566
applying bayes	2.0376102740271858
smt system	2.3178743311782295
corpus data	3.07518421530461
english expression	2.252261568910547
important implications	2.267991541156711
estimate probabilities	2.411548331612362
complex words	2.2007990435146105
fixed threshold	2.1895967860268843
make predictions	2.5359424864217504
differ greatly	2.159746288077376
type b	2.281641357238363
models assume	2.014014536653706
thousand sentences	2.189026860278423
general constraint	2.1659825136431445
uses several	2.227249320716272
intended meaning	2.8246281697741464
10 minutes	2.4638732104449046
one potential	2.3413503145720274
capture long	2.1432314806484003
system builds	2.3355042481939856
trec evaluations	2.1411515379744985
computational process	2.0824927688889554
standard information	2.257186090032611
case study	2.7830816141638297
john told	2.0057196349988935
dependency grammar	2.990434464320368
documents contain	2.3540470303697685
reiter et	2.1449182970306166
complexity analysis	2.177338384924136
previous section	3.9589578910031245
observed distribution	2.031216633777997
nouns occurring	2.0965741974051317
dialogue system	3.1448380894630557
first case	3.35195150898567
line model	2.153552027758611
linguistic features	3.0402729809330977
speech taggers	2.846247860207085
temporal intervals	2.032982255991816
general models	2.1543888787937475
subject field	2.331042941315639
improve information	2.1119430342456553
grammatical relation	2.644086203833148
provide strong	2.1983168585210526
vary widely	2.2633091888721837
text planners	2.0301435463043473
selection algorithm	2.5922487108383687
general model	2.6665350176009515
relative simplicity	2.075729255121515
background information	2.8386558782001945
evaluation presented	2.015795287891615
structural units	2.0116645373544815
first column	3.162660772545568
linguistic constructions	2.6851508054955686
lexical properties	2.64486023183321
similarity metric	2.7398977390184647
english morphology	2.0312646501273113
common verbs	2.2443687102109102
theory underlying	2.022261218861711
possible sequence	2.022261218861711
end result	2.6163224599499793
sentence text	2.014014536653706
corpus also	2.3920948169591325
formalism used	2.588258719432283
several knowledge	2.2939172677378985
simple present	2.1632553551693197
corpus material	2.026260250766906
techniques used	3.1822573648588617
semantic module	2.02007728945488
case particle	2.157536453720155
statistical parser	2.8860056213724654
case markers	2.3848449865014043
generation phase	2.3846748437773893
approach works	2.5768708070487776
functions like	2.116591742190463
sentence pair	2.9668082658829165
wider coverage	2.0817120252117016
syntactic expressions	2.072290098419245
corpus without	2.399252281275852
part 1	2.012901690413823
thanks also	2.893745871391028
brackets indicate	2.3193242962445444
appropriate morphological	2.015795287891615
lexical resource	2.7070871546510573
higher scores	2.787298932855055
bleu scores	2.5265369084924947
total size	2.0588781459323418
computational applications	2.289421100443162
u n	2.2443106664762844
rules operating	2.1023887620661283
limited coverage	2.521311760953824
phrase p	2.0493961548232065
given moment	2.296234813805337
semantic verb	2.107694681935609
best strategy	2.1677686360966497
resulting representation	2.1312839727750825
three possibilities	2.5356198281017184
second question	2.6390266235746176
jose mercury	2.1513738263092224
earlier study	2.086884416965048
corpus may	2.6390272787949414
pairs whose	2.4650397342770933
unrestricted texts	2.4891586776108996
discourse model	2.905291135866992
various methods	2.8806494951132326
following components	2.5356198281017184
interactive environment	2.0577375124403856
segmentation algorithm	2.570333796828371
temporal relations	2.670133092664932
automatically generates	2.435632598873795
dictionary could	2.089026915173973
strategy may	2.1004669767659387
independent recognition	2.453291161886212
yield good	2.363427967514935
representations using	2.045521226679158
barzilay et	2.241752115066583
first approximation	2.9583793807760976
zero probability	2.7965212694345314
interface must	2.2011501968875664
cue phrases	2.432181948143984
separate component	2.25398049501041
formal grammar	2.4005735042651257
complex problem	2.4148184100839467
subordinate clauses	2.72103112300861
constituent words	2.340375825621324
understanding conferences	2.7882959218419616
something similar	2.314007818435079
writing grammars	2.0376102740271858
short summary	2.2623137103139603
head category	2.0309308562575756
contains rules	2.255349860587285
ie system	2.5170259621427724
vary considerably	2.2823116493074065
given state	2.578062444007164
number information	2.0780045127234654
structures based	2.060145782650642
answer candidates	2.015415024648016
language families	2.390928801874167
paper use	2.045521226679158
slot name	2.0382975066978917
techniques applied	2.1774937848124125
full parsing	2.6184793520079808
friendly interface	2.2062448606993037
one query	2.202020216964557
various dimensions	2.1413469986390323
learner may	2.075729255121515
rule 2	2.514431374661197
error rates	3.319196637968772
possible parts	2.362189645466873
role information	2.274594296859412
one method	2.8094665257256297
computational theory	2.353890135057515
greater flexibility	2.366082551445672
sentences randomly	2.200598690831077
words inside	2.1971990564969754
following rules	3.0432488458163784
large parallel	2.227249320716272
continuous speech	3.187520675033451
collect data	2.3362105897148218
causes problems	2.257869825585851
words linked	2.0907374619642662
trees using	2.609763958625415
enough information	3.0541821208498248
maximum possible	2.4711372947173462
score function	2.0600797786585052
categories based	2.2532512925302806
relations may	2.754258196086449
appropriate model	2.286725248739412
special features	2.464532383816498
many noun	2.075729255121515
output pairs	2.102459437819877
word similarity	2.611578520989837
optimal value	2.483586349079205
human intervention	2.9685754684193664
use data	2.435632598873795
text strings	2.2468562711056688
preliminary experiments	3.2728071630090314
standard measures	2.2863172876464515
also serves	2.5327107950191152
bound variable	2.119964339928767
two nps	2.5649648746967766
parse times	2.1129807521176964
language teaching	2.229816380014228
striking difference	2.014014536653706
features present	2.159746288077376
many mistakes	2.045521226679158
semantic distance	2.699059100853313
approach using	2.929637787309186
two test	2.7650624696638704
previous systems	2.6726294902592147
focus word	2.1039028639707924
lexical semantics	3.172181453467386
annotation using	2.075729255121515
complete text	2.2169936022425736
use manually	2.022261218861711
narrow sense	2.172274429303222
level descriptions	2.0350538995209133
word sets	2.0886318658041363
convey information	2.581337457713909
get lost	2.001705936914745
information alone	2.5626703237425104
text corresponding	2.0312646501273113
two senses	3.03946419230069
general vocabulary	2.0506846684939912
pos tags	3.3253316652881133
reasoning processes	2.293773362494991
new language	3.0381621908082823
expression must	2.172274429303222
unique words	2.4418477604382334
results returned	2.0182900715665095
takezawa et	2.012347776929124
careful examination	2.089026915173973
one possibility	2.8705847213941125
following context	2.154388878793747
discourse information	2.648320022970551
language equivalents	2.007315006862137
inflectional affixes	2.0799084977218767
one non	2.3042556674153793
travel agent	2.1898738867574314
select one	2.9359681284381915
world data	2.223354179502502
system evaluations	2.0965741974051317
contains various	2.1479184330021646
system offers	2.272505380879168
semantic disambiguation	2.526879797760604
particular concept	2.2856827436154
tree representation	2.677366984149193
term selection	2.072010358246759
classifiers using	2.318932106792177
worth pursuing	2.200598690831077
draw attention	2.0577375124403856
specific source	2.045521226679158
two distributions	2.702530940387159
running words	2.3929767377741333
supporting evidence	2.1971990564969754
vowel harmony	2.1509258355243768
goal state	2.0054173982725354
require two	2.114299018453181
heuristics based	2.573822910996244
morphological feature	2.1760577188495196
second difference	2.272505380879168
every domain	2.1432314806484003
time limit	2.101499964511544
relation used	2.0743965972347684
algorithm consists	2.751626253140354
hmm approach	2.114707400235429
contextual effects	2.0491394236959737
sense given	2.0656667908183506
one piece	2.324871720725265
tree rewriting	2.069254540745998
punctuation symbols	2.0032261408328793
specific data	2.6212466393742986
algorithm achieved	2.031264650127311
input utterances	2.1754004078368676
phrasal categories	2.353590743686986
singular present	2.270957674496034
logical expression	2.2779298173923235
dimensional vector	2.498962231145669
unseen texts	2.1051956698941945
one token	2.4594527702143942
greater improvement	2.0074734400897967
rules concerning	2.0376102740271858
speech tags	3.4642871827725985
immediate constituents	2.4504857767287103
single analysis	2.144451252664119
corresponding answer	2.055118260693857
one specific	2.3586789940860258
lexical preferences	2.2683597195437413
corresponding surface	2.210530136031284
two di	2.08176436291408
development cycle	2.2354359131356984
resulting list	2.448499087268562
computer dialogues	2.018724697973509
000 sentence	2.3244550549665677
natural interpretation	2.0082269995432465
based mt	2.7800294412973945
second system	2.0862895726458346
pioneering work	2.331780823064821
term appears	2.212365169546574
two segments	2.530408860556757
cannot perform	2.014014536653706
complex model	2.3387607047301717
noun may	2.2112071020073643
web page	2.801758261027934
appropriate answer	2.3926299907916917
particular syntactic	2.813933847692784
linguistic principles	2.4815112726738593
overall processing	2.159746288077376
minor differences	2.227249320716272
previous example	3.1210615631477077
likelihood estimator	2.303767517589812
one derivation	2.282571578913096
document collection	2.9874234965411093
following tasks	2.3605881269336804
generate n	2.099458264360342
work together	2.3489717834648687
gigaword corpus	2.229272965900683
output quality	2.1537782372927268
major task	2.1023887620661283
complex events	2.092758213063373
evaluation tools	2.0763533583416773
semantic distances	2.01622312528832
sections 5	2.6545782769584503
important ones	2.034392286942111
element may	2.278461809175595
existing software	2.0817120252117016
entity type	2.615103972107979
boolean operators	2.2608962684880867
parsed trees	2.1563079384871173
necessary information	2.8372070357944468
system along	2.0312646501273113
corresponding translation	2.2094664196837046
following functions	2.3922788066689504
rules apply	2.6619758288711783
semantic annotation	2.5957499329701155
parser errors	2.067448051519092
scoring functions	2.3105814486946032
project number	2.1023887620661283
resources available	2.4023591301802703
word corresponding	2.049069748176902
main clause	3.038308063016978
select features	2.00238351760398
semantic equivalence	2.237337152516271
b must	2.0601457826506424
gram precision	2.1544949113487455
simple case	2.7635659875230822
verbs occurring	2.105083838552665
different lengths	2.670754691569546
systems must	2.9423274732683162
vertical lines	2.092391982415086
model 1	2.73324733498681
first determines	2.109437912434099
appropriate translation	2.5402889087175597
whole input	2.2823877774700367
syntactic approach	2.143898852826872
comparing results	2.166102255087602
notational convenience	2.227249320716272
standard set	2.5949234410430737
lexical representations	2.5955335478585977
different possibilities	2.257869825585851
different form	2.427896658773757
phonetic form	2.012347776929124
compound verbs	2.0295348506615305
parser used	2.723042864251284
word occurs	2.9632390746333206
2 briefly	2.24825378073324
1 figure	2.1312839727750825
one view	2.2224068987457315
overall organization	2.2823116493074065
sentence test	2.141006899009188
surprising since	2.3763414719464064
structure tree	2.9133318581955465
see section	3.837346210879183
decision rule	2.2241067579529537
corpus text	2.051285638402277
generated sentence	2.4370053515030525
avoid confusion	2.5356198281017184
sense number	2.2024753287379637
50 words	2.564308230489789
sets containing	2.2455581621174368
types associated	2.089026915173973
correctly understood	2.1521748154008313
nlp technology	2.521973998116837
disjoint sets	2.6526415372496883
straightforward extension	2.122246315203648
new relations	2.2700626177648484
key difference	2.1016722214456514
technology based	2.089026915173973
4 presents	3.213619704369796
semantics based	2.194634015772898
different order	2.4368186247856465
file contains	2.270820552349159
current hypothesis	2.0543797753915793
minimum value	2.356443881178913
help identify	2.532031966771681
algorithm selects	2.469909103548966
science department	2.1447558802308735
target verbs	2.0970455044691083
nist scores	2.066486712809637
real speech	2.053986137989304
represent word	2.3229739757638654
input word	3.1226777798343535
statistical association	2.019557844009582
particular goal	2.147342467231957
kappa values	2.146826922427632
selection restrictions	2.3908960083886273
people often	2.3231354352745397
parallel corpus	3.104480626821095
lexical classes	2.3291075251609494
comprehensive study	2.049069748176902
next question	2.3515967636294297
collins et	2.362465163209574
three levels	3.121729105295905
use simple	2.5002982171432886
short sentences	2.9039422757801576
french translation	2.3882802605246125
different users	2.5120747860790003
two collections	2.1882216921535553
results among	2.045521226679158
pairs without	2.0376102740271858
using default	2.009868063569498
linear time	2.918316460222406
information taken	2.0074734400897967
set forth	2.0484178732091096
thus becomes	2.109437912434099
application areas	2.4707569101573013
longest match	2.269348011392596
text input	2.730364810478565
frequent word	2.6774365026657785
recall error	2.155589295660503
surface level	2.7433973952410606
predefined categories	2.0763533583416773
sentence analyzer	2.070283826978425
line 6	2.1339258481610717
1 shows	4.255558464258551
precision obtained	2.1353034784701492
patterns used	2.5429423198957037
parsers may	2.067747107964575
general features	2.278968473400518
whose function	2.3290814369082278
two type	2.070902079606615
text structuring	2.012519747980491
significant decrease	2.1023887620661283
clear preference	2.0023835176039797
user study	2.2178512246312954
extensive research	2.1946340157728974
definite clauses	2.018691645429352
germanic languages	2.259349307018171
considerable improvement	2.4910639166060555
related meanings	2.161700268474605
output strings	2.261589918913124
certain order	2.067747107964575
two alignments	2.094727102798552
linguistic constraints	2.9731917106709447
different descriptions	2.0974923841702804
ten years	2.311316243336451
different english	2.242793580526907
one template	2.4305653885656935
generation research	2.112778664196261
entire document	2.5897023310768943
years ago	2.6104279506479697
methods mentioned	2.116591742190463
initial part	2.297574746882776
language based	2.5364747548415036
computing power	2.177312398504085
l n	2.1252780259970514
pos tag	3.2027114953326397
larger structure	2.1245637044373713
phrases used	2.434479401630983
require special	2.403331244885159
dataset used	2.0376102740271858
spoken sentences	2.2606888045458526
garside et	2.007315006862137
specific terms	2.627890362218005
classification algorithm	2.6770599714711376
mechanism used	2.387261616069604
per second	2.9492006086885723
4 displays	2.1413469986390323
tagged words	2.5504695639162045
main elements	2.022261218861711
incorrect parses	2.2425716069742774
every english	2.1380488015835484
discourse features	2.2505373407091294
sentences might	2.0312646501273113
one developed	2.183647914993236
section 8	2.90174039333537
inflected languages	2.0214242165662735
features available	2.1119430342456553
former type	2.172274429303222
contract number	2.637542154402094
database entry	2.042325972844754
many experiments	2.0918364857418936
class x	2.073527422797919
resolution algorithm	2.5449575687753843
time stamps	2.277797565864412
speech synthesizer	2.6464626395700157
text databases	2.1072480253375248
read speech	2.4931417469987913
30 minutes	2.6783808829674673
additional annotation	2.0676325559456012
ambiguity exists	2.0221951140579186
translation algorithm	2.240342505510239
given part	2.238945374754718
positive values	2.00238351760398
large effect	2.0824927688889554
inherent ambiguity	2.045521226679158
words approach	2.0767328320021536
cannot detect	2.0312646501273113
phrasal level	2.1774937848124125
using dialogue	2.171178031468078
questions used	2.035540181368014
speech tag	3.0961662329532236
input stream	2.6022727125975367
small vocabulary	2.348359550546241
probability theory	2.2246039886507187
relative contribution	2.292677622271089
models include	2.060145782650642
additional complexity	2.1023887620661283
alignment problem	2.3831222844868085
another module	2.1023887620661283
decisions made	2.487927468543251
possible case	2.176234939134731
certain position	2.0312646501273113
combinatory rules	2.1677799926013606
syntactic parses	2.3854857072176583
multiple types	2.248968785663327
realistic applications	2.1453279816524384
documents relevant	2.292510173839442
different strategy	2.1479184330021646
known problem	2.475621859290715
linguistic analyses	2.706853945477269
corpus shows	2.216453662622799
0 means	2.4943326702066217
possible values	3.2630523335741684
another verb	2.4223028454014885
toutanova et	2.378424710353297
precision rates	2.458110372967052
entity tags	2.169927587063312
fewer errors	2.3051397055451046
directed graph	2.96945022027946
appropriate features	2.3883750721503745
three language	2.1859453873826347
coherent discourse	2.5519839487331555
elementary tree	2.80355218863167
like words	2.012347776929124
much linguistic	2.257869825585851
tree grammars	2.0846839150790553
ranking method	2.009281342162774
good predictors	2.2594937395335144
also mention	2.0817120252117016
communication process	2.021237938105666
sections 3	3.090052229438248
hard problems	2.1659825136431445
following operations	2.34443972705697
correct sentences	2.4483237657352603
theoretical foundations	2.089026915173973
advanced learner	2.301436088628126
character type	2.1279322421349525
complex forms	2.061212299166531
et a1	2.2379816377150776
transformational rules	2.370305787485025
performs two	2.223453205129625
additional sentences	2.014014536653706
theoretic measures	2.0074734400897967
clustering procedure	2.2666345489536015
estimate p	2.653286183105789
novel technique	2.1297846672161187
multiple domains	2.270820552349159
input grammar	2.104850998667944
temporal order	2.4628757002858714
syntactic phrase	2.49225958195468
linguistic representation	2.7696167102490508
unigram probabilities	2.2827808432516177
design process	2.0523306879408985
news article	2.5048583234
semantic relevance	2.045229961808099
document processing	2.1855853647783996
worse performance	2.4922686900186415
new sets	2.0074734400897967
input alphabet	2.1954890274836183
also match	2.1543888787937475
fall back	2.0022830992899374
specific dictionary	2.0355401813680145
lexicon would	2.24825378073324
whole sentence	3.312877308935444
specific text	2.434891173024058
3rd person	2.3961071276600108
occurrence counts	2.52218110063927
transcribed utterances	2.0880414073297957
performance gap	2.2128620201854576
statistical framework	2.4616902745572595
case c	2.0341482158118964
errors could	2.399252281275852
related topics	2.1529019727075136
intermediate levels	2.2455581621174376
grammar used	3.0799086121617627
distance function	2.3548365939284936
another interesting	3.0192930022848556
rules containing	2.2664627381026032
see example	2.5723617581846687
general structure	2.64972788992013
confidence value	2.283343869744752
word recognition	2.963996402598701
entire sentences	2.3205010232255585
default strategy	2.064468417437918
dependency trees	2.9926623643499055
tile words	2.014014536653706
important aspect	3.0821942836944562
different fields	2.2434846698300097
possible referents	2.2932309227931627
cannot parse	2.1413469986390323
another element	2.0883166975146934
bilingual lexicon	2.5745198730427417
windows nt	2.031264650127311
adjacent words	2.888798596123449
limited information	2.067747107964575
model according	2.159746288077376
first option	2.0965741974051317
database systems	2.3725346455413945
achieves higher	2.1678854032223676
different aspect	2.135844661161516
matrix clause	2.533706649305097
various degrees	2.5190594440550926
random number	2.2186044279222794
high cost	2.0356174780572673
possible states	2.2233541795025014
multiple categories	2.139617094418227
translation candidates	2.577891964192221
possible variations	2.166102255087602
higher level	3.3053295512115572
several issues	2.288708425389342
object noun	2.4832155435310606
related research	2.4711037122810384
morphological processing	2.682800839884329
second rule	2.8143728222814977
per language	2.40233737298711
particular types	2.6642429192578705
nlg systems	2.55057954884374
text generation	3.1685345580734636
vocabulary items	2.6362217054146573
present stage	2.21826141526814
many nouns	2.4254668385658062
something different	2.1290482690107417
texts based	2.075729255121515
reference data	2.060481905665517
program called	2.0679289775943177
cardinal numbers	2.086551350552231
case 2	2.5451450914345486
likelihood score	2.2484548773629642
every aspect	2.00238351760398
lexical context	2.3315137326460897
ordering relations	2.27524491322031
significantly improves	2.76473517095585
language phrases	2.1712320042435467
two tags	2.6603884105314286
complex features	2.3086379993321238
text according	2.269813783347405
baayen et	2.131569621150367
maps onto	2.012347776929124
noun taxonomy	2.043597524575342
method presented	2.730367755012132
rules 2	2.1413469986390323
complex constructions	2.319324296244544
input without	2.1499148428335464
set contains	3.115289439960434
general principles	2.729552563301712
lexical description	2.291921297197021
description must	2.039007283873737
another document	2.0817120252117016
neural nets	2.2001679239685723
tag formalism	2.2300855106916986
ranked candidate	2.0730662534060538
based query	2.088757010266874
models generated	2.020363647132216
text sentence	2.012134475568287
quantifier scope	2.390210227542365
transition matrix	2.1983168585210526
obvious solution	2.318793079863193
based method	3.260034545420003
rich information	2.2020202169645566
much help	2.014014536653706
traditional dictionary	2.055118260693857
article describes	2.352331304842637
alignment model	2.530486603820832
time requirements	2.1380488015835484
false negative	2.0381154200515335
previous experience	2.458989459396439
traditional text	2.167885403222368
also generate	2.4943690648994172
cannot guarantee	2.3605881269336804
utterances like	2.2492578335740094
sentence pattern	2.0998655675132207
cannot refer	2.1851123324453434
1 table	2.0932945584362135
using parallel	2.52345524702619
one disadvantage	2.109437912434099
smaller corpus	2.2226693635644956
700 sentences	2.135844661161516
system developed	3.1391907253334894
necessary conditions	2.3608514782456753
three relations	2.403472566572053
translated word	2.022410241232288
model defines	2.1222463152036486
noun compounds	2.555618538117276
ones proposed	2.022261218861711
items may	2.4039418597775994
current theories	2.1810671172835483
30 seconds	2.2320627568691824
tile problem	2.089026915173973
extensive use	2.9108487461136634
bold face	2.4658668186702535
lexical redundancy	2.0238218800327443
phenomena discussed	2.0157952878916148
tasks involved	2.0824927688889554
related work	3.582995650206302
various elements	2.0074734400897967
6 compares	2.067747107964575
mixture models	2.1037724397641977
general phenomenon	2.089026915173973
viterbi alignments	2.2075271473511786
vp rule	2.3041603708958487
appropriate structure	2.1290482690107417
10 words	2.8415787441816214
special attention	2.904336309730422
cormen et	2.1405120286798134
free parser	2.2529245580170847
semantic data	2.2999931173281616
physical sciences	2.1413469986390323
control structure	2.6388230459895925
methods work	2.089026915173973
development time	2.640346597511529
total weight	2.0344633948566937
either system	2.257186090032611
based clustering	2.3020856252058017
concern us	2.331780823064821
identify important	2.0215688023747367
possible set	2.296234813805337
file containing	2.4707920875257408
paths leading	2.00238351760398
hong kong	2.379201187967775
make errors	2.264070934029513
processing system	3.3758425488425563
substantial effort	2.045521226679158
core arguments	2.067217982040316
extraction rules	2.364455387893677
subcategorization patterns	2.0434365667908208
lexical knowledge	3.2046750059512794
distance metrics	2.240325778554989
candidates per	2.0074734400897967
document length	2.55415381664755
category pairs	2.0759676676734973
pos tagger	3.1424201955497884
significant advances	2.1275552563638707
given sense	2.407229227142648
order unification	2.0187487952253216
relations across	2.092391982415086
restricted form	2.4746937707709256
obvious way	2.99438360097584
active sentence	2.0880414073297957
english syntax	2.543705691571996
network consists	2.3271110475429713
variables x	2.4347973143714543
corpus would	2.5961912151452204
lexicalized grammar	2.518142279584694
xml documents	2.09436816019584
several distinct	2.594827371013212
function described	2.022261218861711
different training	2.768551448026417
better match	2.1290482690107417
wsd problem	2.045229961808099
atis corpus	2.563325315436447
one run	2.1275552563638707
previous paper	2.2823116493074065
annotation tool	2.5737615778161382
forms one	2.022261218861711
training times	2.105195669894195
optimal word	2.0965741974051317
montague grammar	2.4717490908346647
data stored	2.122563170379007
cannot easily	2.2900704417451556
date expressions	2.3035191229908847
simple definition	2.2020202169645566
nominative case	2.622262231438563
single semantic	2.4674059732124207
feature constraints	2.366813489727024
large sets	2.609451926984696
english translation	3.2996186661818885
clause structures	2.086884416965048
english dependency	2.0397786916969833
different language	3.1116372457709414
word depends	2.4602434646053126
object level	2.009683744521853
multiple readings	2.0074734400897967
great detail	2.2169936022425736
word sentence	2.471156853311011
briefly describes	2.5765298321699723
conditional log	2.0883122837731563
speech tagger	3.42268921562681
presents two	2.3806000578467827
learning procedure	2.430913922844518
deeper levels	2.0312646501273113
linguistic framework	2.3548005898232183
completed parse	2.045229961808099
computational morphology	2.141635865407804
current framework	2.294734406103862
main reasons	2.5692705126751343
long run	2.143383598266076
000 articles	2.1569808507858435
extract key	2.0388643487004234
thesaurus construction	2.005829900519596
analysis component	2.796013916185144
semantic patterns	2.365899115198122
every language	2.53294059514112
correct results	2.50352395005998
time intervals	2.455809660272124
passive form	2.4869751402467286
particular needs	2.067747107964575
large structures	2.089026915173973
syntactic point	2.2532512925302806
including zero	2.043521175114013
sentence meaning	2.2424523017293723
syntactic argument	2.009868063569498
parameter space	2.4510875722374497
segmented corpus	2.00158045957469
null value	2.1297846672161187
vector whose	2.122563170379007
figure 14	2.5677503512626485
even simpler	2.0376102740271858
labeled recall	2.383561375281235
assigns probabilities	2.0880414073297953
journal texts	2.0817120252117016
section two	2.1984572101092845
research laboratories	2.151871591362271
becomes important	2.309782368028973
system deals	2.267991541156711
special treatment	2.788628641487632
general constraints	2.3706473607714544
est en	2.0297445546210575
major approaches	2.1479184330021646
slash feature	2.021304366070059
words given	2.446565811067697
equation 8	2.11638795219105
rule schemata	2.3245253553335967
simple frequency	2.135303478470149
godfrey et	2.055118260693857
crucial information	2.3483606540998085
validation experiments	2.283789268465375
language speech	2.0013487161900905
generate natural	2.3059348486690014
context analysis	2.093294558436213
noun senses	2.27153270818734
original word	2.858752513791791
news story	2.5461896174018044
classic example	2.116591742190463
single sentence	3.2557374729588813
also use	3.5586311040281413
equal importance	2.2917594692280554
relative difficulty	2.050941256998681
multiple instances	2.2877533725324914
training approach	2.0223531762070586
place names	2.765191236810785
discuss two	2.6339219017704405
involves identifying	2.3644803559732956
considerable research	2.075729255121515
boolean queries	2.1234143692948715
control mechanisms	2.175400407836868
6 show	2.565866029392127
following issues	2.3005523645852994
common types	2.3422763733583603
high number	2.863086418844359
pronoun must	2.108179066756418
many classes	2.109437912434099
data sets	3.534880107426202
smoothing method	2.6794779762824477
provide information	3.1608407167628014
simple baseline	2.366082551445672
high correlation	2.5650080760309
remaining sentences	2.4148184100839467
expanded version	2.0880414073297953
corresponding predicate	2.075729255121515
parsing proceeds	2.3485059345603894
particular item	2.0376102740271858
data analysis	2.72700170920088
also covers	2.1222463152036486
earlier version	3.0305895851753735
phrase extraction	2.4910399241578998
first half	2.1339987919337604
based tagging	2.3268499481897527
copyright 1988	2.089026915173973
logistic regression	2.2480142749024576
anaphoric expressions	2.7338373723065237
extraction tasks	2.7617938188545765
human factors	2.1569399962260416
compare systems	2.067747107964575
syntactic representations	2.6230698374475754
whole procedure	2.1543888787937475
scores based	2.3271110475429713
additional source	2.326156671484873
formedness constraints	2.2269723471586764
low cost	2.1413163855758874
every path	2.330001604487663
either correct	2.102921514525577
chart parser	3.124081259796855
performs slightly	2.377838240407285
distribution across	2.168967603804668
adjective phrase	2.5501233042135234
experiment reported	2.376521934900442
ontological concepts	2.0651469271533585
database used	2.1678854032223676
original version	2.5125561039231155
japanese translation	2.396902352116326
data structures	3.470695732347589
marginal distribution	2.140856954368349
additional data	2.7739990622989104
brill tagger	2.4039075886563337
specific senses	2.2863172876464515
structural ambiguities	2.638156381508034
human cognition	2.137726762436956
manual intervention	2.398010503457407
different time	2.487920588919004
sentences found	2.1165917421904625
analysis includes	2.014014536653706
technical writing	2.1964587511211215
earlier drafts	2.684723926233512
une autre	2.0301435463043473
parsing rules	2.062080362809535
three sets	2.8074601978411313
linguistic phenomena	3.4604069478766575
unique set	2.2492578335740094
first list	2.020363647132216
level node	2.0648406357378892
operations may	2.00238351760398
new input	2.681313807438796
existing corpora	2.185240400682336
features associated	2.784757586014816
clear distinction	2.326215389922111
new application	2.5531920573841083
direct representation	2.0539861379893036
limited set	3.0001352350266965
solution proposed	2.1413469986390323
charniak parser	2.2920774850710197
john bought	2.191737602725664
log files	2.0331086751867544
illocutionary act	2.0872096970427547
1 uses	2.0817120252117016
language resource	2.2665806942653073
documents retrieved	2.6175172452431545
lower precision	2.682708463769283
one unit	2.4956344349281183
like information	2.3436055949502137
score assigned	2.434614997609767
current case	2.0355401813680145
similar contexts	2.8544364754471916
preceding utterances	2.0592136405805275
considerable work	2.183647914993236
arc labels	2.2149685568147377
retrieve documents	2.484585827831406
input units	2.036264278667375
case grammar	2.557059700844882
model developed	2.431764288227561
question like	2.260158412295013
expressions may	2.4358964112583816
entire data	2.342422961598934
syntactic classes	2.5148824632954163
new methodology	2.007473440089797
particular function	2.0817120252117016
semantic arguments	2.541589233696846
output sentence	2.6077203237805144
recursive process	2.2717132851229636
remaining cases	2.4328329620168185
pustejovsky et	2.4620442728979866
performance levels	2.4629615424710245
using language	2.435632598873795
system described	3.4015342940590085
construction process	2.5786502394764685
role labeling	2.6702349372693326
r c	2.519593093966715
loves mary	2.0869027647065694
syntactic parsing	3.2588355410911793
corresponding sequence	2.2062448606993037
different ways	3.6810972355866056
terminal elements	2.12995111753158
additional material	2.060145782650642
driven techniques	2.031264650127311
template generator	2.2322680703881694
several projects	2.227249320716272
research direction	2.2924536885149736
different pairs	2.357121432847829
certain time	2.3435927256751126
step 4	3.061184170750608
interesting aspect	2.49449202328214
given instance	2.2147431158325057
training corpus	3.871331084252537
overall process	2.236207490380668
syntactic transformation	2.046096440133587
components may	2.449495104805549
two agents	2.3481055202536805
longer words	2.382637754944021
translations per	2.0817120252117016
np vp	2.8161191559500423
w 1	2.4185788909138823
tedious task	2.060145782650642
several points	2.431764288227561
de r	2.156759649006162
phrase length	2.1693942789149188
bilingual texts	2.180845896390021
scaling factor	2.040222986255761
synchronous tree	2.1240381466107294
correct response	2.2070563180044633
implicit assumption	2.3229739757638654
user community	2.1569808507858435
de recherche	2.0157594426817584
best fit	2.2073697906566268
run experiments	2.060145782650642
rich set	2.7244118306872585
maximum value	2.628164180936047
terminal symbol	2.8983449761686457
direct reference	2.0438015236562186
important role	3.526255064848979
art results	2.464836609822278
many parameters	2.321839114388654
initial words	2.053540033769436
selection strategy	2.1174510428502895
describes experiments	2.335904293904669
e x	2.4310804964562647
concepts associated	2.023537111427965
features employed	2.00238351760398
inflected word	2.4830906185466803
tile information	2.329081436908228
exponential time	2.433770311966939
unique label	2.00238351760398
human performance	2.9529231025920026
performance degrades	2.425466838565806
discourse representation	2.979910776504583
r 2	2.404631818990606
solution may	2.067747107964575
full documents	2.080606834271845
different role	2.0601457826506424
hurt performance	2.194634015772898
one concerns	2.0539861379893036
1 n	2.0811827917300074
word often	2.109437912434099
wherever possible	2.669547566043833
recognition tasks	2.3882695895936537
various combinations	2.658401881402122
data size	2.4712982931728114
efficient generation	2.012347776929124
forward way	2.075729255121515
sequence p	2.016336526582215
specific roles	2.020121462798538
mutual beliefs	2.15544270343453
user utterances	2.6454032011066078
set containing	2.892507674434005
following paragraph	2.1774937848124125
church 1988	2.419639843686028
certain ways	2.1413469986390323
discourse understanding	2.298505254223964
good predictor	2.4476623082632964
original grammar	2.676525377305792
use similar	2.1774937848124125
better use	2.0638211438882195
basic task	2.135844661161516
right order	2.815514470334959
certain pairs	2.0312646501273113
smaller number	2.941584951003004
helpful information	2.0329822559918163
n b	2.1226848278709882
set c	2.710236923793525
since non	2.067747107964575
given words	2.1695427321240506
average case	2.1042904425091606
substantial differences	2.022261218861711
corresponding parts	2.075729255121515
value pair	2.599267006191809
anaphoric noun	2.051738900457298
effort needed	2.2952219642004525
various reasons	2.435632598873795
include two	2.16396351368263
average values	2.2818214018700504
select among	2.533874621058591
parsing tasks	2.0144251740288075
complex constraints	2.1189396399514044
event space	2.031216633777997
np argument	2.179860534116518
also calculate	2.166102255087602
semantic dictionary	2.1712154113555377
following grammar	2.299686872045762
priori knowledge	2.345714156498313
lower error	2.4321499807495774
hierarchical clustering	2.5667276743726366
dumais et	2.212743797827991
including punctuation	2.145857202115015
frequency terms	2.288438693079057
information also	2.067747107964575
asr output	2.2333965229840933
term co	2.043503240473918
question types	2.4337539651131297
maximum likelihood	3.4900825704885023
since multiple	2.109437912434099
one particular	3.0845547715687345
probabilities associated	2.444782663571849
given utterance	2.6043251761486665
shallow analysis	2.3689267664870304
preceding word	2.7223073942839067
7 presents	2.183647914993236
information without	2.4459112107066288
theorem 4	2.225071436636754
action described	2.019557844009582
data becomes	2.257869825585851
systems using	3.0558095490435853
frequency counts	3.06103875190411
algorithm stops	2.218434485750926
syntax rules	2.2269144230335574
taskar et	2.1558224402987203
two elements	3.004852790872609
system could	3.2134172285477582
text categorization	2.8920073668614115
basic form	2.6694757629800034
definite noun	2.863580341284493
regular languages	2.4756238380867392
work correctly	2.014014536653706
section describes	3.514521091604934
verb v	2.6575602891043477
module generates	2.067747107964575
parser tries	2.260688804545853
low number	2.254788682446663
lower right	2.215156728075037
formal specification	2.4016968795954985
nouns used	2.0073178378257417
length model	2.0358411799663316
using relations	2.031264650127311
words like	3.3067871891356226
inflectional endings	2.280293839842196
following diagram	2.2020202169645566
production rules	2.87117928452593
sentence planning	2.5531694808047094
wordnet lexical	2.161246853402207
location name	2.1851508071020307
financial support	2.1663218214831987
qualitative evaluation	2.329593112668035
particular speech	2.0601457826506424
determine possible	2.0817120252117016
one participant	2.2277774537464663
sentence using	2.951623192741314
given problem	2.0759843683478647
sample task	2.4682413717362905
categories assigned	2.140856954368349
intermediate structures	2.3419409044154285
sentences must	2.2112071020073643
relationships within	2.060145782650642
phrase may	2.712656141725647
features make	2.166102255087602
two assumptions	2.5593166380164063
figure 4	4.228662618791079
spoken text	2.2011686204719005
another source	2.361017694193396
whose components	2.159746288077376
harmonic mean	2.7923708765822948
methods may	2.512514922083054
amongst others	2.171178031468078
tile one	2.166102255087602
tree path	2.087764029761046
similarity using	2.232977877789648
wide scope	2.480963291403013
approach yields	2.3448347819209543
subject domain	2.354743042088602
labeling task	2.3075843622031247
provide valuable	2.3763414719464064
per se	3.1319067216917458
formula 1	2.1406341576965144
8 words	2.458989459396439
johnson et	2.594329551760782
algorithm searches	2.2380656204269957
new directions	2.24825378073324
document pair	2.0236897681307644
modification relations	2.0743965972347684
errors due	2.550898850609554
linguistic insight	2.015795287891615
new sentence	2.898498784508574
knowledge engineering	2.7202986068812716
interlingual representation	2.1548498526137263
v u	2.0633132861039947
common property	2.0965741974051317
also exhibit	2.286725248739412
city name	2.0167329186945517
28 aot	2.7303677550121326
30 words	2.5191748617496703
semantic closeness	2.0577375124403856
general framework	3.075482218775066
use dynamic	2.145668076397672
big difference	2.047078298342884
using spoken	2.172274429303222
larger values	2.189546252614866
particular entity	2.435449594411215
four levels	2.5325593802441935
informatio n	2.2188080218191453
threshold values	2.4113470108053594
ongoing dialogue	2.0312166337779973
shows results	2.7006206206497407
level 1	2.264894393512525
100 documents	2.4450164383968658
high accuracy	3.398040421350055
semantic ambiguity	2.666074393917219
unique index	2.1275552563638707
dialogue module	2.0208173083388417
system recognizes	2.39552998400594
rather straightforward	2.2492578335740094
preferred interpretation	2.3099083151891553
model described	3.1645120634235444
copy otherwise	2.850240182896208
similar observations	2.159746288077376
one np	2.348359550546241
object argument	2.088757010266874
single attribute	2.185240400682336
remaining data	2.1543888787937475
phoneme sequences	2.023444414579761
le 2	2.0074734400897967
semantic inference	2.0846839150790553
foreign words	2.3641763606679467
conceptual content	2.3612865440695145
powerful tool	2.548854449890151
principle behind	2.022261218861711
second object	2.095055459489049
particular rule	2.5213477264853457
following points	2.50352395005998
processing capabilities	2.010154214584469
literal meaning	2.388937652761835
syntactic description	2.5317258507200564
formal aspects	2.1290482690107417
simple questions	2.049039082891178
unseen sentences	2.338978613401074
specific pattern	2.082836083424519
local word	2.2268060788765816
lauri karttunen	2.2275895034737268
prepositional attachment	2.2068642602077393
various techniques	2.6339219017704405
notational convention	2.1245637044373713
watson research	2.521020330237949
correct choice	2.429781898789426
either side	2.94522117673109
languages whose	2.192729087350867
word contexts	2.011879506557582
alignment algorithm	2.7942829294176588
tile first	2.636293240915723
similar idea	2.3922788066689504
including text	2.2532512925302806
ambiguity occurs	2.159746288077376
certain amount	2.75803319810529
approaches use	2.5327107950191152
ambiguous sentences	2.6615828247304583
simple example	3.464879195370029
3 training	2.1439026385008653
original formulation	2.3886619127043263
initial model	2.5340448318027557
several feature	2.0355401813680145
separate paper	2.067081229293324
common ground	2.167374656361513
automated learning	2.0423259728447545
actual value	2.102921514525577
independent manner	2.014014536653706
semantic classification	2.75977321641522
2 indicate	2.089026915173973
create training	2.0539861379893036
next paragraph	2.227249320716272
method provides	2.4966020678756835
discourse domain	2.14203372965276
weighted sum	2.885143030099407
following condition	2.5709437502869212
elliptical constructions	2.0376102740271858
example rule	2.0918364857418936
publication date	2.007315006862137
cannot generate	2.313258134832421
certain relation	2.075729255121515
recognition problems	2.139946145565166
entire text	2.660603819821859
one item	2.606081092210735
alphabetic characters	2.43181239995584
logical formula	2.458746150091343
linguistic levels	2.66153523565907
verb groups	2.448572171897954
tag combinations	2.012347776929124
aligning sentences	2.0907174895585374
web search	2.764760576380024
lower part	2.4484990872685626
dependency parsers	2.1861460525243865
given model	2.249257833574009
evaluate system	2.089026915173973
production system	2.010119731972423
th sentence	2.0860913066510656
absolute number	2.230524125991308
chinese lexicon	2.070902079606615
paper uses	2.4103259885026898
tile set	2.4778798432904345
values may	2.253574501397075
deeper analysis	2.4303117287858482
resulting graph	2.236207490380668
morphological process	2.155589295660503
dimensional space	2.9245433019963927
form used	2.2062448606993037
general case	3.2629036845113815
compile time	2.371558184982764
covariance matrix	2.060640253035934
mt research	2.0799926284569095
strategy works	2.022261218861711
rows represent	2.0601457826506424
specialized domain	2.167252082381521
practical value	2.2062448606993037
first strategy	2.5093058454595765
general setting	2.0965741974051317
rule extraction	2.0468551534843473
standard english	2.3681580705446086
systems allow	2.166102255087602
ar e	2.6238827920706576
control information	2.110879244281583
whose value	3.0087123788567114
il n	2.099645604973686
parsing problems	2.330001604487663
capitalized words	2.351603594642249
subtle differences	2.4172716132499064
model could	2.914490903747207
per input	2.135844661161516
verb could	2.067747107964575
recent study	2.078772670603387
free grammars	3.452341622308315
words outside	2.4272873498196885
morphological ambiguity	2.07722862167258
across systems	2.4028303441399395
input would	2.183647914993236
ambiguous cases	2.662279912298005
specific meaning	2.403472566572053
category name	2.1569808507858435
subsequent reference	2.1030670646886467
tests show	2.0186389419293844
clausal complement	2.1219736983354656
computational models	2.919587835338261
names found	2.058162491400105
representation languages	2.5475526839203146
simple task	2.470467437983557
available information	2.837698032067421
resulting structures	2.2685844072039183
primitive concepts	2.2205926003344483
computational model	3.061358008205544
questions correctly	2.130626584104312
two others	2.020056787986086
expository purposes	2.2224068987457315
overlapping features	2.216337423122651
system participated	2.049069748176902
unigram probability	2.2789208048829694
based trigram	2.202020216964557
various concepts	2.014014536653706
set threshold	2.096756183704444
temporal relationships	2.024052009043314
system runs	2.4250738008550288
auxiliary verb	2.9246480329909916
000 times	2.1497135154557805
single tree	2.581810927516102
among linguists	2.1774937848124125
step 1	3.3536009469765915
test texts	2.310564498336531
resources used	2.5266808937314367
human judgement	2.3091749723655277
like algorithm	2.031264650127311
unification formalism	2.2776224095974573
direct comparison	2.6836608058256592
different numbers	2.7877934927169474
sense inventories	2.0529404835311302
information related	2.7088901138689305
unification fails	2.2030887260567305
parser builds	2.3346429909491904
one property	2.2328679513998626
text could	2.267991541156711
case feature	2.074645043072319
separate training	2.3097823680289724
language components	2.2720616231260307
larger set	3.009378216480707
nodes must	2.3207800473379914
trace back	2.0074734400897967
side effects	2.4555107702427565
indexing scheme	2.1094372122819527
approach exploits	2.089026915173973
signal processing	2.4512478318350803
analysis systems	2.5053321500399357
missing word	2.0654273625489092
following probability	2.0817120252117016
conditional distributions	2.317715525859803
system based	3.3006937991367526
alternative hypotheses	2.0875607823387785
length one	2.262815729049285
definition 6	2.362505415324706
entire process	2.4559838465843646
frequent category	2.017570716396185
verbs used	2.2421869127123264
full advantage	2.166136340455187
decreasing order	2.8104130714504
000 verbs	2.017570716396185
role played	2.6340930423263167
temporal precedence	2.0051584301940206
three factors	2.44208952419012
state transitions	2.5927880470474505
polynomial kernels	2.112778664196261
phrase pair	2.1877255232130084
two minutes	2.2320627568691815
decoding algorithm	2.492723616598526
associated information	2.055118260693857
cannot hope	2.089026915173973
one link	2.4020668704166903
simple form	2.563033542520629
one level	3.045359177783911
induction system	2.043229651767234
common properties	2.130459427106893
corpus using	3.2060606616119944
optional arguments	2.1117654159915116
null word	2.3822019004132318
remaining terms	2.045521226679158
corresponding values	2.4028451845347893
temporal relation	2.3883527807220846
relative importance	3.0727798789718817
given information	2.446476790421464
nl system	2.4064132595003915
main sources	2.034392286942111
recognition community	2.012347776929124
two reasons	3.4945726038831317
class 1	2.182192561381698
tree bank	2.4801333537381183
ensemble de	2.091414048327362
set k	2.1358664885908043
appropriate case	2.135844661161516
tiffs paper	2.181370500368545
individual nodes	2.3634279675149354
2 displays	2.431764288227561
tile algorithm	2.102921514525577
situations involving	2.045521226679158
speech labels	2.1956179388517136
resolution component	2.3225702227198886
system assumes	2.3741226029687725
also support	2.3955781761503503
low accuracy	2.5212015420754312
algorithm first	2.583124765145555
language identification	2.2920053127830182
cannot use	2.6494568948431523
expected agreement	2.104384481649786
translation component	2.2822101355443998
programming languages	3.0245712101277755
information produced	2.2384438317931394
specific content	2.11911577017412
develop systems	2.0376102740271858
observed data	2.521043712893417
character error	2.0092030223109587
corpus evidence	2.353676249882287
relation name	2.159301094522771
set also	2.0376102740271858
often co	2.1353034784701492
clustering approach	2.289222373298476
individual speaker	2.085366556257962
consistently outperforms	2.081242005652375
permits us	2.550616835304045
node types	2.0089841291585113
annotated training	2.9074411358668186
new insights	2.109437912434099
complex domain	2.189546252614866
language tools	2.048642073271141
process described	2.804024362064602
transformational grammar	2.649942010391258
tree substitution	2.036766869839977
identification system	2.089909994226249
infinite set	2.663418866110875
clearly shows	2.5998248828102515
tile parser	2.2016158367443897
tree structures	3.118721935176719
dependent variable	2.061097666135009
theorem 1	2.6996573349264086
cannot handle	2.7349801258791695
chomsky 1981	2.2541760535899478
foot node	2.531361401089174
important application	2.1816335351833818
different input	2.3483606540998085
method 2	2.239791731075982
wsd algorithm	2.122250102505777
enables users	2.1588487997915733
tables 7	2.060145782650642
f f	2.090098433948502
three stages	2.865100700915209
gram models	2.9449920372989995
following feature	2.496488433113517
argument roles	2.051285638402277
model allows	2.6902426722536297
two dependency	2.241991728299518
different environments	2.0965741974051317
gram model	3.0750462811926247
mohri et	2.1972376264683557
descending order	2.8670216701938593
partial results	2.5087058549621184
human speakers	2.1609391252430954
p c	2.0131322911008747
automated systems	2.064362096240994
unique id	2.0376102740271858
sentence within	2.2776740307447065
development environment	2.4814729557961286
help much	2.067747107964575
language description	2.394251797375938
additional research	2.045521226679158
frequent use	2.1942667452387457
modified noun	2.2624623918373445
two humans	2.045229961808099
two datasets	2.0355401813680145
every application	2.026260250766906
using em	2.3365802522225954
every position	2.1413469986390323
second element	2.6268931537476226
elegant way	2.3422763733583603
full power	2.020056787986086
keeping track	2.9419018785485043
empirical comparison	2.1299549710919283
statistical tests	2.468831387492778
let h	2.1911147757555276
features used	3.4901742165611322
f e	2.428556185388687
data contain	2.086884416965048
complex queries	2.1563079384871173
large extent	2.7367316715194994
sense selection	2.1329569867441203
coordinated structures	2.073066253406054
compare performance	2.2062448606993037
phrasal lexicon	2.118284546306079
common nouns	2.994056702856271
correct senses	2.233352240274619
use different	3.1677830502920385
brennan et	2.245781827711935
several kinds	2.6896863519611975
learning mechanism	2.1976976468804086
lexicon must	2.3332911335775464
two inputs	2.0904334515920637
small domain	2.2062448606993037
figure 1	4.635656390503238
le syst	2.0243941674525656
accuracy drops	2.0577375124403856
rr 1992	2.3953272468316316
primary motivation	2.314007818435079
scheme used	2.4631653324704343
extensive discussion	2.1774937848124125
occur independently	2.1312839727750825
acquisition system	2.1147180619144357
relevant portion	2.040238153498075
information carried	2.4280505365551512
search algorithms	2.5989558759567286
scale corpus	2.3178448553704087
constraint must	2.0817120252117016
single linguistic	2.022261218861711
class name	2.310192420565465
grammatical categories	2.8671166930255825
evaluation measure	2.634979157023286
initial consonant	2.108846551096652
several sentences	2.8159447711649115
lexical variation	2.0813511401067935
technical papers	2.0904334515920633
statistical systems	2.3209654505061703
morphological analyzer	3.275791505133448
barbara grosz	2.255349860587285
logical operators	2.4175346564046687
trivial case	2.109437912434099
section outlines	2.3005523645853
per query	2.1922918625134216
complex ones	2.4561333536596983
higher likelihood	2.031264650127311
rule also	2.2532512925302806
distinction made	2.1946340157728974
word orders	2.5005955263927704
entire test	2.4488803960715435
need information	2.014014536653706
processing speed	2.48073635253244
input specification	2.1003886985625235
information gathered	2.3365869267409933
second iteration	2.261642506055714
threshold th	2.1000868080586823
language knowledge	2.5377657802535367
first solution	2.1590976696462425
analysis task	2.1297846672161187
small grammars	2.080218100597496
first language	2.352002086984809
spoken dialogues	2.5645026942742404
3 presents	3.2229907821569994
name may	2.303241766231916
automatic question	2.1050838385526642
semantic type	3.1371232609741324
agglutinative language	2.4359041476270598
memory space	2.280589386460209
crucial factor	2.060145782650642
main parts	2.6792868185392495
network model	2.08681834552062
shallow semantic	2.5002409784664
examples illustrate	2.724961745321722
possible errors	2.3260125287756366
rule may	2.8437279467454437
beam search	3.0447561583062344
answer questions	2.986787996167925
directly use	2.296234813805337
singular form	2.541733686793081
much detail	2.242793580526907
three approaches	2.59302066402624
per sentence	3.316489558540467
post processing	2.1232429349093938
available features	2.192723676340907
model captures	2.3205010232255585
various entities	2.014014536653706
processing since	2.067747107964575
h 1	2.0763533583416773
transition probability	2.608061023848965
word basis	2.3717508441912747
press et	2.1245637044373713
automatic acquisition	3.1205459707047836
language system	3.3254051614096456
uses multiple	2.222406898745732
tagging methods	2.1954890274836183
process could	2.414320698244546
ideal case	2.3107504571819204
results seem	2.5385603545976756
two techniques	2.6543246246293988
computer scientists	2.5640264231336745
discourse component	2.197870891426191
statistical analysis	3.146901049908792
rule requires	2.190982236626934
earlier papers	2.072290098419245
practical problems	2.5009289214631294
overall meaning	2.093294558436213
special way	2.2251427952863687
boundary marker	2.2743180934903515
given discourse	2.446115845819681
new contexts	2.00238351760398
arbitrary size	2.075729255121515
city university	2.209256749394556
trigram language	2.9762795317177564
mother tongue	2.236251084088013
greedy search	2.538819714129086
joint ventures	2.0144824746589816
frequently co	2.229959192980847
features including	2.3956433384568747
model based	3.2831518990361994
sentence boundary	2.7181846288735807
good indicator	2.6214131337199413
perl script	2.070902079606615
isolated word	2.348574642775343
window sizes	2.464024322890984
data sparseness	3.326686280299095
important factors	2.111650518923389
two base	2.10319560143998
disambiguation systems	2.26470180877317
several cases	2.227978605693532
using confidence	2.053986137989304
raw texts	2.227249320716272
4 lists	2.5995605116974647
third experiment	2.5184301437280587
real texts	2.7206721378382848
mckeown 1985	2.109803271841388
possible attachments	2.1023887620661283
analysis used	2.2062448606993037
sentence translation	2.073037418952196
simple regular	2.1413469986390323
syntactic rule	2.6421569215318734
whose probabilities	2.0699983678896388
forward application	2.061009249259874
map directly	2.0918364857418936
solution would	2.3285192394219965
important semantic	2.2917594692280554
line 3	2.30018490760141
problem even	2.0817120252117016
precedence constraints	2.0781045944618195
standard language	2.1731100105449546
concept definitions	2.0177035618572665
identify key	2.0312646501273113
strong indication	2.2169936022425736
various sources	2.7669113109852526
chinese named	2.120065560513333
generated summaries	2.3266632211033302
chinese texts	2.507019859183598
involves using	2.2823116493074065
3 displays	2.166102255087602
prep np	2.009868063569498
quantitative measure	2.240851989911108
per token	2.141006899009188
upper case	2.730532743133546
information across	2.377282352832986
memory usage	2.2654494808841723
model assumes	2.6630384065311845
conflict resolution	2.202125096658182
full range	2.8756851971900397
tree used	2.014014536653706
learning approach	3.1506118931846245
expressive power	3.170832603069039
variables representing	2.0490697481769016
rules work	2.1597096509760467
distance matrix	2.0500240918697994
time performance	2.463455943273898
subsection 3	2.239252982739327
certain criteria	2.1023887620661283
probabilities used	2.172274429303222
four sets	2.541293313648083
search strategies	2.589915122392228
accuracy figures	2.2802007927104944
hirschman et	2.4055027735681156
less coherent	2.007473440089797
window around	2.333178632543555
handle various	2.2421869127123264
noise level	2.0882752360578962
terminal labels	2.208775653151079
coverage grammar	2.708497281319176
first time	2.86703462495816
k n	2.1275452238699737
graph structure	2.6258475659233573
abstract level	2.7434519793398437
restrictions imposed	2.115657387463218
english machine	2.589432531681637
data sparsity	2.547444315683009
task using	2.6730033760375407
two interpretations	2.6302555456501118
key problems	2.109437912434099
following scheme	2.116591742190463
resource grammar	2.1553929608484212
typically contain	2.3567860333521553
roman alphabet	2.19110807291794
cases without	2.2328679513998626
probability model	2.9815213159255776
dialogue situation	2.213318758370463
syntactic realization	2.5954904990169014
like representation	2.1861280921309323
different query	2.014014536653706
possible english	2.3333329581503177
moldovan et	2.3770237731337236
second reason	2.4552186484386915
lambda abstraction	2.1497135154557805
associated set	2.1597096509760467
various places	2.0376102740271858
tree representing	2.3451079538244812
introduce another	2.126823141441437
rule says	2.482555376675485
earlier work	3.381337655593265
work better	2.1786566600374835
words beginning	2.1266359682615787
b r	2.0824927688889554
given set	3.1486722063432344
lexical anchor	2.15845517841567
learn rules	2.3454519419159627
improve upon	2.4333473178778693
surface realization	2.78610242354933
many documents	2.5570843432582175
poster sessions	2.889561746555762
language lexicon	2.0355401813680145
special class	2.086884416965048
relevant documents	2.962025813053168
present experiments	2.4873413864592644
arbitrary order	2.1499148428335464
standard notion	2.0379203068673784
different stages	2.745906850968284
next module	2.0790198876097175
different directions	2.0918364857418936
senses may	2.172274429303222
explicit word	2.2691363048828297
phonemic transcription	2.1667196458467277
independent variables	2.0631872863523464
system goes	2.045521226679158
original work	2.33582780054359
formal theory	2.15378453651139
steps 2	2.5683856589058234
different sentence	2.602446094626907
5 presents	2.9717279489682755
free skeleton	2.0460964401335864
last character	2.34454000239269
information would	2.862390727217232
multiple knowledge	2.3430594840483843
labeled trees	2.089450032304352
kaplan et	2.065923000064049
probabilistic grammar	2.280557185725618
abney et	2.064362096240994
summarization process	2.115251618848125
better precision	2.6169782971273055
recognition rate	2.497190432471097
linguistic systems	2.0514353566256416
grammatical function	3.0305929146660846
current systems	2.8761738626025455
experiments involving	2.443147787685106
contains sentences	2.1413469986390323
general definition	2.386811743778047
confidence measure	2.2818971683505875
following kinds	2.314007818435079
many incorrect	2.081242005652375
correct alignments	2.072290098419245
techniques developed	2.697776880737977
every sentence	3.1756379424191477
automatic system	2.7025748899331217
relations must	2.267991541156711
initial conditions	2.1001142412056417
1992 l	2.082836083424519
recognition grammar	2.1027772408407204
zero otherwise	2.1499148428335464
constraint logic	2.112681746357865
absolute difference	2.0402381534980756
section 2	4.3884638123796025
particular object	2.4348902877946643
complete translation	2.0514054040135554
selection task	2.0029889602877082
data consist	2.2380656204269957
three constituents	2.2339270481759206
2 introduces	2.7033596236321222
label c	2.022410241232288
another research	2.067747107964575
following sentences	3.0666866855468715
single target	2.1353034784701492
model predicts	2.5468147871476705
ordered pair	2.574491073858254
human annotated	2.100503945208036
intrinsic evaluation	2.1537099818799033
est pas	2.220740478208941
synonymy relation	2.1397843193813024
right node	2.0333762149816526
system obtained	2.012347776929124
linguistic analysis	3.3203851124828985
structures rather	2.189546252614866
higher probabilities	2.1424461801257175
years old	2.420211946448981
missing arguments	2.031097094307942
lattice structure	2.1497135154557805
method performed	2.122563170379007
let w	2.707973434361595
relative entropy	2.28723312745233
compound nouns	2.684474705674208
different corpora	3.08822138715932
semantic tags	2.669723888122611
model approach	2.079019887609717
referring expressions	3.1069570995719267
choice points	2.145564298255952
lexicon consists	2.458989459396439
returns 1	2.1438988528268723
subjects could	2.0965741974051317
parsing grammar	2.1066211566745716
classifying words	2.0329822559918163
given label	2.014014536653706
combining information	2.3000015484793472
billion words	2.0360685889145866
performance figures	2.5115297397318246
system cannot	3.046507520125202
future extensions	2.116591742190463
evaluation scheme	2.501165891286116
module determines	2.079019887609717
automatic conversion	2.1390445946327175
one point	2.4930509270390204
long documents	2.1784840626689177
physical objects	2.693829937672656
several studies	2.7592043699465174
one argument	2.930634966013342
tagged brown	2.1161528435313297
linguistic structures	3.061458456862245
distance dependencies	3.0186080676842426
lexieal entries	2.1882216921535553
specific assumptions	2.014014536653706
n nodes	2.1659825136431445
two labels	2.264636058202776
language provides	2.3398754723868356
boundary detection	2.407106222494317
feature path	2.153709981879903
various parts	2.4552186484386915
phrases according	2.045521226679158
distinct nodes	2.060145782650642
search methods	2.3239101964323794
let v	2.538139668739646
disambiguation task	2.7955779540165953
functional categories	2.0235139020014183
category assignment	2.1490413269979767
certain degree	2.8340268715273456
recognition output	2.4411167370701654
last decade	2.629948014730134
server architecture	2.136855588077802
direct way	2.5298496353373254
higher f	2.2443687102109093
relative frequencies	3.049914314382038
particular emphasis	2.074322125359227
particular value	2.435632598873795
within sentences	2.4511389563643227
help improve	2.090723484061317
interchange format	2.0846971567714903
previous stages	2.0701690642083435
scores assigned	2.4006692790727913
n n	2.7634170517200936
given string	2.7452962873450675
two modes	2.710887122814867
following fragment	2.1432314806483994
specified set	2.1946340157728974
null finally	2.5715673631957685
parse failure	2.010845255290387
subsequent modules	2.045521226679158
ted briscoe	2.017790445239557
main information	2.0355401813680145
assign semantic	2.177338384924136
best described	2.1023887620661283
current example	2.45078770424615
structure based	2.5099315661219403
work addresses	2.200598690831077
sentence 6	2.075729255121515
cunningham et	2.37282406569921
manual tagging	2.2639203346314627
actual word	2.5102766482657013
gram features	2.1611284553216574
finite automaton	2.2971524833605574
following cases	2.6182878861798575
used word	2.5270204646552323
second word	2.860268482962968
predicate logic	2.6036512947797323
two positions	2.5697125607389486
automatic construction	2.7424210151959274
generation techniques	2.305097752546449
parse selection	2.1073755208484686
lexical choices	2.4866260692901037
research problem	2.319324296244544
work suggests	2.3005523645853
theoretical foundation	2.2458302187516197
est de	2.1039800387717347
rules use	2.1882216921535553
whole corpus	2.9340035319180733
better result	2.5453426558731893
machine dialogue	2.1325320464545423
linguistic competence	2.5788504024618675
algorithm computes	2.506551527861537
compact form	2.1774937848124125
search technique	2.0730662534060538
propagation algorithm	2.0860845524331517
similar experiments	2.3229739757638654
head words	2.958847297581307
certain verbs	2.533874621058591
several data	2.070902079606615
third person	2.981447529844556
varying number	2.135844661161516
practical issues	2.0824927688889554
first examine	2.2380656204269957
unseen text	2.2741633166997204
describe two	2.945343942761738
process based	2.318793079863193
theoretical assumptions	2.1432314806484
higher recall	2.883651799299681
regular patterns	2.07817541032735
resulting sentence	2.296234813805337
role fillers	2.2662334281228462
et les	2.4352511916402713
combined approach	2.2371895639205435
bnc corpus	2.0301435463043473
structured information	2.396605332922542
tasks include	2.0376102740271858
close proximity	2.2916287452490876
using sentence	2.1939531715857408
semantic content	3.3414489614325666
problem may	2.5871936349478175
second version	2.417618427399807
best lists	2.6046350666224183
prior discourse	2.1889018612382527
error reduction	2.9513899110104123
declarative representation	2.197648576685241
semantic formalism	2.1710936856104794
significant number	2.8291155627113778
collection process	2.139044594632718
mean number	2.340979884332822
particular languages	2.2609206202950274
lexical verb	2.099458264360342
single noun	2.48504420336149
much difference	2.189546252614866
subjective judgments	2.075729255121515
relation defined	2.3365869267409933
significant correlation	2.1533170290508146
new idea	2.0053972830294953
output based	2.1023887620661283
little attention	2.550836618281068
narrow domain	2.2823116493074065
unsupervised system	2.1073394803862087
process involves	2.5750881661299045
bilingual sentences	2.1089536884889815
probability values	2.392329141858811
word alignment	3.0686859914874076
entropy tagger	2.029056699046527
linguistic task	2.11638795219105
current task	2.426468633715252
syntactic frames	2.246853384477596
subsequent references	2.113667226638083
boundary markers	2.2376153966180508
di cult	2.231075794793252
yields good	2.086884416965048
frequent tag	2.0564340940627437
values would	2.031264650127311
different attributes	2.380830363798011
local trees	2.3542235863886627
text corpus	3.2205179204324996
minor changes	2.0755911374995546
classi cation	2.5322761793574573
sections 4	2.963597744134697
notation used	2.611814665828829
initial hypothesis	2.1053337479124172
correct meaning	2.432661019236081
category v	2.105195669894195
two entities	2.85991078163197
algorithm seems	2.0817120252117016
logical framework	2.026260250766906
grained classification	2.2063039107322684
research contract	2.2427935805269072
verbal communication	2.0054483617135577
translations produced	2.231235308000747
documents based	2.4810888967838034
speech events	2.11309470603799
method called	2.4949866697330707
research labs	2.0918364857418936
variable names	2.1405120286798134
necessarily correspond	2.227249320716272
classification systems	2.203615537330491
following situation	2.1072480253375243
international workshop	2.846281229978029
general class	2.3357920273960415
pattern recognition	2.8607122757161187
figure h	2.166102255087602
lingual information	2.463511893729932
accusative case	2.56536905331494
annotation effort	2.453833107257463
translation quality	3.0127639614999016
taking care	2.0073046923332467
whole phrase	2.419651923103911
notable exception	2.250704742761071
result might	2.0601457826506424
directed graphs	2.538443214636903
lexical element	2.1513738263092224
grammatical properties	2.3661314342895565
specific models	2.090030695424222
rule applications	2.578978550232726
criteria used	2.6127947423729774
indexing terms	2.1186417152370036
two mechanisms	2.2965916473768724
discourse relations	2.744938692676026
low agreement	2.165115471861699
resolution systems	2.258293323434577
key word	2.3874067089578785
neural network	2.7596159632910466
query q	2.4635558641939106
processing problems	2.512514922083054
algorithm finds	2.609451926984696
penn treebank	3.5914045876044662
sentence contained	2.089026915173973
function symbols	2.130027088095199
linguistic form	2.71335755902022
current form	2.2147431158325057
tight integration	2.1677847630386546
techniques described	2.7772820149736175
slight improvement	2.27314861737366
wordnet synonyms	2.0410184476447633
agent must	2.1882760634479266
different constituents	2.1954890274836183
separate file	2.189546252614866
word subset	2.0221951140579186
another type	3.0802455813072434
many irrelevant	2.2224068987457315
various machine	2.2917594692280554
information might	2.509365157065348
convenient way	2.5577806922873396
efficient manner	2.3448347819209543
system designer	2.255617582058247
textual input	2.1943253913209606
sentence structure	3.184419247146664
parser developed	2.4403483572687596
similar nouns	2.051405404013556
tools used	2.088762691620893
automatically extracts	2.2421869127123264
university press	2.289694481867339
model obtained	2.0817120252117016
following results	2.4625444951357887
model theoretic	2.238087700472332
two sources	2.925016806105453
contains much	2.181370500368545
six words	2.189546252614866
demonstration system	2.1542109899205135
various sizes	2.278461809175595
final word	2.430609642275889
literal translation	2.414663984844162
possible solution	2.7003926770803184
formal logic	2.192729087350866
generator would	2.189546252614866
lessons learned	2.4279310877776368
structural characteristics	2.1816335351833818
rule makes	2.045521226679158
sequence may	2.0965741974051317
task model	2.0029410592386276
case marking	2.522318423627032
sun workstation	2.2917594692280554
definite description	2.3497764285726674
model cannot	2.5405134381151706
verb cannot	2.075729255121515
lexical disambiguation	2.5600961869731305
information coming	2.084486237350305
two paths	2.478652299331841
often contains	2.33582780054359
one child	2.31541170926451
domain corpus	2.3871780619167895
every combination	2.278461809175595
complex reasoning	2.060145782650642
theoretical models	2.1462245538298697
million sentences	2.329081436908228
theoretical model	2.27202920771712
word corpus	3.017817937831156
initial letter	2.0709020796066153
knowledge resources	2.5339906570738426
simple combination	2.1432314806484
answering systems	2.9541053528221295
full word	2.3548005898232183
since people	2.067747107964575
corresponding class	2.1414753566437956
common knowledge	2.635005932467724
enables us	3.290777588562077
last years	2.028632806380823
maximum probability	2.4656711150836816
null ment	2.475621859290715
speech information	3.0707791429441196
horizontal axis	2.5080857654066757
corresponding entry	2.2685844072039183
final version	2.7422900345248378
syntactic sugar	2.172274429303222
two words	3.8131272497310986
two places	2.2685844072039183
syntactic variation	2.289831262074191
example 7	2.0114255700800174
different weights	2.6233058216269347
simple finite	2.1774937848124125
language texts	3.0603623353334926
emotional states	2.0167106848265535
positive effect	2.2132531992189786
underlying grammar	2.4427752364021327
parameters based	2.045521226679158
various information	2.4282856154916748
individual models	2.084809533470605
recent version	2.1563079384871173
new target	2.02585040092793
distributional hypothesis	2.284270190673964
small differences	2.296234813805337
language side	2.089632357857702
structure level	2.3729739559312844
journal data	2.168608828146297
2 explains	2.2328679513998626
lexicalized tag	2.168418591097131
adjunction operation	2.194131831584423
reduction rules	2.1231758826106404
research field	2.053986137989304
200 sentences	2.6003267729353774
linguistic descriptions	2.71466331037246
predicate names	2.1477417983652516
several simple	2.1119430342456558
system integration	2.0219742875792464
human evaluation	2.3720560275201645
require considerable	2.045521226679158
full version	2.0880414073297957
strong impact	2.089026915173973
another technique	2.1413469986390323
mouse click	2.063693154062279
telephone speech	2.2900431969261206
similar argument	2.099563140976809
set would	2.4778575630550566
particular problems	2.352331304842637
grammars without	2.154388878793747
current position	2.4799228318381727
looking centers	2.2594250704245225
training portion	2.10637101627493
personal name	2.188337587319723
thank david	2.022261218861711
text classifier	2.1463886151886746
occurrence within	2.0460967409737787
grammars may	2.4261542590040044
