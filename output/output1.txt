quite small
scarce resources
newspaper texts
91 \],
significantly fewer
ambiguous structures
likelihood solution
individual characters
special form
cannot expect
texts containing
procedures may
errors per
corresponding output
common part
true value
based disambiguation
different user
initial approach
expository text
1 includes
learning algorithm
special function
approach seems
1 given
observed differences
ordering relations
limited extent
sidner 1983
training required
associated weights
single individual
relative ranking
predicates may
represent knowledge
ir systems
speech effects
using rules
terrorist events
features without
strategies based
-- l
paper demonstrates
shieber 1986
2 words
nl applications
articles published
state 0
parent nodes
also help
;(& quot
new patterns
universal quantification
original paper
semantics would
expression ).
texts describing
daniel marcu
smart system
al 1998
output sequence
web pages
text documents
search procedures
frequent patterns
phonetic transcription
learns rules
interactive approach
prolog clauses
next version
methods presented
dans la
verb structure
estimation using
pruning algorithm
alpac report
link together
grammars allow
worst case
5 words
segmentation rules
depends critically
example rule
recognizer used
also identifies
english \[
p value
example concerns
pitch accent
level processes
hash tables
constituent trees
original tree
estimation techniques
4 );
lemma 4
trigger word
expensive task
grammar specification
alignment error
bnc data
like p
introduce additional
problem addressed
lexical relationships
possible worlds
derivation structures
interesting insights
object --
generating language
antecedent must
training program
first probability
three semantic
extreme values
resources used
types described
original implementation
automatic approaches
original utterance
brief summary
linear transformation
relative distance
algorithms work
rules include
among n
nodes created
words already
experimental framework
syntactic surface
theory predicts
tree according
also facilitates
special problems
proper name
initial conditions
several nodes
semantic tag
parsed data
second question
modelled using
additional constraints
original list
1982 \].
could conceivably
extract candidate
control system
cannot simply
anaphoric expressions
treated differently
one tends
multilingual corpus
w j
parallel elements
segment words
different areas
\[ allen
local minima
rater agreement
handling large
), ...,
knowledge provided
produced without
constituent coordination
based knowledge
representation contains
detailed exposition
make clear
different lexical
specific concept
design process
human assessors
40 ),
average entropy
multiple derivations
hypothesized word
three evaluation
category must
express certain
estimated probabilities
based machine
independent linguistic
single state
easily integrated
de meulder
purely compositional
say ),
semantic units
data increases
distributed processing
augmented transition
sentence also
complex phrases
grammar currently
): \[
show promise
total frequency
class contains
often associated
natural english
base nps
specific linguistic
feature functions
cf rules
restrictions ),
f ~,
function h
computational tractability
informatio n
n increases
various syntactic
large numbers
several characteristics
certain regularities
), written
two pieces
exponential complexity
wordnet hypernyms
05 ),
accuracy across
source words
finding word
pragmatic knowledge
various combinations
lexical context
0 9
new architecture
corrected manually
severely limited
process might
one defines
beneficial effect
across languages
usually obtained
implementation level
dependency tree
necessary background
previous result
empty sequence
tl ).
occurring together
partially implemented
experiments comparing
efficient tool
translated directly
remaining four
current corpus
path length
small data
26 ).
reported herein
clause type
word dependencies
tl \]
model performed
noticeable improvement
two fields
specialized language
appropriate measure
results indicating
domain training
large extent
although word
state ),
26 \]
recall without
build two
c using
system translates
semantic nature
level interpretation
syntactic frames
approximately 0
help resolve
one option
tagging problem
concept names
particular verbs
automatically using
morphological synthesis
structure found
deep semantic
logical object
en particulier
universal set
dialogue interaction
point de
optimization procedure
elegant solution
australian research
complete analyses
general lexicon
application tasks
best result
senses per
freely available
rambow et
adaptation data
atis data
system incorporates
types used
compound nominal
86 \],
limiting case
rules may
identified automatically
left side
data type
c /&
sentence represents
final translation
arbitrary features
method needs
different group
independently motivated
wider context
elements within
reference time
dependency graph
past experience
independent models
f ~'
edges ).
lowest level
produce better
method utilizes
\[ karttunen
methods like
existing language
null alignment
name ),
uppercase letters
extensively studied
single speaker
search paths
sets contain
useful feature
sample rules
us start
content based
different circumstances
verb usage
specific models
select one
one proposition
also explain
correct model
vary across
assisted language
also called
must go
sized training
require substantial
sense ambiguities
speech technology
relation ),
correct form
case grammar
weighted linear
incorporate syntactic
semantic objects
). 7
dating back
efficient algorithm
items within
three cases
logic \[
currently testing
driven bottom
large list
kanji character
combining syntactic
translated sentences
assigned values
low coverage
data bases
related word
difficult problem
concept nodes
one currently
model information
always true
root category
h h
attractive alternative
best answer
manual transcription
strong association
end nodes
minimal pair
whole dictionary
first put
lesser extent
basic question
social security
phrases associated
often much
without sacrificing
nsf grants
tile definition
root symbol
multiple copies
models must
remaining word
resulting automaton
representation called
discourse may
plural ),
average number
figure 3b
n ~,
sections 2
comparative analysis
dialogue behavior
two anonymous
200 ),
earlier approach
treebank pos
general public
cannot model
clearly understood
feature might
drawn randomly
tile meaning
approximately 35
distinction made
repeat steps
three simple
perform much
canadian hansards
fixed threshold
either directly
three distinct
information needed
intermediate result
becomes apparent
single run
given question
proper treatment
potential value
varies depending
used 1
explicitly stated
english phrases
like figure
value matrix
system applied
making progress
shallow processing
close correspondence
using stochastic
propagation algorithm
results obtained
semantic definition
defined categories
given time
previous noun
years ).
computational treatments
000 running
fisher et
distinguish whether
boundaries ).
different test
). subsequent
automatically aligned
simply ignored
different aspects
certain distance
context could
concern us
scale system
risk minimization
single text
either lexical
(( v
noun entries
system within
translations found
asr system
name appears
first six
procedure continues
7 million
recognition community
worse still
spontaneous dialogues
small collection
report experimental
response time
principle ).
conceptual distance
multiple values
agreement figures
based parsing
volume 20
would find
speaker wants
standard evaluation
semantically consistent
showed better
works best
might think
identi ed
(( 1
columns contain
specific purposes
~. 1
numeric score
significant proportion
formal grammars
small sub
requires large
comparable accuracy
every node
many definitions
speech recognizer
multinomial distributions
produces multiple
contain semantic
practically impossible
several groups
confidence measure
grammar needs
multiple classes
add additional
statistical data
figure 18
perfectly acceptable
single line
sentences contain
various examples
annotated text
statistical methods
ten runs
deep cases
since different
additional experiments
person singular
system applies
likely parse
detection rate
output words
), results
67 %.
flexible system
mes de
approach produces
1 r
way ),
useful starting
interpreted using
narrow domain
template definition
short documents
annotators agree
mellon university
syntactic templates
new classification
improving accuracy
several tasks
formal definitions
important advantages
yet known
concept learning
multilingual lexical
quite accurate
information carried
simard et
10 [?]
also part
r (;
different subject
event space
assume without
verbs tend
statistical classifiers
information available
multimodal input
different reference
single pos
np must
sentence meanings
classification rules
semantic equivalence
prosodic units
checked manually
two participants
entry may
constituents within
71 %,
wsd tasks
thus consists
one trained
unique identifier
one idea
actual translation
basic architecture
similar goals
case 3
express relations
-~ e
two corpora
ii system
word task
given c
performance would
output without
significantly easier
appendix c
f e
retrieved information
connections among
shift action
simpler task
tagged data
semantic selection
), two
evaluating performance
cases discussed
text rather
work still
system incorporating
intermediate language
relative difficulty
k clusters
subsumes another
binary tree
within 10
effective approach
closer look
independent model
simply choosing
language words
particular input
2 %.
containing x
category names
used effectively
partial solutions
like verbs
basic research
several utterances
quot ;...&
primarily concerned
theories may
templates would
general english
next element
entropy values
practical experience
relational databases
detailed examination
specific model
quite naturally
collect information
;. even
producing summaries
complication arises
computation required
original motivation
example rules
important subtask
syntactic variation
de mots
syntactic sugar
four columns
one another
definite reference
average performance
use x
human judgements
function k
largely determined
specific form
b }.
current stage
automatically extracting
score higher
research workshop
tile following
., 1997
different information
full parse
lower right
w c
problems due
two semantically
parsing experiments
five groups
may note
one needs
declarative representation
particular constructions
specific function
several smaller
main word
performance computing
training run
many feature
always occurs
without increasing
language models
paper compares
planning mechanism
r rules
several years
study using
first approximation
word representation
since sentence
understanding module
results may
corresponding word
g .&
probability value
label sequence
english question
far better
usual approach
right sub
test different
linguistic properties
training text
formula p
kay \[
number agreement
next subsection
probability mass
acceptable quality
2 sets
1 k
new field
using brill
two extremes
radev et
task described
verb agreement
10 \]).
character string
german language
seems likely
parsing technology
frequent ones
types defined
unification would
algorithms designed
imperative sentences
mathematical properties
three patterns
generally accepted
make parsing
york city
things like
would imply
scenario templates
tense information
learning literature
present research
greater efficiency
consistent set
seem appropriate
order constraint
word type
system operation
single output
right sides
lexical class
specified semantic
must follow
ten sentences
distributional properties
current rule
name suggests
speech production
additional layer
also reflect
method proposed
immediately preceded
news corpora
larger test
learner must
processing results
several analyses
parsing framework
form n
head verbs
epsrc project
also incorporates
core language
linearly combined
), vp
last constituent
classification trees
best use
specific problems
main verbs
system obtains
proper subset
partial tree
derived using
clear differences
dialogue fragment
computer systems
category assigned
textual description
e cient
european project
data described
method ),
sentence planning
;. another
grammar rather
text ),
increased accuracy
third method
top portion
rule must
corresponding verbs
lexical signs
two right
(: el
generalized markup
also must
features mentioned
100k words
). throughout
agree upon
syntactic formalism
either correct
generating multiple
second corpus
token frequency
cosine similarity
quot ;+
processing strategies
various actions
classified examples
discourse phenomena
target representation
documents contain
uses wordnet
., 1991
modular system
error reduction
structural properties
minor problem
two structures
partial list
local linguistic
whenever one
already knows
predicate arguments
node corresponding
therefore require
actually occurred
human evaluators
many names
35 %.
general reasoning
usually occur
task consisted
structures \[
xerox parc
state set
6 times
method extracts
possible situations
use f
words encountered
default values
per utterance
similar strategy
though still
1983 ))
college students
parsers used
full natural
machine learner
using full
class tags
first observation
translation community
representation must
level feature
telic role
intonational phrase
domain dependent
automatic text
digital library
achieves good
far enough
system usually
significant linguistic
linguistic approach
appropriate action
., 2004a
orthogonal dimensions
1979 ).
00 american
randomly extracted
task used
alternative definition
two observations
later .)
simple verb
represent syntactic
hpsg grammar
scaling factor
\[ det
even possible
reasons described
correct prediction
1979 \]
maximal set
data support
document ranking
better modeling
aha et
stochastic language
also begun
case element
en un
certain questions
underlying assumption
common characteristics
hoc rules
last letter
two subsections
node 7
data consisted
multiple types
words include
highly specific
module using
parallel parsing
ranked documents
}) e
4 note
initial node
investigated several
significantly reduced
analysis since
research team
compression ratio
increase performance
many nodes
2002 );
baseline performance
rm task
temporal modifier
constructed according
., 2004b
tile translation
case relation
possible category
involves determining
appropriate verb
use tree
function words
following theorem
x )))
negation operator
extract knowledge
pilot study
make fine
would work
line ).
data used
learning strategy
column 3
fed back
template fills
also seen
intermediate representations
lavie et
given p
time course
performance reported
surface string
utterance within
sense distribution
two arbitrary
4 per
automatic pos
whose role
may reveal
one non
left unchanged
using cross
cannot recognize
simple frequency
users actually
section contains
word problem
/( n
collocation information
null string
simpler problem
many large
likely one
current segment
create additional
language model
grammar ),
first words
higher similarity
generation ),
another application
linguistic elements
architecture provides
representing knowledge
full evaluation
b cannot
misspelled words
hand side
extra cost
methods perform
towards one
language form
event structure
dictionary size
newswire text
speech may
one last
order logical
zero value
far back
computational aspects
comparison ).
structure representing
different level
temporal adverb
strong relationship
similar sentence
l (;
length one
relations using
low levels
set increases
sequences may
6 %)
null 3
sql query
provide assistance
simple task
multilingual information
also labeled
anywhere else
lexical expression
sentence without
states ).
discussed two
sum total
remaining candidate
quot ;);
line 1
\[ ic
1 presents
precision metric
almost always
may combine
size 5
structures according
different trees
overall strategy
rather complex
algorithms proposed
compound terms
linguistic object
arrows represent
possible paraphrases
probabilistic generative
formal characteristics
wrong word
phrases attached
occur much
minimal recursion
existentially quantified
easily obtainable
de base
language analyzer
n u
use much
english would
data stored
two concepts
category 2
candidate according
pointwise mutual
5000 words
similarity may
positively correlated
human judgement
understand whether
3 given
features improve
new predicates
question posed
derived compositionally
often expressed
natural way
role within
pos sequence
previous papers
segmentation procedure
attributes used
concise way
r 1992
generate additional
remain open
contain various
e )).
many learning
multiple language
traditional sense
index term
operational environment
changes occur
np nodes
feature set
parser may
text units
structural elements
resulting decision
adding syntactic
score ).
human cognition
network may
differs depending
complete representation
based probability
discuss possible
views expressed
level nodes
), shown
current information
specific context
wordnet word
two procedures
3 provides
written text
using wordnet
language problems
application development
might also
g contains
might occur
user starts
large part
probabilities given
examine three
development since
various linguistic
identify information
deeper semantic
domain texts
initially set
\] r
thus one
following characteristics
corresponding input
time axis
correct ).
structure described
highly complex
reasonable level
). furthermore
provide tools
recognition rules
sophisticated version
word triphones
terms extracted
evaluate three
working hypothesis
relation could
92 \],
defining new
normal parsing
one ),
). experiments
made publicly
search query
example presented
occurring less
w '.
automated procedure
approach adopted
training algorithms
features ).
complete analysis
c given
h l
patterns based
multimodal system
second half
anaphoric np
). though
information necessary
three steps
evaluation shows
formedness conditions
idea underlying
useful insights
processing begins
de ned
syntactic word
corresponding document
words surrounding
avoid redundancy
others ),
june 2005
would indicate
widely adopted
adjacent pairs
handle certain
native speaker
large negative
simple discourse
easily interpretable
different sense
new rule
appropriate representation
multimedia information
must belong
gale program
label l
embedded sentences
last item
noise introduced
every nonterminal
particular constraints
thorough discussion
surface features
noun like
\[ bateman
chinese dictionary
speech categories
tags within
activation values
linguistic notions
mental representation
since neither
semantic material
moved towards
manual checking
second stage
discourse entity
interpreted relative
introduce another
including additional
information gained
new possibilities
words extracted
functional relations
correct parses
-- v
words given
combined using
million chinese
rule names
particular system
leads us
2 ).
john went
noun meaning
contains multiple
better ways
w --
rules contain
thereby avoiding
many states
appropriate type
used mutual
degrades performance
different characters
4 displays
kyoto university
david yarowsky
local model
recall tradeoff
appropriate discourse
various models
iff p
rst analysis
relevant literature
template fill
appropriate analysis
present evidence
figures 2
infinitive clause
muc evaluation
methods also
leftmost derivation
order n
previous tags
less complete
incorporate linguistic
corpus must
word tagged
5 ).
(~ x
still better
increasing size
primary source
many groups
rst step
alignment system
document indexing
obtain different
model like
3 b
5 \]
several layers
table 12
significant percentage
based interface
three readings
built within
following grants
one letter
text plans
represents information
proper noun
using fewer
2 v
user cannot
current token
second rule
atomic feature
structure used
usually required
model enables
lexicon contains
accuracy score
declarative semantics
obtained better
cohn et
becomes necessary
second group
method makes
empirically based
one grammatical
case complexity
oriented systems
isolated words
\] k
user intends
critical importance
whose representation
features derived
given user
general strategies
2 1
ith word
rhetorical predicates
first point
depends directly
gradient descent
commonly occurring
correct class
discourse planning
1993 )),
another phrase
context consists
segment ).
une fois
thus ensuring
seems quite
may change
adequate model
useful tool
last set
system utterances
related elements
actual data
allow users
dependency grammars
one deals
well worth
lets us
major disadvantage
one elementary
larger chunks
+- 1
paths leading
build one
two paths
english corpus
different strings
null first
coarse level
5 1
discriminative power
preliminary analysis
using translation
may decide
since x
another program
discourse processing
match either
human summaries
two kinds
phase ).
remaining elements
first form
recall ).
first built
relevant parts
measures may
whose type
optimization methods
2001 ):
field ),
component within
coming year
slightly larger
algorithm since
c b
automatic thesaurus
examples presented
also evaluate
motivations behind
preliminary step
texts used
example like
specific annotation
word used
defined analogously
must find
complete problem
n 4
annotation task
open test
1984 \].
also large
source ).
discourse contexts
genera \]
2nd person
aquaint corpus
clustered together
random field
r ))
average ),
finite automata
simple example
linguistic techniques
good features
sentence corresponds
weighted using
probabilistic parsers
also assigned
various purposes
knowledge structures
probably also
realization process
recall error
various entities
position index
automatic labeling
quot ;%
one second
translation correspondences
r ('
verb relations
qui n
interesting challenges
development work
structural disambiguation
graph g
language within
reference sentences
speech phenomena
concluding remarks
shows similar
defined recursively
basic lexical
general topics
additional categories
given directly
illustrative examples
significant problems
new segment
strict subset
hmm parameters
muc evaluations
major importance
recursive transition
describe two
agent case
linguistic knowledge
np 2
potential referents
h (',
treebank sentences
thus need
data format
described herein
le plus
role ).
verb subcategorization
phase 1
time allowed
following tasks
original feature
parsing becomes
different experiments
resulting structures
accuracy may
coverage grammar
preceding utterance
completely correct
german newspaper
--* b
given lexical
leaves us
performance increase
cognitive psychology
rules without
spanish word
). later
different tests
equal numbers
problem consists
tagging result
individual probabilities
person ),
error analysis
consider another
translating english
resolve conflicts
fixed size
many human
line shows
ditransitive verbs
answering track
also working
). case
different forms
morphological patterns
grammatical context
finite clause
matrix containing
fifth column
called upon
higher correlation
theoretically possible
similar improvements
level tasks
73 %)
scientific research
statistically independent
complex lexical
recommendations expressed
~( r
better choice
system description
general way
computationally intractable
answering system
detailed error
2001 ),
grammar makes
varying amounts
stochastic part
information shown
). additionally
independent modules
separate task
physical sciences
tag assigned
start node
marked according
source word
;), etc
2 percent
similar values
manual classification
good first
assumption holds
grammatical roles
3 f
linguistically relevant
lexical approach
expected counts
low accuracy
richardson et
many analyses
based decoder
similar senses
basic rule
performing well
ary predicate
specific modules
following utterances
classification procedure
several variations
important factors
argument label
1 %.
tree rooted
european commission
ordered set
unsupervised methods
possible meaning
given sentences
composition operations
rule takes
important topics
structural knowledge
introduces another
currently available
also takes
4 \]),
kept track
5 percent
present stage
used due
quot ;<
two outputs
subjects ).
partially automate
sophisticated features
iterative process
every tree
tags ).
better translation
reason may
proper use
selection scheme
manually produced
research efforts
directly compare
important effect
account also
000 nodes
method could
fairly consistent
dialog systems
one item
every utterance
programming language
one english
8 words
[?] 0
manually disambiguated
also determines
pattern ).
including ones
discourse state
add another
cunningham et
examples like
training runs
cannot rely
major research
technical text
internal data
greatly simplifies
p e
answer types
set described
des relations
still requires
quite impressive
significantly larger
pl ).
v l
army research
automatic acquisition
complex rule
structure theory
formal description
speech interfaces
consonant clusters
sentence templates
two versions
c f
viterbi alignment
systems cannot
study provides
case study
qualia structure
text sources
verb morphology
two significant
important way
maximum margin
terms associated
right ordering
costs associated
using chinese
gone wrong
q ,,
successfully exploited
short summaries
issues discussed
c r
different character
annotated sentences
\] presents
tree kernels
new speakers
rule states
contiguous words
first column
template consists
phrases within
additional mechanisms
mixed initiative
search results
small hand
standard ones
mere fact
approximately 75
use precision
large feature
3rd person
highest likelihood
database structure
minimum score
technique allows
another state
three relations
reasons discussed
stemmed using
good results
baseline ),
incremental system
sentence generator
syntactic constructions
revolve around
speech sequence
html documents
shown earlier
work within
relative frequency
frequency terms
subtrees rooted
99 ).
first occurrence
evaluated according
output ).
given configuration
present day
ldc ).
entities found
new items
28 aout
various problems
data structures
lowest cost
moved around
new edges
significant factor
temporal intervals
define several
strictly limited
effect may
pages 1
basic aspects
several times
tile data
provide direct
apply two
several candidates
two sample
search strategies
possibly complex
incorrect information
parameter c
atis corpus
following description
many works
classes defined
null value
prototype version
key concept
complete dialogue
discussed briefly
set threshold
4 evaluation
woods et
still provide
., 2000a
electronic dictionaries
give similar
new entries
local context
planning operators
crucially important
spoken sentence
writing style
one set
new rules
structure defined
english lexical
subsequent sentences
using kernel
relations cannot
correlates well
given test
possible ways
use part
always choosing
gram features
th (;
conditional likelihood
cannot give
semantic data
rows correspond
robust way
related techniques
relational information
originally conceived
unique label
model predicts
\[ tl
bracketed corpus
level description
input format
techniques based
verbal complement
containing multiple
). firstly
6 senses
2 shows
class 2
new edge
condition ).
desirable feature
cannot resolve
following issues
times news
set used
de ces
data collected
generation process
would generate
particularly useful
especially true
structures would
text compression
represent linguistic
requires little
succeeding words
system allows
distributed evenly
would become
segment boundary
regular rules
extracted automatically
bn ),
existing large
talking microphone
new situations
contexts may
almost never
5 shows
assigned weights
structural ambiguity
extraction technique
trees generated
mostly used
another speaker
newspaper corpus
automatically acquires
version ).
iterative method
become clear
). whether
parsing mechanisms
naive users
rhetorical information
explicit description
key words
representing syntactic
come close
approaches described
framenet frames
based processing
may follow
journal text
since language
r g
scheme based
output word
agent may
uses data
common prefixes
another alternative
form dictionary
approach first
final question
several parts
four categories
would receive
left edge
2003a ),
recognition software
also measure
method gives
selected features
may undergo
discuss future
emnlp ),
built manually
test result
vocal tract
zero frequency
standard annotations
entries containing
distinct features
barn fell
th (,
models developed
certain action
sun sparcstation
simple majority
single input
process begins
conditional probability
essential aspects
word subset
whose name
concept type
stage involves
4 one
condition 1
must work
set u
results without
conditional log
module provides
null ity
lee et
task oriented
algorithm performance
values given
may achieve
based concept
partial path
five times
l ))
language 1
robust statistical
derivation relation
potential antecedents
two views
three possibilities
distributional similarity
provide little
fifteen years
algorithms could
sometimes called
shed light
term project
moore et
analysis proceeds
tables ).
particular case
best hypothesis
higher f
smoothed probability
l ('
concatenated together
new category
truth values
generation systems
sister nodes
grammar systems
people would
long sentence
boundary marker
parameters used
whole discourse
<) f
european communities
support vectors
quite similar
based linguistic
certain degree
computer speech
pronoun ),
basic information
generic concept
large textual
), respectively
new symbol
provide useful
may need
4 describes
may fail
certain verb
four rows
last two
1 introduction
21st international
clearly demonstrates
value given
related events
sense ),
b )).
several different
entity data
automatically recognized
end times
capture syntactic
first slot
disambiguation algorithms
shows us
systems deal
allows many
different interpretations
syntax analysis
phrase p
rank 2
), provided
following algorithm
determining semantic
nearest neighbors
negative values
data size
r ',
narrative text
one process
shows average
using probabilistic
structural similarity
j .,
whose semantics
), position
similar types
dialogue systems
behave similarly
e must
human listeners
following lemma
model assigns
process generates
phrase table
language parser
estimation technique
approaches used
empty word
similar result
topic identification
compare several
many thousands
un autre
broadly classified
shows better
resolution component
tasks including
31 ).
correct tagging
correct antecedent
summaries generated
system yields
complex information
frequently omitted
specific source
two grammar
mt ),
11 (;
error rates
individual classifier
selected 10
cky algorithm
eight times
underlying technology
input structures
computational modeling
substantially less
database retrieval
number n00014
possible choice
natural one
initial tree
several dimensions
generation method
recent years
local maxima
frequent terms
first review
automatically converted
generates exactly
figure ).
source document
entry point
making reference
query terms
global properties
present information
paper ).
statistical evidence
problem lies
similarity metrics
initial utterance
take account
assuming independence
carefully selected
usually corresponds
used along
document containing
generally contain
first syllable
labels like
second level
based models
user types
parsed sentences
). none
l '&
features selected
unary predicate
existing computational
avec des
stock market
); also
new variable
two configurations
sense distributions
pp complement
default interpretation
tree information
relatively close
correct attachment
adverbs ),
work ).
initial estimates
two bilingual
pronouns ),
essential information
elliptical constructions
3 presents
structured objects
pronominal references
formalism \[
global variables
graph represents
adverb ).
target term
remaining ambiguity
total size
time rather
much stronger
centering algorithm
enough training
knowledge might
find useful
could perform
assumption may
paper introduces
c /),
mechanism could
des deux
basic rules
possible translation
stage uses
various roles
pairs generated
vilain et
\[ g
confidence threshold
two baselines
character recognition
12 different
top ranking
potentially large
research \[
sophisticated way
successive words
set theory
action ),
web data
also contained
shall discuss
3 proc
automatically identifies
new scheme
() ill
figure 1
performance level
english subject
one location
score across
estimated based
distortion model
function ).
manually defined
integrated system
reflect different
theory would
chart parsers
english reference
b contains
vital information
parsing approaches
implicit assumption
analysis proposed
information structure
describe briefly
models significantly
anaphoric reference
thus every
using data
would wish
algorithm needs
enormous amount
50 documents
semantics framework
strategies may
recognition procedure
item ).
first four
previous computational
book ),
various authors
best parameter
3000 words
evaluation using
phrase --
estimated directly
extraction model
processing mechanism
resulting alignment
transition matrix
corpus 2
key point
first ).
similar language
frame would
complex rules
., 0
., non
one order
monolingual corpora
quot ;?
computing resources
information services
features across
per feature
pp ),
grained analysis
alignment score
small group
sentence error
state changes
overall result
segmentation methods
individual clauses
one method
human readers
scores given
specific issues
f ,~
lexical transducer
distinct components
dependent lexicon
complex multi
promising result
human factors
precision obtained
real problem
hmm ),
technique using
highly skilled
latter problem
incomplete sentences
category type
). also
specified number
less attention
verb complement
syntactic constraints
consistent semantic
frequent tag
requires different
substantial amount
document space
full range
occurrence relation
item 1
dialogue control
two training
common core
user language
cue phrase
1986 \],
documents related
simple decision
parse using
eight different
becomes one
1 v
inherent difficulty
certain discourse
leftmost column
strategy requires
taking care
inheritance network
two graphs
first analyzed
assumptions underlying
especially well
resources needed
using good
like case
derived tree
\[ mary
japanese parser
analysis reveals
evaluating systems
describe natural
dictionaries often
1 1
great majority
definitions given
input size
could derive
n n
posterior probabilities
several paragraphs
implemented system
features provide
language expression
finite string
extra time
information gathered
spatial relation
copyright notice
multiple feature
latter strategy
phonetic information
basic level
organized hierarchically
remain ambiguous
length principle
20 %.
sentence word
parsing program
h ).
parser without
89 \].
greatest lower
quite challenging
regression model
years old
whic h
machine code
true distribution
h \]
also several
languages l
10 iterations
detailed enough
). many
perform tasks
variable c
general evaluation
text automatically
grosz 1977
canadian parliamentary
), among
among linguists
label ).
carnegie mellon
little time
88 \],
interesting consequences
japanese nouns
1991 \],
central component
clearly separated
value depends
given level
cannot ignore
easy modification
document pair
almost 10
handle non
small test
one looks
incrementally adding
quite general
large impact
additional attributes
get information
,+ 1
grammatical utterances
illustrative purposes
problems associated
low tone
number ),
62 %,
many students
less informative
practical system
following tables
supervised learning
incremental parsing
verb ),
widely known
also highlighted
could also
predicates ).
american chapter
judges whether
could occur
concept name
third message
algorithm proceeds
description language
first class
two members
formally described
information provided
robust parsers
step 2
argument pairs
phonological constraints
detailed investigation
values associated
natural manner
null scribed
individual users
also encode
important open
mean either
h 1
elementary structures
existing tools
fundamental problems
resources available
fold crossvalidation
often give
interface design
generic model
~-& quot
segmentation errors
6 points
recursive structures
phrase based
n ,~
every part
expert would
might mean
three entries
van eijck
temporal relation
x occurs
summarization methods
also carries
would capture
third question
robust natural
negative result
search space
fixed grammar
major areas
2 yields
returns 1
content may
independent approach
already noted
inconsistent information
modifiers may
sentence adjuncts
collocational patterns
). la
identi es
interesting facts
data extraction
application specific
production may
web searches
ne recognition
two features
international chinese
used elsewhere
qualitative evaluation
par l
combining operation
variable whose
classify words
cell contains
pragmatic function
translation equivalence
distance relations
one effect
however one
efficient implementation
lexical node
approach exploits
1967 ).
systems make
object may
l ',
lowest possible
richard sproat
verb lexicon
could obtain
general one
numerical methods
relative strengths
improve results
l '(
already found
quite feasible
partially specified
'. however
terminology ).
interesting result
detection method
appropriate term
meaning structure
automated scoring
three situations
g [?]
whole dialogue
disambiguation based
), repeated
estimation problems
place holder
1 percent
plan may
syntactic disambiguation
best known
constituent phrases
evidence shows
method consists
single system
mother tongue
simplified view
general structure
acquisition system
tile time
order relation
tagged sentences
particular categories
verb appears
one concerning
program could
mean value
general cases
many times
trained linguists
negative answer
final corpus
example dependency
given sense
terminological data
6 hours
many candidates
page ).
active sentences
would otherwise
new definition
grammar automatically
node becomes
first paragraph
string position
target noun
\[ church
different contextual
user access
since multiple
-- although
one per
lexical similarity
similar representations
large database
common parts
new meaning
sparck jones
objects mentioned
attribute may
., grammatical
system along
different branches
another table
parentheses indicate
top element
real dialogue
also significantly
null ter
cannot say
electronic systems
even sentences
describe one
section 2
spanish system
language modelling
2002 evaluation
initial motivation
specific conditions
texts often
japanese expression
successfully used
translation could
advanced techniques
figure shows
level representation
two first
explicitly indicate
examples also
made significant
average recall
section 23
lower results
linguistic examples
much weight
every single
parsing efficiency
several stages
second goal
equal weights
con dence
entities described
real life
important one
words form
0 ).
corpus contained
unlabeled training
maximal projections
entropy markov
language environment
john wanted
would hold
relevant facts
fragments may
alignment probability
3 %.
top 10
statistical test
property values
equals 1
0 \]
algorithm proposed
rules ).
data sample
simple structures
''& quot
6 contains
key factor
several improvements
current translation
contain information
work shows
baseline systems
also effective
independence assumption
verb second
first applying
leibler divergence
provide support
total effort
high efficiency
bilingual word
unbounded dependencies
simple constraint
much lower
tactical component
different mechanisms
01 iv
syntactic knowledge
comprehensive list
;. note
oov rate
us give
discussed extensively
level analysis
morphological rule
translation applications
algorithm ),
adversely affected
several values
joint likelihood
e [?]
trivial case
de r
filtering methods
isolated word
always positive
word unit
several senses
resolution strategies
subcat list
resulting grammar
ainsi que
without significant
accuracy without
experience gained
classification methods
identify relevant
known names
preprocessing steps
context would
compares well
machine translations
2000 ))
default feature
including non
processes described
operation used
could assign
classification approaches
several classes
extremely high
previous study
grammatically well
significant source
0 1
word ei
three terms
several specific
corpus consisting
october 2005
written instructions
general mechanism
making sense
encode semantic
grammar consists
rules 1
powerful formalism
scores associated
orthographic words
different dependency
n )&
variable p
form shown
use manually
document structure
surface structure
research question
word n
arabic text
taken directly
semantic checking
first person
two results
reference translations
g 2
helpful information
major aspects
another --
category name
answer must
intelligent text
threshold 0
analysis makes
triphone models
different application
respective languages
question could
gain insights
recent development
particular relevance
relevant answer
1 shows
\[ pred
annotation format
new ways
would call
valued functions
optimal alignment
baseline method
probability theory
anaphoric pronouns
dependent training
must become
retrieval method
information among
framework within
spoken utterance
requires making
6 n
help make
crucially depends
higher precision
sentences selected
column indicates
two subtasks
new procedure
structure might
may imply
one conceptual
format suitable
natural hmguage
-- see
), since
representing linguistic
correctly analyzed
bare noun
two perspectives
currently implemented
conversational english
next year
would facilitate
random variable
wordnet hierarchy
1998 )),
2 using
rules derived
several drawbacks
company ).
v \]
information produced
made public
therefore necessary
two strategies
four words
extract words
subordinate clause
higher percentage
word bigram
module identifies
right information
following parts
~: l
produces three
topic marker
ever increasing
language allows
common root
algorithm appears
also quite
examples illustrating
unsupervised algorithms
al 2000
measure using
model showed
could interpret
percentage point
query could
would describe
discourse related
irrelevant words
providing access
learn word
8 ).
knowledge needed
\[ br
5 using
way one
prompted us
set size
xue et
produced using
first major
distinguishes three
ir community
critical information
8 \]
much improved
absolute position
processing steps
position may
first document
meeting corpus
generalization capability
mapping problem
also employs
null 6
achieved high
., english
basic goal
complete noun
1994b ).
apply one
tool based
attention paid
top 2
abstracted away
basic framework
b must
resulting string
four models
traditional methods
head child
initial part
compare different
automatically constructed
~- r
abstract syntactic
2005b ),
rules produce
objects --
traditional approaches
allow us
x 5
-- thus
many challenges
clause grammar
rules employed
method \[
dictionary construction
word alignment
adequate description
germann et
descriptive power
another related
consists entirely
might imagine
increase recall
2003 ].
given goal
classification errors
would tend
one consists
algorithm builds
grant n0014
50 ),
4 reports
another similar
cause errors
certain property
\] v
phrase constructions
two topics
less similar
general patterns
capture linguistic
name followed
entry ).
word extraction
sparse data
syntactic ambiguity
ov coling
e 2
must consider
significantly increase
monetary amounts
logical form
could extract
parsing text
verbal complements
appropriate points
constraints cannot
cross word
well represented
intensive process
node represents
considerably easier
pragmatic context
\] 1
put another
text resources
desired properties
considered ).
precise interpretation
automatic system
spectral features
characters ),
entire sentence
threshold ),
previous decisions
~, h
annual meeting
constituent c
results improve
error types
introduce two
richer information
also removes
early experiments
expanded set
currently open
itr grant
nodes labelled
simple present
final stage
called recursively
two days
knowledge available
using larger
dependent features
detailed discussion
connectionist models
long list
syntactic parsing
different methods
art methods
terms within
). third
could assume
much use
mt systems
different formats
second section
different approaches
determined whether
present framework
distributionally similar
expert user
generally higher
30 times
surface position
meanings ).
approaches require
() lie
), show
subsequent experiments
prolog interpreter
contains several
names ),
two consecutive
given character
remaining examples
value l
whose weight
sentences provide
development set
traditional phrase
date information
inference mechanism
extracted patterns
empty elements
syntactic restrictions
particular sequence
later processing
lexicon construction
1989 )).
analysis could
exist within
parser fails
processing since
theoretic measures
simple past
tree like
1999 ).
noun pairs
\[ 1988
words immediately
sentences .)
entity whose
computational cost
equally useful
data sizes
higher weights
also confirmed
also demonstrated
essentially based
). naturally
results ).
simply takes
might improve
logic programming
first kind
main categories
final rule
words within
module called
patterns obtained
french translation
initial weights
partial analyses
surprising result
broader context
successful systems
template matching
human speech
actual user
semantic classification
\[ 1975
effective technique
know whether
). recently
sequence containing
(( b
terminal string
finite amount
last one
1996 ))
female speakers
constructed rules
extrinsic evaluation
arises whether
), functional
together constitute
following sort
intuitively clear
whose size
handcrafted rules
dry run
unit ),
perfectly well
partial description
documents selected
noun phrases
natural communication
following sentence
)). although
knowledge sources
detailed knowledge
standard notion
kappa value
human resources
trees ),
ad hoc
formally evaluated
different phrase
new translation
system cannot
unification procedure
research would
well aware
using c
text one
clause 3
spread across
discusses future
recent literature
determine word
two common
generating discourse
atomic type
4 )-(
include features
put aside
l '~
adequately represented
several types
university ),
claims made
orthographic form
little evidence
query translation
less costly
data needed
prepositional phrase
akiba et
ultimate goal
main directions
given later
conceptual primitives
three objects
9 %),
simply states
set n
human evaluation
w ,,
second choice
operation defined
two orders
tags using
syntactically marked
extremely fast
rule 5
based annotation
involves using
possible sub
general agreement
syntactic markers
taken place
previous event
much room
particular entity
perform significantly
subject case
seven categories
document summary
well adapted
forms may
different errors
various issues
right level
generate summaries
30 %)
semantic readings
n 3
based clustering
user must
electronic texts
previously reported
top row
another experiment
algorithms \[
best current
quite clear
also different
single case
one natural
dependencies within
novel aspects
sentence requires
one change
combination results
clear advantages
multiple analyses
broadcast news
set inclusion
consider two
whether using
actual lexical
computer communication
associated values
first approach
ever used
sample text
pp complements
based ).
000 times
generate english
class problem
algorithm makes
current application
johnston et
network formalism
much closer
morphological rules
x ?&
original representation
depends upon
extracting relations
constituent ordering
statistically trained
partially determined
head h
simple clause
memory ).
grained senses
81 ).
development process
built using
inferred using
morphologically ambiguous
computationally intensive
r j
81 \]
inferential processes
dictionary containing
sharp contrast
phrase alignments
semantic field
would fill
geographical names
input grammar
around 70
~' c
category symbols
entropy measure
nearest neighbor
e \].
systematically related
20 percent
combination strategies
), thereby
interpreted within
quite low
modified algorithm
continuing work
level semantics
derivation structure
data available
test collections
although still
first finding
child ).
common subsequence
parse forests
often occurs
one speech
simple bigram
16 words
lexical mapping
realistic application
already said
system actually
conversational systems
connectionist model
errors ),
word tags
also including
subsumption hierarchy
considerably reduced
analysis gives
create training
original analysis
share information
400 words
one semantic
words contribute
x )/
particular instantiation
electronic mail
introduces two
others \[
varying complexity
final segment
recognition methods
another assumption
actual sentences
included three
description must
syntactically ill
uses hand
2000b ),
development team
word gets
existing discourse
patterns may
bad ones
strong independence
sentence elements
nodes n
several language
maximum size
event ),
empirical study
probabilities ).
7 %).
features include
.) however
). researchers
via telephone
decision support
processing work
first provide
holding among
present section
less general
good clue
either left
help explain
slot values
add two
appropriate response
new material
particular contexts
sentences differ
nagao et
... tn
~. r
(( r
including english
precise definitions
case theory
constituent labels
possible pairs
sense per
right sense
4 minutes
language using
specific situation
70 %)
improve recognition
enhance performance
whose use
natural number
minus one
last position
presented two
particular concept
good summary
1986 ):
morphological disambiguator
relations expressed
simple extraction
per collocation
specific languages
length ),
end result
use automatic
briscoe et
), even
must capture
grammatical constituents
since large
problematic cases
x )\]
verb complements
alternative would
corpus analysis
n features
word grammar
already described
sequences ).
\] shows
morphological category
lexicon containing
fixed value
mechanism \[
nlp tasks
simpler version
methodological point
next best
perhaps also
\] give
qa task
specific node
parsing algorithms
first object
phrase chunks
generate training
putting together
perform quite
frame information
fairly robust
mathematical models
algorithm attempts
base language
previous topic
similar feature
1 yields
standard english
structure resulting
previous discourse
extremely well
primary reason
remaining arguments
revised submission
possible phrases
significant contribution
turn leads
1we use
)). thus
larger phrase
elements x
suggests two
three sections
important thing
psycholinguistic evidence
even humans
several hours
sister constituents
engineering effort
almost perfect
input string
current data
previous three
eight types
three ways
systems performing
particular domain
current methods
task involving
contained information
tile user
returns true
individual word
another place
base de
given frame
notoriously difficult
listed first
shared task
section reviews
occurrence frequency
unique referent
input query
methods based
1991 ):
makes us
automatically evaluate
). next
four sets
case 4
parser ).
answer questions
abney et
particular words
network ).
giza ++
testing set
grant iis
translated words
languages ).
include two
formed structures
ce qui
class name
clauses introduced
automatically assigned
probabilities derived
latter result
semantically relevant
mutually believed
lie within
ranked parse
many senses
randomly distributed
seem likely
three following
text retrieval
research reported
recognition errors
feature agreement
task specifications
potentially useful
sample output
also changed
\[ j
quality results
repeated words
rome air
usual interpretation
target grammar
1986 ),
includes information
standard drt
words 1
module must
specific sentence
based filtering
representative examples
systems must
). ii
task difficulty
later point
one thousand
may lead
linguistic purposes
cases arise
look like
selected articles
b [?]
storage space
linguistic entities
), another
structured data
old one
containing several
technology research
portability across
third stage
example text
various conditions
also straightforward
three applications
text spans
available yet
possessive forms
achieved using
human intuition
individual verbs
ittycheriah et
standard set
already used
computational methods
end points
parser parses
larger contexts
following stages
manually determined
bigram frequencies
general ones
word string
subject may
written forms
must hold
disambiguation system
3 percent
short one
raw text
syntactic categories
use several
improve readability
encouraging results
frequency data
joshi et
related concept
overall view
figure 16
efficient computation
would reveal
using svm
tested three
300 sentences
alignment models
random walk
classifying verbs
rich enough
tile lexical
(~ 2
discourse properties
problem posed
string matches
purely syntactic
including punctuation
highly redundant
score given
natural interaction
knowledge concerning
one distinct
content words
relations holding
best represent
1991 ),
hold true
knowledge necessary
preceding utterances
categorial grammar
phrases using
much difference
likely tag
10 points
may determine
entropy reduction
word 3
nominal groups
greater length
base contains
planning process
text editor
usage frequency
previous example
lines ).
following values
bias towards
without first
single target
noisy environment
;. one
results compared
broad variety
john knows
lin et
semantic predicates
relations must
third rule
semantic lexical
alternative readings
traditional text
different surface
without involving
two tokens
determining lexical
work using
sentence plan
accurate results
third group
express semantic
established using
language family
work well
features extracted
following classes
quite restricted
knowledge gained
features given
also employ
would combine
k nearest
main purpose
speaker intentions
). 12
already stored
global syntactic
write rules
explicitly addressed
textual representations
cheng et
tile sentences
best example
us turn
time necessary
backward search
test questions
input features
1990 \],
parsing --
closely resembling
must process
also helps
linguistic model
position k
million sentences
identify possible
important since
easily determine
corresponding derivation
target string
edit distances
testing process
function using
important ones
shieber 1985
unified representation
shallow parsing
learned classifier
time per
good thing
computer interaction
precision without
values within
certain structures
uses lexical
texts available
mathematical model
segmented discourse
strings may
la phrase
null course
ts ),
extremely important
model instead
tight integration
decision theory
1974 ).
also involve
without difficulty
query consists
whose initial
algorithms need
conceptual structures
spanning tree
way classification
additional word
constraint solver
following reason
structure features
different text
l j
manual selection
planning systems
model whose
lisp functions
eight hours
lewis et
polynomial kernels
random choice
nouns like
therefore one
automatic production
worked well
spontaneous speech
often different
corresponding original
also started
linguistics research
algorithm gives
set difference
semantic rule
grammar developer
parsing involves
1 using
following considerations
also identified
r c
human brain
articles collected
many complex
secondary stress
may ask
corpus including
made two
must describe
whose internal
never occur
~, l
different frames
statistical framework
incremental processing
fertility model
general vocabulary
frame level
2 provides
upper half
b 2
general principle
research community
cahill et
particular predicate
language independence
many irrelevant
also come
parse tree
abstract concept
provides evidence
link grammar
texts written
iii ).
models discussed
quot ;@
cognitive models
f denotes
second application
section three
research areas
x x
case structures
33 ),
without specifying
presents experimental
existing web
two binary
theoretical foundation
information even
links represent
different scales
one need
program \[
possible explanation
processing needs
discusses related
approach described
5 provides
functional generative
first proposal
lingual text
correctly annotated
low probabilities
annotation effort
abstract linguistic
word features
particular point
cannot deal
model would
questions ),
tschira foundation
model theoretic
common types
vax 11
main processing
linguistic problems
accuracy ).
select relevant
high error
could instead
even though
work along
negative feature
extraction tool
different pos
16 %.
word dictionary
create two
\] allows
simple version
modern standard
professional translators
vp nodes
experiments ).
respectively ),
objects involved
simple grammar
), adjectives
training may
straightforward way
french data
usually requires
., brown
various proposals
), rather
equation 3
communicative actions
), 0
3 years
), non
may generate
particular form
vector contains
word hypothesis
two partial
also influenced
major goal
translation \[
extra information
fundamental issues
stored information
absolute number
algorithm looks
-- b
specific instances
without either
highly polysemous
using user
adding extra
well understood
labeling scheme
grammar checking
solely based
discourse elements
antecedent noun
general relations
interesting feature
fully developed
2004 ):
ungrammatical utterances
reasonable time
main concept
generation architecture
set sizes
since human
two potential
alignment model
context information
key properties
phrase translations
new data
like l
provide improved
user questions
system implementation
related entities
input specifications
expressed directly
one chunk
requires access
bleu score
grammatical sentence
partially parsed
fewer rules
corresponding concept
semantic account
relatively narrow
appropriate grammar
given label
may receive
semantic aspects
sentence length
figure 11
beeferman et
pipelined architecture
learn new
figure 13
structured according
special thanks
language comprehension
techniques may
p );
involving several
maximal number
parsing based
voting method
made concerning
classification algorithms
analysis consists
avec les
high word
german example
implementation based
partial understanding
research group
extension would
every entry
., 1998b
becomes even
corresponding linguistic
tile correct
bell laboratories
2 b
details concerning
character codes
temporally related
1987 )).
cluster words
detailed syntactic
system acquires
generate two
14 \]).
human agreement
whole clause
entire utterance
describe work
extraction algorithm
structures like
identify phrases
certain concepts
likelihood score
performance comparison
best described
lexicalized pcfg
infinite set
distinctions among
set si
82 %.
large window
potential advantages
also clearly
cannot modify
speech input
additional experiment
multiple news
relative clause
chinese corpora
names tend
extreme example
recent theoretical
principle behind
optimal results
information needs
main conclusion
hidden units
entire list
processing functions
actually relevant
structure building
agent slot
previous dialogue
rules already
form produced
development efforts
important work
structure imposed
john gave
already defined
sets may
direct translations
tree kernel
longer documents
model p
machine interfaces
widely used
ii ).
adjacency pairs
matched words
second iteration
practical solution
traced back
tile corresponding
ones described
better scores
., grosz
database ),
2 x
corresponding words
text classification
standard method
set 3
several papers
thus avoiding
article describes
important relations
topics related
g ~,
heavy burden
information specified
vector representations
arithmetic mean
sentences include
common language
unique lexical
using lexical
processing proceeds
\[ c
simple string
w h
different output
language queries
tile whole
2004 ),
pragmatic issues
leaves much
summary produced
algorithm terminates
give details
least half
4 0
different templates
5 x
reciprocal rank
following facts
another system
science foundation
larger text
two orthogonal
best illustrated
confidence intervals
relevant feature
node corresponds
hierarchical semantic
verbal expressions
theory presented
tree traversal
therefore propose
com ),
actually perform
top three
noun belongs
\[ q
\[ shieber
following types
ambiguities may
considered equivalent
results across
one side
gles de
orthographic word
., forthcoming
corresponding models
categories c
flexible word
introduce one
brackets per
corpora without
based tagging
understood correctly
alternative lexical
;. similarly
effect ).
two sorts
also appropriate
chinese segmentation
term frequency
recognition time
4 ):
reasonable coverage
appears frequently
contain fewer
sentence parsing
two differences
almost impossible
1988 ))
free phrase
shows examples
cognitive model
), consider
;. finally
e analysis
reasoning component
function w
likelihood estimator
involves choosing
multilingual text
first parsing
could get
coder agreement
main clauses
four modules
entire language
generally better
b \].
etc ...
feature principle
bilingual knowledge
using bleu
many points
improves parsing
symbols like
seem quite
relatively infrequent
better represented
parsing experiment
absolute performance
representation format
two articles
semantic information
simply say
following list
manual translation
estimation algorithm
elementary school
single solution
etc .);
.) c
information supplied
mettre en
translation pairs
value ).
new sub
r p
framework allows
answered correctly
-- r
often cannot
show experimental
numbers indicate
table 9
may express
second phrase
many noun
include references
positional information
deictic reference
plausible interpretation
less clear
grammatical gender
japanese sentence
word choices
type relations
parser performs
1 w
clause containing
typical example
small constant
ones used
results depend
results provide
formal language
). initially
definition ),
makes clear
1985 \])
graph based
\[ 1987
also carry
multiple times
somewhat related
linguistic context
sentences may
ne ...
multiple candidates
text generated
contexts ).
naturally occurring
key elements
graphs ).
6 \]).
argument place
additional input
contain relevant
action denoted
somewhat similar
modern approaches
possible referent
syntactic derivation
slot contains
related problems
particularly significant
adjectives ),
)). given
2 f
call p
also discussed
correct tree
always takes
worth noting
also interested
value v
major obstacle
used part
agent ).
different algorithms
approximately half
operations may
present empirical
et l
equivalence relations
lexical association
pairs could
data containing
type specification
logical expression
may wish
optimal way
based approach
current time
called word
remaining problems
word predictions
far higher
either atomic
great difference
value 1
general idea
1997 );
equation 10
4 ),
shallow syntactic
minimum error
several interesting
developed specifically
expression generation
shown ).
l c
right choice
detailed examples
category n
6 2
possible labels
basic methods
abstract model
retrieval mechanism
size 2
data shown
less expressive
matching process
might provide
also seems
classified using
method computes
1 sentence
new arc
quot ;).
r '.
iterative fashion
chunk types
often involve
practical purpose
makes explicit
15 words
bayes rule
information rather
simple features
ran experiments
x )).
single sequence
categories would
different media
e ~'
already exist
jose mercury
among documents
tile final
compared across
error measures
intended use
component provides
chinese documents
increased attention
generate texts
analysis \[
similar analyses
). figure
processing ),
would consist
see van
using test
semantic rules
module takes
interesting questions
graphic representation
assigned different
structural relationship
deterministic rules
contextual factors
example --
de faire
taxonomic knowledge
news articles
removing words
sense clusters
150 words
correct arguments
primary motivation
disambiguate among
base must
steps 2
null ical
extraction module
confidence measures
various languages
whose top
shieber et
domain terms
), assuming
describes two
45 ).
characters per
generator using
human communication
consider one
attachment errors
fairly detailed
common use
sentence score
english dependency
language provides
main point
final goal
phrases cannot
word together
use bayesian
actually occur
around 5
temporal adverbs
information status
module determines
discourse knowledge
rule types
space precludes
). similar
experimental system
next constituent
letter strings
rich knowledge
comparative study
system running
one np
composition operation
lexical lookup
syntactic processing
rule induction
r --
recall r
must cope
length greater
separate grammar
human generated
initial description
act types
also result
mechanism would
interpretation component
automatique des
preference mechanism
possibly containing
present methods
technique called
example involves
discuss later
utterance types
consider cases
one classification
later ).
overall complexity
np --~
syntactic contexts
syntactically tagged
sample sentences
last argument
system treats
object relationships
transcription system
). word
), leaving
); others
minimal amount
algorithm consists
currently developing
main function
tree data
without introducing
goal may
new phrases
j )).
discriminatory power
\[ p
expressions ),
also make
supervised wsd
learning tools
per minute
utterances containing
possible permutations
segmentation accuracy
correct links
decade ago
accuracy compared
constructions involving
single derivation
current coverage
gram lm
use different
different prepositions
single entity
conversational speech
sentence 9
additional properties
grammars ),
given phrase
combining different
cases described
individual elements
words approach
particular slot
standard statistical
dialogue corpus
channel model
time expressions
sections 3
events within
tree classifiers
models use
two modifiers
semantic compositionality
yet available
variant forms
test split
filtering based
2a ),
many reasons
la traduction
separate component
pertinent information
little sense
still maintain
among concepts
de 1
go hand
likely translation
add one
quot ;.
recent paper
several measures
decision procedures
large improvements
various sentence
classification based
precise description
data revealed
main aim
linguists working
16 ).
information ),
starting position
syntactic domain
2005 )),
different search
understanding tasks
word reordering
previously considered
model directly
grammar augmented
partly due
well documented
need information
process needs
generalization ability
existing corpora
technology group
system processes
word orders
., speech
set results
yield better
). verbs
model uses
scores 1
system called
r ,'
matching method
also decided
verb semantic
relatively fixed
patterns match
answer key
), english
language b
prosodic information
tagging task
determined experimentally
correctly predicted
accurate recognition
interactive environment
., semantic
level contains
state information
using information
patterns without
linguistically correct
purely linguistic
several feature
prime minister
paper includes
)= p
linguistic viewpoint
labeling process
software systems
loosely coupled
al 1986
objective evaluation
clauses containing
would look
modifier relations
highest average
unified treatment
japanese mt
formal representations
individual system
quot ;=
de ce
rule language
null another
representation formalisms
linguistics community
also points
heavily dependent
known example
may select
communication among
example based
simple declarative
substantial number
association strength
word accuracies
one generated
visual display
phrase may
one field
also brings
work includes
japanese spoken
would need
would fail
done either
funded project
simplified example
known problem
reliable estimate
right siblings
provides information
three data
de fa
thus gives
legal domain
dominance relation
user study
arcs connecting
link two
whole grammar
syntactic models
solving problems
tags associated
performance increased
knowledge structure
chart items
lies somewhere
novel ways
implemented without
=& quot
notable exceptions
experiments showing
quite efficient
decisions must
must combine
lexlcal items
better accuracy
boundary symbols
table h
could directly
presents several
speech system
substantial work
might differ
object slot
string ),
nlp literature
example uses
whose first
potential sources
order used
columbia university
hierarchical manner
issues arise
1985 )),
relation information
calculate p
use sentence
possible user
work provides
difficult task
chart entries
top 30
cannot help
less restricted
would happen
information appears
intuitively speaking
processing units
focus articulation
;)& quot
structure shown
time one
text refers
1995 ))
two additional
lexlcal entries
different levels
ce cas
matching two
whose relation
resulting structure
high enough
disambiguation may
might specify
meaning postulates
widely discussed
prenominal modifiers
good idea
current prototype
clearly better
short ones
four major
figure 14
would apply
concrete example
... n
two arcs
build systems
discourse parsing
linguistic clues
interface also
sentence given
upper level
~- 1
ontological concepts
past form
communicative function
different runs
robert l
corresponding sets
word containing
thus conclude
graph corresponding
partial derivations
first extracted
avoid redundant
single classifier
() n
level representations
sized units
first given
currently exist
subject domain
may hold
may try
system designers
new question
significant correlation
equally valid
contribute towards
g u
briefly explain
first convert
framework using
state rules
relative sizes
conventional methods
context modeling
satisfaction problem
previously presented
contemporary english
comparison purposes
dev set
;. since
propose new
defined earlier
1992 1
final section
bilingual lexicons
single domain
open text
make new
grouping together
following processes
million characters
quot ;))
conventional approaches
arbitrary set
negative integer
possibly infinite
process proceeds
english phrase
transcribed utterances
example sentences
., one
ground truth
%, whereas
two implementations
,~ l
boundaries within
translation research
alignment ).
another case
alignments using
free text
using multi
annotation style
rightmost column
1 provides
input space
explicit representations
boolean combinations
also improved
generation community
single words
verb clusters
2 presents
methods differ
18 words
genia corpus
integrated model
rule contains
highly specialized
sentence probability
domain objects
possible explanations
keyword search
., dr
whole string
also contains
exactly three
also produces
strong positive
cutting et
., cases
possible lexical
used feature
algorithm \[
commonly known
based techniques
two criteria
somewhat better
john ate
parsing requires
formal interpretation
1989 \]).
vectors representing
flexible enough
similar role
1997 ],
program would
examples based
constraints must
5 presents
show high
given state
2 proc
adjunct distinction
line model
lexicon used
also let
involving different
standard parse
tests indicate
quot ;(&
generate natural
annotated words
news stories
really want
one argument
binary values
considered important
\] w
r (!
get good
fall short
supporting information
different uses
insufficient information
following optimization
default strategy
content ).
second time
passive sentences
task structure
modifiers ).
spanish words
simple way
alia ).
substantial performance
rather limited
3 using
may call
particular word
different verbs
three possible
position within
translated word
task independent
leaf node
three sub
require many
many papers
still higher
natural lan
whose components
specify information
5 proc
quot ;;&
human interaction
000 noun
dalrymple et
speech disambiguation
events ).
general ),
calculated using
object ).
vast amount
question q
typed input
context like
still exists
might thus
answer type
dimensional vectors
european language
fourth section
also frequently
may facilitate
)/ 2
technical details
language capabilities
short story
suzuki et
automatic derivation
classifier used
initially designed
several large
w l
discourse markers
di \[
tree pairs
formatted text
precision ).
appropriate value
translation would
p denotes
~; e
yet fully
discourse segments
e u
generally low
medical reports
one agent
human performance
tasks involving
syntactic point
programming problem
probabilities using
also use
\[ 1983
discrimination net
plan ).
following interpretation
), adjective
particular class
items must
control module
may describe
model accuracy
final choice
4 depicts
function mapping
must realize
general formulation
theoretical results
generate semantic
recursion semantics
data structure
possible rule
made one
precision error
grammar rule
little influence
language utterance
detailed linguistic
spoken utterances
strong indicators
cannot explain
x ~.
classification accuracy
terms using
~.& quot
predicate symbols
cluster centroid
general terms
functions whose
corpus collected
paper considers
large portion
1 b
syntactic form
collection ).
must change
likelihood estimates
reference point
bayesian model
different sentences
automatically annotate
complement clauses
answer candidates
classify new
generated summary
collins parser
approach addresses
make explicit
incorrect pos
following part
attribute ).
handle cases
statistical dependency
syntactic function
new research
possibly partial
given speaker
evaluation phase
must decide
original language
2002 ):
great difficulty
could assist
content word
existing nlp
method performed
hand coding
database systems
get around
successfully integrated
two ambiguous
average scores
clarification questions
measure called
several questions
practical point
show several
9 million
less fine
human behavior
w (:
word pair
absolute terms
selected documents
categories correspond
create one
several learning
1 x
given surface
arguments ),
semantic tests
often make
medline abstracts
hand sides
6 seconds
~, v
solution space
middle part
system must
speech knowledge
words using
limited size
simple means
word recognition
explore ways
could benefit
usually include
whose value
probability density
class n
interpretation results
de nition
equal parts
general notion
data indicates
score function
constraint violations
term used
verb followed
entities introduced
compression techniques
parsing system
simulated annealing
relatively poor
bound variables
hypothesis testing
concept dictionary
training words
neural networks
process ),
~, 1
including speech
n 6
equally often
larger scale
general grammatical
anaphoric definite
extracted terms
n indicates
general scheme
two combination
automatic processing
question would
remaining 1
k \[
word found
computer laboratory
vice versa
infinitival clause
also leads
linguistic expression
research presented
systems developed
languages using
three pairs
state variable
including semantic
detailed models
typically correspond
basic text
ask whether
16 shows
various classes
full texts
defined syntactic
across sentence
important terms
large volume
correct part
psycholinguistic experiments
), location
intermediate structures
modifying adjectives
10 %,
bilingual concordance
individual events
necessarily contiguous
fairly standard
many interesting
graph unification
phonological form
single predicate
even small
every 5
intensive research
normalization constant
simply cannot
realization rules
operations required
treebank ii
sentence depends
verbs could
sub j
several decades
given text
long utterances
associated feature
decoding algorithm
set containing
query would
corpus size
vectors used
unique sense
standard semantic
recognition based
resulting summary
integrate speech
work remains
2002 ),
generate one
parser finds
tedious task
three phrases
8 9
constituents must
framework outlined
triangle inequality
row vector
go far
thus made
single point
pos tagset
disambiguated using
automatic classifier
would exclude
score decreases
uniquely identify
several computational
considerable degree
agreement features
confidence value
semantics --
function f
complex text
ar e
rules extracted
semantic meaning
independent processes
p ()
3 5
speech events
exclamation marks
robin cooper
rules given
type ),
null consider
84 \].
project started
data provided
lexicon theory
connectionist approach
xi ).
function r
t1 ...
case );
longer version
ive bayes
five questions
example 7
4 describe
function associated
syntactically complex
preceding sentence
personal computers
multiple senses
current user
specific entities
news reports
automatically added
immediately surrounding
separate components
resolve references
four elements
3 different
local tree
similar representation
cluster ).
simplifying assumption
may conclude
possible reference
white space
top 200
words aligned
plural pronouns
denote sets
parses found
contain errors
vi ).
straightforward manner
limited amounts
1 f
somewhat worse
surface syntax
present time
words along
different input
art accuracy
correctly extracted
word frequency
statistical approaches
text fragment
like word
initially proposed
shallow linguistic
utterance interpretation
observations made
whole relation
feature ),
~( w
overall frequency
magnitude larger
1989 \].
directly connected
news agency
new referent
x must
estimating p
class assignments
questions concerning
large size
results include
reasonable assumption
hence one
oriented programming
el \[
tagger trained
., 2005b
one wishes
phoneme string
position ).
anyone else
common view
local coherence
rich syntactic
much finer
additional knowledge
processing applications
integrated within
two letters
similar type
trees \[
input structure
mechanisms used
english speaker
great value
systemic grammars
), co
maximum value
drastically reduce
target verb
algorithm returns
similar analysis
visual inspection
two annotations
including one
surface grammatical
underlying word
would therefore
research goal
speaker uses
69 %.
particular person
different constructions
corpora ).
speech ambiguity
general nature
based similarity
base 2
common feature
last token
order hmm
conjunctive normal
two primary
give preference
system learns
goto step
overall task
four new
extract new
feature terms
expressions could
corpus contains
automatically classified
processing systems
source model
case scenario
performed best
comprises two
), recall
problem could
nodes containing
corresponding error
limited use
objects may
ordering information
two operators
modular approach
sentence john
different scope
possible information
improve performance
avoid zero
work much
10 ],
semantic heads
operating systems
pos categories
standard one
two networks
every sentence
test example
syntactic relationships
often thought
different formal
typed unification
might include
analysis would
modifications made
koskenniemi 1983
us much
4 illustrates
current lexical
position 1
following definitions
third section
include additional
psychological reality
linguistic issues
already indicated
verbal communication
nodes marked
h r
help identify
focus tracking
user request
reasonable accuracy
statistical phrase
handle various
main modules
identify lexical
may fill
performance gains
translation scores
would create
de marcken
general methodology
various subsets
one segment
french corpus
stemming algorithm
5 %.
0 b
tipster program
4 gives
perform morphological
particular document
whose resolution
entire feature
head word
feature appears
linguistic environment
cooperative responses
also handled
longer needed
carefully constructed
human speakers
system failed
label associated
figures 3
size increases
empty words
detailed model
preliminary study
systems development
filled templates
specific part
de facto
english text
semantics based
answer returned
learning step
two best
quite sensitive
lexical clues
absolute discounting
information sources
greatly facilitated
1963 ),
noisy environments
left branching
certain features
question asks
several human
slot names
implemented within
first k
new corpus
processing method
\] provides
introducing additional
predicates associated
model estimated
grammar fragment
large amounts
,, g
last category
syntactically valid
agreement results
fully resolved
words w
experiments using
common names
scope reading
description logics
breiman et
much current
plus two
structured lexical
character set
identify sentences
sentences without
take time
takezawa et
differ greatly
density function
dans l
quot ;,&
primary focus
news story
computer applications
last symbol
computational lexical
german noun
previous n
categories corresponding
extend beyond
grammars tend
corresponding class
shall assume
left hand
trees according
overall discourse
weak point
two minutes
individual segments
information systems
similar example
table l
dependency representations
ungrammatical sentence
6 ].
\[ hayes
describe objects
precise characterization
acl 2006
also adopted
illocutionary force
feature logic
per cluster
ordering rules
general set
method handles
q ')
positive evidence
ordered rules
previous clause
several tools
natural order
time stamp
1994 )),
approach might
might yield
systems today
partial result
applications including
possible rules
similar problem
providing useful
n e
could take
english pos
best paths
within two
semantic interpretations
1990 );
top right
-) automatic
describes one
distinct kinds
current design
features within
longer available
best n
partial implementation
single occurrence
new evidence
sufficient number
6 indicates
tree whose
quite acceptable
also interesting
concepts could
;=& quot
unification algorithms
following feature
must look
3 pm
binary vector
sentence processing
written without
15 ).
principle states
modifier ),
level view
utterance must
one correspondences
large n
successful communication
language called
theoretic semantics
noun sense
ner system
result seems
chinese bilingual
answer keys
15 \]
linear representation
thus reduced
comparing different
first processed
recognition accuracy
bc used
basic algorithms
representations used
fairly good
5b ),
element ),
markers like
material used
two parameters
th ('.
empirical observations
data produced
interesting task
meyers et
current configuration
results given
particular properties
certain relation
table 15
though much
might learn
logical operations
made among
particular kind
produce new
still produce
three dimensional
partial answer
next two
location names
relatively rare
canonical forms
decay factor
\] b
loss function
word overlap
conceptual relation
used measure
first converted
using character
global context
process makes
basic ideas
simplest possible
often called
significant positive
year period
verbal argument
one fragment
1962 ),
learning problem
), statistical
riloff et
previous alignment
additional restrictions
relation like
tree would
semantic contents
contains n
test described
much useful
two documents
parsed text
popular method
randomly choosing
would incorrectly
often leads
;, whereas
beam size
processed using
current test
termination condition
higher node
generative probabilistic
quite differently
0 f
one immediate
determined based
elementary structure
comparable across
coreference relations
explicit feature
would distinguish
communicative intent
across classes
similarity score
clearly see
wrong ).
care must
., np
dialogue situations
efficient representation
special characteristics
analyzed using
recursive structure
output function
methods may
real value
also problematic
work presents
\] x
six times
since nothing
i0 \]
references cited
falls within
degrade performance
see tables
verb classification
several works
sense number
rules r
textual cues
entities referred
problems using
retrieval performance
new user
thank prof
rely solely
full semantic
arc labels
though perhaps
best alignment
rules associated
): c
gain access
present algorithms
simple binary
grammar proposed
near zero
performance --
causal relationships
grammar symbol
values using
subsections describe
english output
including various
). although
sentence words
immediately preceding
one token
logical languages
goal state
particularly common
special issue
12 \].
based mt
semantic translation
related issues
wordnet ).
hierarchical relations
requires human
general process
measured using
training sets
one edge
overlapping features
high ambiguity
specific grammars
83 ).
possible trees
without additional
thus possible
null set
syntactic annotation
analyses presented
small text
low error
backward pass
based syntactic
paragraph structure
another solution
per language
two morphemes
patterns ).
83 \]
translation rule
would attempt
framework provides
2003 );
translated back
likelihood function
pruning strategy
previous versions
develop robust
greater precision
first technique
condition part
interesting topic
z ),
valuable resource
similar vein
subject slot
single formalism
general domains
verbal predicates
1987 \]).
vertical dimension
identified according
onto one
time since
features contribute
choice point
word wi
xtag research
sentence l
germanic languages
work properly
', etc
automata theory
documents without
high parsing
without generating
maximization algorithm
integrate various
also extend
initial sentence
single slot
case results
operation denotes
feature makes
test used
infinite loop
model presented
parallel sentence
transcribed using
whose verb
link structure
database consists
simply counting
focusing mechanism
definite conclusions
new areas
causal relationship
lexicalized tag
wordnet 1
99 %.
corpus encoding
two things
news documents
evaluation tasks
yet found
suitable set
1 proc
man saw
performs better
thereby providing
new lexical
systems represent
v r
theoretical linguists
base consisting
vector machines
alternative ways
frames ).
performance levels
many questions
already introduced
system takes
empirical testing
low word
using svms
examples .)
7 9
occur inside
two separate
phrase without
may know
different constraints
partially successful
us explain
system turns
use language
one category
), consists
yarowsky et
languages exhibit
., since
shown using
one particularly
also believe
\] f
one proposed
noun forms
candidate pairs
development corpus
observation suggests
system determines
simple structure
express information
level descriptions
boundary identification
,..., tn
step closer
generating coherent
one symbol
newspaper text
actual application
comprehensive analysis
discriminating features
syntactic parses
automatically identified
high similarity
show different
90 million
possible sense
plural morpheme
lexicon size
style ).
utterances used
motion verbs
clustering procedure
help students
also translated
would rule
list ),
group contains
linguistic module
whose frequency
improving system
patterns derived
root morpheme
six different
correcting errors
common prefix
automatically transformed
specific roles
build semantic
specifically developed
information concerning
good set
dependent relations
second uses
construction algorithm
processing texts
using local
path equations
100 randomly
negative example
length normalization
evaluation measure
senses according
structure among
string positions
window around
x [?]
disambiguation models
direct match
local optimization
candidate phrases
certain value
among features
important applications
sequences given
already available
fit together
big advantage
log p
two individuals
give higher
model assumes
performance based
classification experiments
presented work
score 1
simple head
argument may
specification language
particular object
discourse purpose
words actually
several thousands
word e
useful tools
text generators
three lines
structure produced
treebank parses
observed distribution
letter sequence
november 1992
whose task
selection rules
strictly less
many choices
general method
different partitions
structured information
muc scoring
words spoken
technical domain
probability space
could specify
interesting example
related approach
different effects
weight assignment
upon whether
reference word
many computational
alternative interpretations
grammar writing
achieves state
resulting formula
greatly enhance
output generation
6 e
feature selection
right time
recursive manner
lexical description
tipster text
syntactic class
central idea
short ),
would increase
subject ).
component must
interesting problem
ambiguous english
four languages
underlying properties
duda et
annotated treebank
g n
reader may
term weight
string fill
monotonically increasing
typical use
frequency information
features 1
natural texts
-- 1
lexical entry
hard task
type could
word bank
parameter setting
one variable
approach combines
inference process
sequence alignment
referring back
living room
high score
legal word
great degree
correctly recognized
factors affecting
existing grammar
scores using
cb ).
18 ).
). thus
synonym set
another entity
texts produced
additional details
every x
n \],
induction process
set provided
two functions
18 \]
san francisco
l {.
semantic content
et us
~( x
pruning threshold
allowing one
still performs
possible strings
empirically derived
small improvement
j [?]
pragmatic considerations
8a ),
new configuration
single error
would account
node .)
overall dialogue
parser provides
perform slightly
hierarchy would
shown along
incorrect sentences
class members
1997b ).
shows four
running example
matching mechanism
empty sets
include relations
several key
highly interactive
specified time
participant roles
subsequent search
linguistic variations
systems including
general consensus
steps described
feature could
30 %,
used several
analysis stage
language generation
interesting aspect
significantly increased
unification based
especially suited
verb argument
unmarked case
language grammars
tagged text
spanish ).
investigate several
robust systems
superior performance
compilation process
significant part
time relations
verb number
3 x
fairly strong
conversational agent
simple greedy
clark et
may however
method enables
15 shows
first model
information automatically
new test
\[ 1992
sentence form
came across
., number
main properties
last decade
necessarily true
english prepositions
capture important
fairly broad
top n
absolutely essential
previous version
candidates per
text meaning
;~ l
highest total
additional syntactic
model differs
31 %.
production system
highest score
1977 \],
relevant portions
semantic label
polynomial kernel
whose head
completely general
class features
1 ~.
parsing may
passive forms
content determination
translations produced
strings ).
e use
particular area
writing styles
john said
models together
two adjectives
single word
semantic nets
eventual goal
level components
), natural
sentence analyzer
features employed
linguistic basis
two rather
translation service
containing n
extracting data
two non
e n
either system
proposed two
appears likely
large difference
1987 \].
second assumption
100 times
longer necessary
one tag
database \[
analyzed sentence
quinlan 1993
successfully implemented
different slots
different modes
yamamoto et
possible forms
variable ranging
parses produced
feature systems
usually contain
increasingly difficult
nodes represent
le 2
c x
larger system
combines information
variation within
word wk
evaluation tool
speech interface
language equivalents
implementation details
data show
multiple representations
new subject
level 2
rich linguistic
annotated word
therefore consider
robust method
performs worse
produced manually
single class
(: lass
would lead
). nevertheless
based primarily
providing support
new reference
second test
originally intended
produces results
incremental approach
sentences taken
error type
two last
differently depending
becomes active
10 seconds
tell us
sentences within
0 proc
current level
), speech
must therefore
distinct english
verb predicates
less accurate
always straightforward
discourse domain
quot ;>
articles containing
also supported
standard classification
strategy employed
). parsing
often seen
nouns whose
collected using
erroneous word
initial stages
), semantic
interactive spoken
approach introduced
entity tagger
uses standard
matching patterns
words x
wsd task
adjacent text
domain data
rule learning
brevity penalty
70 %,
hmms ).
row labeled
9 \])
selected 100
across language
data distribution
conversational interaction
coverage english
training ).
time performance
phrases must
semantic dependency
uppercase letter
sheer size
uses many
syntactic expression
technique works
observed behavior
national natural
different translations
among two
development stage
2 ,...
~, z
\[ 41
partial interpretation
also treated
j 2
higher values
systems participated
would determine
translation rules
placed upon
valuable discussions
tree correspond
upon receiving
optimal combination
normalizing factor
85 ).
another domain
specific characteristics
role assigned
r ,,
training error
great variety
initial values
surdeanu et
tense forms
85 \]
second place
lexicon might
must create
unsupervised approaches
different occasions
89 %,
new information
fewer errors
medical terms
immediate goal
generating descriptions
previous query
also argue
null two
useful generalizations
labelled data
communication ),
textual representation
punyakanok et
.) nemlap3
based tree
notation \[
n best
discourse models
alignment using
nothing new
second heuristic
becomes easier
like system
york times
set e
al ...
obvious approach
still allows
list item
technical point
particular knowledge
complex sentences
text could
basic design
alignment techniques
research institute
found useful
constituent label
regina barzilay
candidate c
action performed
100 %)
extracting semantic
channel approach
might still
black boxes
probabilities associated
time behavior
working together
1959 ).
ratnaparkhi et
resolve ambiguities
informative features
3 types
handling unknown
particular emphasis
less sensitive
graph may
underlying system
interesting property
outgoing arcs
model obtained
specify whether
generally speaking
simply use
chosen according
textual analysis
aotrr 1992
helpful discussions
actions may
several discourse
feature co
another without
selected training
main elements
automatic clustering
great flexibility
processing natural
4 \[
clause structures
technique developed
string consists
system works
perform feature
corpora annotated
parser depends
hard problem
additional examples
indicating whether
basic assumption
sets ).
kasper et
corresponding elements
also created
intonation patterns
may consist
words preceding
nodes representing
english based
analysis performed
large collections
included features
process terminates
main memory
domain \[
;*& quot
also illustrate
rule 2
entry words
set produced
four basic
high probability
thus producing
experience suggests
sophisticated systems
techniques similar
general texts
often translated
retrieval ),
), one
1998b ).
conversion process
one makes
arcs ).
running linux
xml representation
systems employ
structures instead
efficiently implemented
controlled language
b .&
least 90
commonly used
transfer grammar
garden path
et ai
could think
existing features
lexical anchor
features give
structures whose
main object
section focuses
proposed statistical
one definition
also done
statistical algorithms
trained two
baker et
system consisting
raising rule
english uses
sense entries
words associated
existing dictionary
100 iterations
seems impossible
corresponding object
three areas
model achieved
null note
two distributions
subcategorization list
particular events
following formulas
competitive linking
one meaning
classifiers ).
might contain
core grammar
permit us
english verbs
using contextual
avoid making
strongly influenced
three lexical
added advantage
set consisted
categories like
based japanese
function within
less acceptable
x \].
best available
one full
similar pattern
linguistic phenomenon
every language
run time
closed classes
processing resources
u \[
semantic methods
general conceptual
called basic
first factor
collection using
first applied
sometimes use
semantically correct
individual entity
step 3
sentence taken
writing systems
completely unsupervised
two grammatical
difficulties arise
7b ),
extract syntactic
\[ 71
2 give
recent computational
sentence within
system developed
predicate must
highest probability
complementary distribution
four combinations
pairs would
context around
harvard university
recognition experiments
strong impact
whose antecedents
news ),
documents within
clausal complement
h ;~
grammar could
n documents
tile left
su et
structure annotations
different case
restrictions placed
sophisticated method
gives preference
transfer component
parsers trained
use multiple
term extraction
used quite
reported results
software architecture
using standard
natural speech
two elementary
combining multiple
somewhat simplified
real task
given input
also exhibits
automatic word
nl text
evaluations show
selecting features
latter task
could offer
right branching
2 years
final text
recurring patterns
sc ).
sentence context
university corpus
). 9
3 )).
sets derived
actual state
sentence hypotheses
expressions \[
plus one
mittal et
integer values
types ),
english sentences
ambitious goal
must distinguish
two advantages
using many
problem \[
four alternatives
magnetic tape
core component
words list
recognition technology
whole structure
also considering
5 concludes
constructions used
la base
local feature
conjunction reduction
process texts
grammars \[
male speakers
sentences ).
first applies
6 4
whose sole
features performs
empty ).
segmentation standard
relation label
best features
event detection
verbs according
assigned tags
5 years
size 4
performance metric
takes three
generation applications
random sentences
purely structural
hypotheses produced
considered acceptable
good effect
approximately 90
made even
choice ).
mostly concerned
tried using
table ).
syntactic properties
specific tasks
one attempts
earlier stages
right hand
static knowledge
provides many
thus would
karttunen et
clue words
fuller description
discourse model
rich model
using small
already appeared
speaker assumes
higher numbers
possible interpretations
gate system
overall organization
connected text
comprehensive treatment
follow directly
may look
action rules
relations identified
version 1
text translation
single document
large dictionary
resolve ambiguous
wsd problem
acoustic properties
segmented training
forced us
... etc
predetermined threshold
integrated approach
algorithm creates
former approach
c language
using dependency
4 cases
look similar
planning stage
simple fact
inter alia
say something
10 ].
traitement de
also known
vector representation
frequent category
steps 4
c )).
root label
current experiments
good way
small scale
discourse function
two units
abstracting away
ann copestake
information resources
instance ),
another point
brackets indicate
grouped together
input specification
mixed case
new level
must sum
sentences 1
big problem
dependency link
unique solution
using probabilities
string edit
speci cally
still able
also removed
plausible candidates
friendly interface
one big
like knowledge
annotated document
shallow parser
boundaries using
section 3
sentence fragments
expert users
0 %.
make much
new trees
requires three
binary feature
semantic errors
multiple tags
transcribed spoken
list could
almost 3
like machine
analysis rules
next one
one could
weather reports
maximizes p
syntactic attachment
), followed
simple template
model becomes
individual concept
central notion
pronominal anaphors
approach results
additional functionality
speech processing
many decisions
full np
latter type
relevant pieces
phrase (&
world ),
may happen
tree corresponding
., 2002
coherence relations
rate obtained
tables 1
best first
previous subsection
assumption underlying
manually generated
system constructs
underlying knowledge
upper left
subsequent input
text mining
based technique
knowledge rather
current goal
fourth line
ai work
thirty years
b --~
apply statistical
many linguists
expressing relations
patterns across
atomic value
several iterations
may apply
inherent features
component takes
dans ce
word segmentation
current framework
1999 association
speakers may
general statements
approximated using
english test
two summaries
pair may
three aspects
point 2
considering whether
moldovan et
feature bundle
2001 ))
une langue
provides valuable
provides empirical
47 ).
linguistic significance
million people
process consists
initial implementation
nice property
also motivated
centered around
syntactic approach
paragraph breaks
edit operations
entropy estimation
single candidate
parallel implementation
2 .,
existing components
also utilized
two english
include non
may add
5a ).
equation 4
bayesian approach
initial list
two extreme
priori knowledge
equations 1
word appearing
four examples
individual words
temporal reasoning
must obey
previous results
(~ n
corpora using
rate achieved
nonterminal node
local focus
often treated
hovy et
tested first
could learn
either case
becomes increasingly
), tile
attitude towards
different classifications
either alone
input documents
intuitive understanding
based simply
1 iff
system development
internal state
actual text
information one
high proportion
relative position
positive effects
related languages
classifying words
symbolic representation
svm learning
final output
discovery procedure
current character
main knowledge
new sense
two resulting
highly significant
im walde
27 words
period ).
contains 10
paper 1
simple combination
fleischman et
another extension
attractive properties
practical approach
best results
computational framework
rule works
uniquely defined
algorithm performed
text material
describes work
individual models
also works
manual post
verb entries
complete theory
easily add
used 5
questions regarding
;) 2
punctuation mark
take place
using methods
transfer dictionary
two projects
word structures
simple ones
sp ),
corresponding vectors
training document
also likely
sentence ).
involving multiple
start symbols
8 %.
syntactic elements
tie together
defined within
documents ).
adverbial phrases
conditions hold
also study
already shown
many current
verb token
also marked
since n
form two
dialogue translation
~, w
l ~:
trees rooted
general natural
obtained using
c /~
structures --
used different
full corpus
lexicon component
western european
crossing dependencies
analysis trees
small experiment
produce reliable
five iterations
official results
investigate different
representation containing
already stated
). secondly
selected two
kernel function
one choice
rules applied
conceptual structure
two set
le cas
speech models
98 ).
5 algorithm
knowledge ),
contain sentences
additional processing
sentence ei1
alternative methods
across multiple
scaling algorithm
clarification dialogue
electrical engineering
sense tag
similar size
relational constraints
relevance judgment
w ')
selection procedure
modus ponens
feature consists
class containing
set defined
ranked set
null tion
previously computed
second sense
phrase ).
syntactic object
different target
strategy allows
rules within
top 3
examples may
woods \[
predicate names
using left
using maximum
language tasks
grammar assigns
e si
clausal units
shows promise
also suggested
closed list
sophisticated statistical
must account
detection process
large computational
systems discussed
prague dependency
constraint grammar
en route
bracketed sentences
also identify
model also
sentence 1
briefly reviews
rapid development
1990 \]),
next position
select information
corpus construction
may optionally
clustering methods
actually correct
give us
evidence suggesting
first 1
often done
eurotra project
general semantic
information transfer
system increases
also developed
case structure
b n
prototype systems
deep subject
single terminal
one translation
intermediate results
automated methods
may introduce
recent works
et ah
mainly based
systematic ambiguity
target structures
translation framework
e 3
time ),
common representation
copy otherwise
loves mary
domain question
7 words
define three
using phrase
poorer results
ces deux
\] must
., german
additional linguistic
section first
six types
tagged word
finite set
simplified version
going work
lexi (:
use feature
nombre de
disambiguation ).
data rather
hierarchical nature
give better
relevant subset
constructed based
task might
reference system
resolution process
prediction problem
verb category
occurring dialogues
precision score
infinite number
automatic document
). examples
language utterances
l g
bayes classifiers
black et
become important
maximal projection
section 10
system employs
common type
order features
tile speaker
process natural
infer information
first 500
many useful
key problems
diverse range
-- typically
lexical redundancy
:// www
60 %,
first issue
arabic words
still lacking
parse times
looking centers
model represents
example also
contains 2
always make
space representation
clear idea
first manually
surprisingly good
currently active
target documents
based model
completely specify
cause many
best value
pretty good
additional words
resulting lexicon
many theories
expressions need
segmentation program
earlier paper
clearly outperforms
would treat
first parser
using five
specified lexical
three items
structures represented
cannot yet
better methods
also looking
already implemented
solved using
), 8
discourse context
classical approach
92 ),
focus attention
data might
developing methods
communication process
beek et
actual output
important semantic
different categories
least five
76 %,
moore \[
object relative
process \[
value f
fact may
pragmatic interpretation
singular ),
arrows indicate
three rules
natural question
structure specified
evaluated separately
overall size
table shows
usually taken
directed toward
current local
structures based
insufficient data
,..., n
correctly recognize
html ),
various nodes
information need
\[ cat
third time
30 seconds
compound nominals
equally plausible
also noted
value associated
stumbling blocks
functions like
formal representation
model probabilities
particular syntactic
new software
node may
1997 \],
xml files
write p
many discourse
using supervised
particular combinations
alternative possible
given system
increasing training
schulte im
indefinite determiner
several pieces
without reading
choices available
still important
question like
single object
would like
context c
many unknown
complex constraints
operational semantics
also arise
thus represented
figures 6
past participles
level task
possibly non
two domains
idea behind
also found
time slice
ordered lists
cooperative agreement
valuable input
dependency analyses
new goal
would select
uniform representation
world objects
tile text
two sources
help clarify
must determine
cases involving
relatively good
could detect
give enough
different directions
new framework
four arguments
grammatical analyses
1 ,...
major limitation
single new
robust speech
dative case
information retrieval
association score
rough approximation
biomedical domain
evaluation strategy
~+ l
various characteristics
language description
computational linguistics
using individual
text consists
analysis presented
cue words
surface expressions
definition 8
formal analysis
smaller constituents
shorter ones
improved accuracy
also explains
general interest
retrieval process
near term
al 1993
similar reasons
sense definition
robust semantic
communication systems
processing architecture
theory --
salient entity
core components
blank spaces
commercial software
lexicalized probabilistic
query like
grammatical description
average translation
c )~
large fraction
increasingly sophisticated
semantic composition
following representation
c ~.
extensive analysis
existential quantification
agreement among
training requires
general world
discuss related
verbs whose
mother node
inflectional paradigm
improve recall
two systems
partially automated
trigram tagger
higher quality
defined ).
1 states
linguistic considerations
resolution method
translation procedure
following recursive
), np
proposition 1
extracted according
details ),
whose primary
current frame
negative weight
mapping onto
constant symbols
4 8
example illustrating
one chinese
potential answers
81 %.
significant impact
state j
identifying semantic
polynomial time
remain unchanged
context windows
also incorporate
compare three
word often
formal characterization
full lexical
common problem
fully explored
links together
relevant portion
us define
queries used
little attention
retrieval applications
may therefore
following type
paper shows
complete structures
parsing without
understanding language
feature \[
bilingual parallel
parsing models
;~ 1
.. l
linguistic sophistication
limiting factor
,, c
way \[
implemented using
journal treebank
following analysis
kaplan et
stored separately
uses text
would try
two complements
step toward
involves several
candidate sentences
similarity function
incorrect translations
den berg
san diego
exact answer
improve overall
speaker may
identify whether
extraction phase
network model
could prevent
following step
corpus containing
different phenomena
computational burden
particularly concerned
little improvement
information storage
np head
use large
verbs would
recall due
e first
retrieval systems
must ask
true even
subject area
explicit representation
score using
,, q
level discourse
sentence boundary
particularly easy
apply semantic
~- x
500 documents
topmost node
prosodic phrases
bottom part
verbal heads
several anonymous
student essays
th ('
users must
interesting points
possible dependency
almost equally
corpus study
averaged across
classes rather
sections 4
two random
domain would
long term
news sources
linguistic grounds
), therefore
f (~
discourse annotation
correct constituents
lfg grammar
first define
word compounds
right place
inferences made
user response
directly relevant
sense selection
possible application
analysis software
may create
communicative situation
pos annotation
structure rather
adjacent sentences
communications technology
generated string
multimodal interface
hypothesis space
first draft
text data
always contains
empirical results
current target
main characteristic
null one
features using
-- using
performed two
algorithm extracts
every path
sentence shows
noun phrase
17 \],
one respect
document clusters
useful property
1993 ).
based generation
character position
lattice structure
discourse ).
based grammars
surface cases
techniques provide
first give
-& quot
1993 \]
pos ).
must generate
). still
practical machine
coordinated constituents
smaller parts
one made
vertical bar
ie tasks
may cause
first source
boolean expression
second layer
group 2
semantic coverage
size n
two co
also increases
topic shifts
denote events
[?] g
theory based
joint probability
v ).
lexicon building
system including
semantic contributions
deep level
american journal
tagged input
also useful
<& quot
sense could
position relative
short texts
line 5
inflectional features
tile output
), locations
sophisticated natural
basic forms
grained distinctions
may continue
typically also
using speaker
typically occur
priority queue
whose right
must receive
.) also
associated semantics
markov random
ambiguous tokens
another ).
semantics may
rules manually
ones shown
module contains
could eventually
cover different
clause modifies
module produces
likelihood approach
requirements may
information regarding
thus far
always use
following example
slot name
spell checking
frame based
filled pauses
roughly correspond
j ~,
generate paraphrases
substantially improves
terms must
second series
data suggest
1993a ).
problem would
using surface
\[ 51
frequently occur
roughly half
many iterations
95 ),
following problem
annotation work
simple idea
c must
several corpora
anything else
v v
various tasks
syntactic operations
input ),
penman project
acquisition techniques
insert new
method showed
preference order
second case
muc conferences
une phrase
word might
always sufficient
uniformly distributed
underlying syntactic
net ).
(- 3
multiple choices
v 1
based context
devices like
two positions
next steps
information fusion
full account
classes ),
common practice
complex forms
resulting set
yet exist
rule type
present experiments
null tween
sophisticated semantic
preferred center
text \[
compute p
subsequent discussion
around 2
list consists
logical interpretation
group together
phone models
features pertaining
act type
two factors
structure ),
hard constraint
within certain
e ectively
methods mentioned
good one
importance given
articles used
would surely
information improves
additional problems
heuristic method
mani et
combination process
semantic selectional
,: l
containing 2
grammatical dependencies
independent core
de smedt
utterance type
even better
following strategy
features along
8 1
une autre
manually designed
also indicates
efficient access
technical report
est donc
leaf nodes
possible approaches
system starts
systems use
first entry
-~- dire
1978 ).
without assuming
also led
spoken language
relative entropy
derivational morphology
nonterminal x
recall using
structures corresponding
high precision
~, b
clausal complements
using text
1978 \]
health care
different meanings
1 \].
ambiguous expressions
objects ).
wide array
two rounds
om one
limited degree
appropriate concepts
another word
xml format
efficient use
original verb
several complex
whole range
10 \],
different inputs
heavily weighted
either explicitly
4 corpus
constraint imposed
concepts instead
properties associated
high percentage
single transducer
less precise
may distinguish
automatic determination
data suggests
potential performance
solution might
concepts whose
verb meaning
phrases consisting
include punctuation
system dialogue
models described
possible complete
nous ne
every feature
domain corpus
questions posed
dialogue data
sentences annotated
punctuation ).
speech within
il n
human transcription
allows users
without leaving
grammatical role
common property
judged relevant
possible left
tile main
location name
develop methods
corresponding syntactic
,, p
analysis procedure
formally defined
another class
considerable effort
describe various
system participated
could contain
corresponding synset
probability distribution
future works
nl sentences
syntactically motivated
computing research
chomsky normal
pattern occurs
derivational morphemes
relatively strong
also represent
verbs belonging
), number
grammatical constructs
information detection
best recognition
1 .,
year ).
question whether
distance information
using pos
structure appears
voting scheme
read speech
dialogue acts
significant reduction
dynamic language
morphological information
better alternative
tag indicates
set aside
). words
also examine
available tools
already provided
negative response
possible phrase
would almost
generally taken
priori probability
different entity
generator takes
string matching
many distinctions
may attempt
specific goals
sentences typically
di er
dimensions along
standard dictionaries
automated text
medical texts
style parsing
de nes
n );
general architecture
syntactic similarity
performance obtained
immediate left
iv ).
sized corpus
syntactic analyser
keyboard input
seed set
context --
functional uncertainty
word lexicon
document retrieval
single hierarchy
makes extensive
\[ i0
grammar \[
arbitrary number
generation \[
word sample
one data
2 5
iv \]
english experiments
automatically labeled
work focused
generated first
le choix
three tests
training techniques
space used
may act
text according
full information
research objectives
deemed relevant
focus word
word distributions
art phrase
basic case
two studies
cooperative principle
). without
search mechanism
language development
evaluated based
). l
low score
possible errors
general research
run test
discriminative models
surface patterns
two statistical
automatically learns
rules define
formal semantics
semantic labels
expected given
data set
2 different
5 5
also serve
... e
main conference
8 percent
time could
(( x
ion l
quite fast
first algorithm
propositional attitude
parser applies
multiple human
different agents
models used
corpus frequency
combination method
form expressions
conditional model
different variants
l },
given case
state q
modern linguistics
material available
f1 score
author wishes
unordered set
use human
lexical classes
two active
large degree
final configuration
english translations
9 %).
local dependencies
automatic disambiguation
two places
speech waveform
somewhat differently
human judge
5 different
ambiguous cases
performance achieved
particularly helpful
randomly assigned
daughter node
two texts
predictions based
special treatment
research prototype
single pass
would llke
cause confusion
using traditional
\[ 1994
2 }.
additional factor
would perform
significant advantage
increasing amount
rules give
user gets
following dialogue
reasonable approximation
complex case
1992 )).
~, f
entity mentioned
null finally
yet another
system helps
depth analysis
standard logic
entropy value
also play
time applications
information since
output alphabet
information generated
much longer
lexical network
different processing
independent parts
different metrics
ranking based
verb could
~, r
final test
cl ...
user knows
correct sentence
automatically translated
proved successful
shall take
good fit
sentences already
key issue
present case
wild card
could match
lower ranked
summary evaluation
sets using
standard tag
current solution
19 \].
lexical anchors
existing theories
specific parts
program also
interface system
continuous text
much effort
proper way
different contexts
usually rely
acoustic score
nsf iri
computational mechanisms
existing structure
recent example
\[ 33
using templates
pervasive phenomenon
combined use
otherwise noted
simple non
computed precision
important research
modules may
total set
order shown
could certainly
following paragraphs
additional context
formalisms used
list \[
output using
automatic techniques
p ):
situation may
one \[
discriminating among
art supervised
construction types
roughly corresponding
although much
psychological research
global sentence
methods ).
correctly resolve
also limited
different people
initial phase
street journal
best partial
answers questions
implicit knowledge
arda aquaint
tasks require
mccallum et
et du
expression language
extracting lexical
ordered according
lesser degree
several nlp
certain range
preprocessing stage
different classifier
classifiers using
even worse
formal treatment
search patterns
input english
may increase
automatic knowledge
temporal references
rich source
speech annotation
unsupervised word
several points
task domains
created two
automatic machine
dollar sign
many fields
.). however
form one
human evaluations
context based
fewer sentences
source languages
relation whose
also excluded
expensive process
true ),
way consistent
different domain
alignment links
case since
one possible
agreement rate
performance may
one sub
statistical system
type checking
string generated
uniform treatment
new distinction
horse raced
source node
auxiliary inversion
semcor corpus
dependency representation
ordered pairs
different attributes
selected one
om tile
distinguishes among
27 ).
news texts
alone would
normally considered
one correspondence
component analysis
software manuals
;. however
)/ n
dif cult
different words
joint ventures
27 \]
trend towards
)) c
though several
language understanding
one judge
selection methods
direct approach
declarative grammar
often quite
impressive results
speech ).
yield higher
x u
may account
minimal units
would derive
8 shows
adjectival modifier
internal word
might produce
tagging results
rule name
candidates generated
22 words
general classification
special symbols
developed two
processing community
p ~)
also described
r (:
tree obtained
fixed amount
better coverage
high confidence
linked words
learning component
selectional constraints
german verb
quite well
data sources
scale grammars
paper examines
particular linguistic
maximal matching
based parser
p ),
dependency analysis
experiment 2
individual modules
section explains
using algorithms
relation would
without going
initial substring
sentences using
without fee
morphological level
hand crafted
current iteration
probabilities assigned
grammatical analysis
complete constituent
k ))
event variable
two previous
automatic creation
new content
base noun
initial feature
discriminative model
rescored using
darpa resource
multilingual entity
correlate well
word english
also tagged
must select
whole documents
input set
current approaches
large variety
representation described
\[ di
substantial progress
already produced
wordnet synset
semantic levels
another major
like john
linguistic motivation
formula given
relative simplicity
optimal set
another document
extraction tasks
different state
fairly obvious
longer ones
segmentation algorithms
elle est
treebank tagset
one arc
two parse
future development
2003b ),
salton et
us say
10 documents
last question
list according
words usually
source sentence
improve speech
indexed using
patterns extracted
also illustrated
translation application
already familiar
2004 senseval
different clauses
million word
also support
highest confidence
semantic decomposition
sponsored research
algorithm could
). taking
250 sentences
single underlying
reliably estimated
transformational grammars
using p
likely sense
corresponding f
first module
two statements
matching terms
model corresponds
wordnet provides
original problem
case letters
new label
incomplete knowledge
c [?]
7 ).
increases performance
development cycle
within documents
clustering algorithms
standard definition
problem areas
linguistic processes
financial domain
constraints discussed
suffixes ).
;@& quot
7 \]
2 cannot
xinhua news
low probability
words assigned
large margin
tied mixture
thereby enabling
gives us
forthcoming ).
intermediate structure
6 );
second mode
fig 2
additional annotation
28 aol
problems must
estimate whether
alignment technique
straightforward method
whether different
component parts
simple lexicon
direct object
world texts
20 years
., co
rule used
general corpus
fully understand
different persons
names like
also used
used language
data compression
research involves
figures 4
afonso et
possible alternative
actual input
computational approaches
values must
merge two
must try
1997 ):
good agreement
current phrase
smaller values
)) would
combining several
lexical types
response may
possible surface
gives better
contains three
given target
accurate method
term containing
support vector
results shows
definite clauses
automatically learning
viterbi parse
assumptions behind
also asked
12 hours
relation p
results give
weather forecasts
way would
quality control
simple notion
88 ),
common pattern
common term
still remains
indirect object
active area
third sentences
corresponding tree
document detection
current program
tree adjoining
two changes
stressed syllables
7 1
upper levels
relation labels
step 6
following symbols
account ).
two parsers
al .'
generative rules
larger linguistic
automatic parsing
representation used
using hand
word training
14 %,
phrasal nodes
english named
subordinating conjunction
nlp tools
simple matter
theory requires
lexical realizations
rules allows
recursive nature
actions taken
baseline methods
3 states
theoretical status
type np
pure bottom
concepts represented
different researchers
somewhat higher
one remaining
humans use
6 3
larger domain
minimal set
several paths
output node
based tasks
estimating probabilities
web search
cluster centroids
case relations
either words
currently underway
size 3
template based
remarkably similar
several reasons
two natural
since either
another kind
problem concerns
new dependency
type classification
become much
deeper level
user selects
different users
also stored
initial focus
refer either
utterance u
linguistic perspective
higher nodes
require knowledge
numbers reported
could mean
new application
gigaword corpus
linguistic interpretation
per verb
evaluation result
ambiguity may
), designed
20 \].
appear anywhere
base access
large portions
e -~
e par
9 sentences
basic distinction
adequate representation
translation methods
tokens may
quite important
lines 1
basic categories
1997 ),
probabilistic information
steps 3
different point
system discussed
arabic word
adjacent characters
crucial importance
corresponding arguments
w ~-
monolingual english
either l
would know
1a ),
sentence using
unsolved problems
make extensive
binary representation
normalized scores
two speech
within another
documents using
single syntactic
document categorization
abbreviated form
correct values
linguistic part
research based
standard ways
first using
p (~
). suppose
pivotal role
automatically created
comparative results
directly affects
explicitly modeling
correct senses
two semantic
sufficient data
called context
emnlp 2006
manually segmented
nn ),
parsing ).
sentence basis
naive approach
comes close
test results
level 4
table 11
data file
techniques include
table 13
foreign origin
adequate analysis
inflected forms
available today
hidden variable
huge amounts
two resources
simply selecting
two expressions
', n
underlying concept
several heuristics
corresponding japanese
provide another
parse result
system seems
output sentence
possible pos
qui ne
knowledge automatically
cfg ),
similar relation
extracting information
example 12
temporal structure
database entries
still much
di erent
parsing error
units like
following format
null since
results showing
complex categories
activation value
explained later
graphical representations
signal processing
section 6
two evaluation
second solution
underlying linguistic
retrieval \[
lower one
25 sentences
prior discourse
associated weight
machine interface
mistakes made
included among
psychological evidence
probability 0
across subjects
complete grammar
powerful enough
transition probability
key issues
annotation ),
one message
query expansion
null ent
single annotator
english may
like words
example output
case performance
provided useful
underlying domain
smoothing parameter
thus providing
described briefly
following pattern
one disadvantage
particular model
automated acquisition
2 senses
recognition engine
larger samples
used frequently
ten words
6 %,
great effort
never explicitly
english target
meaning relations
method would
2b ),
united kingdom
successive utterances
translation program
text would
cue word
suitable corpus
also adopt
one question
corpus described
arc label
). within
language sentence
domain information
lowest error
syntactic grounds
particular function
poor results
5 senses
l (:
term recognition
disambiguation techniques
figure 5
data concerning
act theory
analysis programs
morphological forms
2 clearly
simple english
possible future
new methods
template design
please tell
data necessary
), pages
translation errors
scientific center
partial representation
input method
sentence aligned
new jersey
unique word
new approaches
current state
corpus tagged
data per
case even
phrases occurring
diagonal matrix
could ever
mutually exclusive
several versions
carry information
scope ambiguities
classify nouns
-- x
find another
candidate translations
tile input
algorithm assigns
underlying models
log likelihood
cognitive processes
generation research
representation theory
various constituents
pairs like
using regular
basic mechanisms
ai systems
single nonterminal
noun (&
well .)
\[ ii
phenomena like
selecting among
knowledge consists
appropriate english
issues concerning
also argues
allen et
partial analysis
recall values
possible output
speaker utters
good ones
head words
jones et
morpheme sequence
), german
vowel harmony
immediate future
across several
grammar whose
semantic accuracy
less memory
patterns associated
two clusters
quot ;/'
two evaluations
phrase length
rule 4
scheme may
particular problems
two cases
considerably larger
two thousand
theoretical advantages
n denotes
order differences
collocational information
possible improvement
clause boundaries
human judgment
corpus consisted
segmentation based
also defined
best among
3 \].
occurrence patterns
cosine measure
object model
also refer
total amount
choose different
tree also
decision criterion
preliminary version
per topic
another object
supporting evidence
many valid
learning results
computational efficiency
insufficient evidence
per discourse
complex phenomena
first conjunct
dialogue research
pasi tapanainen
adjective noun
multiplied together
requires considerable
thus defining
accurate statistical
improved upon
initial states
document number
generalization hierarchy
40 sentences
different sets
widely acknowledged
commonly available
using statistics
one c
subcat feature
string concatenation
en fonction
based tool
based automatic
comprehensive overview
additional machinery
1986 ))
x .&
language faculty
large tagged
predicative nouns
new phrase
using essentially
simple set
1 %)
corpus used
input two
free derivation
describe lexical
along different
specific values
must conclude
common noun
occur quite
joint model
f ()
1 5
various times
linguistic grammars
complete training
two simple
also develop
recursive process
specific senses
two distinct
within 0
strongly supported
therefore need
clustering based
current speaker
7 shows
concept extraction
resolution mechanism
following constraint
general rule
..... n
rule format
grammar would
generation would
recursive call
aligned corpora
new program
takes longer
improve retrieval
full meaning
specific classes
technical knowledge
adjective modifier
input file
step described
papineni et
6 ],
following result
data automatically
miller et
following principles
annotated examples
seem less
directed acyclic
level structures
score associated
wide coverage
linguistically based
key advantage
language cannot
many nlp
also evaluated
c \].
\[ 12
interesting features
first situation
best performer
implicitly defined
errors could
abstract entities
genus term
overall results
syntactic tree
new errors
seven words
parameters set
\[ schank
correctly interpreted
tree represents
personal pronoun
derivation steps
unlimited vocabulary
using sentences
1979 )).
defined using
parliamentary debates
expert system
promising results
descriptions rather
containing three
resulting parse
dominating node
1991 ))
might want
models require
type raising
valid words
e 6
error bars
lead us
different relationships
noun --
first node
features f
remaining part
analysis methods
-- f
sciences research
average time
type =&
efficient indexing
groups together
probability distributions
sentence w
1 }.
entire noun
based trigram
learned model
two assumptions
/~ e
generally performs
appropriate training
could imagine
(' n
estimate probabilities
result indicates
would permit
lazy learning
many concepts
based algorithm
use three
automatic syntactic
shared among
features associated
one whose
japanese joint
times longer
ca n
run experiments
,~ r
three lists
diminishing returns
[?] j
precision decreases
decreasing order
terms contained
taken care
instances ),
large majority
average rank
rules using
text corpus
subsequent examples
concepts corresponding
various parts
diverse set
1998 ).
e ~,
fixed expressions
values assigned
becomes difficult
simple keyword
1 minute
c .,
., 1998a
1998 \]
expression must
acquired knowledge
information requested
ambiguous ones
subjects completed
sentence similarity
current text
actual system
aided translation
major bottleneck
main purposes
model --
like model
another area
may suffice
examples taken
prolog implementation
complex interactions
syntactic arguments
good number
made easier
fully exploited
lehnert et
provides facilities
class distribution
differ significantly
desirable property
), collins
). four
requires significant
figure 4a
recall curves
speech tagging
32 ).
user decides
adverbial modification
develop algorithms
manually parsed
correctly using
given sequence
one would
testing material
stopping criterion
directly apply
n ()
underlying model
dependent semantic
term structure
informally described
determined without
process creates
temporal modifiers
first sentence
200 sentences
gets higher
following points
labeled directed
simple process
underlying form
system contains
see figures
basic approaches
original theory
sense according
system produces
language analyzers
certain kinds
computer manuals
connecting two
., 1988
efficiently compute
model involves
reports results
analysis showed
whole lexicon
long distance
language spoken
annotators could
one matching
virtually identical
2 types
various different
sentence comprehension
human tutors
take many
wide variation
training examples
correct answer
w r
following noun
linguistic feature
could improve
using general
technology based
open domain
command line
results due
one ordering
structured text
xml tags
2000 )),
structures obtained
global coherence
including chinese
least partially
certain non
thus adding
solid lines
grammars generate
., data
cannot make
competing systems
extra constraints
knowledge \[
two existing
performance metrics
declarative way
engineering applications
automatically identify
93 ).
log probabilities
negative results
classification error
noun word
recall numbers
sense pairs
provide significant
system always
development data
particular context
big difference
recall f
level concepts
larger sets
appropriate features
words consisting
system retrieves
second classifier
zero ),
sentences extracted
). 1
verbal phrases
sentences given
., via
following logical
satisfactory result
machine processing
x --~
recognition output
generative framework
inference rule
cannot answer
model similar
two patterns
various dimensions
knowledge resources
automatically selecting
reference links
system use
become part
one referent
word per
interesting results
forms without
third level
analyses based
score equal
next consider
less ambitious
corresponding model
necessarily reflect
representation structures
causative verbs
given entity
predicate name
strings associated
every action
system successfully
base grammar
display screen
reported work
lower side
first author
pair ).
g e
whose members
include semantic
replace words
time \[
intuitive interpretation
small probability
right child
proof tree
time speech
suggest using
model word
methods provide
also collected
unsupervised system
valuable suggestions
give examples
preferences among
first train
whether x
limited subset
space reasons
data preparation
selected ones
correctly aligned
exist several
cannot avoid
rather simple
using heuristic
30 documents
quite reliable
also valid
word appeared
must respect
-- perhaps
apply rule
'( x
state corresponding
word brown
present purpose
figure 2c
logical formulas
qui est
general information
human behaviour
model based
behave like
possible --
performing feature
like lfg
r ..
label sequences
100 %,
embedded clauses
original format
unclear whether
category assignments
diverse sources
frequently mentioned
0 5
parser allows
1 )/
enormous number
one encounters
effective learning
many kinds
start working
specific facts
whose features
common denominator
processing scheme
verbmobil corpus
good performance
order among
system response
another round
translations per
diverse applications
network approach
conservative estimate
user requirements
expert knowledge
also wanted
boolean queries
direct connection
translation time
2 )).
quality speech
find significant
specific types
north american
better chance
., \[
human scores
phrases identified
rapid access
still fairly
errors caused
word boundaries
language processor
semantic level
simple method
truth value
may span
convincing evidence
rst relation
main problems
involving three
nested within
word means
q ).
allows us
scores assigned
example word
78 \]
probabilistic modeling
taking part
adds new
., 2004
extensive use
expository purposes
clearly shows
look ahead
q \]
possible mappings
generally true
parses per
new speaker
5 )).
specific implementation
primitive actions
every event
recognition algorithms
commonly referred
adequate treatment
thank mark
estimated probability
readable versions
method uses
0 }.
examine two
steps toward
one corpus
extracted information
optimization techniques
2004 ))
cardinal numbers
verb sub
negative value
partial support
1968 ).
strictly greater
.) table
correctly classified
automatically constructs
last stage
correct morphological
match exactly
e e
current dialog
tile system
person would
classifying new
obvious problems
three methods
segments ),
feature assignment
structures may
'~& quot
complex processing
important information
gave us
results using
paper provides
gram matches
per dialogue
american news
grover et
improve classification
2 hours
22 ).
procedure proposed
separate training
cannot estimate
performs poorly
canned phrases
include one
four years
slightly simplified
;. therefore
case ),
22 \]
following reasons
reported performance
), makes
logic programs
isolated sentence
increasingly larger
models might
two difficulties
selection algorithms
serious difficulties
examples ).
traditional word
acoustic models
different error
process information
participants could
next tag
x n
many alignment
scope relations
corresponding weights
french text
full translation
1000 times
extraction step
general rules
applied whenever
three classifiers
() tit
semantically incorrect
accurate speech
5 hours
structure consists
left corner
task evaluation
gave better
domain ontologies
linguistic characteristics
aspectual class
probabilistic framework
potential applications
one reported
computer programming
large subset
simple two
contain many
l p
best guess
single concept
annotation process
resulting clusters
ongoing work
discourse situation
see 2
accurate semantic
also present
), meaning
questions could
sentence includes
sample input
around 30
fixed position
level n
first transformed
human agent
rewriting process
jth word
000 documents
rich languages
last rule
incremental generation
given domain
retrieval research
different parses
contains 4
second alternative
almost certain
appropriate responses
must know
identification module
new text
independent language
lose information
derived fl
unique index
last group
data resources
potential benefits
already specified
another important
rank documents
less ambiguity
), otherwise
single linguistic
causal links
large corpora
less important
viable alternative
\] 5
quot ;--
[?] c
step 4
would rank
probably due
explicit use
gratefully acknowledges
2 contains
specific language
outside world
computer assisted
significant improvements
technical university
without change
people may
given words
1996 )),
using unsupervised
broad range
would add
relatively difficult
rst two
standard probabilistic
reduced forms
choices among
might turn
previous best
sentence provides
em iterations
different word
passive verbs
). applying
semantic effects
high success
new line
future improvements
auxiliary verb
following heuristics
additional conditions
subsequent processing
5 contains
type information
[?] q
use support
high mutual
preliminary results
accomplished using
structure parse
designed according
full detail
regression analysis
formed english
tree classifier
consider tile
function values
majority vote
j n
several features
variable length
possible uses
certain advantages
thus becomes
second run
case letter
also often
identifying information
motivated grammar
structural characteristics
verb pairs
whose results
larger datasets
often discussed
illocutionary acts
10 folds
consider various
based understanding
translation output
4 ))
poor recognition
given l
e used
best segmentation
different class
perform complex
parsing approach
little knowledge
output tree
choice depends
whole set
cannot use
rewriting systems
strategies employed
senseval workshop
de plus
double quotes
every type
three errors
utterance contains
fl ),
aquaint program
general category
1990 ):
verbs like
next item
subjects agreed
lexical definitions
feature information
way information
character ),
one user
testing data
information units
textual descriptions
typically involves
without incurring
make effective
level alignment
collect statistics
compared using
direct syntactic
recognition search
scoring mechanism
meaning ),
whole training
output list
elements must
two np
slight variations
every step
trace back
verb phrases
text summarization
practical purposes
using unification
june 21
intelligent system
including data
theory underlying
results comparable
node ).
extraction problem
1973 ),
shall also
second line
000 concepts
two steps
paragraph boundaries
defined relative
actually required
clause structure
20 different
small proportion
one wants
give much
10 training
across different
feature weighting
two classification
). compared
pragmatic analysis
15 %.
se (:
dialogue act
l --
.) figure
entries ),
language types
05 level
subordinate conjunctions
possible sentences
pos sequences
pattern language
compound sentences
input tokens
email address
present state
mor e
preceding example
greater extent
since discourse
null tions
n one
white et
three options
particular parser
appropriate way
errors made
section 4
combined together
standard data
corresponding context
encode information
annotation method
subsequent words
also successfully
), could
rule n
president bush
especially interesting
node v
tense verbs
syntactic model
earlier works
first reason
direct correlation
avec la
thousand sentences
choose appropriate
salient properties
64 %.
agentive role
done automatically
time limitations
query q
), following
identification using
red ),
last segment
primary difference
substantial increase
german corpora
acyclic graphs
3 times
speech using
rules cannot
recently mentioned
1993b ).
would introduce
highly portable
inference rules
five features
often result
categories based
theory may
algorithm searches
acoustic model
cormen et
related nodes
pcfg parsing
frequency count
language teaching
sequences using
correct identification
one half
large structures
various stages
structures also
1990 ),
next set
cognitive load
engine used
text segment
permits one
recognition error
simple statistical
children learn
one lexical
grammar design
negation ),
l ..
could handle
search proceeds
input speech
52 %.
flexible approach
two readings
connected subgraph
initial development
strictly necessary
three months
linguistic tasks
syntactic analyzer
frame may
two identical
basic predicate
conducted three
hypotheses per
actually contain
spoken human
represent different
xml element
similarity using
de cette
original size
certain english
one stage
occurrence relations
two vertices
slot grammar
typically based
-- hence
example input
using constraints
63 %.
mt research
correct interpretation
semantic forms
lexical feature
model achieves
necessary knowledge
simple texts
fully described
1980 ).
column ),
bob moore
word tokens
formalism presented
apply rules
various values
probability greater
mechanism based
one limitation
l ,'
every example
either word
another characteristic
co \]
computer software
1980 \]
using bayes
system knows
estimation problem
especially problematic
given predicate
various senses
1998 )'
inactive edges
parser using
specific use
tests performed
logic program
raw input
2003 ):
handled differently
another participant
grammatical formalisms
propose using
network using
simply used
roughly proportional
tag n
83 %.
evaluation measures
clear cut
include various
pronoun would
information makes
personal names
cut across
system focuses
chinese ),
r /,
data gathered
syntactic problems
mechanisms described
similar task
printed dictionary
two schemes
recognition word
x )&
sense would
null symbol
seems necessary
b ~,
given point
document ),
average user
linguistic utterances
approach needs
semantic framework
labeled arcs
specific representations
also divided
related issue
rule extraction
phonemic transcriptions
segmentation system
4 could
one rule
translation table
relatively robust
etc ),
one concerns
something else
occurring texts
text information
25 million
[?] p
one group
representation might
direct mapping
reference problems
traditionally used
inactive edge
explicit mention
larger class
better understood
object nouns
among terms
somebody else
pronouns whose
given form
poorer performance
several components
experiment described
thereby improving
b respectively
accuracy due
also build
3 %)
e ect
argument slots
comprehensive study
one day
every aspect
manual verification
certain set
possible input
single model
passive constructions
thorough description
phrasal constituents
transition function
thus obtain
system never
practical problems
much space
possible correct
human subject
thus represents
acquisition model
typed variables
like 1
successfully parsed
development time
processing could
lower number
chinese lexicon
available corpora
fairly small
ne ),
meaning units
sentence f
human reference
single form
), gives
retrieved document
automated system
ibm statistical
four conditions
produce high
.~ n
new algorithms
selection based
learning task
two answers
sometimes used
temporal relations
shows precision
arbitrary order
2003 ),
hierarchical classification
additional techniques
answers returned
training samples
formal grammar
semantics ).
rather easy
performance improvement
also significant
two conjuncts
s2 ).
saying something
current word
often overlooked
third case
optimal one
around 4
directly represented
report results
state calculus
one suggestion
would get
motivated features
la ).
14 ].
head ).
new ideas
useful features
separate file
presents three
conceptual graphs
significant result
linguistic analyses
redundant information
benchmark tests
identify word
child node
forward looking
verbal stem
would count
3 }.
e 4
concepts like
tool provides
radial basis
first paper
architecture described
idf score
us make
great amount
training purposes
different document
component produces
terms occurring
;~ b
modeling toolkit
individual languages
tag assignments
low confidence
crucial information
linguistic description
three alternative
way humans
value true
three broad
search ),
rules deal
wall street
shows part
satisfy certain
data retrieval
may perform
better system
translation --
initial analysis
based solely
12 \]).
(~ e
tree corresponds
human information
particular source
english surface
must consist
;). null
syntactic generation
lexical nodes
role identification
special feature
cp ),
tile case
first tree
), temporal
;+& quot
two argument
language contains
small domain
experiment used
also mention
achieve robustness
initial step
tools like
grammars could
yorktown heights
empirical distribution
statistics collected
head driven
certainly true
first implementation
first utterance
unannotated corpora
speech situation
far smaller
mitchell et
based sentence
second main
directly correspond
recognition experiment
summarization system
tm ),
discriminative approach
focus may
specify constraints
system extracted
aspectual properties
mckeown 1985
share many
40 million
measures using
major approaches
articles written
patterns within
empirical work
specific interpretation
previously obtained
like nouns
search terms
first list
\[ 22
coherent text
model requires
words occurring
semantic case
c }.
minor differences
weights based
word one
level grammar
unrestricted english
information could
inchoative alternation
explore whether
english resource
strong negative
often contains
logical analysis
structural analyses
produce several
often produces
parsing techniques
thos e
system reaches
intrinsic evaluation
., 2005a
sufficient evidence
commonly found
raw counts
much weaker
closer examination
systems incorporate
related tasks
-- must
useful first
one subject
widespread use
incremental parser
japanese language
correct path
null second
multiword expression
since e
82 \].
one reference
whenever possible
language access
word structure
r ')
clear cases
extracting relevant
three text
business news
overall structure
two relations
conversion rules
financial institution
structural description
1992 \].
org ),
make different
entity recognizer
chunk tags
thus requiring
consider every
frequency based
significant challenge
different candidate
research center
judge whether
define features
ten percent
main issue
possible continuations
finite number
unlabeled corpus
readable dictionary
pilot studies
verb whose
accuracy using
would cause
architecture used
section addresses
event denoted
certain domains
generalize beyond
syntactic context
building process
recent papers
proposed methodology
alphanumeric characters
lower performance
syntactic relationship
right contexts
1969 ).
appointment scheduling
resolve semantic
oepen et
path leading
2 must
6 describes
gets better
representations produced
less reliable
immediate children
section 24
unification fails
two n
statistical processing
already determined
particular entry
general definition
mary dalrymple
server architecture
take next
two predicates
seems plausible
full data
would continue
lexical preferences
current experiment
different properties
new search
provide results
often use
following measures
., 1987
use complex
different kind
several orders
general meaning
play important
different annotators
methods include
particular preposition
highly ambiguous
magnitude faster
may derive
quite simply
includes also
syntactic roles
b e
~. 2
good job
significantly correlated
model .)
alternative forms
contained many
relative positions
accessing information
head nouns
given higher
voted perceptron
u ',
text level
\[ 7
state framework
grammar rules
control strategy
generation rules
various types
one information
referring expression
natural choice
equivalent english
1 )).
~. 23
upper case
procedure could
technical terminology
number would
first blush
clustering experiments
sparse matrix
retrieval community
fairly complete
different variables
verb would
program uses
small samples
document summaries
vocabulary words
tree output
also gets
6 6
target structure
useful results
valid word
speech sequences
sources include
1965 ),
automatically induced
lexicalized trees
retrieval conferences
increasing number
increases recall
v b
translation based
), chinese
alternative translations
al 1997
pietra et
tile total
computer aided
provide two
;~ r
also knows
turing machine
statistical classifier
automatically tagged
old information
building systems
system dictionary
1972 ).
onto semantic
corpus might
p \[
additional languages
may occur
word similarities
rules map
possible alignments
1972 \]
among objects
includes many
japanese morphological
message level
). due
worth pursuing
plural forms
could provide
merging two
interactive problem
200 million
name like
natural translation
systems operate
forms ).
level 3
classification system
holds even
10 days
also highly
root forms
e structures
final solution
object description
example .)
v x
data regarding
average accuracy
many features
much less
gold standard
computationally costly
two substrings
approach ),
recognizer performance
entire dictionary
structures contain
two possibilities
close examination
paper presents
smoothed using
term distribution
less efficient
instances per
numeric values
simple semantic
relatively long
handled within
ordinary text
list items
modified noun
quite adequate
wang et
word usage
made regarding
common first
phd thesis
two nominal
one expects
unification formalism
r .)
word ending
somewhat surprising
structure sharing
word followed
asymptotic complexity
web browser
input patterns
performance drop
word position
like spanish
details regarding
multilingual document
semantic frame
semantic classifications
must apply
language identification
much structure
'~ uk
85 %.
significant amounts
fully defined
aligned bilingual
estimating word
2002 ))
linguistic adequacy
slightly extended
transitive verb
including question
human conversations
may obtain
2005a ).
single context
functions based
string fills
). notice
clear need
accuracy comparable
particular algorithm
contract n00014
situation ).
relation holds
manual correction
nl interfaces
data improves
anonymous reviewer
system involving
1981 ).
first identifies
26 \].
us access
would presumably
rule component
general procedure
sources may
van den
determining whether
current input
structure patterns
null definition
car ),
must add
various language
1981 \]
document set
substitution operations
least 50
tagged words
better translations
total agreement
tree algorithms
problem caused
progressive form
grow exponentially
1 contains
language interpretation
algorithm instead
nist scores
find two
one final
epsrc grant
fair amount
computed using
existing lexicon
user said
r system
yet implemented
us use
singleton set
tag grammar
verb stem
formal descriptions
gives examples
alignment algorithm
july 2002
analysis --
also explored
set intersection
form ),
given slot
labour intensive
equation 6
would correspond
problem becomes
base containing
estimate p
tile target
particular attribute
appropriate evaluation
fixed number
type definition
forms derived
easily obtain
using case
florian et
); see
particular approach
manual text
c u
approach appears
free word
typically involved
le monde
table lookup
different elements
ordering constraints
words cannot
linguistic constructs
l )(
also denote
motivated way
1988 )),
2006 interactive
following figures
future use
machine ).
utterances produced
individual instances
., 2001
possible constraints
target position
word patterns
preparation ).
order specified
formula 1
analysis involves
interesting differences
also needed
level features
standard hpsg
treebank annotation
v f
following hypothesis
open class
templates ).
., every
become smaller
accurate classification
incorrect parses
parser cannot
new verbs
algorithm would
map task
ones ),
fail due
important ways
second word
4 lists
bracketed text
selected terms
rule 3
). moreover
lp format
also introduced
verbs also
simple one
main source
reference ).
even without
almost exactly
aligned training
verbs occur
deleted interpolation
complete structure
selectional preference
even clearer
k c
method presented
meaningful comparisons
general statement
substitution node
topic segmentation
reasonably close
approach builds
pronoun refers
although several
interest lies
subsection 2
corresponding source
right corner
gets worse
); thus
text strings
template may
structures without
incorrect word
also contributes
becomes relevant
preliminary evaluation
treebank ).
french sentences
prepositional object
use xml
consider lexical
discourse tree
\], etc
). using
approach offers
systems generally
preposition phrase
please refer
must wait
estimated model
different system
art system
simple cases
tree learner
context also
usually associated
regular verbs
disambiguation technique
tied together
one level
three algorithms
written form
single syllable
higher threshold
process could
coder reliability
extraction pattern
get higher
may assign
appropriate one
various morphological
time spent
provide means
form suitable
many results
line learning
earlier example
increase accuracy
system components
sur le
baseline accuracy
highly productive
synthetic speech
sound basis
significant gain
large sample
sheds light
new sentences
task since
36 ),
discourse coherence
identify candidate
probability parse
np chunker
lexical database
also carried
classification techniques
must introduce
syntactic clues
also gave
describes tile
small loss
relations represent
appropriate structure
successive stages
). see
output must
interactive dialogue
consecutive sentences
certain amount
x 10
3 senses
first explain
also available
adjunct clauses
techniques applied
research may
resolving anaphoric
2005 ).
study described
category vp
either approach
independent feature
section discusses
score assigned
certain way
system 2
tense system
final phrase
semantic distinction
entity may
four different
john walked
pi ),
rarely occur
map onto
automatic generation
following questions
specific feature
application task
inflectional morphology
translation accuracy
level relation
us closer
appendix ).
method assumes
analysis based
(& quot
highest similarity
syntactic parser
original version
slot value
careful analysis
different base
original grammar
data since
higher perplexity
recognition requires
constituent may
inner products
data generated
semantically related
parameter l
vector containing
added complexity
lm ).
inference procedure
representing different
response ).
mcnemar test
different notions
semantically similar
word among
conditional independence
spoken discourse
also written
john broke
tree --
corresponding entry
features assigned
performance statistics
precise semantics
common value
problem also
per session
english slot
transformation process
per unit
small sets
grammatical tags
errors occurred
ai research
analysis uses
seems promising
varying degrees
sorted according
language must
main issues
judged correct
proposed method
among systems
separate tokens
categorization information
system correctly
four dimensions
topically coherent
linguistic intuitions
nearest neighbour
grammar presented
model thus
shown graphically
94 %,
deutsche forschungsgemeinschaft
evaluation tools
section outlines
text span
w ).
second experiment
whose structure
technique known
could see
complement ).
two speakers
simple mechanism
inflectional morphemes
one sense
basic word
noun hierarchy
w \]
lowest score
event occurred
certain common
): p
usually much
line 2
possibly ambiguous
1985 ).
improves precision
existing english
text like
style parser
whole parse
application independent
corpus version
graduate fellowship
problems including
every pattern
1985 \]
relationship holds
every term
bell smoothing
compute probabilities
existing bilingual
semantically based
sense assignment
000 training
probabilistic methods
first interpretation
tried several
traditional techniques
used ill
system decides
efficiency problems
three matrices
equally frequent
th (!
linguistic representation
probabilistic approaches
may extract
given word
following components
study used
see example
2000 sentences
order ):
nippon telegraph
two edges
dependency grammar
would benefit
sequence ),
search process
watson research
new mexico
linguistic constituents
active form
capture different
1 )~
approximately 300
un mot
perhaps better
2 [?]
description used
potentially unbounded
briefly illustrate
lexical cues
column gives
space complexity
role names
also observed
23 ).
occur frequently
incremental algorithm
two entries
algorithms presented
language barrier
l .)
w 1
15 years
europarl corpus
first part
complex word
names would
trees instead
gives much
case studies
23 \]
already parsed
often highly
surface order
could thus
describes research
method differs
., agent
may assume
adding appropriate
x 2
tag features
trees whose
given class
barbara grosz
\] pp
approach makes
japanese newspaper
terminal vocabulary
classification hierarchy
multiple paths
original string
linguistic analysis
new input
english treebank
tiffs paper
sufficient information
), however
similarities among
run without
different thresholds
module described
single finite
systems described
johns hopkins
pronoun reference
appropriate position
george miller
basic statistics
asks whether
semantic phenomena
explore methods
). 11
). 13
systematic approach
theoretical interest
use n
indexed grammars
great potential
corpus sizes
predefined set
correctly identifies
still high
constituent categories
effective strategy
different techniques
another observation
separate evaluation
segmentation may
reliable results
estimate based
recognition system
node z
main algorithm
large grammar
texas instruments
coreference information
distributional analysis
based purely
fillmore 1968
directly estimated
stronger evidence
two situations
reasoning mechanism
col pus
one presented
algorithm computes
one unique
), \[
algorithm stops
new constructions
limited training
possible translations
research directions
best english
specific nodes
lexicon rather
disambiguation model
highly trained
parser operates
several potential
another concept
search systems
produce different
h ~.
cannot fully
chen et
linguistic literature
different knowledge
review previous
incorrectly recognized
., lexical
generates sentences
good generalization
always used
resulting performance
distribution p
trees would
lexical sign
tree based
document frequencies
simple patterns
multilingual system
different machine
art machine
words share
grammar framework
order ),
mirror image
model alone
subsequent modules
many common
distinct objects
clustering may
14 \],
document co
computer programs
formalism also
focus spaces
large bilingual
word detection
z ))
46 ).
grammar like
information consists
,' rod
low compared
hierarchical information
range 0
researchers may
translated using
structural analysis
e ,~
produces two
dictionary ),
look forward
relations described
shows experimental
000 features
units whose
final state
languages share
default settings
24 ).
every constraint
use heuristics
work must
2 \]).
sense information
first published
achieve significant
1995 )),
also write
three levels
result might
test corpora
iterative procedure
numerical score
much additional
entries per
rule results
avoid generating
corpus 3
24 \]
distinguishes two
incorrect input
also results
log scale
b ...
98 %.
uses dynamic
second major
two proposed
perplexity reduction
recursively applied
testing corpus
person pronouns
np vp
exhaustive search
analyze sentences
important practical
sections 6
longer texts
1979 \].
several independent
fully specified
one long
three runs
current system
1999 )).
et el
lexical dependencies
5 \]).
dividing line
\] contains
systems used
based implementation
approach attempts
search method
within natural
selected set
thus enabling
structure generated
verb group
directed graphs
another technique
nlu systems
total number
20 hours
function must
2 illustrate
distinct set
identifying relevant
contains 100
2 2
directly mapped
good method
obvious advantage
slightly faster
different length
space requirements
widely available
learning theory
algorithms like
disambiguation problems
undergraduate students
rules called
automatic annotation
different events
last section
telephone speech
also choose
well without
english word
item must
geometric mean
specific questions
som e
adj ),
linguistic problem
parliamentary proceedings
preceding noun
probability one
+- 2
high correlation
standard corpus
5 2
scheme ).
taking advantage
task performance
present experiment
corpus available
one goal
errors would
original features
1 must
word combination
two terms
case markings
major improvement
using log
german version
procedure consists
manually compiled
le syst
means clustering
nodes dominating
one framework
lexicon ),
), according
8 proc
sentences uttered
german grammar
bnc ).
tags assigned
without affecting
sentence generation
several differences
original dictionary
individual utterance
also referred
reliable way
varied widely
vary greatly
sentence expresses
pairs whose
user defined
multiple versions
applied successfully
notice however
traditional parsing
integrated environment
graphical representation
unrestricted texts
\] use
empirical distributions
simpler ones
relations used
computational system
precision using
equal weight
derivation process
ellipsis resolution
general electric
processing \[
two dictionaries
dialogue progresses
g .:
complete agreement
), j
substantially reduced
need also
full coverage
dependent information
string comparison
takes less
slightly higher
1993 association
total sum
restriction may
became available
others like
semantic meanings
management database
signi cance
output nodes
actual words
interesting fact
three verbs
discourse entities
formed expressions
extra processing
generally associated
solve problems
might affect
information via
possible case
model score
add information
interrogative pronouns
grant daal03
contains 3
usual practice
6 \],
de rijke
phase 2
conventional word
good recall
highest correlation
probable sequence
single expression
based language
pour le
phrase generation
even higher
technical problems
remaining constituents
cognitive representation
particularly promising
one copy
derived fi
free variables
1992 5
method achieved
semantic approaches
generation procedure
one character
move towards
idiomatic expression
second kind
every noun
data indicate
low enough
development test
automatically collected
thus instead
functional information
situations involving
12 ].
main contribution
equal size
dimensional vector
fair evaluation
translations generated
first phase
correct analyses
following requirements
different parsing
art parsing
name lists
involves learning
adverbial phrase
sense discrimination
problems remain
single person
child language
overt subject
sufficiently close
con gurations
current plan
3 points
text contents
optimal number
pattern learning
noun may
rule vp
requires less
identifies two
frequency effects
spoken dialogue
broad set
program code
extensive study
would take
learned using
single source
syntactic transformation
information relating
three sentences
analyses may
data even
arithmetic operations
blind evaluation
almost 100
acoustic modelling
syntactic environment
distinctive feature
concept hierarchies
given us
techniques employed
relation also
parses generated
predicates must
words make
nominal modifier
without attempting
tile original
could include
phrase within
select sentences
alternate approach
small error
state represents
preferred candidate
stochastic grammar
given document
way distinction
generating two
part contains
also relatively
distinguishing characteristic
certain natural
restricted use
morphological variation
porter stemmer
significantly faster
automatic conversion
acoustic environment
axis indicates
highly desirable
several sites
achieves higher
., subject
illocutionary act
remaining candidates
au moins
automatic parser
sure whether
multilingual machine
two tag
operations ).
chinese texts
low attachment
test documents
phrase chunking
e .:
', respectively
highly non
might expect
within one
information \[
practical issues
probable derivation
particular situation
extra words
1970 ).
significant feature
dependent rules
pattern matcher
model may
bring together
five articles
ru (:
word vocabulary
(: e
particularly simple
time information
used three
syntactic tagging
particular languages
building natural
id slot
one morpheme
four values
complex constructions
particular phenomenon
easily computed
recall measure
prediction task
thanks also
scoring functions
online news
one particular
also appear
add rules
particular node
3 hours
system designer
sign test
\[ owever
sense level
must attempt
raced past
hierarchy based
generally much
extremely large
grammar g
6 show
logic based
must correspond
capitalization information
fragment shown
word number
four classes
special role
learning rate
relied heavily
based morphological
word matches
optional slot
approach gives
performs well
example 9
distribution based
training dataset
multidocument summarization
original results
first feature
v ~.
1982 \],
contains features
elliptical utterances
generally agreed
allows efficient
without even
similar structures
although different
stage 1
another model
dependency relation
2 \].
relies upon
contain similar
initial cluster
word distribution
table 1
words inside
current machine
number information
dynamic semantics
certain expressions
also means
another form
whose arcs
current technology
darpa tides
system found
80 %),
surprisingly small
edges represent
vocabulary ).
including lexical
cannot tell
important application
sentence uttered
bigger corpus
., information
must precede
language questions
synsets ).
research asia
building semantic
visual feedback
another function
different segments
5 \].
posterior probability
remaining parts
mapped directly
direct relationship
techniques rely
could yield
example may
alternative solution
minimal number
g );
language learning
given candidate
data needs
small class
feature lists
times articles
word relations
three general
theorem provers
similar observations
events occurring
300 words
unsupervised models
generally recognized
words away
basic properties
acceptable results
independent human
10 articles
overall best
three test
3 contains
new constraints
evaluation task
). syntactic
similar documents
included two
usually expressed
one section
performance across
expressions according
lexical representation
properly understood
procedure \[
actual users
input parameters
r may
standard lexical
improved word
l ,,
). given
declarative specification
marker passing
particular sentence
3 let
translated sentence
optimal parameters
gram frequencies
brian roark
given enough
oriented towards
independently generated
tense markers
high f
4 summarizes
unbounded amount
corresponding constituents
end position
ascii characters
examples include
optimal performance
gram co
remaining 5
general data
tree 7
different lexicons
goal ).
general methods
additional arguments
human translation
noun modified
), question
one local
trained automatically
explore alternative
completely new
classification given
comparatively simple
10 ms
set since
find one
lexical type
document consists
rules must
transparent way
orthographic conventions
index ),
current events
english input
new code
two objects
complex enough
every n
different queries
information according
different underlying
stopword list
rule schemata
becomes possible
top 100
lexical analysis
various measures
us focus
research program
primary task
used whenever
tile words
control language
first names
relatively small
specific tools
,, l
remaining 25
c contains
million words
;~& quot
vocabulary word
different occurrences
second object
9 words
questions whose
considered part
system checks
many open
actual form
ordering may
frequent sense
r ~-
wider variety
commercial system
vertical bars
latin american
english document
semantic attributes
also displays
1994 ).
press ).
masculine singular
), add
would suffice
group 3
english constructions
quality assurance
program committee
state methods
overcome data
1994 \]
al 2002
identical except
morphology component
cannot identify
index terms
pcfg model
initial weight
previous two
model probability
access information
functional level
newly formed
sufficiently flexible
state approaches
avoid sparse
various feature
highest f
utterance containing
syntactic conditions
second system
., rules
standard rule
). 14
users interacting
condition 2
better solution
essential properties
rule shown
common task
right word
describe methods
would meet
extracted data
possible target
klein et
e );
tile l
also shown
limited value
row 2
detailed description
recursive calls
programming effort
diagonal line
especially useful
way towards
limited amount
language 2
author would
translation experiments
x -~
research activity
directly applicable
unrestricted natural
representations rather
avoid overfitting
certain cases
appropriate combination
main drawback
completely clear
correct dependencies
uses domain
limited way
project focuses
like notation
e ective
also easily
categories include
sentence usually
similar concepts
nouns based
analysis .)
acquisition algorithm
\[ woods
parser identifies
best two
largest number
questions would
metric called
statistical word
dependent variable
later stages
meaningful unit
dative alternation
toutes les
simpli ed
alternative approaches
', e
implemented yet
text content
marked improvement
fall back
glottal stop
frequently encountered
generalization error
case one
), c
unification succeeds
large volumes
traditional syntactic
found among
tagging ),
word morphology
often referred
selection restrictions
unexpected input
features encode
translation technology
one closest
see 4
), producing
key part
discourse generation
computational power
generator must
prove useful
c n
rate may
certain simple
\[ 9
first evaluate
typically include
problems discussed
around 3
current case
). b
following verb
single attribute
first large
spatial location
get better
html pages
domain text
logical language
candidates according
polysemous word
fourth column
new plan
says nothing
first assigned
prosodic phrase
contextually bound
group members
optimization problems
full stops
normalization factor
english candidate
reasonably successful
produced several
many potential
string pairs
within tile
introduce noise
method also
quot ;<&
fonction de
conditions could
individual entities
corresponding parts
explicitly represented
grammatical construction
special null
paper takes
f ~)
human taggers
tested two
cost per
recent study
basic elements
oriented approaches
\], e
one relevant
proposed \[
robust methods
algorithms perform
answer extraction
f ),
7 presents
user testing
n 0
available electronically
grant gr
example demonstrates
contains either
extractive summaries
structures similar
1 [?]
syntactic module
different syntactic
available features
appropriate answers
significantly improve
initial estimate
proposed semantic
shows several
spreading activation
different combinations
often happens
possible categories
\[ kaplan
model explicitly
typical features
vector representing
\[ 1996
une certaine
perform equally
four types
based interpretation
speaker chooses
easily calculated
mean squared
special type
longer one
), whose
dictionary could
existing statistical
arbitrarily large
count nouns
categories may
7 proc
recognition score
differ slightly
highly technical
communicative goals
chinese natural
columns give
another factor
tagger assigns
concatenation operation
occurrence frequencies
polysemous verbs
section headings
constituent words
two sections
n ):
forms using
best way
morphological decomposition
suitable data
large quantities
karttunen \[
also revealed
large value
two ways
intervening material
base used
correctly labeled
whose range
b ,~
examples extracted
2we use
large amount
function takes
fairly similar
database would
examples given
greatly improves
particular instances
also remove
small compared
97 ).
klaus tschira
based part
.... n
candidate word
modelling techniques
basic system
pairs based
given object
french translations
speech synthesizer
john took
4 c
links two
scale language
formal theories
figure 2
partial descriptions
exponential number
determined empirically
object cannot
direct manipulation
model considers
null however
head node
possible directions
task ),
resolution systems
entire discourse
standard reference
constraint set
certain position
vast number
language database
thematic roles
weighting parameters
new technology
user often
recursive rule
national research
results reflect
h [?]
), would
parser processes
act recognition
equivalent expressions
temporal order
que nous
two following
another context
parser called
different probability
theorem 1
third line
give feedback
match one
west germany
less context
clause ),
previous statistical
corresponding node
significant computational
semantic predicate
better approximation
often appear
us illustrate
depth study
simple relations
language tools
adjacent word
ascii character
everyday language
null \[
biological entities
portable across
first determines
2 reviews
improve translation
dramatic effect
occurs without
semantic resource
give high
including information
several fields
interesting observations
complex system
many differences
work done
audio file
grammar also
important requirement
little loss
complete specification
domain coverage
two applications
\[ reiter
multiple features
n ~)
window ).
data ),
lower order
deerwester et
using domain
syntactic variations
1 \]).
-- say
;'& quot
tile next
linguistic processor
automated processing
pattern consisting
generated summaries
requires additional
section two
present new
). f
recognition may
u c
present system
n ),
patterns using
generate rules
almost two
appear explicitly
caused us
verb sense
'~ e
training approach
children ).
could put
conditionally independent
computer time
al ),
). r
telegraphic style
objects within
unique words
called free
using words
standard information
may get
work correctly
phrases together
). recent
parsing technique
automatic tagging
many similarities
effort needed
tag pair
coverage statistical
aircraft maintenance
four language
np structure
markov process
semantic frames
l ~;
currently consists
senses based
level structure
syntactic constituents
processing research
definition would
integrate information
main sentence
certain patterns
acquisition module
systems require
1 2
system recognizes
therefore conclude
distinctions made
q x
always present
involves selecting
syntactic well
theory ).
primitive elements
nlp component
maximum entropy
., 1992
simple transitive
specific role
system believes
evaluate performance
corresponding sentence
n terms
selection may
set performance
following discourse
deeper levels
theoretic notion
approach consists
get back
special problem
second round
(' e
various learning
dominance relations
applying two
one content
table entry
particular classes
share similar
also exploited
frames using
following three
1977 ),
using l
trees corresponding
go ahead
ideal case
provide additional
would replace
bayes classifier
coding scheme
preliminary work
specific action
paper concludes
fixed set
celex lexical
whose distribution
extracted pairs
bar theory
l ~-
adverbs like
4 would
either syntactic
easily get
pronouns like
web browsers
english equivalent
logical representations
total ),
cassell et
contextual information
different examples
partial results
using state
input token
around 10
wi ).
labeled training
search \[
one typical
minimal information
post processing
place predicate
training conditions
correct representation
solved problem
treebank using
attributes ),
becomes larger
wi \]
order co
henceforth called
larger syntactic
;, 3
small enough
au niveau
matching rule
get worse
also implements
lower recall
produces one
marker ).
front ends
versa ).
full sentences
brand new
labels may
require us
quite high
righthand side
roughly similar
x ~,
relevant documents
various parameter
way around
edge weights
extracted phrases
possible solution
one label
specific verb
constraints described
evaluation described
possible exception
automatic approach
processing would
three constraints
always produces
without compromising
correct type
google search
lexical semantics
minutes per
definite determiner
company names
phrase boundary
\[ h
), every
correct analysis
utterances ),
000 tokens
user might
original space
en une
default reasoning
limited space
standard rules
expressions whose
first measure
test sample
clearly superior
often marked
nonterminal symbols
one standard
value vector
clearly identifiable
english system
new case
highly frequent
sections show
43rd annual
two translation
web server
grammars whose
). much
every grammar
text classifiers
weights may
0 [?]
technique described
stylistic features
daughter constituents
sciences institute
also less
present discussion
~' l
best ranked
notational convention
word rather
specific event
english newswire
computing time
theoretical work
must take
language developed
possible interactions
well ).
directly proportional
author wants
error rate
basic knowledge
correctly assigned
experiment 3
string c
smt system
systems often
reported speech
volume 22
also raises
reason behind
free parsing
relations present
top two
related senses
selected 20
mechanism also
information gathering
final word
trigram probabilities
british national
6 0
efficiency issues
input symbol
event reference
two temporal
key feature
approach aims
tentative conclusions
expressions would
sri system
handled using
contextual rules
'. l
broadly defined
yields results
finding appropriate
one small
evaluation consisted
textual corpora
fully integrated
semantic output
handled well
better match
look back
maximum number
second pass
given knowledge
best previous
human involvement
less abstract
clauses ),
fairly general
unix operating
symbol e
identical strings
also describes
single turn
computational expense
grammars would
node b
whose parameters
semantic templates
one dependency
possible sequence
structure proposed
\[ man
produce language
previously mentioned
linguistic facts
w using
whose performance
three characters
methods applied
every component
e two
relatively short
6 ):
chain rule
relevant concepts
). ideally
one application
increasing order
general question
wsj training
annotated using
slightly better
best parse
7 %.
constraints used
program may
software design
100 documents
6 discusses
evaluation used
complex knowledge
representation structure
4 %),
significance test
could still
n (~
possible meanings
human data
particularly since
information would
2 times
), augmented
speech generation
measuring agreement
still cannot
typical way
system described
phrases containing
national corpus
system returned
includes word
world applications
stage approach
right kind
elements used
last five
comprehension system
left unresolved
words must
grants council
would offer
rule 6
rule indicates
generation methods
relatively minor
occur either
provides users
parsing model
object x
applying statistical
tests showed
use either
5 times
fig 3
path feature
string whose
cannot represent
u p
verb like
difference list
also differences
technique used
), charniak
linguistic realizations
text collection
issues addressed
statistical properties
features added
applied first
parser must
wordnet synsets
hav e
null ment
translation may
network must
recent theories
large space
absolute time
languages must
et al
independent representation
l h
mean something
positive score
2 represent
). nonetheless
whose behavior
items used
focus ).
different actions
given length
)-( 5
syntactic phenomenon
also demonstrate
candidates whose
texts ),
specific representation
low similarity
substantial body
word ),
identification phase
dimensional feature
syntactic node
based feature
present participles
least ten
algorithm presented
interface allows
although sometimes
quot ;):
certain criteria
including pronouns
handle long
interested readers
template generator
described method
variable ).
language constructs
important question
0 2
model relies
\[ 81
attentional structure
primary concern
dynamically generated
could affect
modifying phrases
complex events
-- 5
cfg rule
data objects
string would
comme un
incorrectly assigned
recursive rules
linguistic formalisms
task definition
achieves better
human body
existing resources
specific type
entire dialogue
specific topic
highest precision
verbs --
cannot serve
specific discourse
2 example
precision curves
n objects
system generating
models produced
specific analysis
future study
yet even
often found
outgoing edges
word terms
np modifiers
descriptions generated
different derivation
noun without
easily represented
many additional
solution proposed
idf ).
knowledge could
also provided
final experiment
b );
partial orders
occur together
ltag grammar
materials used
reasonable compromise
coverage parsers
u '.
us identify
particular tree
inversely proportional
domain --
may correspond
parsing problems
system used
rewriting rule
words seen
salient entities
variable v
rely heavily
different arguments
systems might
since training
data makes
sufficient detail
many ambiguities
among tile
training using
empirical studies
one iteration
functions may
2 %)
particular implementation
scoring process
tagging process
particular utterance
fixed order
three experiments
performed automatically
x e
generation program
various tools
used corpora
nlp technologies
briefly described
also recognizes
word dictionaries
possible variants
problem stems
newly acquired
human errors
maximum performance
also hope
among various
actual word
question may
form \[
whole story
one thinks
6a ),
\[ john
simply choose
node f
basic parsing
semantic labeling
three million
three forms
whole vocabulary
outgoing edge
argument relationships
z \[
satisfy two
canonical form
5 %)
atis system
reflexive transitive
e denotes
complements ),
significantly better
strict definition
mirella lapata
node r
also captures
word appears
algorithm assumes
like x
model without
\[ 18
english machine
semantic formalism
developed systems
concepts mentioned
optimal tag
consistency checks
effective use
corresponding values
last resort
extended period
features including
must meet
-- including
described two
proposed architecture
challenging problems
new company
unsupervised clustering
main classes
h .,
words labeled
distance metric
consistent improvement
mining techniques
classification models
several observations
experiment involving
techniques using
distinct semantic
previous evaluation
textual documents
relative number
already existing
improved system
query may
example words
input units
), lexical
ranked lists
structures involved
variable bound
several documents
ii project
graph model
one left
quite effective
resource development
priori reason
along several
telephone calls
take care
possible contexts
new target
thus improving
set 0
telephone number
approach lies
modifier pairs
explicitly listed
1998 association
phrase attachment
claim made
us begin
cosine distance
traditional linguistic
high weights
coverage semantic
scoring systems
tagging systems
fuzzy logic
object types
five minutes
pcfg parser
local decisions
one vowel
one good
existing one
original structure
involves three
hearer would
single node
j e
potentially infinite
system comparisons
scheme using
dependent phone
problem --
whole relations
main reason
reduce data
., adding
000 articles
tagger \[
\] 2
argument relationship
two conjoined
one phrase
matching rules
phrases found
reduced word
technical issues
positions within
translation directions
input device
well formed
reduction ).
following property
two conceptual
1984 \],
specific example
would cover
section briefly
algorithm follows
left child
compilation time
three syllables
far lower
extra word
languag e
right track
semantics interface
planner would
different concept
slightly worse
specific problem
disambiguation algorithm
un ensemble
real work
official evaluation
also said
larger tree
since features
nl input
verbs based
task could
continuous function
r l
., wordnet
coordinating committee
context word
people involved
graphical presentation
nlp components
problem involves
significant role
adopt one
dictionary consists
experiment results
), rule
obvious reason
found even
lexical organization
part supported
eac h
highest weights
far away
different linguistic
structures across
easily adapted
child nodes
sometimes referred
fundamental questions
parser failed
word units
compute word
previous one
earlier papers
another expression
possible words
algorithm differs
find answers
must ultimately
type c
using higher
every element
scoring method
tagging method
assumption behind
binary decisions
constraints defined
selecting one
10 minutes
hobbs et
), philadelphia
optional elements
extremely useful
argument slot
language representation
study focuses
often useful
single sentence
scientific abstracts
automatically selects
measure described
several concepts
information scores
learning english
also compatible
practical significance
japanese verb
specified constraints
lexical pattern
data could
9 ).
new categories
although perhaps
another person
observed frequencies
specific aspect
top ranked
best one
., g
shallow analysis
highly dependent
every relation
many ambiguous
given earlier
problem related
9 \]
every 10
possible without
relatively weak
positive probability
monolingual dictionaries
extracting sentences
analysis may
smaller pieces
another source
necessary ).
words constitute
characters long
set ),
noun form
well compared
acts \[
approximately 12
language type
times real
language discourse
). precision
full unification
annotated sentence
enough data
knowledge relevant
des textes
language analysis
object language
evaluation corpora
third word
possible state
structures provide
literal meaning
top 20
process would
words starting
spanish sentence
ti ),
r /.
parsing context
topic shift
saw mary
default value
zens et
retrieval application
relative performance
similar training
linguistic reasons
based learning
typically contain
common theme
chain rules
result shown
different models
may benefit
press newswire
longest string
table using
given moment
new directions
applicable rule
4 sentences
uses four
adaptation techniques
constructions ),
structure makes
contains less
processing stage
automatic discovery
word segmented
linguistic representations
related language
would also
also produced
would occur
asking questions
seems desirable
features cannot
user query
far superior
oriented dialog
ace data
applying natural
helpful discussion
}@ cs
language front
model built
classification model
problem based
34 ).
improves significantly
one incorrect
seems rather
protein names
001 ),
often indicates
significance level
tile word
may place
dialogue state
verb ',
last time
next stage
performed via
semantics associated
together two
la r
following condition
%, depending
expression contains
certain number
translation probabilities
whose last
second annotator
part 2
learning paradigm
state p
systems allow
performance given
using shallow
sense label
low correlation
argument annotation
recall rates
feature whose
exactly two
mean one
find relations
written using
simple terms
july 2004
system defined
., whether
significantly worse
formal theory
representation produced
measure used
require explicit
estimated using
ever since
complete lexicon
reliably annotated
semantic word
ideally suited
european parliament
surface ordering
methods produce
quality translation
type would
determining word
methods employed
strong support
different samples
large differences
space defined
corpus provided
inference would
position paper
notational conventions
clear definition
processing group
\[ l
two data
), subject
rigorous evaluation
approaches proposed
speech conversion
would obtain
topics within
analyse de
individual nouns
argument ).
8 \].
statistical disambiguation
nodes ),
), reference
produce multiple
r (',
multiple references
place ).
certain relations
uniquely determined
structural representations
appropriate terms
probable word
next rule
approximately 95
intentional structure
topic boundaries
cfg rules
put forth
entity ).
could form
new phenomena
temporal expressions
word set
often represent
sentence occurs
intransitive verbs
deep case
different purposes
syntactic environments
several functions
conceptual relations
manner similar
performs much
n order
concepts related
therefore possible
feature would
work quite
former president
extend existing
3 \]).
procedure calls
raw form
like japanese
document classification
\[ nirenburg
useful comments
training sentence
uses word
wordnet classes
significant increases
rules developed
classification problems
specific dialogue
data gives
next day
automatically building
applications involving
simple scheme
.~ e
features identified
also concerned
clearly seen
similar features
positions ).
rewriting rules
mental lexicon
value must
8th international
pos tagging
using latent
might define
voice ),
unpublished work
uses finite
ensemble des
since two
also easy
direction would
well enough
disambiguation module
third experiment
retrieval methods
system evaluates
resolved without
figures show
also learned
traditional model
plural suffix
tedious process
phrase occurs
\[ 15
basic syntactic
automatically discover
3 illustrate
daal 03
tree may
6 training
passive form
meaningful results
dictionary \[
alignment quality
accurate translation
3 2
several kinds
statistical techniques
quickly become
generated texts
)= l
syntactic classes
local discourse
rule says
), information
fairly clear
2001 )),
almost one
appropriate grammatical
input could
algorithm achieved
testing procedure
phonetic segments
tool allows
created automatically
generated automatically
grosz et
following decision
naive algorithm
best fit
short phrases
might give
methods rely
corresponding answer
agent must
given syntactic
93 %.
speech signal
initial hypothesis
null therefore
extraction tools
new solution
also prove
abstract away
total order
score values
1998a ).
understanding component
classes could
upper bound
statistical knowledge
every value
l words
thereby eliminating
state --
generation time
target set
would prevent
several non
hmm tagger
inherited information
de sa
translation approaches
formalism based
linear string
word selection
many helpful
statistical machine
x 4
words beginning
possible next
still make
sentence constituents
similarity calculation
time cost
one speaker
relatively shallow
structure could
system integrates
seamless integration
quite different
better predictor
ie task
improving precision
substantially increase
section describes
analysis also
parser takes
complex syntactic
certain parameters
news data
centering theory
fairly low
bayesian network
1982 );
inversion transduction
different model
l ,.
;, etc
one alternative
situations described
significant step
smaller corpora
tools based
~+ 1
chinese corpus
main challenge
whose syntax
substitution operation
c 2
standard procedure
knowledge intensive
based measure
gives high
extensively used
different form
recognition models
automatic sentence
often impossible
tables 5
document corpus
shall see
entity class
text instead
specific piece
syntactic considerations
would assign
cannot afford
central data
l l
thank john
empirical evaluation
computer hardware
subjects participated
also certain
different function
text output
l '\]
first analysis
corresponding language
appropriate tag
comprehensive grammar
four feature
frame elements
even using
intermediate position
annotator agreement
also ambiguous
computational processing
processing performed
two possible
current domain
two sub
powerful tool
still leaves
relation --
first step
measuring performance
:~ e
isa hierarchy
), anaphora
texts could
quot ;:
recursive definition
word could
contingency table
semantic net
dominant role
\] developed
process stops
words 2
another approach
ranking candidate
graduate students
reduction compared
successful application
test two
another special
individual utterances
little better
words plus
five kinds
cases one
two sides
model fails
highly ranked
.. 1
particular interpretation
us examine
also accept
likely translations
new sequence
component described
improved parsing
identify words
~. let
1 times
negative training
mixture models
object pronoun
\[ 1978
1983 ).
bare plurals
(, n
detailed treatment
constraints require
one text
three target
possible extension
segmentation error
already generated
practical considerations
2002b ),
interpretation ),
also reflected
78 %.
using similar
1983 \]
automatically detected
operators used
consecutive sequence
test file
1 ~,
state devices
disambiguation using
funding agencies
much training
written languages
clarity ).
based analyses
quantified variable
functional dependency
different problems
one time
associated concepts
perceptron algorithm
based tools
three weeks
following criterion
introduce errors
used successfully
particular part
another direction
current models
speech utterances
etc .).
1m words
use frequency
used n
similar results
may take
functional application
whose second
performance depends
10 speakers
provide insight
words correctly
ongoing research
briefly discussed
unlabeled data
propositional logic
always selects
cannot refer
many real
using word
grammar coverage
nl system
low f
solution could
final system
text databases
experimental environment
consistency among
algorithm becomes
systems available
(: omt
facilitate comparison
p ))
per article
world data
words wl
significance tests
constraint relaxation
often described
top one
checks whether
two passes
six features
several significant
general lexical
phrases play
pentium 4
technical manual
literal translation
significant problem
state based
considered several
training instances
... ...
full forms
asked whether
~. n
,~ x
text summarisation
22 %.
evaluation procedures
empirical evaluations
prolog term
first sight
processin g
system prompt
et de
ranked according
generated parse
one pos
baseline classifier
\[ elhadad
occur infrequently
language dialogue
unique class
using class
english syntactic
mt approaches
short time
right parsing
alternative word
figures indicate
57 %,
17 ),
many documents
approach improves
way --
worked example
rose et
b denotes
semantic input
one explanation
verbal phrase
matrix subject
wilcoxon signed
would extract
various discourse
null lowing
example taken
annotation scheme
new entity
given examples
significant degradation
optimal translation
applicable rules
proper semantic
1 example
constraints specified
language semantics
two pairs
given explicitly
automatically predict
1976 \],
shows clearly
involves making
better ).
essentially equivalent
fully implemented
graph ).
directed arcs
first problem
level specification
.) next
easily understood
combinatory rules
occur less
module selects
including whether
user responses
\] -~
complete set
component used
statistical parsing
studies using
state verbs
extracted directly
simple definition
abstract representations
trees obtained
theoretical justification
\] le
linguistic regularities
also developing
sentence 5
word order
recorded using
closest match
clusters based
complex examples
three categories
interaction among
e structure
extends beyond
grammar developers
randomly split
recursive procedure
first 5
see 3
significant aspect
would assume
different scenarios
human would
relation based
header information
th iteration
log probability
following structures
discourse module
analyzer uses
authors use
sample ).
extended domain
multiple independent
10 ):
language used
randomly selecting
unseen documents
-- especially
two phrases
successful approaches
tensed verb
similar topics
lexical access
yet addressed
learning strategies
present examples
6 illustrates
filling slots
frames associated
without deep
root word
also show
seem promising
possible relationships
resolving ambiguities
coded rules
first strategy
semantic scope
second purpose
surface realisation
may grow
schema may
save time
highly flexible
syntactic types
threshold th
also recorded
whose english
processing rules
question processing
7 concludes
works reasonably
different generation
earley 1970
linguistic realization
appropriate set
following observations
two remaining
task easier
often used
third kind
interpreted correctly
another module
models trained
word frequencies
general design
1989 );
must perform
framework developed
utterances per
data made
thank michael
main motivation
use discourse
could recognize
following subsections
allow different
several advantages
word co
active node
acquired using
concept ),
lexical representations
better word
syntactic realizations
automatic mt
string corresponding
structural regularities
extracted sentences
bigram probabilities
observed data
contains additional
previously selected
similar way
next token
unlimited number
present preliminary
one output
initial value
sentences cannot
list whose
simplest case
selectional restrictions
verbs take
translation without
generation algorithms
cl ),
surface lexical
word removal
types according
many things
additional source
89 \],
mixture model
strings x
without regard
parse failure
usage patterns
dictionary look
also semantically
easy integration
nsf grant
ny 10598
context requires
important sentences
new contexts
handle many
apply various
null lexical
majority sense
current point
also defines
l )-
may stand
task consists
hotel reservation
quot ;$
previous steps
form np
unsupervised approach
particular path
frame slots
times ),
ohta et
straightforward application
noun ).
general outline
algorithm also
correct result
links among
increasing interest
process sentences
10 ),
key role
given lexeme
ascertain whether
noun \]
analyses ).
tagger would
set could
resulting data
methods outperform
various genres
practical implementation
90 ).
thus indicating
incorporating additional
much greater
yet unclear
little worse
), sense
information items
scope readings
current form
weight l
three judges
usually occurs
90 \]
word positions
time data
new classifier
different context
flow diagram
left contexts
different hypotheses
would naturally
stack decoder
list would
could contribute
experimental conditions
value assigned
support multiple
matching procedure
words contained
method --
combinatorial explosion
grant ist
translation engine
fundamental role
like part
text --
answers found
either single
smaller size
lies within
system often
ss 2
annie zaenen
data consists
new domain
anaphoric phenomena
information requests
experiments done
agreement ),
composition operator
makes little
english examples
relevant features
specific pattern
advanced language
specific term
second observation
ontological relations
model ).
always choose
centers around
two widely
features make
methodology described
new attributes
., 2006
existing ones
relations involving
30 minutes
processing level
discourse understanding
virtually every
extensive testing
output symbols
verb conjugation
also confirm
classification ).
also exist
new words
many diverse
must pay
sequences consisting
overall effect
). usually
analysis program
computational point
incremental interpretation
phrases used
identification algorithm
also allowed
limited range
go home
results significantly
partial orderings
pairs obtained
weighting method
., 1975
words marked
context .)
brought forward
translate english
object must
method involves
may differ
atomic formula
particularly suitable
evaluate systems
variable z
sentential structure
would certainly
preceding verb
single tree
electronic text
domain constraints
sentences spoken
well using
help solve
constraints might
hidden layer
longest common
different authors
discourse goals
users interact
goldstein et
briefly present
following procedures
may specify
nlp task
single utterance
complete systems
') e
considerable amount
entities ),
l ~&
target positions
nouns may
several english
regular words
,' l
important aspects
promising way
higher accuracies
annotation projects
less well
single dimension
de chaque
includes knowledge
retrieve documents
certain situations
biased toward
3 seconds
daily basis
questions like
single list
example ).
accuracy measure
two spans
previous ones
understanding components
strong constraint
web document
intermediate steps
produce large
square brackets
new state
intended reading
noun class
likelihood values
input texts
dependency theory
must also
template merging
speakers ).
given k
must occur
answer pair
la premiere
2c ).
pronominal reference
yarowsky 1992
stylistic differences
extensively discussed
thus obtained
four groups
method similar
resulting semantic
), providing
topic structure
elapsed time
grammatical theory
main part
units may
concepts involved
best combination
., 1994
somewhat ad
differ depending
approaches could
using document
one suffix
one p
extraction task
r ).
reasoning process
may ultimately
node immediately
average improvement
integer value
automatic mapping
professional translator
indefinite nps
retrieval time
corresponding part
obtain two
). conversely
r \]
assigns one
model derived
bene cial
-- noun
different datasets
best ones
node within
generated one
language specific
incorrect words
roughly 50
provides us
arbitrary strings
f \[
fundamental frequency
methodology used
four tasks
english penn
taggers use
wilson et
verb phrase
:= 0
problem .)
0 %)
\[ 34
appear ).
evaluation effort
grammar --
one table
construction grammar
4 characters
sentence cannot
also vary
every verb
minimal cost
grammar acquisition
passive ).
example 1
words ill
adjacent segments
certain category
increasing need
pereira et
2 points
call ).
entire clause
extended form
coordinated noun
articles ),
either within
3 explains
data via
propose several
r v
objects using
row 4
make several
living thing
might say
method based
table 14
provides better
category ),
2000 words
text based
basic processing
quite clearly
subsequent utterance
relatively unconstrained
third system
developed according
also addresses
many adjectives
p ',
therefore would
transitive relation
ralph grishman
fundamental task
potentially ambiguous
dictionary research
including many
r 1
p '(
large range
5 points
helpful suggestions
different annotation
many non
possible ).
modern linguistic
normal human
90 percent
weighting factors
interface provides
analyzed correctly
well founded
forschung und
generation involves
higher priority
negative effect
naive bayesian
system might
implementation issues
weighted features
wonder whether
also trained
initial study
remains relatively
tile object
also require
\]. e
text directly
c -~
1 u
one hand
reasoning systems
used consistently
surface subject
performance without
appropriate order
communication research
graph shows
larger context
often defined
separate set
task \[
possible error
basic concept
often refer
node n2
corpus show
central question
koehn et
infrequent words
always appear
better handle
across three
third columns
without losing
appropriate method
recognition hypotheses
pairs may
also dominates
better characterize
may influence
tile new
restricted type
final pass
file contains
\] ly
bilingual sentence
simple translation
two research
transduction grammar
existing work
input consists
spanish language
called generalized
strong points
efficient way
three approaches
semantic constraints
analysis without
basic linguistic
would run
glass box
arise due
human user
given concept
base query
distributional hypothesis
simple co
20 times
est en
train two
leaves room
use e
;. table
context sensitivity
error sources
;, respectively
new users
cat ),
new predicate
possible parses
following kinds
system outputs
), like
template elements
recall points
young et
designed around
becomes obvious
find suitable
complete control
semantic system
go ).
increase coverage
central importance
proved difficult
forward way
interesting way
1961 ),
rate ).
data \[
one --
given linguistic
could produce
also needs
following clause
first identified
move back
combined results
additional advantage
question mark
source language
pronominal anaphora
dragon systems
format required
tense ).
type corresponding
n \[
sensitive languages
preceding discourse
basic words
possible word
target texts
parser constructs
richer syntactic
compound word
key step
document information
surface information
provides enough
c ++
represent discourse
thus may
complex linguistic
matches ).
build upon
3 task
also specified
esprit project
previous work
different modules
section reports
lefthand side
ordered based
certain grammatical
also address
interpretation process
difficult one
might well
systems operating
specific points
best matches
\[ nom
almost identical
granted provided
parsing algorithm
grammar based
generation based
manually prepared
examples 1
correct use
bootstrapping method
reversed order
level rather
heuristics based
4 million
consistent across
complex domain
relevant factors
\[ \]
level 0
function fi
8 different
per year
appropriate temporal
every article
one mappings
set fill
common lexicon
\]). however
blocks world
possible class
relevant topics
particular nodes
backward algorithm
must assign
word consists
head grammars
specific noun
senses may
2 let
thank mr
c /~,
models ),
even given
lauri karttunen
syntactic processes
make direct
extremely complex
among speakers
function symbols
dimensional representation
based representation
difficult cases
major categories
cannot always
applied iteratively
approximately 7
corresponding path
textual features
another phenomenon
speci ed
complex words
grammar directly
free model
fellbaum 1998
made possible
x ,~
unsupervised techniques
poisson distribution
syntactically ambiguous
categories ).
may later
another node
wsj articles
undirected graph
structures given
phrases refer
., j
grammar uses
symbols may
constituent elements
cfg g
ullman 1972
efficient means
sufficient coverage
would mean
free form
adjectives like
\[ v
three variants
project described
second point
constraints allow
dimension reduction
easily ported
np complement
compound verbs
low precision
two subclasses
trigram word
general conclusions
language seems
relation involved
perform best
singular definite
efficiency considerations
relations appear
input data
without semantic
several sources
slightly modified
ibm translation
separate process
p <.
second model
particular characteristics
original documents
incorrectly classified
segment contains
20 %)
nominal head
lexical realization
), unless
human post
based analysis
algorithms based
research system
processing software
us refer
screen shot
information presented
inferences based
various pieces
documents judged
reduction techniques
complex models
semantically acceptable
second form
16 \]).
interpretation systems
errors also
comparative evaluation
minor changes
correct response
errors occur
brief explanation
physical proximity
mhz pentium
morphological categories
dialogue partner
language .&
control verb
three metrics
general level
\[ 1982
model shows
answers provided
second function
methods using
2 n
first baseline
different nouns
always agree
mean number
55 ).
prevented us
general cognitive
phrasal verb
final score
dependency structures
data according
individual constituents
object nps
semantic disambiguation
may think
dt jj
latter set
test instance
binary decision
grammatical structures
open problems
canned text
boolean values
enough corpus
two lines
performance model
alignment link
recent past
key problem
gure 1
model compared
company name
tree without
phonological processes
lexical component
compare favorably
narrowly defined
seems unlikely
john made
one based
another sentence
grammatical constraint
brackets around
3 indicates
attachment point
made according
nonterminal nodes
actual parsing
higher degree
2005 association
standard methods
syntactic interpretation
1 4
;. figure
), adding
three people
np ),
using annotated
functional composition
standard approaches
words increases
naval research
grammatical principles
)= 1
np constituent
recognition ).
two labels
minimum description
data consist
make little
20 iterations
level constituent
readable version
constructed examples
node 1
fundamental problem
text stream
close approximation
l ).
speech understanding
1996b ).
two monolingual
nearly equivalent
earlier study
word identity
investigated two
special tags
best parameters
one uses
example shows
contract number
three individual
seconds ).
l \]
important implications
generate possible
heuristic would
effective tools
meaning postulate
highly successful
mercury news
types would
less stringent
syntactic part
best performance
sentence x
subject pronoun
pairwise comparisons
year old
also non
en (:
many advantages
tagged examples
;, e
1992 \]).
using memory
tile knowledge
computational overhead
whose effect
|& quot
output language
phonotactic constraints
would bring
either 1
thereby creating
key aspect
remaining nodes
note first
section show
three attributes
research area
new sets
following sample
preference ordering
explicitly represents
social context
also compute
zero otherwise
briefly mention
may offer
due mainly
practical perspective
standard maximum
enough evidence
selection ).
deep processing
expressions like
main feature
position feature
unique object
model performs
english words
empty list
input semantic
easily obtained
system gets
global optimization
state constraints
3 represent
used features
higher frequency
must assume
current annotation
work discussed
letter sequences
present purposes
semantic knowledge
text corresponding
select words
corresponding feature
speaker ).
grammars like
much wider
structure \[
emotional states
whole process
many technical
general sense
computer technology
also discusses
unification categorial
might wonder
independent reasons
1992 2
first pattern
first term
using new
weighted equally
1984 );
rule might
mt output
coreference chains
template slots
standard phrase
), g
thus also
january 1
lower scores
spatial relations
method achieves
transform one
c ~,
conceptually simple
issues raised
6 \]),
conventional linguistic
u ,,
user evaluation
basic model
contribute much
information like
morphological phenomena
surface level
identify four
linguistically annotated
word lattice
system combines
either manually
appropriate places
1987 );
approach performed
bound algorithm
much previous
labels ).
typically produce
follows \[
left column
also seem
basic form
equally weighted
prior distribution
provides new
maximum recall
null ture
general picture
parsing module
turn boundaries
free rewrite
initial task
common english
three state
section defines
absolute sense
easily achieved
describes methods
uses knowledge
fully disambiguated
linguistic complexity
context alone
known fact
turn means
given model
several strategies
individual parts
prior work
texts \[
uses machine
\] used
word \[
related questions
average word
word speech
often require
des (:
based strategy
phonetic models
., text
category set
already proposed
derivational suffixes
preliminary studies
second column
accuracy significantly
three clauses
system configurations
infinitival complement
entire grammar
local structures
). subjects
bond et
verb frames
complex model
distance dependency
single interpretation
next sections
remarkably well
particular tasks
set consists
43 ).
either nouns
used first
., time
de mani
short sentences
local constraint
nous avons
word expressions
significant portion
28 ).
sentence numbers
theory provides
classifier combination
example showing
directly related
c ~'
word substitution
al ,...,
different phonemes
second occurrence
12 ],
desired result
correctly identified
similar structure
): l
different source
words represent
specific heuristics
present model
system component
merging process
used 10
output contains
underspecified representations
embedded within
arbitrarily complex
may convey
null ple
2004b ).
oriented system
string like
answering questions
found two
two scores
30 ),
single part
somewhere else
present form
system );
overall evaluation
combination would
many english
various corpora
constant value
\] u
loan words
), whether
additional complication
correctly resolved
arbitrary choice
basic problems
likelihood estimate
times greater
several factors
subsection 3
also consider
several topics
classes according
syntactical analysis
semantic hierarchy
check whether
structural complexity
human mind
defined classes
first cluster
monolingual data
ranking score
dramatic increase
simple probabilistic
nominal modifiers
6 evaluation
representation ),
news corpus
linguistic tools
tree built
use decision
general background
second factor
1 st
likely sequence
-~ c
structure according
16 \].
careful study
different colors
one general
sentence types
experiment consisted
token identity
using explicit
vary significantly
3 iterations
also compare
creates two
also proved
framework described
stack decoding
class ),
one test
problems arise
desired answer
~& quot
relative order
directly based
systemic grammar
may learn
syntactic path
program ).
problems found
noise level
several days
\[ person
order relations
usually involve
semantic ambiguity
would lose
require much
xml file
logical representation
first result
syntactic grammars
de nitions
complete parse
new formalism
corresponding nodes
developing new
extract key
lower half
verbal stems
negatively correlated
complex problems
target expressions
following grammar
report two
rules specific
pour chaque
two user
correct tags
lexical descriptions
., c
basis function
explicitly encoded
linearly separable
vertical axis
1964 ),
also enable
symbolic features
discussed later
spelling changes
topic detection
provide greater
1988 \])
associated features
entire string
good reliability
conference poster
texts according
approximation method
fifth sighan
shown ill
model approach
slightly outperforms
also indicated
treebank wsj
takes two
parsing using
done via
analysis fails
semantic parsing
terms representing
first make
corporate designators
recent versions
may cover
order predicates
one correct
implementation using
remaining 2
systems capable
writing rules
atis domain
main characteristics
knowledge would
possible candidate
performance within
., q
classification approach
selected information
output representations
sure alignments
3 e
output quality
lexical element
contain domain
frequency word
results cannot
previously encountered
features must
different structural
relations found
empty productions
one place
relative frequencies
also deals
predicate structures
similar mechanism
using techniques
quot ;',
first answer
method requires
word clusters
incremental learning
far removed
semantic restrictions
contain words
style grammars
many training
soft constraints
english model
'. although
briefly summarize
japanese noun
ambiguity problems
associated press
general goal
incorrectly identified
idiosyncratic information
x 100
right processing
7 5
based dialogue
optional arguments
examples showing
strategy adopted
pronominal forms
one corresponding
touch upon
used either
language use
individual sentence
formal query
general framework
describe experiments
simple data
around 60
obtaining information
considerable interest
matching methods
x );
system available
surface strings
linguistic cues
using knowledge
6 cases
online learning
time would
passage retrieval
possible antecedent
words including
timing information
framework used
using machine
based semantics
end positions
wide scope
two areas
requires two
linguistic study
two explanations
annotation efforts
subject must
inversely related
partial structures
operate upon
always less
finitely many
several statistical
state technology
generic dialogue
), none
two lexical
general unifier
also determined
study indicates
space available
89 ),
verbs may
c e
certain applications
five days
thei r
training part
phonetic form
semantic associations
monetary expressions
let pi
core idea
corpus rather
minimal pairs
signi cantly
direct implementation
psychological experiments
trees t1
first language
largest possible
spurious ambiguities
). yet
automatically detecting
extensive training
languages including
23 %.
(( 2
two stage
achieve performance
nl interface
frequency statistics
also allow
also suggest
play different
translation word
learning mechanism
common features
set denoted
cannot possibly
). typically
one representative
us return
latter two
clustering techniques
understanding task
potential application
lexicon could
next cycle
generate text
000 terms
halteren et
occurring data
first noun
orthographic representation
improved models
question ).
1990 ))
quot ;~,
context may
simply defined
based wsd
following query
system selects
response generation
argumentative structure
tree sets
single path
automated techniques
text chunks
entropy principle
otherwise specified
still working
tightly related
point ),
received much
incident types
important respects
hand built
system features
every year
set \[
dialog management
text .)
living things
traditional approach
linear structure
second syllable
semantic markers
x 3
lambda terms
important goal
phrases often
syntactic feature
score measures
complex relationships
8b ).
used together
paper contains
another reason
input message
may also
automated knowledge
cases even
solutions may
}& quot
3 used
took approximately
evaluation exercise
right words
approach proposed
research laboratories
\] 4
information contents
gain insight
rules discussed
phrases headed
ranked sentences
l /,
la forme
found automatically
les r
two rule
problems faced
without producing
interrogative sentences
marked explicitly
automatic interpretation
negative one
typologically different
hierarchical levels
closer inspection
complete semantic
prepositional attachment
fall within
morphological variants
lexical choices
(: tion
might involve
occurrence information
lists several
compression rates
procedural knowledge
contains exactly
graph using
lexical components
1966 ),
full phrase
words tagged
constraints imposed
expression containing
goes without
certain order
obtain one
input question
north america
parameter k
avoid unnecessary
parser starts
given context
left ).
46 %.
fast match
specific tags
semantic lexicons
also suggests
many domains
evaluation results
acquisition phase
might actually
developing techniques
derivation trees
linguistic generalizations
statistical technique
many sources
automatic part
aligned english
interesting one
different approach
change according
thus indirectly
times per
automatically generating
including text
terms used
french word
relationships may
testing time
question word
within reasonable
,' 1
also apparent
main groups
-~ p
present within
simple substitution
already segmented
might come
ambiguity exists
disjunctive normal
morphological processing
24 %.
problem may
summarization techniques
much interest
protein name
sont les
validation results
contains non
baseline experiment
paper use
issues regarding
skewed distribution
l ~(
various nlp
derivational affixes
second ).
polysemous words
differs considerably
system first
go back
corpus ),
inflected words
global view
sentence contains
statistical models
multiple documents
line dictionary
interesting cases
human cognitive
appropriate semantic
linguistic decisions
main tasks
various points
improvement could
present context
arabic sentence
rather complicated
new word
brill et
information relevant
hill climbing
standard text
four times
improve information
classifier performance
., p
global feature
done according
support tools
independent grammar
ten years
ambiguity within
parse results
positive class
existing dictionaries
manual examination
small percentage
tailed paired
drastically different
recent version
presented evidence
words used
), many
resulting corpus
mapping process
'. thus
2003 ))
fairly accurate
tokens within
alphabetic characters
entries could
1 let
specific grammar
semanti c
constructed without
current document
atomic units
frequently enough
grammar .)
often hard
medical subject
performance ).
approaches \[
following features
results clearly
two aspects
one alignments
describe later
internal syntactic
tile basic
2 3
french verbs
automatically learned
new class
clearly demonstrated
none ),
seems natural
different conditions
2000 ).
research include
yield different
refined version
several previous
graphical displays
ones proposed
open issue
useful way
reasonable approach
verb stems
purpose language
partially funded
restricted subset
define two
across corpora
grammar constraints
system building
general linguistics
right association
also set
related works
file names
rules make
attachment ).
complete one
5 3
side effects
l )'
1991 )),
fewer examples
farther away
classes c
determine appropriate
reading time
explicitly state
first use
heuristic rule
candidate words
possible new
likely state
terminological knowledge
standard pos
enough information
shieber 1987
index number
syntactic nodes
difficult since
fully functional
whose semantic
input side
independent component
free parser
frequent verbs
difficult ones
system results
may prevent
interested parties
garden paths
4 also
certain positions
produce useful
recall measures
random sampling
hand column
retrieval test
generating summaries
statistical association
experiment show
national institute
four parts
1 n
become known
real texts
logical consequence
un livre
verb belongs
another recent
tile syntactic
decision process
th word
two reference
time pressure
system responses
qa ),
c /=
medical information
could reduce
second class
naive implementation
approximately 50
variables ranging
matching pairs
tile given
words making
adjacent words
section 0
classifier trained
inheritance hierarchies
resulting scores
considered irrelevant
concepts may
previously unseen
typically requires
training phase
one obvious
also satisfy
step ),
system tries
syntactic characteristics
1958 ),
dialogue system
l ')
1994 association
unification ).
likelihood estimation
1989b ).
large lexicon
three main
dictionary information
classification using
based baseline
oriented parsing
retrieval accuracy
elements together
filtering techniques
shriberg et
000 english
itr iis
verb --
correct hypothesis
single operation
clearly different
totally new
tag set
biology domain
following relation
classes instead
). basically
notice also
words remain
another tree
system either
conclusions drawn
dialogue management
classes whose
speech recognizers
intended effect
added value
computationally feasible
manual inspection
f scores
following subsection
shortest possible
working systems
knowledge represented
consider sentences
connected graph
np complements
structure whose
produce three
global focus
actually exists
george bush
another dimension
foreign word
clusters obtained
newly constructed
critical point
semantic categories
informal description
focus primarily
feature indicating
correspond exactly
caused problems
1 ,~
l .~
dependent lexical
another list
seven years
two information
tile probability
action verbs
2 %,
tagset used
mckeown et
official policies
different segmentations
better parsing
ten subjects
uses syntactic
data corpus
11 ).
syntactic preferences
processing mechanisms
example using
linguistic task
previous tag
input would
subject nps
11 \]
slightly differently
compare results
information content
statistical model
semantically anomalous
ben franklin
may eventually
daughter nodes
4 \])
could well
contrastive stress
speakers using
automatically constructing
either direction
expressions also
either true
especially considering
3 4
hmm based
mentioned previously
uchimoto et
say whether
bayesian learning
accuracy rates
directly involved
classes would
detect errors
b \],
represent complex
exactly matches
worth exploring
significantly reducing
german ),
de souza
based smt
second paragraph
grammars also
basic modules
hierarchically organized
special value
common way
following results
preliminary evaluations
rich language
., figure
original english
structure would
various reasons
nps ),
present ).
variable r
ways --
average across
solely responsible
nl processing
overall agreement
like algorithm
many factors
somewhat different
best tag
1993 )).
occur often
object role
della pietra
speech classes
infinitely many
single nodes
many topics
processing programs
appendix 1
thorough analysis
computational properties
greatly reduced
cn ).
data reported
il est
quite interesting
\[ z
dictionary entries
tree ).
compare two
several open
similarity values
future versions
therefore take
processing application
sentence interpretation
brief outline
section ),
inflected form
bits per
sentences must
would handle
based pattern
based term
major point
target side
long ago
null g
unification parser
another challenge
restrictions may
search procedure
two immediate
language grammar
retrieved using
richer model
worth emphasizing
event time
proposed translation
two items
assign grammatical
2 first
c 4
spurious matches
ambiguity ).
total corpus
pa 19104
following two
called synsets
hard cases
wu et
well recognized
current approach
macleod et
last character
tokens ).
subcategorisation information
include lexical
user says
generic term
recall figures
german lexicon
\] points
past years
), maximum
primary means
8 )).
slots filled
acoustic signal
reliable method
majority voting
separate semantic
two rules
several evaluation
german syntax
transfer lexicon
state may
omitted ).
conversational partner
significantly outperformed
hyponym relations
1996 ).
would translate
87 ).
x )=
algorithms differ
arcs labeled
retrieving documents
various heuristics
simplest form
frequency counts
translation candidate
linguistic discourse
). 5
1996 \]
whole data
87 \]
level control
preference information
many spurious
., 1983
., based
open source
natural solution
alone may
two final
morpheme boundary
100 articles
limited set
16 times
evaluate various
everyday life
core lexicon
mail address
structures built
helped us
also expected
vector space
original query
(\[ 1
standard algorithms
), phrase
theory behind
formal way
novel words
semantic relevance
providing additional
35 %,
one popular
designed based
may contain
problems described
disjunctive feature
word delimiters
). different
representation systems
ways similar
specific dictionary
algorithm --
paraphrase acquisition
related verb
system operating
one vertex
human translations
language component
became apparent
lower level
2002a ).
two pages
corpora consisting
semantic interface
containing non
contained words
null ing
name may
new document
generation technology
historical reasons
existing set
post office
examples using
simple question
fundamental principles
est une
better score
original training
tagging performance
structural representation
w2 ),
inverse document
clusters may
modified versions
particularly attractive
second person
chinese translation
semantics allows
values returned
word fj
tree derived
using simulated
\] present
2004 )),
computational approach
., verbs
rate using
g ),
strong relation
relation may
full text
annotated manually
similar ones
systems produced
english ).
rules use
semantics approach
direct interpretation
cases ),
identical word
.) although
leaving aside
end product
large english
left us
allow multiple
particular representation
provides details
e ):
word classification
algorithm involves
right column
last case
properly handle
causal link
many statistical
bigram statistics
measure results
section deals
powerful language
1 )&
additional classes
correct predictions
document relevant
language string
treated equally
15 \].
following way
news items
form like
cannot choose
-- namely
includes words
marked words
differences may
ce que
great interest
infinitival complements
support verb
using syntactic
processing methods
-- 2
might make
generator produces
ir engine
system exploits
input corpus
using combinations
method alone
50 %).
ambiguity class
satisfactory results
main advantage
usually thought
equation 8
example 11
make appropriate
example 13
many places
well studied
,~ 2
practical tasks
short strings
,, r
process becomes
algorithm maintains
many texts
various versions
order terms
representation method
going research
express constraints
three related
particular analysis
relations among
row 3
attention recently
thus taking
function tags
see shortly
., sentences
several thousand
p c
opposite order
preference rules
english parallel
performance compared
theoretical assumptions
two level
specific relation
key points
directly linked
susanne corpus
raising verbs
positive feedback
identify new
found one
one assumes
related task
hand ).
algorithm similar
one million
recursive descent
algorithm runs
\[ see
work focuses
dependency graphs
new candidate
wordnet database
logical theory
noun sequences
two trees
heuristic rules
see e
best set
x denotes
good choice
4a ).
must search
efficient implementations
open issues
rows 1
correct value
event whose
local grammar
computer interactions
model reduces
three word
p q
computer interfaces
). sentence
japanese grammar
particularly salient
explicit way
probability according
content representation
end evaluation
e ~)
significant difference
translation requires
sentence must
values used
also two
provide natural
large set
several simple
important distinction
system displays
discourse representations
performs significantly
e ),
critical factor
main role
quite encouraging
base structure
using probability
simple architecture
temporal logic
new properties
real speech
may match
specialized domains
entry contains
cartesian product
new kind
difficult even
change would
actual linguistic
tool called
probabilistic classifier
text production
.& quot
different operations
xml structure
else ).
word corpus
k --
large training
bin laden
ambiguity inherent
possible ambiguity
english parser
algorithm based
r w
l ;'
approach could
based formalisms
good translation
future version
types listed
makes use
first difference
yield improvements
high values
human conversation
short forms
n refers
right context
content analysis
engineering research
previous attempts
limited domains
accuracy improvements
syntactic analyses
manual review
83 \].
ideas presented
recent results
acquisition tasks
new variables
standard search
still somewhat
given expression
two pronouns
processing program
stressed syllable
covers cases
involves first
object type
;. furthermore
either using
internal arguments
depth first
paradigmatic relations
candidate sets
--- one
\] n
driven fashion
paying attention
clearly identified
provide semantic
entities denoted
syntactic description
takes one
classifier ),
important content
larger constituents
high baseline
global measure
parallel corpora
algorithm uses
since even
words .&
canadian hansard
handle sentences
using prolog
possible parsing
english nouns
bottom right
characteristic properties
complex expression
order logic
processing errors
independent components
translation equivalent
fewer words
recognition using
report describes
rank order
five cases
kupiec et
liu et
prima facie
existing applications
similar distribution
97 %.
automatically deriving
discuss problems
lexical verb
recognition techniques
problematic case
11 shows
major factor
thus eliminating
specific components
clear evidence
translation memory
baseline system
given source
vanlehn et
%. note
similar work
single utterances
cross validation
dependency features
explicitly mentioned
perl scripts
bene ts
highest values
tabular form
2006 ),
different languages
also note
morphological generator
model underlying
final point
language dictionary
distinct ways
useful resources
past several
single measure
often written
successfully tested
existing systems
addressed using
grammatical features
di erence
least upper
figure 3
web interface
x =~
un tel
document content
us denote
current discussion
next possible
linear combination
acts like
.) thus
roughly corresponds
column lists
also necessary
sentential complement
usually called
knowledge information
next sub
cannot predict
cannot appear
remaining three
one layer
many previous
number correct
final model
., 1974
requires one
reduce noise
process also
comparative performance
using examples
even within
selection techniques
four stages
cannot understand
modified version
might help
randomly sampled
alignment problem
initial segmentation
involving non
semantic entity
final form
transformational theory
contextual evidence
individual language
language descriptions
translation system
combination rules
ap ),
positive polarity
successively larger
still significantly
previous applications
null ation
office environment
greatest number
imposes constraints
berger et
structures within
\[ 11
structure represented
modal verb
\[ 13
little linguistic
include information
term ),
state q0
simple matching
grammatical descriptions
using dynamic
trees --
decision made
batch mode
latter one
discourse interpretation
software packages
provides examples
chunk boundaries
syntactic cues
component technologies
performance varies
years ago
one accepts
find semantic
system implemented
entire space
smoothing parameters
chooses among
also appeared
actually produce
different similarity
\[ 1990
formed according
previous systems
phrasal structures
ibm research
current techniques
computer program
context words
assigning semantic
four senses
several existing
abstract representation
quite large
28 aot
one case
task performed
negative ones
250 words
matching algorithms
provide one
core set
subsequent utterances
also common
specific value
objects called
language technologies
preferred antecedent
brief sketch
verbs without
large gap
users often
two strings
supervised training
method may
additional types
overall number
generate sentences
classes correspond
second approach
various grammar
practical tool
tests whether
another perspective
whereas one
text may
different sentence
false negatives
works like
sequential model
real world
communicative action
certain semantic
great importance
factual questions
new elements
may run
multiple views
current knowledge
context sensitive
pocket dictionary
several patterns
\[ w
best systems
step process
considerable number
single description
monolingual corpus
given preposition
scores used
current proposal
tag per
perhaps due
interesting since
significantly higher
weighting schemes
1 3
become quite
unsupervised technique
rule means
poor performance
simple classification
p p
expression representing
conducted another
grammatical relation
context without
information around
aligned texts
useful one
... xn
approaches would
including verbs
context models
preceding clause
possible score
english expression
different sequences
single entry
chose one
generally lower
modeling framework
parallel structures
similar performance
average length
semantic processing
prior training
appropriate inferences
also automatically
18 \].
precise specification
another part
related problem
measuring semantic
contain word
previously studied
applications require
linguistic semantics
considerable work
). actually
new object
first translated
physical location
feature also
term appears
13 \]
previous method
text becomes
original two
boolean feature
problem domain
nouns ).
theoretical research
sports co
semantic contexts
computational techniques
user interaction
speci es
many tokens
topic ),
head constituents
right ).
remaining problem
reduction technique
frequency thresholds
unified medical
already made
training utterances
experiments discussed
tile linguistic
task information
include rules
various sizes
translational equivalence
cluster analysis
vocabulary v
nominal compounds
resulting rules
cannot infer
new system
model includes
figure 19
semantic concept
two tests
many natural
testing sentences
one obtained
inference mechanisms
qa systems
local features
concrete terms
negative integers
structured knowledge
), text
model given
system capable
analysis requires
7 )).
query system
words present
que le
quite satisfactory
expression e
arises due
introduction ).
best method
strategy works
functional unification
50 sentences
general approaches
japanese dictionary
darpa spoken
naval ocean
learning parameters
acquisition tool
), time
constraint logic
similar behavior
three input
set generated
state sequence
still quite
); b
z ',
woods 1970
lexical head
based n
common evaluation
corresponding type
syntactic dependencies
dates back
separating hyperplane
learning performance
marcu et
variable bindings
dictionary definition
cannot easily
model provides
structure rule
main goals
interactive presentation
systems show
assigning one
result suggests
semantic distances
randomly divided
might explain
often results
compact form
transfer process
accurate information
based representations
may mean
made difficult
sufficient expressive
arbitrary text
semantic domain
every clause
feature represents
problem without
trees based
specified ).
multiple english
retrieval experiments
enables users
many speech
stative verbs
also generates
(~ 0
integrating speech
three major
word refers
around 100
provided training
language components
higher weight
phrase marker
without reference
additional language
final column
available training
richard schwartz
take us
relevant ones
modal auxiliary
grammar may
different relative
adequately model
generation may
could actually
detailed analyses
pragmatic reasons
relations defined
following pair
many semantic
proposed methods
heuristics may
principles underlying
markov modeling
text categories
conjoined phrases
human target
grammatical choices
3 n
four phases
training database
user behavior
la classe
define one
similar idea
increasingly popular
segment containing
many expressions
descriptive adequacy
), pos
worth investigating
system implements
example given
uses linguistic
). neither
hoc queries
per query
genuine ambiguity
average case
took place
binary features
bigram counts
advancement organization
syntactic role
nam e
external sources
intuitively appealing
retrieval technology
ranking model
negative logarithm
1 %,
adverbial clauses
issue concerns
basic approach
perhaps surprising
figure 10
highly variable
synonym lists
intended one
several difficulties
overall performance
presents another
systems work
different gender
~' r
discourse must
argued elsewhere
version 2
worse results
anything like
users might
generating texts
may decrease
work involving
based heuristics
longer possible
equal status
first hypothesis
fully grammatical
best recall
annotation would
us first
\]) l
columns 2
new area
wsj ),
necessarily representing
dialogue may
resources like
ranking function
really make
one frame
). clearly
semantic models
translation knowledge
parser produces
syntactic constructs
specific requirements
since non
2004a ).
given attribute
evaluation indicates
algorithm achieves
calculate similarity
network contains
several additional
perform two
washington post
imposed upon
identify important
right parser
always exist
either sentence
date ).
byrd et
medical diagnosis
frequency events
full search
common one
already know
sentences 2
would differ
sur un
correct decisions
25 words
). whereas
algorithms may
english preposition
morphosyntactic features
much work
thus improve
structures ).
without information
almost entirely
using k
whether human
perhaps using
e set
achieved results
determined using
john wants
interpretation would
data items
ten different
work supported
lfg ),
first page
performance due
predicate adjective
reuters corpus
current parsing
chinese data
reduced form
error detection
interesting work
84 \],
stop words
expression used
quantified np
resulting trees
son names
statistical parser
common structure
semantically annotated
seems therefore
stochastic process
science research
relative importance
would specify
hypotheses generated
inflected word
competitive results
85 \].
), q
least k
precision rates
main parts
model b
analysis system
best overall
possible syntactic
extensive evaluation
german compound
initial testing
example 14
slot filling
possible combinations
tables 2
initialization step
phone numbers
resolution procedure
ldoce ).
major modules
maximum matching
different instances
text also
plain text
indirectly via
identically distributed
semantically equivalent
name entity
strings consisting
require extensive
present approach
syntactic bracketing
less computation
another path
possible given
weight vectors
contains two
success rates
processing text
system structure
actions associated
likely word
n rules
summaries based
fully determined
extracted phrase
main problem
verbs taking
hmm system
one may
edges e
different mt
use training
cases could
quot ;~.
phenomena related
standard test
situation occurs
user needs
2 seconds
make use
1989 \],
three properties
processing time
tree approach
languages use
clustered according
binary relation
time periods
certain assumptions
tagged manually
given conditions
also briefly
), instead
used e
central goal
others may
syntactically parsed
arpa order
every constituent
slots may
actually performs
one target
constraints provided
german federal
large effect
two heads
next question
one child
candidate parses
time adverbials
supervised method
verbal forms
resolve lexical
c2 ),
several criteria
entire structure
aspectual classes
3 outlines
entry must
inference methods
next time
rare cases
b ):
5 seconds
often necessary
\], among
various features
ge et
network representations
c ,~
section illustrates
let g
weighted combination
new techniques
two particular
correct verb
many clusters
three variables
many hundreds
obvious problem
third point
also implemented
major class
types like
many cases
classes corresponding
complex conditions
statistical filtering
internal model
inheritance networks
often confused
weights w
well within
complementary information
answer ),
natural text
different alternatives
significance levels
linguistic theory
partially correct
objects must
~. e
useful patterns
text engineering
structures derived
following partial
1 ],
dashed lines
identified based
scoring scheme
tagging scheme
user clicks
previously built
morphological word
different environments
like sentence
average score
random variables
symbols ).
node cannot
suggestions made
72 %.
generation mode
overview ).
several steps
new knowledge
imperative sentence
also changes
provide consistent
average ratio
formal structure
single unified
r b
terminal category
38 %,
substantially larger
idf weights
relevant work
full scale
2 explains
15 times
relations exist
input information
could modify
valuable source
previous studies
sentence beginning
get high
using concept
without modification
terminal symbol
many simple
many distinct
40 words
training algorithm
pages containing
1 results
appropriate number
). sometimes
john arrived
model depends
produces semantic
former type
full word
whose answers
first parameter
actual generation
method considers
automatically build
search methods
would thus
personal name
never occurs
longer phrases
choose either
symbols v
using linguistic
versus non
palo alto
good data
basque country
international conference
tile model
concrete objects
also change
similarity measurement
corpus automatically
structure information
development system
find tile
management task
main challenges
rich feature
accessed via
results must
mt technology
generally regarded
less structured
tile form
perfect tense
semantic point
http ://
seven different
among possible
wahlster et
branching factor
pos feature
reliable evaluation
attachment disambiguation
theoretic terms
grammar books
testing service
find patterns
somewhat simpler
commercial machine
trigram probability
right boundary
b ),
singular pronouns
common usage
word lengths
correctly translated
incorporate features
use features
human judges
following structure
em training
higher n
restricted version
must check
2002 )),
sufficiently powerful
final syllable
human expertise
possible extensions
categories given
multiple domains
rst relations
surprising since
3b ),
line indicates
simple lexical
basic notions
two interpretations
annotation schemes
previous efforts
multiple sources
80 sentences
system provided
first created
performance using
research purposes
four points
syntactic representation
semantic form
sentence 2
key difference
general setting
~, u
score increases
hpsg \[
discussed next
also capable
new events
future tense
tree families
independent recognition
less preferred
first 2
accuracy improved
semantic function
overlapping phrases
run ).
systems trained
training materials
also go
particular values
word information
whose argument
extract noun
natural extension
typically includes
database schema
work together
close enough
shown schematically
particular senses
large texts
whose number
polynomial function
verb endings
one mode
five steps
encoding schemes
batch processing
adaptive language
\] 3
syntactic type
fundamental difference
unit corresponding
1992 4
actual context
grammar engineering
term selection
logical conjunction
may lose
using models
syntactic analysis
14 ):
incorrectly labeled
test ),
word processors
introduced two
phenomenon occurs
x 6
word correspondences
models achieve
), p
discourse marker
three elements
conventional approach
matching sentences
frame ).
relevant parameters
heuristic procedure
15 %)
language containing
annotation guidelines
anaphoric elements
relative ordering
bootstrap method
dialogue phenomena
resource requirements
non --
means algorithm
person may
usually translated
extraction component
scheme allows
last iteration
relatively accurate
clustering words
give several
simple rule
computational lexicons
less restrictive
four hours
phrases appear
classification tree
easily imagine
structure rules
model outperforms
matrix c
grammar covers
list also
sentences contained
original parse
sophisticated methods
-- null
one also
operation would
alternative models
models fail
models need
subjective evaluation
unigram probabilities
15 different
course possible
search errors
adequate way
theoretical frameworks
sophisticated approaches
binary value
email messages
7 ~.
potential user
parse chart
r f
valuable feedback
upper model
model estimates
hard work
quite likely
initial alignment
phonological representation
elaboration relation
clarification dialogues
work toward
200 words
9 %.
w occurs
generating natural
lexical idiosyncrasies
\[ b
interest group
enough detail
structural organization
messag e
r r
root words
former problem
experiments use
correct head
two closest
high quality
various strategies
style analysis
contain enough
generalized quantifier
overwhelming majority
mitch marcus
made sense
standardly used
word wall
hand annotated
near real
appropriate rule
', since
stored within
third column
includes four
many existing
constraints among
various components
lexical property
document pairs
us understand
date expression
may improve
way people
new parsing
discourse focus
convey information
word entries
context within
example tree
trainin g
system summary
precision values
disambiguation tasks
model still
highly sophisticated
human interactions
14 ),
assigned two
given using
telephone lines
contain variables
database search
inanimate objects
recent evaluation
sentences consisting
many patterns
63 %)
descriptively adequate
also one
global discourse
generative devices
factors like
1971 ),
loosely defined
would think
word token
translation score
different solutions
global analysis
two relevant
exponential increase
word usages
case c
systematic treatment
tells us
performance comparable
initial effort
different subsets
larger values
2 ].
processing algorithms
linear functions
veri ed
helpful feedback
e *,
current mt
second important
free languages
three columns
modified value
matrix whose
dependency parse
categorial information
everyday use
significantly slower
arbitrary depth
particular term
12 \],
terrorism domain
). language
even smaller
distribution across
current syntactic
translation lexicons
last phrase
34 %.
vp complement
analysed text
also cases
time span
larger classes
;. also
great number
attribute values
german texts
identify syntactic
relations specified
rule describes
optimal level
5 ].
less constrained
similar terms
around two
take precedence
ordered sequence
recall increases
verb forms
). perhaps
third factor
average probability
2 6
different numbers
slight difference
greedy method
much bigger
cpu seconds
user ),
2 indicates
includes 1
making explicit
better models
general text
coordinate conjunction
networks \[
single analysis
4 table
specific task
standard unification
questions based
linguistic structures
parse produced
containing two
hoc solutions
based ranking
f ))
statistical modelling
research problems
existing parsers
final class
proposition 2
still allowing
probability scores
structure level
svms ).
one solution
features represent
5 6
part 3
5 indicates
information theoretic
one exception
step consists
since english
planning system
whole corpus
singular forms
common among
use two
way without
lower cost
lambda expressions
structure trees
linguistic principles
fast enough
4 %).
similar distributions
problem mentioned
yields two
2 ~,
first row
node x
proper order
situation arises
possible actions
denote different
functions include
neural nets
entropy method
old man
existing natural
linguistic observations
est la
explicit confirmation
remaining 4
text planning
based measures
attachment ambiguity
present proposal
uniquely determine
rules specifying
v [?]
embedded sentence
approximately 15
would include
word category
presented later
operator must
semantic relationships
entirely based
ne sont
every string
using model
come next
based oil
shows three
would score
language pair
behind using
stuart shieber
therefore also
either x
based view
20 %,
several relations
vp ),
long documents
3 rd
different tree
ru \]
system produced
extracted text
unification operations
bearing words
), figure
sentential complements
multiple topics
used decision
known method
level patterns
description ),
limited semantic
first possibility
another issue
approach \[
nine words
conversion program
analysis alone
discussed previously
simulation system
existing speech
basic operations
likely candidate
segmentation model
full document
two taggers
afin de
\[ r
possible derivation
systems rather
limited resources
link information
tile context
sentence contained
shorter sentences
every training
structural constraint
rules created
cannot hope
also follow
made experiments
proper translation
task like
state corresponds
). morphological
syntactically realized
documents contained
noisy channel
existing semantic
otherwise stated
answer set
6 ),
case would
resolve anaphora
educational testing
category np
edge labeled
fully parsed
size ),
rules 2
process text
memory capacity
many difficulties
free skeleton
highly subjective
thus better
notation ).
goes back
randomly selects
message processing
term frequencies
linked list
one sequence
4 --
c );
procedure would
thus creating
identify proper
automatically identifying
particular types
term co
figures 8
two chinese
still remain
text classifier
3 3
semantic compatibility
almost exclusively
method relies
main stages
value n
phonetic alphabet
three slots
categories associated
chosen based
40th annual
english using
comparisons among
relative scores
simple information
sun 4
features play
highly indicative
explored using
error prone
target category
n ))
first study
three techniques
surface text
morpheme sequences
module might
useful since
semi -)
detailed analysis
individual feature
development center
pearson correlation
first priority
underspecified representation
dependent upon
might constitute
data like
two content
;.& quot
functional linguistics
test item
would yield
hypothetical example
require special
)= f
systems become
quot ;~-
corpus resources
sentence consisting
c denote
interpretation rule
acquisition problem
usually takes
), based
reader must
verbs ).
would inevitably
natural sounding
directly usable
number indicating
processing --
additional computation
robust text
alignment result
also fail
systems seem
also propose
thereby reducing
classification technique
linguistic concepts
statistical approach
typically associated
translation given
three knowledge
could help
possible links
root form
several parameters
contain new
uses context
two typical
show significant
three machine
96 ).
consider whether
new opportunities
agreement scores
section 20
evaluation methodologies
shows example
better precision
right direction
sense may
first suggested
given situation
morphological properties
assigned automatically
question answering
best speech
rules together
two lists
already occurred
context ).
hardly ever
programming systems
4 figure
investigating ways
c 3
provide answers
application needs
first identify
), uses
39 ),
000 distinct
4 introduces
frequently associated
8 \]).
theoretical account
key element
depends solely
one complete
reference sentence
one purpose
could expect
candidate antecedent
theoretical aspects
conditioning information
phrase consisting
applied two
), verbs
processing involves
larger numbers
given node
first split
also noticed
better model
first developed
method without
large speech
original one
v 2
last state
text without
would fall
., case
research within
weight given
entries whose
complex situation
would learn
classical example
intonational cues
\[ chomsky
one entity
achieving good
\[ k
occurrence data
place within
500 rules
relations might
w occurring
correctly detected
separate rule
mel '~
complex languages
large semantic
tree notation
systems ),
lexieal item
following additional
classified correctly
file name
significant generalizations
diverse information
achieves high
completely ignored
initial version
translation probability
audio data
case labels
irregular morphology
initial grammar
two standard
words 3
negative ),
segmented data
expressions --
important future
labeled recall
positive instances
8 2
); hence
structure presented
1982 ),
l r
actually using
real number
earlier versions
language task
speaker intended
improvement would
knowledge engineer
comes closest
distance matrix
international business
elementary trees
dialogue processing
expected values
simple rules
ambiguous phrases
), others
test set
output representation
important contributions
argument applies
main drawbacks
contextual effects
5 iterations
grammars --
match would
particular language
must specify
possible linguistic
type ii
absolute improvement
turing smoothing
bnc using
discuss issues
much broader
features described
template slot
w }.
800 sentences
usually done
packed representation
occurred within
%. finally
pragmatic features
three events
occurs immediately
simplest approach
quot ;-
involving two
sentences randomly
construct new
), sentences
simple modification
3 %,
strong clue
later use
programming method
null c
token level
particular noun
[?] log
four papers
closely matches
desired action
occurs within
e et
july 2006
argument roles
deep knowledge
corpus one
pronouns may
similar objects
integrated treatment
propositional information
1987 \],
keyword list
simply note
parsed using
information --
secondary role
feature corresponds
automatic analysis
xml documents
fair number
method outperformed
search time
performance relative
detailed example
extraction results
problem ).
alternative sets
short vowel
new syntactic
report performance
score threshold
2 e
appropriate rules
somewhat unusual
given sentence
could simply
email message
verbal categories
highly unlikely
expressed within
single root
subcategorization features
must somehow
sequence data
action may
compound words
f g
paper reviews
two dependency
short phrase
predictive power
correctly predict
two programs
greatly increase
computationally efficient
often less
semantic annotation
strategy described
sense distinction
strongly suggest
manual disambiguation
assign semantic
machine readable
bonnie webber
judges agreed
dependency structure
language independent
following criteria
incorporating semantic
selected according
grammatical theories
common ones
occurrences ).
individual nodes
near 0
choices made
,, v
previous evaluations
processing based
positive correlation
thousand words
performed independently
grammatical structure
move forward
es ).
system combination
c features
several things
another measure
declarative nature
corresponding term
labels associated
punctuation marks
50 million
additional feature
a0 a1
frequent categories
,, 1
structurally ambiguous
using confidence
perform one
linguistic unit
broad semantic
possible models
resolve pronouns
grammar without
parsing phase
first action
partial parse
temporal connectives
similar set
thus requires
significant degree
time frame
significantly outperform
suffix ),
related formalisms
otherwise go
[?] h
mainly due
words written
translation examples
). hence
could explain
key components
wide spectrum
randomly choose
selected pairs
user makes
clauses like
subsection 4
one third
well suited
np np
new probability
probability assignment
initial query
features used
partly supported
weighting scheme
modeling approaches
whose entries
resulting translation
relation exists
standard forms
criteria based
preceding two
rare ones
one classifier
object noun
scale grammar
large clusters
general schema
constraint propagation
background material
object oriented
lexicon consists
sometimes necessary
classes like
correction techniques
different instantiations
weights associated
semantic ontology
using context
language differences
national library
around 20
one individual
f ',
resolution methods
therefore required
9 1
incomplete constituents
initial training
first verb
prepositions like
structure like
physical world
models generated
manually sense
\[ joshi
four tags
tbr example
contains one
simply means
directly encoded
expressions based
third class
ill addition
1989 ):
verb frame
document would
candy sidner
stochastic parsers
develop models
original position
represent either
strongly suggests
appropriate category
figure 1b
information represented
external resources
1993 \].
last ten
25 ).
basically two
rules containing
interactive query
improved using
). part
unlike english
also belong
may translate
one conclusion
entire range
grammars based
step one
dictionary data
sequence \[
25 \]
subsequent sentence
trigram models
learning set
3 ],
2 used
complex queries
thus provide
evaluation criterion
open classes
cyk algorithm
strategy used
relevant terms
newly added
lower left
routes les
alternative hypotheses
good coverage
last line
set forth
rationale behind
new nonterminal
b --*
n g
concepts ).
feature templates
telecommunications advancement
relative increase
human speaker
information based
words observed
transformational grammar
morphologically related
empirical data
rules describing
atn formalism
assign one
). multiple
aho et
without detailed
relatively higher
alexandersson et
tile parser
morphological system
texts like
one match
therefore assume
3 results
give different
never actually
consider also
analysis given
low values
require manual
see equation
accuracy measures
one without
applying semantic
lexical structures
word definition
deterministic finite
based computational
words first
two methods
positive examples
every two
information directly
columns represent
perplexity 60
particular use
verb may
research ).
adverbial adjuncts
also improves
many answers
exploring ways
main types
still seems
animate subject
taken advantage
currently implementing
progress toward
word meaning
evidence indicates
functional categories
describes experiments
different processes
ci ).
semantic class
90 %.
main result
one state
coherent segments
deep parsing
phonetic similarity
time proportional
wordnet senses
following ones
different entities
full system
), corresponding
naive subjects
phonemic representation
earlier version
new examples
whose category
factoid questions
human users
approximately 80
manual creation
local structure
segmented according
relevant grammatical
discourse cues
first task
either positive
1989 ),
two classifiers
theory allows
like language
pragmatic components
similar process
g \[
x \],
100 ),
cause problems
originally created
structures already
system tends
proven effective
please note
query interface
fully automatically
high costs
system networks
transformational component
three segments
must pass
would put
probability information
line 4
crude approximation
certain lexical
certain predicates
additional nodes
seems particularly
inference algorithms
stop word
meteer et
two left
long history
n3 ),
search would
n ',
underlying language
application ),
induction algorithms
interpretation rules
correctly identify
items appearing
powerful tools
typical semantic
karlsson et
figure indicates
1978 \].
would start
value difference
available resources
controlled way
enables us
language l2
also find
), 7
useful even
offers several
simply stated
capture regularities
collection contains
source files
novel approach
40 ).
seven types
possible model
null table
software system
far beyond
semantic filtering
candidates based
two phrase
previous question
match scores
sequence according
data relevant
semantic closeness
sentence representation
x axis
;. l
words results
multiple tokens
image processing
constructed using
q ~.
extreme cases
explicitly specify
quirk et
line corpora
conducted two
learning process
), except
quot ;/&
partial evaluation
world application
restrictions ).
procedure uses
form larger
four measures
similar systems
different perspective
english queries
tagged using
particularly difficult
05 ).
better prediction
specifying whether
table 5
needed information
separate entries
parse structures
j \],
using clustering
separate rules
roughly equivalent
input modalities
sentence type
tutorial system
another algorithm
contain important
sont des
using four
%, respectively
grammar corresponds
factor analysis
placed within
morphological disambiguation
way forward
many relations
smoothing method
two errors
significant differences
sentence analysis
truth conditional
along three
current linguistic
verb categories
first type
data mining
1 ].
state ).
model constructed
consists primarily
class classification
heuristic methods
matching strings
-- indeed
parent category
let j
least four
expected answer
).& quot
different part
main noun
automatically construct
phrase constituents
certain circumstances
multiple speech
processing capability
definition 7
first three
parsed sentence
process --
correct decision
one predicate
inherent limitations
intelligent tutoring
uniform distribution
also reports
lexical concepts
words either
make choices
though less
,' r
)-( 4
third person
1 6
full disambiguation
following formula
labeling system
overall process
structures using
1 indicates
multiple semantic
correspond one
constraint satisfaction
extensive work
markers ).
relationship exists
tree given
extremely limited
statistics gathered
always available
interactive computer
predicate structure
sentence candidates
one run
word following
possible problems
hobbs \[
sentences including
performs morphological
order according
learning systems
tense form
solid line
subjects used
seems intuitively
identical syntactic
unless one
performed according
one point
tags used
rightmost noun
similar method
[?] k
algorithm may
text generator
using giza
accuracy figures
syntactically distinct
usually several
%. since
two options
np argument
4 7
another advantage
-- 4
product line
single symbols
driven systems
ir system
bigram probability
arbitrarily chosen
presents results
process involves
constructive comments
syntactic pattern
called feature
using 1
also generated
corresponding logical
forces us
relation ).
pattern used
formal specification
form c
experimental setup
direct representation
using parallel
free backbone
make predictions
subsequent mentions
null tactic
three basic
respectively .)
significantly reduce
describe related
correctly found
semantic expression
must include
agreement information
two varieties
logical relation
computational linguistic
substitution nodes
would eventually
clustering ).
derived features
language described
comparing results
conditional markov
exact string
model within
include many
techniques cannot
least 1
partial structure
typical cases
close connection
early stages
morphological knowledge
meaningful way
thematic role
9 shows
assigned one
every member
correct spelling
tile source
x ...
directly without
l /.
one line
last mentioned
verb also
separate category
type --
ascending order
transition network
certain aspects
classification phase
optimal choice
1 respectively
plural ).
lc ),
12 ):
200 ).
2 4
alternative parses
substantial improvement
initial results
many parameters
first examine
cluster contains
5 decision
sub -)
user gives
optimization algorithm
also intend
root ).
presents two
disambiguation step
wsd ),
representation schemes
appendix b
model context
previous research
additional computational
two months
words together
extracted word
using finite
minimum distance
planner uses
short text
used rather
suc h
subsequent stages
merging information
used non
level relations
labeled data
focused primarily
3 means
cosine value
learning method
process similar
(( n
english mt
full knowledge
dorr et
lookup process
ordered sets
5 4
c denotes
approach would
wrong result
comparing two
hwa et
three syntactic
entire documents
possible sets
spoken english
different weights
incomplete sentence
similarity measure
terms appear
around one
frequency words
ranking algorithm
comprehensive set
driven method
resulting dependency
difficulty arises
kappa statistic
may depend
may provide
many possibilities
may encounter
experiments involving
extracted based
tool development
fast parsing
h ~,
first example
one comes
testing whether
complete definition
etc .),
situation types
language skills
right node
output files
base ),
given x
containing one
ney smoothing
analyzer based
long tradition
traditional grammars
case assignment
applied natural
second tree
explicit relations
giving rise
action sequence
underlying representations
dependency arcs
~' 1
information corresponding
following proposition
lines 2
interpreted either
developed independently
speech database
incident type
harder task
., many
use less
computational models
combination methods
completely free
several dictionaries
\[ 1989
augmented phrase
outside probabilities
criterion used
second utterance
p holds
correct segmentation
relevant set
calculate precision
human dialog
explained earlier
). feature
topic changes
substantial variation
two short
correct rate
terminal label
process based
horn clauses
rooted tree
tree b
word choice
system trained
encoding scheme
current purposes
compare performance
generated data
multiple clusters
words appear
succession event
joint project
agreement rules
user control
still contains
wrong answer
promising method
lexical unit
statistical independence
larger project
20 seconds
features discussed
w2 ,...,
total cost
confusion set
generally known
recognition model
following parameters
find information
appear within
several grammatical
good evidence
quite straightforward
generated via
mutual information
inflected languages
main use
appropriate definition
would contain
form would
sun sparc
largely based
either expressed
27 \].
computer scientists
based study
increase efficiency
clear whether
process uses
): given
using pre
similar effect
computationally demanding
topic change
6 gives
main reasons
appropriate meaning
syntactic parallelism
research programme
different grammars
larger part
subcategorization requirements
rule number
research shows
database system
comparative studies
rule matches
3 displays
characters may
word translation
also matches
unsupervised algorithm
2 ...
developed methods
must learn
deep syntactic
three rows
satis ed
resource used
genitive case
twenty years
ordering within
major languages
full power
level performance
summary would
certain information
run using
use simple
higher value
paper described
information service
possible within
way similar
among lexical
also helpful
disambiguation problem
sentence including
high computational
system needs
less effective
approaches like
combine multiple
database management
conceptual information
pragmatic inference
mary likes
different definitions
morphological ambiguity
feature percolation
best matching
last month
also work
query processing
correct structures
sag et
queries submitted
translation models
handle phenomena
obvious reasons
still use
bottom row
computational purposes
work described
en la
defined three
inference based
names may
linguistic component
possible context
test texts
level morphology
overall recall
reduction algorithm
ir techniques
possible hypotheses
grosz \[
general assumption
retrieval engine
telephone conversations
enough time
parsing tasks
55 %.
problems involving
reference list
without prior
additional human
new concept
point since
u .~
sophisticated search
oxford advanced
four questions
strategy seems
decision rule
current model
case filter
[?] l
show results
two broad
whose definition
automatically generates
syntax ),
morphological parsing
recognition problems
various sites
one occurrence
martin kay
textual units
case information
specific corpora
feature based
originally developed
human labeling
sequences containing
700 sentences
function described
0 indicates
often find
two modifications
automatically infer
semantic properties
whose meaning
theorem 5
major causes
text search
new linguistic
daily news
coordinated structures
1997 ))
choose among
). starting
reference answer
effective features
together via
using linear
wordnet noun
simply one
french words
target translation
question words
simple procedure
7 \].
tagged sentence
original ones
document sentences
surface sentences
information embedded
linguistic string
1 e
immediate context
form p
best classification
discourse representation
realistic model
4 .)
highly salient
certain items
first name
full parsing
three examples
active sentence
unit may
grammar formalism
also counted
constructions could
first described
words containing
distribution given
single cluster
input chinese
illustrates several
typical patterns
existing corpus
third approach
local ambiguities
efficient method
trees may
machine language
different roles
n '~
still beyond
verb meanings
precise way
strongly dependent
previous accounts
directly corresponds
applied one
given tree
explore new
appropriate ways
relative pronouns
improve coverage
would match
swiss national
structural component
recent work
also took
let c
iff x
node must
-~& quot
certain rules
statistical significance
empirical question
frequent words
given utterance
parser works
translation purposes
earlier results
query words
method ).
consecutive words
competing interpretations
first tagged
increases exponentially
two text
morphological analyser
new technique
tree associated
next level
discourse analysis
similar texts
figure 21
computational model
also currently
two thirds
time required
usually quite
penn english
null cal
following another
us government
labeled according
user studies
work used
\]& quot
4 system
often ambiguous
every pair
clear distinction
considerably higher
explicitly include
lexicons used
let q
high complexity
set g
corpus since
four components
schematic representation
using conventional
general heuristic
one main
still room
previously annotated
perform certain
categories within
electronic form
ambiguity remains
sentence semantics
demonstrative pronouns
section 8
new models
model 1
complement types
;, one
transformation rule
h e
chomsky 1981
vander linden
medline database
previous corpus
generate many
processing constraints
optimality theory
task data
1987 ):
unbounded number
involving one
lfg \[
language theory
briefly address
straight line
whose heads
frequency function
set fills
semantic attribute
also satisfies
function used
mixture weights
several objects
execution speed
1992 3
ibm pc
semantic approach
message ).
boundary detection
first property
ca ),
l ;~
considered useful
side ).
document summarization
also indicate
length ratio
remarkably high
categorization frames
lower coverage
expository reasons
also distinguishes
particular view
larger grammars
small values
single answer
symbol 0
specialized vocabulary
wordnet contains
prime example
last word
generated according
theoretically well
second technique
conversational agents
first used
see subsection
parser developed
also serves
\] 6
one formalism
school students
\] indicates
approach extends
entity tagging
system rather
something like
zero pronouns
2005 )).
particular feature
errors may
several projects
high inter
(; c
toutanova et
constraints ),
key task
,..., xn
process input
limited vocabulary
tendency towards
bootstrapping process
data formats
index j
always necessary
lexical sample
one slot
computational problems
classi cation
specific structures
8 times
generative power
output format
since natural
16 %,
translation model
category symbol
discriminative learning
quite specific
currently runs
multilingual generation
fixed phrases
represent partial
evaluated using
feedback loop
28 %.
%& quot
fo r
sample tasks
1984 ),
successful match
conceptual category
many entries
family name
significantly reduces
original number
increasing availability
special status
limited number
establish whether
iff f
processed correctly
language defined
word lists
exceptional cases
structural risk
news text
monte carlo
information .)
thus include
features present
given equal
linguistic features
translation function
user enters
two dimensional
version 4
automatic term
subject noun
best reported
necessary conditions
single language
application domains
), though
grammar ).
semantic elements
tjong kim
genuinely ambiguous
1987 ),
probabilistic techniques
terms found
using structural
lamel et
structure represents
following functions
phrase cannot
considered three
closely tied
occurrence count
statistical mt
node labeled
recent developments
since speech
original work
may thus
second motivation
complete translation
\] respectively
empirical research
verbal predicate
foreign words
phonetic transcriptions
ones reported
parse constituent
tests based
low quality
since semantic
strict left
lines suggested
one month
method employed
c /:
precision measures
appropriate choice
current generation
treebank contains
many situations
indicator function
english tree
weight function
temporal ordering
linguistic descriptions
every instance
adjective ),
language corpora
best scores
three actions
ambiguous lexical
avoid spurious
detailed presentation
phenomena may
semantic object
make decisions
1985 )).
domain plan
default assumptions
1999 ].
application systems
development activity
dialogue ).
otherwise identical
makes sure
82 %,
context plays
consuming task
see two
order would
data showed
words found
srl systems
generative parsing
computer system
currently uses
foreseeable future
four human
associated grammatical
logical inference
two output
othe r
translation problems
symbol ),
annotation procedure
predicates used
hold among
every potential
rule needs
randomly chosen
exactly one
official scores
speci c
accurately model
dictionary would
distinct levels
part describes
ranked output
several attempts
issues relating
appropriate translation
research direction
good test
contract mda904
butt et
done efficiently
lexically marked
chinese information
system ),
gram based
would form
reduced using
contain examples
). 2
syntactic representations
semantics must
notable difference
first condition
actes de
following non
8 %)
al \[
restrictive relative
x 0
q [?]
last experiment
head must
immediately dominating
major syntactic
0 e
interpretation procedure
ambiguous sentences
common terms
extremely sparse
large vocabulary
new model
space ),
segmented corpus
(' es
definite nps
data collection
high priority
different characteristics
edit distance
null 7
models proposed
tit (;
three hundred
another strategy
context using
let p
particular focus
practical use
new form
time efficiency
dependency relations
structural descriptions
independent concepts
almost every
f j
grammar specifies
current context
propositional contents
briefly discusses
denotes one
world text
based theories
rules described
eliminate many
tasks could
using mutual
take arguments
concept definition
description consists
large number
new function
practical reasons
good accuracy
incorporate additional
1953 ),
attitudes toward
particular hypothesis
take much
x ):
use additional
quite complex
sections describe
n possible
specific concepts
speech signals
icsi meeting
full details
linguistic specifications
answer lies
fundamental differences
distinguish different
others ).
skew divergence
slots ).
figure 4
rules induced
un module
one error
classification tasks
terminology used
pairs given
domain knowledge
test \[
essentially different
discusses several
come back
structures extracted
forms part
general unification
automatic induction
without knowing
system lexicon
one introduces
prominent role
semantic notions
location ),
results represent
-- n
defined semantics
would necessitate
defined threshold
two databases
node names
since one
consists mostly
decision rules
lexical grammar
process involved
contextual probabilities
segmentation algorithm
,~ n
also designed
several domains
whose translation
two previously
context vectors
many terms
one related
whose frequencies
lambek calculus
come across
ground transportation
lexicon would
\[ 1
1 \],
different experimental
obvious difference
used training
small numbers
character level
), indicating
conjunction ),
intonation contour
based discourse
par le
phonological representations
existing rule
system appears
regular polysemy
total errors
correct reading
lemmatized version
previous stage
get much
structural relation
using special
processing technology
anaphoric links
two algorithms
3 describes
several applications
entire lexicon
relevant material
sufficiently robust
th (',
equal footing
bigram models
corpus currently
read sentences
like feature
among rules
algorithm compares
new problems
acceptable performance
distinct lexical
correctly tagged
two competing
many dictionaries
unlike previous
case characters
combined effect
3 ].
must eventually
language structures
people actually
becomes clear
second part
one word
various fields
function defined
words task
partially instantiated
sun 3
thus given
), case
tagger based
several benefits
identical words
system deals
ontological information
du syst
class information
declarative statements
two ideas
would decrease
several systems
term research
alternative approach
baayen et
linguistic choices
attribute names
clearly marked
level category
x ),
coverage lexical
must ensure
compared three
japanese corpora
linguistic components
remain constant
next goal
method shows
usual way
phrase coreference
actual situation
tree models
3 6
correct alignment
related features
descriptions like
rules used
branching node
first 4
module generates
approximately 40
wsd systems
could define
many grammatical
formal syntax
probabilistic parsing
le ),
first 200
previous rule
parsing speed
\] e
term ti
senses given
object descriptions
tagger uses
relevance score
one class
much easier
.) 1
unrelated words
fidditch parser
n j
selected sentences
level ),
1990b ).
clustering approach
running texts
language documents
general constraints
similar semantic
three linguistic
relevance judgments
specified explicitly
question sets
application dependent
tile basis
fairly reliable
anaphora ).
24 hours
markov chains
entities like
generally useful
may bc
automated approach
also ask
qualitative physics
varies considerably
topological fields
transformation rules
summary sentences
text input
original idea
bender et
discourse semantics
translation within
field ).
simpler methods
briefly compare
similar expressions
statistics show
high speed
k k
2 discusses
automatic language
21 words
base systems
effective means
different states
social sciences
general system
correct outputs
detect whether
algorithm relies
could give
employs two
oriented approach
6 data
character code
metrics used
lexical string
generally quite
constraint would
recursive application
average ).
highly inefficient
block diagram
differences across
speech analysis
procedure presented
slightly modify
p (.
slight differences
rules learned
feature counts
often indicate
head feature
external argument
radically different
<: h
1975 ).
leading us
1 show
information contributed
text document
treebank project
events involving
particular parameter
sentences collected
next character
5 discusses
different groups
three words
translation hypotheses
information technology
frontier nodes
every position
class items
relevant texts
1975 \]
whole procedure
3 respectively
first evaluated
l v
summarization algorithm
decision trees
average error
feature matrix
mit lincoln
j ),
guide us
network representation
comparative evaluations
shallow nlp
intermediate stage
one crucial
sentence pattern
may include
daily life
following equation
wsd method
words chosen
word penalty
questions may
would ever
/, c
processes like
test cases
x appears
l 1
left branch
performance decreases
seems reasonable
forms must
wrongly classified
20th century
deletion rule
different tasks
arnold et
future work
compares favorably
use certain
method performs
two matrices
term matching
learning semantic
better approach
verb predicate
variables may
interpreter uses
also generate
must contain
five systems
process .)
data consortium
actual sentence
mass nouns
), named
hierarchically related
specific kinds
document level
involve multiple
learning curve
time frames
also become
grammatical morphemes
2 outlines
frequent occurrence
three models
associated set
p represents
less straightforward
incorrectly tagged
utterances according
e2 ),
utterance might
annotations ).
true probabilities
time line
long way
nearby words
functions within
algorithm without
patti price
partial answers
deep linguistic
longer strings
evaluation paradigm
93 \].
worked best
), prepositional
2001 ).
propose three
19 \],
also receive
one experiment
remaining 10
difficult question
1 ...
locally ambiguous
using held
following advantages
system suggests
planning domain
possible source
quality text
domain speci
lower levels
feature unification
phrasal structure
empirical evidence
human input
free grammars
1996 association
parameter b
varies according
also proposed
clearly visible
predicate argument
similar one
research using
right tree
overall semantic
results described
v u
6 \[
word errors
features encoding
results returned
propositional content
automatic morphological
given part
information included
almost half
choice points
standard metrics
modal operator
cannot compare
character strings
rule ),
segmentation module
previous segment
specification defaults
94 ),
figure 1a
000 items
words respectively
real applications
second generation
abstract descriptions
act ),
collected together
may yield
similar cases
illustrative example
translated correctly
100 texts
verb taking
various participants
simple application
two adjacent
case like
quot ;]
7 times
since tile
utterance ),
additional type
line version
syntactic tags
much data
done independently
new context
nominative case
existing information
genetic algorithms
new hypotheses
000 entries
provide detailed
internal node
random samples
constant amount
additional analysis
parallel structure
w ~.
tree model
semantic operations
network represent
candidate solutions
robust system
database may
online dictionary
frequency ).
give access
type constraints
anaphor resolution
compute similarity
japanese documents
partial matches
also extracts
event sequence
generic summary
written language
lexical features
., pos
linear indexed
additional step
many objects
linguistic means
common set
information indicating
also combined
efficient parsers
typically rely
multiple answers
main focus
also calculate
f c
results support
directly derived
type system
figure 17
syntactic regularities
including words
level units
whose presence
resolution based
manually encoded
first constraint
two levels
remains open
relatively new
next section
software engineering
expressed using
k words
also reveals
left context
inference system
may fall
essential part
semantic interpreter
good level
), may
current parser
training sessions
baseline ).
output unit
classification algorithm
,, w
000 pages
must match
make sure
different analyses
small positive
capture generalizations
find ways
1976 ),
much reduced
useful applications
structures associated
first principles
induction system
feature constraints
language often
chinese characters
role assignments
using manual
two runs
many projects
trees contain
tag ),
generative probability
optimal setting
passive transformation
method avoids
special training
previous information
model allows
~( e
xi [?]
~'& quot
baseline error
:& quot
). several
really useful
action taken
work addresses
speaker turns
also appears
feature system
technical term
system training
moving towards
john left
web using
phrase types
continued development
middle east
thorough study
word according
also suitable
entire set
one input
0 4
k l
definition 9
accuracy increases
). ill
~, 2
error propagation
syntactic realization
certain ways
translation ).
deterministic parsing
1991 \])
words wi
also express
fully exploit
evaluation exercises
}( c
rules 4
following english
volume 18
bearing elements
latter method
expression denotes
pair consisting
partial specification
relatively quickly
rules defined
namely one
atomic category
types --
originally described
rules refer
corresponding element
nil ).
22 \].
competing approaches
characters without
quot ;)&
applying rule
particularly relevant
ou des
problem reduces
word j
lines represent
chunk tag
argmax operation
state techniques
one major
table contains
existing entries
used two
entire training
grammatical functions
atomic symbol
one document
even simpler
complete data
il ne
multiple matches
synonymous words
existing rules
text requires
maps onto
investigate two
7 %)
et qui
analysis within
word pairs
better able
definition may
generate surface
4 9
parsing step
reduced relative
agent role
ranked based
prolog version
obtain semantic
without question
representation scheme
lexical relation
feasibility study
also builds
much semantic
would improve
segmentation using
unification grammars
examining whether
problem encountered
weight ).
added features
pattern extraction
quite frequently
definite description
simple methods
total time
words representing
also wish
specific lexicon
quality output
computing power
test various
matching approaches
also offers
top half
prominent example
two uses
several studies
extensive set
pseudo code
state machine
level objects
information obtained
\[ marcus
development sets
directed edges
parsing times
precision measure
english part
quantitative measure
tile sentence
direct result
semantic similarity
line 3
two verbs
vary depending
additional problem
1994 )).
relatively low
potentially lead
never seen
word wj
multiple relations
derived structure
native language
parsing literature
allow arbitrary
always one
labeled corpus
good sense
n c
[?] v
biomedical literature
classi ers
network representing
quite successfully
word phrases
retrieval based
given path
initial position
recognizer uses
using ibm
surprisingly well
various structures
different subjects
pretty well
al .(
clustering using
stem ),
appropriate data
number generator
automatically classifying
varies widely
functions ).
associative memory
;, since
several places
previous rules
less specific
proposed framework
domain may
a2 ),
75 ).
dialog system
smaller training
many attempts
several texts
new annotation
87 %.
following set
already presented
french ).
qui se
75 \]
simple left
following conditional
($ 1
state transducer
text elements
possible attachment
similar manner
oblique object
serious limitation
mdl principle
l shows
equivalent forms
large vocabularies
special features
metric used
provide relevant
builds upon
two others
linguistically well
model yields
complex grammars
considerably better
etal .,
4 details
also addressed
words e
readily applied
automatic identification
huge amount
time algorithms
processing may
shared information
extract useful
words forming
\[ pereira
human transcripts
several efforts
roughly speaking
\] argues
originally used
one candidate
speech tagged
missing argument
trees without
three problems
argument node
user need
titles ).
coreference chain
easily modified
two sentences
temporal aspects
annotator reliability
improve accuracy
brennan et
finite verbs
;<& quot
parameter estimates
parsers would
also received
also eliminated
tree constructed
regularization term
simple voting
different algorithm
automatically produced
surface forms
country names
building upon
speech dialogue
yet know
language might
syntactic transformations
query ).
acoustic feature
similar patterns
first present
20 \],
possible approach
task would
significantly outperforms
model due
parser described
describe events
varying number
grammar requires
generation requires
input text
considerable progress
p )),
question 1
one kind
target phrases
spoken natural
models could
users find
correct string
frequent error
relevant semantic
maximal length
heuristic based
comprehensive evaluation
machine translated
density estimation
text system
methods cannot
clause would
types based
probabilities used
parsing problem
punctuation symbols
deletion rules
words produced
original formulation
association measure
context surrounding
bell labs
must maintain
)-( 3
wil l
summarization using
2001b ).
less appropriate
), also
2 );
must form
filtering module
average document
spatial information
obtained results
expressions may
noun occurs
one index
problem solving
consider figure
distance metrics
variable values
two stacks
\] show
4b ),
possible direction
x precedes
temporal reference
many applications
similarity threshold
ai researchers
data would
new modules
\[ el
general case
incremental construction
sufficient statistics
grammars may
context includes
chart parsing
modi cation
conference registration
also stores
lower accuracy
efficiency reasons
various degrees
perplexity results
multiple possibilities
5 );
context given
wsj corpus
-- 3
find relevant
also benefited
), h
n would
pronouns ).
aligning sentences
always follow
possessive pronoun
source grammar
parsing strategy
three sets
decisions concerning
word w2
challenging problem
make distinctions
idf values
used statistical
like human
directly compared
also capture
many systems
information may
extent possible
frequent word
;#& quot
conversion system
abstracts using
effectively using
chinese character
language versions
th ).
word occurred
sequence information
action ).
common method
least important
various concepts
computational linguist
context provides
verb (&
specific version
~, fl
parameter vector
100 messages
following process
rule makes
journal portion
hpsg framework
phrases rather
also makes
provides important
every ambiguous
consider word
van rijsbergen
two test
president clinton
horizontal axis
set according
phrase contains
knowledge --
software agents
always followed
write f
new parser
would produce
formal systems
;/& quot
knowledge acquisition
2 training
parser used
null tem
subordinating conjunctions
usage information
english generation
\] ...
network used
book ).
quantifier scope
forward chaining
frequent class
one terminal
english grammars
extremely expensive
languages used
constraint violation
temporal information
may start
np could
corpus reveals
way round
let 7
small part
one requires
various settings
1980 \].
single feature
level process
underlying database
rule set
added two
)- th
otherwise 0
f '.
information becomes
automatically find
analysis ).
improved precision
final tree
every combination
project aimed
representation makes
unification operation
adversely affect
.' however
4 demonstrates
could say
value indicates
significantly affect
analyses using
algorithm exists
~, etc
processing mode
generation system
grammar system
intuitive way
good translations
compare systems
references within
database records
phrases might
british english
dependency syntax
1999a ),
variable x
tagging guidelines
length l
algorithm fails
additional support
terminal labels
basic operation
hmm ).
allows different
first n
time --
much progress
decision procedure
lowest node
possible due
pairs without
56 %.
based theory
text plan
many fewer
systems achieve
different layers
type hierarchy
string may
\]. another
second issue
two place
using similarity
nlp work
error messages
reiter et
first clause
learning architecture
total ordering
performance characteristics
another cluster
n p
order information
current issues
tree fragments
full words
source string
co .,
map one
one hour
model using
database interfaces
corresponding human
another paper
possibility would
smallest distance
trees built
particular role
two corresponding
programs written
surface case
candidates may
english equivalents
phase 3
often appears
systems need
also hold
develop new
using sentence
seems possible
analysis modules
3 \],
recognition algorithm
level systems
boundary ),
primarily based
large dictionaries
many translation
word c
still relatively
use another
69 %,
particular action
near one
possible segmentations
current automatic
functional word
selected sense
rule selection
). step
considerably worse
make comparisons
simply indicate
thus reducing
direct access
nodes according
approach presented
\] exical
separate data
garbage collection
specific query
several changes
applying rules
partial functions
following seven
la description
overall architecture
cannot solve
independent training
combine several
phonetic context
one object
crafted rules
initiative dialogue
efficient manner
occurs twice
direct use
slot fill
individual senses
various functions
2 features
suggested earlier
controlled vocabulary
1 might
\[ gazdar
carbonell et
following method
specific training
experiments described
net result
ii est
\[' l
computational issues
efficient retrieval
tipster project
new light
); 5
otherwise ),
clauses would
particular verb
results discussed
machine learning
one new
total count
corresponding row
particularly valuable
separate classifiers
n '.
evenly across
one system
first divided
knowledge based
replacing every
\[ subj
several natural
least squares
right part
c \],
type restrictions
deeper linguistic
yielded better
involves identifying
variants ).
chooses one
5 %,
complex nps
promising research
significant effort
annotation errors
carletta et
rule could
default parameters
monolingual text
central repository
comparison results
also could
considerable research
computational environment
immediately dominates
based format
extractive summarization
sensitive language
also call
texts whose
formed dependency
gradually increasing
hearer may
word whose
order rules
current algorithm
first build
june 2002
high threshold
lexicon information
various kinds
statistical part
inheritance mechanism
selection approach
psychologically plausible
participating sites
e (;
clear improvement
extends previous
special way
agent believes
conceptual content
broader range
state university
branching tree
nominal argument
~: e
required since
also facilitate
different news
two subsequent
tutoring systems
particular event
information also
may still
recent attempts
quot ;+&
linguistic theories
model along
cross language
time based
h ,~
japanese lexicon
representation could
compositional interpretation
entity mentions
3 show
test corpus
rule applications
linguistic structure
research includes
several semantic
phrase identification
also describe
soon et
small grammars
using relative
maximum possible
actual implementation
;, even
16 ].
,, x
actual utterance
projects agency
technical reasons
single large
start position
wordnet 2
another language
20 documents
explicitly mark
three datasets
left boundary
sometimes even
across documents
13 %.
text length
syntactic components
texts would
., 1982
multiple speakers
possible using
). whenever
word would
purely declarative
1000 documents
regular expression
focus list
zero probabilities
rules often
corpora containing
., word
text handling
rule systems
ones presented
new source
ordinary context
thes e
equally applicable
\[ jacobs
distribution within
simplest model
highly inflectional
particular task
generation techniques
..... xn
drastic reduction
single link
sun workstation
experiments used
correct results
based segmentation
independent features
sentence initial
also parsed
always coincide
either zero
one area
6 would
;~ n
close relationship
human knowledge
word matching
previous tests
resource grammar
verbal constructions
expressions referring
blackboard architecture
approach like
also tend
different constituents
verb class
semantic mapping
propbank corpus
axe used
learning classification
sample sentence
le cadre
words defined
specific features
model reflects
features results
original set
output string
use exactly
dp ).
word ordering
varying lengths
around 0
following factors
negative side
correct responses
eventually lead
separate sub
alignment task
equally probable
approximate p
approach enables
computer interface
news service
3 defines
2 ],
graphs representing
w [?]
references ).
entry points
simple text
21 ).
first mention
automatic feature
interface must
linguistic differences
arguments may
summer workshop
two closely
additional improvements
lexicon entries
morphological form
include sentences
indexed grammar
scores provided
21 \]
novel technique
noun synsets
compares two
smaller amount
resulting sentences
lisp program
two alignments
procedure also
explicit control
)( c
right branch
experimental result
distance function
uniform way
develop techniques
differently according
corpus makes
solution would
recent applications
real data
process may
different goals
actually uttered
5 ],
temporal adverbials
usual manner
represent documents
use non
monotonic reasoning
three ).
treebank 2
95 %).
might result
also illustrates
second source
distributional differences
2 results
words show
present experimental
significant advantages
de coling
reasonably good
exponential time
entity classes
major problems
approach differs
available ),
much research
many studies
financial news
state transducers
system begins
syntactic construction
typically occurs
algorithm ).
system automatically
topic segment
particularly good
group ),
full form
bit vectors
morphological analyzer
overall classification
detection system
recognition module
system classifies
prep np
3 minutes
rule gives
f1 measure
semantic grounds
one sees
also gives
recent systems
physical objects
\[ 16
generative models
van halteren
1992 6
general categories
detailed descriptions
interesting research
-- either
example 5
several passes
value returned
;. although
unsolved problem
provide data
occurring text
causal relation
although less
term weighting
appropriate time
detail later
based ir
two conclusions
handle word
produced results
,, f
extended algorithm
memory structures
c ...
frequently occurs
text parsing
inflectional information
single parameter
rather difficult
using spoken
called dialogue
language query
terminology extraction
text alone
required number
quintus prolog
particular type
set c
particular topic
condition 3
johnson et
vice chairman
particular discourse
next iteration
per ),
word count
spelling corrector
always easy
c /'
2 two
word stem
structural differences
paper focuses
several hundreds
several clusters
called support
syntactic complexity
several cases
usually contains
language training
selection module
;). since
38 ),
made based
terms appearing
analysis shows
relation instances
marcus 1980
similar fashion
version 3
three modules
categories using
noise ratio
context free
type may
two obvious
many efforts
alternative syntactic
set q
allowing users
columns 3
database domain
stand alone
binomial distribution
always find
statistical grammars
lines 4
basic tasks
one knowledge
ones like
two chunks
\[ johnson
chosen domain
one finds
key concepts
precision increases
), obj
translation approach
constraint rules
easily extended
recall results
processing required
resolved using
whole collection
system generates
news transcripts
february 1991
entity x
unseen texts
include general
correct way
rule looks
several distinct
nouns within
provide good
chosen words
less frequently
two humans
relational database
background information
classes --
different parts
second entry
cognitive science
mary loves
.) using
represent concepts
fully automated
r }.
john would
features together
feature may
v n
method reduces
words appearing
another use
frequent enough
1981 \].
go along
completely automatic
structure --
mechanism using
two constraints
cooper storage
one round
answering task
l {,
): 1
constituent x
z ,,
lexical tokens
may l
tables 3
tipster phase
use ),
also ensures
set whose
overall corpus
w 2
99 %,
methods make
usually use
scaling factors
complex tasks
initial letters
value used
4 compares
translation direction
units within
error message
tile tree
maximal probability
grows linearly
phase two
slot filler
single phoneme
factors contributing
technical papers
phrase must
agreement errors
ambiguous interpretations
within five
space consists
2005b ).
words exist
first version
seem particularly
human parsing
information encoded
similar argument
would undoubtedly
based nlp
resulting accuracy
first grammar
greatly increased
n sentences
reversible grammars
underlined words
mckeown \[
helping us
kind described
\], one
similar number
si ),
handled correctly
previous sections
generate word
sentence realization
unary productions
gives access
third arguments
without lexical
quantitative analysis
four fields
tile list
text segments
grammar parsing
k ).
semantic tree
5 days
example sentence
hierarchy ).
examined whether
word belonging
considers two
), without
would want
corpus could
), l
main verb
specific strategies
k \]
different strategy
categorization system
problem still
last week
distribution ).
set would
using equations
used one
particular example
north korea
features appear
corresponding verb
sequence modeling
language features
,..., wn
complex utterances
also reasonable
useful semantic
specified context
threshold ).
retrieval system
manual annotation
two partitions
strong effect
candidate generation
particular problem
behave differently
satisfactory way
semantic coherence
two effects
probabilistic models
domain concept
attribute type
decision lists
framenet project
\[ moore
concepts associated
cubic time
memory size
information required
find good
target corpus
previously recognized
initial work
future plans
merely one
generic concepts
selection using
new attribute
engine ),
distance relationships
;, rather
work needed
two characters
possessive marker
new approach
evidence available
per second
first component
standard error
\] discusses
constructions would
independent factors
user wants
main event
overall scores
higher rate
et par
k 1
p2 ),
generative model
passive sentence
feminine plural
different dimensions
nps like
rather poor
names ).
possible causes
et le
entire parse
nwo ).
use grammatical
previous section
related articles
pairs within
corresponding event
;. thus
previously processed
1 );
recall value
may relate
scores produced
semantic arguments
equally relevant
model cannot
general solution
tree shows
like representation
second algorithm
specific topics
approach achieved
4 words
original order
little help
modi ers
including word
preliminary research
associated semantic
~, ~,
verb constructions
together information
used simple
grained semantic
magnitude smaller
particular aspect
control structures
performs quite
automatic learning
generated rules
usually regarded
original method
\[ 25
encode syntactic
time stamps
classes based
verbal elements
system aims
systems since
one pass
would depend
would provide
automatically trained
four anonymous
annotations using
earlier ones
15 seconds
two popular
french equivalent
highly idiosyncratic
null also
argument types
first string
underlying representation
strategies described
main task
also experimented
user community
structure based
ne tagger
given description
currently pursuing
like type
acoustic cues
core meaning
ordered sequences
basic structural
simple algorithms
different window
independent way
become less
possible sequences
english tasks
~- e
error checking
additional constraint
column represents
markup language
standard word
later used
system changes
involves two
es de
el ).
different sort
find possible
basic issues
sentence 3
scale corpora
acquisition tools
performed experiments
relevance assessment
work carried
trees ).
1990 \])
el \]
many data
set p
among many
file containing
business letters
specific way
learning community
structure directly
nodes would
first introduced
another interpretation
first 3
1995 association
viable approach
empty element
surface phenomena
via e
semantically empty
another project
e >,
define semantic
domain without
may point
data stream
semantic links
correct resolution
domain models
university ).
handwriting recognition
argument list
underlying discourse
(: ed
second advantage
continuous values
constraint programming
knowledge involved
schemes used
rather obvious
weights using
constituent tree
human judgments
\[ mcdonald
character sequence
first quarter
comparing performance
determiner ),
sentences involving
computer processing
numerical values
null many
utterance boundaries
much harder
might use
(: ti
element may
various configurations
depend heavily
anaphoric expression
existing translation
processing efficiency
), predicate
initial parameters
systematic analysis
follows directly
senses within
complex structural
13 ).
g ))
may form
labeled trees
one vs
built two
two modes
neighboring words
restrictions associated
past perfect
initial performance
ten times
english utterances
slash feature
bilingual lexicography
represent non
assisted translation
empirical tests
lexical structure
tagging rules
may function
technology conference
simple count
labor intensive
input sentences
arrival time
table 10
course also
)) b
features shown
particularly strong
alternative solutions
using f
analysis allows
common semantic
two experiments
also made
mutual beliefs
normal distribution
features describing
u l
distinctive features
cases like
1985 \].
relatively efficient
particular name
shall concentrate
7 points
describes related
one chooses
english source
excellent results
hyphenated words
theory must
combine different
next state
still useful
recent studies
avoid repeating
choice questions
2 displays
two million
two forms
multimodal dialogue
corpus frequencies
pragmatic functions
word lexical
specific common
slot fillers
best choice
class label
strategies used
nlp ),
four corpora
parameters p
units ).
lists used
main topic
van deemter
directed translation
unification algorithm
upper bounds
program using
syntactic generalizations
raises questions
natural l
always work
leaves open
represented via
total accuracy
address issues
john ran
p '.
corresponding discourse
)/ np
significant features
strong generative
dialogue participant
could involve
method given
5 displays
translations based
trees derived
mt system
text given
probabilistic model
basic algorithm
parsing result
complete corpus
expressions without
daily newspaper
average ambiguity
two translations
23 \].
two respects
boguraev et
errors ).
would already
course many
arabic language
noun classes
new module
frequency vectors
000 chinese
w .,
kurohashi et
transcribed dialogues
feature engineering
relation f
true positives
models \[
system evaluation
stage takes
level data
grammars without
occurs much
particular dialogue
like example
structural ambiguities
), taking
use english
using tree
require large
general purpose
lexical differences
syntactic decisions
first selects
explicit statements
every non
extended using
relation r
expressive power
experiment ),
method provides
selecting relevant
best translation
system understands
proposed approaches
translation using
b (;
whose referent
distinctions may
must establish
could come
based bigram
immediate parent
paper first
different derivations
word sentences
added one
several research
one week
37 ),
provide strong
average parsing
finite forms
describe actions
rule consists
words often
1 features
pos tags
e ))
translation techniques
various training
also combine
category pairs
nl generation
therein ).
still unknown
l w
volume ).
p --
contains fewer
would reduce
year project
cannot parse
particular property
sentence hypothesis
user wishes
complex algorithm
one characteristic
formal semantic
used patterns
earlier work
certain pairs
80 ).
target lexical
word group
information state
9 proc
short segments
dependent system
one participant
hybrid systems
identified four
parse structure
small groups
two syllables
connectionist networks
percentage points
primary interest
80 \]
events described
sufficiently good
using graph
length ).
billion words
relative time
whose labels
one underlying
times larger
extensive semantic
present algorithm
using hidden
second module
pairs ).
]& quot
two character
part ),
subsection describes
future systems
representation consists
critical issue
supervised systems
different treatment
based alignment
sensitive grammars
larger window
lower case
verb requires
enhanced version
le texte
proposition 3
formal evaluation
current situation
financial support
reliable data
3 ):
write grammars
stage 2
three issues
using much
tree algorithm
within text
table 2
certain phrases
3 discusses
~, e
better f
sense disambiguation
distance ).
\[ al
based extraction
0 );
seven times
must handle
used extensively
volume 1
also encodes
discourse grammar
average similarity
correct recognition
inner workings
system one
false positives
likelihood value
another interesting
good solution
2 briefly
coordinate structures
compute precision
first element
current node
less interesting
parser always
common cases
24 \].
particular concern
contained within
input stream
formed text
following semantic
string match
functions using
concatenation operator
context model
efficiently computed
high weight
particular piece
np \[
grammar includes
attribute name
c 0
corpus material
spoken dialogues
implementation language
variables within
present several
improve upon
generally use
provides much
following resources
arabic morphological
following expressions
text books
grammar given
... \[
level \[
using japanese
question using
time .)
general processing
list may
constructed manually
labelled precision
test speakers
evaluation mechanism
(: oml
family members
sentences correctly
hybrid method
examples involve
first relation
recognition rates
features found
p ,'
utsuro et
potentially relevant
level semantic
first 10
desired results
would turn
possible alternatives
three annotators
knowledge management
human annotator
detailed study
c ):
principled approach
time constraints
word test
automatic tools
word collocations
-- specifically
pretty much
sentences together
using vector
nearly equal
major issue
obvious example
des mots
test items
systems currently
rules needed
aligned data
must unify
information coming
%, recall
one basic
indicates whether
semantic association
likely class
recently added
thank dan
original texts
would see
semantic entities
discourse component
attention within
query using
0 3
subject role
produces good
1990 )),
two case
past three
graduate school
senses ).
dynamic range
significant results
time system
dowding et
scale nlp
important prerequisite
unigram probability
explicit semantic
rules introduced
polysemous nouns
approach also
2 \]),
active learning
observation probabilities
cannot change
rules 3
highest weight
next line
generate candidate
long ones
less problematic
people usually
different theoretical
nigam et
different stages
structure corresponding
section gives
simply assigning
constraints need
sense knowledge
various sources
current sentence
german translation
models consist
third model
corpus studies
better estimates
full parses
1988 ).
rather long
sighan bakeoff
mathematical theory
word accuracy
verb object
matching word
5 tokens
identify sentence
5 \]),
mostly based
1988 \]
formal evaluations
us two
cover cases
general domain
relative contributions
good indication
single action
standard document
., object
words might
clause level
first results
long range
example x
satisfying certain
g g
relation links
). especially
also reduced
additional use
regular basis
general conclusion
different symbols
small database
simple regular
increase parsing
must keep
collection used
daelemans et
one syntactic
direct link
value structures
lexieal items
general words
different values
96 %.
morphological parser
sentences appear
hand tagging
extremely common
manually checked
senses 1
different senses
cannot go
scores show
chain ).
well structured
subject verb
whose scores
pattern shown
following one
languages might
one given
%. however
r x
special mechanism
word corresponding
sparse network
c ),
approximately 1000
highly inflected
1992 \],
locality constraints
different classes
trained classifiers
may observe
e following
bilingual dictionary
sets containing
lexieal entries
parsing failure
describe linguistic
result could
h .'
method depends
complete lexical
--~ c
lexieal rules
document collection
agglutinative languages
similar items
single verb
first tries
also decide
proteus system
human evaluator
skut et
phrase beginning
computer model
level according
corresponding phonetic
possible reason
give priority
)?& quot
practical task
briefly describes
valuable insights
another view
following cases
corpus made
semantic model
line 6
4 sets
algorithm requires
rather like
best lists
nodes connected
software modules
rule \[
noun types
v 3
several readings
observation sequence
also \[
certain time
one annotator
trivial problem
13 shows
approach towards
particular theory
still limited
robust processing
fairly extensive
partial order
take advantage
syntactic variety
parser outputs
another feature
set sentences
techniques developed
may recognize
human interface
children nodes
also semantic
semantically interpreted
rules written
linguistic annotations
underlying semantics
1 )=
extracted words
necessary part
process relies
output shown
immediately available
indirect questions
conjunctions like
500 sentences
two named
preposition ).
probability p
thus produce
g ',
immediately precedes
component generates
details .)
single event
bias problem
first construct
general preference
feature spaces
8 3
analysis using
often realized
increasingly important
noisy training
additional heuristics
following simple
3 deals
0 %,
5 evaluation
background noise
\] introduced
appropriate lexical
ir ).
\[ 14
important words
speaker intends
mle ),
semantic analyzer
short period
2003 )),
probabilistic context
x2 ),
representation \[
robust accurate
confidence scores
analysis techniques
applications ),
information comes
approaches based
state models
une r
multiple systems
among individual
[?] w
user requests
significant amount
similar situations
related fields
correct attachments
three conditions
minor modification
statistical measure
function value
representing concepts
simple sentences
ne peut
text preprocessing
feature bundles
;. given
significant way
weight w
correct structure
case time
tagger also
gram probabilities
handle new
sections discuss
based qa
sentence construction
make significant
feature instantiation
grammar compiler
case frames
acoustic analysis
largely domain
access system
relevant rule
conditions may
implemented three
van dijk
argument identification
somewhat lower
.... ),
mechanism provides
learning rules
common patterns
developed software
open problem
sense ambiguity
corpus consists
sentence together
first value
module needs
acquisition task
semantic problems
roughly 1
litman 1993
information seeking
information loss
following assumptions
clustering technique
processing flow
1999 );
variables ).
john got
respectively ).
key component
consistently better
sentences containing
pioneering work
shall give
postpositional particles
spoken interaction
valuable tool
different tenses
score indicates
strict sense
makes available
verbs within
visual representation
su (:
baseline score
target information
lexical ambiguities
carefully chosen
particular subset
method outperforms
29 ),
semantically well
rough estimate
providing information
also implies
manual tagging
possible solutions
new situation
reference times
features chosen
previously constructed
three new
strategies discussed
whose lexical
completely different
possible subsets
new languages
quite easily
scoring software
upper part
preliminary stage
based version
travel agent
rules first
methods must
equally good
1998 )).
determined solely
sentences describing
american english
independent evaluation
based grammar
whose existence
second conjunct
questions related
rules involving
boolean value
statistical analyses
n characters
could result
al 1992
new node
pattern recognition
query languages
conventions used
ambiguity occurs
theorem 2
substantial proportion
graph contains
-- 6
major source
\[ x
2 describes
limited data
weighted sum
light verb
slightly smaller
specific evaluation
episodic memory
southern california
identifying words
connected nodes
found evidence
single representation
automatically determined
corpora used
identify instances
word final
1 denotes
language parse
structure unification
complete account
uniform manner
wi [?]
existing data
remko scha
speech tags
every item
existing methods
every english
regular languages
5 describes
particular pattern
ccg parser
could make
8 %,
main concern
specific instance
multiple translation
terminal strings
systems designed
factors influence
negative information
various factors
expression operators
\], since
speakers make
potential uses
25 %.
aligned pairs
previously applied
.) b
architecture makes
phrasal lexicon
manually creating
expansion process
two target
various topics
simple test
rapidly growing
general point
synonymy relation
problem exists
marti hearst
divided among
b >,
infer new
10 %),
set accuracy
highly probable
something similar
single type
data presented
final issue
obtain information
single topic
c (~
segment structure
much information
encode linguistic
still used
critical issues
meaningful units
single discourse
use co
resources may
could answer
directly extracted
interpretations produced
following manner
must depend
must provide
statistically based
based component
two weeks
used since
one considers
city university
database ).
pour un
variables bound
improved algorithm
consists solely
unrestricted text
3 english
information flow
achieve reasonable
interesting information
corpus data
., machine
one notable
phrase modifying
application rule
single step
general form
tree using
first member
adjacent utterances
1 means
compare various
1994 \].
65 %.
strong bias
discourse relation
2004 ).
new sentence
relationships within
successful parsing
quality translations
possible tree
student research
grouped according
combining two
problem within
response times
category would
initial lexicon
correct position
using precision
normalized form
contain special
previous methods
features provided
earley parsing
could avoid
structure .)
automatique de
following patterns
one intended
specific one
gender agreement
com ).
)& quot
proper choice
may 1
lingua franca
among alternative
evaluation criteria
life cycle
different types
3 );
variable names
inference engine
state model
get lost
l b
practical problem
initial letter
original speech
spelling checkers
important resource
text processing
occurrences within
every set
2 respectively
competence grammar
author gratefully
language recognition
cannot achieve
dependency parsers
simple heuristic
specific structure
structure constraints
based query
word level
based paradigm
precise formulation
common subsumer
real corpus
p ~(
recent interest
1995 ).
geared toward
important point
involve different
\[ f
could perhaps
major issues
branching nodes
highly detailed
across domains
linguistically adequate
occurrence probabilities
country name
%, 20
1995 \]
regularization parameter
using part
different realizations
\]. note
far apart
dialogue actions
large data
one lexeme
nlp applications
many research
mainichi newspaper
authoring tools
5 respectively
r )).
called n
grained classification
without providing
template generation
l x
subsequent analysis
alternative model
anywhere within
foreign languages
indefinite noun
two categories
second sentence
), similar
wsj sections
possible arguments
particular structures
sentence containing
network grammar
form required
classifier assigns
based reasoning
par rapport
complete information
term translation
automatically assigning
documents containing
f ,,
pure statistical
significantly smaller
coherence constraints
traditional language
,- 1
additional tags
argument must
key features
timit database
nlp systems
definition ).
human revision
semantic context
40 %.
different answer
relevant information
sentences found
1 displays
dans le
semantic relationship
corpus \[
entity must
probability based
manually selected
standard system
various statistical
chinese text
since otherwise
3 training
semantically different
b ))
considered first
), word
less satisfactory
). let
contains data
scoring procedure
tagging procedure
1993 \]).
human agents
single example
related semantically
separate lexical
likely antecedent
next month
system improves
majority class
document text
actually occurs
expansion method
patterns described
also look
bayes model
text domain
systems generate
anonymous referees
last words
easily derived
require several
spelling rules
w ~'
relation expressed
may produce
;& quot
depth one
tl ...
various means
could estimate
casual users
constituent must
000 pairs
semantic roles
two directions
problems might
4 ).
words );
another parameter
trees provide
depending upon
category labels
action sequences
warfare systems
relations rather
also need
plural form
a1 ),
conll shared
intended referent
contextual constraints
text words
given different
4 \]
arbitrary word
seem reasonable
contribute significantly
greater degree
pronoun may
allegro common
annotated example
appropriate information
help determine
features representing
provide evidence
mainly focused
like format
concept represented
domain ).
human processing
concepts within
whose subject
ocean systems
probabilistic finite
situation semantics
formally represented
comparison among
null next
models would
level architecture
function ph
different language
grammatical resources
nist score
exponentially many
similarity measures
knowledge obtained
low threshold
human activity
enabling technology
language evaluation
word sense
algorithm finds
allowing us
answer selection
model pr
second author
experiments indicated
issues like
distributions p
two branches
(; h
good alternative
probabilistic parser
separate sentences
allows multiple
., 1990
us briefly
1 previous
bill clinton
). n
combines syntactic
becker et
statistical dependencies
tous les
estimation process
processing ).
simple metric
several possibilities
best scoring
using single
tree family
fairly high
one derivation
perform semantic
alternative formulations
vp ellipsis
training times
., parsing
relevant items
input strings
may intervene
domain ontology
bit vector
nie et
numerical optimization
p ')
li et
node types
case p
potential benefit
also apply
earlier ),
computationally infeasible
based features
machine dialogue
clear picture
use statistical
defines two
one hundred
without causing
originally written
two judges
larger portion
best possible
sixth message
combining statistical
(\[ 5
disambiguating information
u ).
ullman 1979
particular nlp
syntactic distinctions
wider range
specific position
greater frequency
relevant rules
questionable whether
latin america
u \]
-- e
german data
description logic
extraction method
gaussian mixture
partially overlapping
go away
web site
two phenomena
w1 ...
makes predictions
may lack
algorithm thus
always appears
use active
particular points
2 ....
intonation contours
23 %)
,~ e
seems difficult
internet users
deletion errors
patterns used
two mechanisms
low perplexity
answer could
score produced
feature function
may rely
new instances
1999 ],
thus yielding
major advantage
basic sentence
value assignments
system resolves
contains semantic
given root
harabagiu et
states government
many classification
corpus according
identity matrix
independent manner
using neural
group consists
automatic topic
highly relevant
paper explains
tree node
rules operating
u v
especially since
parallel english
internal nodes
null tences
following consonant
linguistic insight
medical records
expressions ).
relationships ).
text samples
closely connected
model called
also consistent
find evidence
agreed upon
utterances like
darpa contract
parallel bilingual
unique path
semantic operation
tha n
contribute little
analysis goes
\[ de
two small
paragraph level
u 1
may suffer
0 means
np would
relations ),
grammars ).
related documents
welch algorithm
sense given
value ranges
typically much
three operations
system reports
across two
adding features
careful reading
word strings
words available
precise nature
users tend
quite useful
co .&
l ,\[
different morphological
language structure
training example
2a ).
literal translations
first pair
called initial
standard parseval
maximum accuracy
distribution using
complex sentence
often one
ambiguity arises
two probabilities
marginal distribution
single name
separate test
roles like
one observation
active role
simplified chinese
thus allows
simple unification
commercial mt
make available
different amounts
touched upon
also thank
main phases
including knowledge
directly associated
node labelled
documents found
tasks performed
system since
similar meaning
three important
system generated
including machine
[?] b
grammar models
whole sentences
linear logic
standard techniques
possible instantiations
interlingua approach
generation models
published literature
types also
current paper
lexical functional
unsupervised learning
data drawn
positive values
one concept
better performing
following steps
might write
potential candidate
conversational participant
inflectional suffixes
substitution grammar
empirical comparison
brown et
knowledge may
10 sentences
initial set
estimation method
considered equally
might argue
greater variety
provide lexical
proposed algorithms
hidden variables
another human
term lists
house ).
take different
child relation
possible outcomes
working prototype
information related
arcs represent
one linguistic
parser written
class c
current tree
clear separation
three languages
either language
better reflect
parser selects
last point
standard machine
must thus
represent two
\] denotes
surface generator
extremely simple
application rules
annotation instructions
test procedure
disambiguation decisions
1979 \],
current implementation
data obtained
considered together
target strings
current utterance
syntax rule
closest preceding
word forms
manner analogous
greatly benefit
natural class
dialogue models
reverse direction
different use
one domain
less predictable
adequate semantic
james allen
applications could
parallel training
within sentences
likely due
underlying principles
lexical specification
used even
case involves
current list
parameters estimated
3 illustrates
frequency f
string ).
quot ;,~
investigating whether
principles behind
available via
numeric value
related concepts
rule 8
input character
based results
aravind joshi
without adding
automatically parsed
time may
formed sentences
relation within
approach advocated
would b
training iterations
major contribution
text model
precise definition
encoding information
appropriate level
takes account
discuss several
system answers
future evaluation
feature sets
get different
different representations
table indicates
greater flexibility
require little
figures reported
entity names
text form
nou n
contextual knowledge
present author
rst trees
rules shown
multilingual language
normal english
several information
first checks
shall say
semantic clues
\] means
tables 6
empirical methods
two meanings
morphologically analyzed
matrix based
input layer
kipper et
although often
based strategies
analysis includes
differences within
word like
smaller ones
6 ))
written discourse
less noisy
two inputs
document summarisation
actually occurring
une part
matching system
relevance feedback
unique feature
space prevents
list l
class whose
moderate size
among competing
number may
subjective judgment
constraint dependency
highest ranking
especially helpful
really needed
edu /~
later versions
computational implementation
since almost
language usage
smoothed version
decided upon
two approaches
est pas
similar ways
whole test
english joint
short words
closed set
de recherche
squared error
english sentence
algorithm reduces
trained separately
next experiment
semantic parser
overall meaning
arbitrary input
cannot find
similarity relation
main principles
rule matching
achieve good
deterministic parser
based either
differs substantially
tag ti
procedure similar
syntactically possible
algorithm chooses
goes well
unannotated data
analysis provides
parser first
press news
nouns using
another possibility
node contains
argument nodes
choose one
preliminary investigations
longman dictionary
finite sequence
morphologically derived
representation would
., 1996
r ~.
novel algorithm
meaning --
3 two
intermediate phrases
little impact
requesting information
crucial point
different countries
dramatically reduced
frequency estimates
syntactic theory
remains unclear
1992 ):
tree cannot
two complete
available speech
work describes
)- based
mutually known
extract bilingual
like noun
design issues
top left
[?] f
entire vocabulary
class would
mcdonald et
affect performance
individual components
second reason
two entity
quasi logical
distributed among
allow partial
specifically designed
rule np
showed significant
[?] r
figure 6
tool developed
pr ()
parametric model
arcs representing
often need
often fail
output summary
simply consists
results appear
partial semantic
full stop
cl reference
seems obvious
adding two
procedure based
dm ),
rather small
appropriate sense
noun must
including parsing
larger amounts
less likely
user could
1 one
weight associated
statistical analysis
abstract concepts
time period
directly affect
degraded performance
another study
pragmatic factors
long ),
interactive process
specified using
grammar model
generation model
n th
unit length
progressive aspect
several rules
less marked
act classification
learning procedure
sentence null
sgml markup
quot ;\['
general ).
technical writers
correct ones
various resources
fair test
indicate whether
resources required
previous time
generative approach
evidence provided
effectively applied
assigned semantic
task becomes
usually marked
thus able
also exploit
chapter 1
labeled dependency
experiments carried
well established
., syntactic
certain sentences
external world
functions f
model incorporates
selected text
decide upon
decided whether
two spaces
patient ),
theoretical implications
discriminate among
matsumoto et
optional constituents
underlying logical
first describes
clearly needed
two agents
particular alignment
research issue
possible part
2 reports
still valid
search algorithms
standard parsing
disambiguation task
new york
parser results
two variants
grammatical patterns
received little
different interpretation
use natural
maximal marginal
1 describes
numbers ).
., given
revolves around
either use
confidence level
major constituents
%. therefore
theoretical level
type slot
considerations suggest
non terminal
top scoring
hundred thousand
model must
weighted terms
exist two
sets described
details may
improve efficiency
specific criteria
main points
cluster size
phonological theory
dialogue model
well developed
12 speakers
contributing factor
5 reports
also accounts
3 note
last column
top five
wacholder et
question remains
e \[
goes along
head noun
ito grant
express linguistic
thus reduce
project aims
mary ).
application programming
includes several
demonstration system
message passing
data becomes
missing data
selection technique
proven quite
main concepts
additional questions
de son
always hold
mary \]
2 since
intuitive idea
much like
much noise
following principle
two metrics
trigram model
first 6
pairs using
character word
parser tries
next input
description could
must bc
corresponding concepts
use speech
system took
c /-
quite limited
structure obtained
many areas
major goals
~, n
build new
medical domain
combining speech
full discussion
requires information
additional difficulty
probability matrix
though one
using computers
bayes theorem
phrase type
2 \],
given values
linguistic notion
see fig
many lexical
two contexts
perform similarly
would respond
use semantic
returns one
salient feature
jason eisner
5 since
arguments ).
third source
class labels
phrase analysis
research grants
may want
name ).
general word
system currently
still require
word pronunciations
scored according
7 \]).
system interaction
clusters ).
aligned corpus
upper limit
consistency checking
complex symbols
also create
output produced
bracketing accuracy
systemic functional
5 \],
design allows
features defined
dictionaries may
process ).
system parameters
state 1
virtual world
magazine articles
recal l
paramount importance
tree provides
knowledge engineering
one model
two people
expression ),
based ou
five rules
bayesian networks
dialog context
described methods
alarm rate
system performance
independent continuous
contains various
fairly well
user responds
decide whether
small window
content extraction
one form
unique characteristics
additional parameter
looked like
control strategies
used exactly
point would
new tree
general class
category assignment
chart parser
adjacent constituents
terms per
produce coherent
major drawback
r must
one function
thus using
following relations
experiment consists
two individual
7 2
method applied
mean time
hypothesis would
simple forms
new utterance
provide information
high density
two di
inflectional affixes
similarity value
results shown
discover new
orthographic features
proof theory
sets used
may enter
various cases
value 3
three instances
approaches differ
called top
later version
additional components
longer sentences
propositional calculus
two attributes
new list
generalized quantifiers
rewriting system
empirically evaluated
substantially better
poesio et
also reported
text within
complex network
infinitive clauses
positive answer
2002 ).
generated independently
sentences described
makes decisions
2 )-(
like n
still present
require different
bounded number
seem possible
intensional logic
variation among
senses using
also computes
would still
using manually
v -~
000 lines
\]. one
second paper
useful information
etc ..
mail messages
important word
state parser
., h
short form
weight equal
atn parser
system behavior
weight vector
also discuss
one aim
null figure
defined task
text chunking
type ).
probabilistic approach
(: es
task also
linguistic tradition
segmented text
major parts
local patterns
experimental study
grammatical agreement
many circumstances
another action
method applies
systematic use
ambiguity classes
update rule
parts may
next candidate
four features
extract features
classifiers used
constructions like
syntax rules
indirect request
de plusieurs
relations hold
algorithm includes
2 show
given appropriate
multiple passes
efficient ways
usually found
specific ones
memory requirements
pruning method
test material
typed feature
level lexical
algorithm given
... wn
understanding process
various sorts
systems could
model learns
architecture consists
typically expressed
use one
dynamic nature
e j
important class
world entity
three alternatives
corpus would
words appeared
second dimension
lexical functions
intuition behind
tl ),
model incorporating
foreign broadcast
graph nodes
5 show
possible expansions
labels attached
type v
two state
feature ).
way ).
hence may
second list
26 ),
six systems
interesting ways
surface word
internet search
\[ aho
classes may
reinforcement learning
collection effort
another key
construction ),
), provide
data also
crucial factor
henceforth referred
reflexive pronouns
another verb
syntactic structures
preliminary tests
n2 ),
grained information
less direct
cannot work
12 ),
approximately 1
null word
processing involved
structure may
simple character
two clauses
copy without
noisy speech
ones whose
syntactic constraint
average precision
slightly different
would affect
mapping rule
direction ).
turn makes
9 \].
), memory
single pattern
observed sequence
single term
p ,,
understanding applications
baseline models
embedded inside
hatzivassiloglou et
retrieval models
decoding process
pushdown automaton
speaker identification
l ~.
accuracy results
features appearing
applicable across
null based
average percentage
entirely clear
many aspects
category information
sentences used
/~ c
soft clustering
edges ),
taggers trained
2 defines
rules operate
tagger output
)) 1
text fragments
analysis tree
), object
overall translation
concept level
zero counts
also distinguish
np rule
already represented
1989 ))
take values
incorrect answer
whether certain
main clause
could view
relations could
free language
theoretical background
embedded noun
among verbs
level rule
one branch
distinguishing feature
motivated rules
understanding systems
earlier systems
strong argument
chinese verbs
identifying word
another event
actually spoken
additional complexity
context allows
used ),
approximately 500
finite state
low weight
carlson et
processed first
structure becomes
qui sont
formal account
clearly wrong
system utilizes
whose goal
wsj data
knowledge encoded
systems tend
linear order
plural nouns
transformation based
broad coverage
one column
finite clauses
n represents
second language
without also
correspond roughly
features structures
working group
2 minutes
scoring methods
tagging methods
may already
1 ....
every parse
may notice
among others
lisp code
1981 )).
basic facts
two alternative
straightforward solution
dramatic improvement
latter category
relevant sections
traditional view
current project
possibly also
taxonomic structure
also attempt
including syntactic
specific vocabulary
crossing brackets
syntax trees
c ()
also affected
0 6
categories must
particular version
el al
directly model
nl understanding
simple case
seems like
equivalence class
5 minutes
sentence e
among sentences
particular grammar
basic types
would expect
chinese sentences
discourse theories
quot ;?&
worth mentioning
effect relation
step would
original corpus
sekine et
advanced nlp
dimensional space
program called
also lexical
one exists
knowledge required
sufficiently general
structural configurations
retrieval purposes
sequence c
good word
still considerable
german research
methods developed
agents involved
semantic tagging
also act
perfect agreement
dialogue context
(~) 1997
annotation experiments
memory space
subsequent phases
segmentation tasks
may reduce
existing hand
translations may
text annotation
term pairs
documents returned
last four
search based
recognized words
recall rate
natural approach
standard syntactic
rich variety
system designed
tile algorithm
\[ mckeown
technical problem
many difficult
another useful
one factor
show two
different measures
1992 );
less useful
printed dictionaries
investigating methods
less severe
high value
competition among
training corpora
hierarchy provides
theoretical framework
significant additional
might arise
classifier returns
parsing complexity
various patterns
input mode
), although
also refers
single constraint
much fewer
computational perspective
complex types
different view
almost invariably
n },
discourse topic
exhibit many
becomes evident
measure performance
text ).
governing verb
3 briefly
english counterpart
present paper
large bodies
collins english
many information
specific relations
research issues
reference points
implicitly assumed
substantially worse
deterministic algorithm
45 %,
significantly different
pilot experiments
option would
nouns cannot
particular component
complicated cases
prior research
class lexical
2 indicate
., john
extraction evaluation
different feature
polynomial algorithm
approach permits
practical nlp
boston university
free productions
approach --
modifier ).
resulting phrase
candidate sentence
satisfactory performance
using smaller
two frames
annotation framework
based recognition
scoring program
one context
language since
cannot even
system proceeds
discharge summaries
one relationship
also rule
would simply
rules provided
\] describes
element ).
g c
algorithm considers
information across
presentation sessions
recognition must
another type
encoding initiative
research tool
without exception
present implementation
another topic
np subject
manual editing
entries corresponding
8 ].
computer terminal
roughly comparable
small improvements
another discourse
engine returns
grammatical ones
russell et
separate experiments
tagging errors
every natural
may well
knowledge processing
also creates
ninth international
representing two
number k
strong performance
baseline model
processing system
method 1
charniak et
use existing
particular string
direct consequence
text 1
algorithm depends
may turn
1962 ).
three subsets
factor used
approach involves
including proper
). 10
manageable size
composition process
source structures
left behind
fourth step
consists simply
sentence used
des r
., k
structure also
two pos
00 ).
basic language
use tile
procedures described
normal way
mentioned explicitly
operating system
documents used
3 \]),
tasks like
grammars provide
human annotation
bootstrapping procedure
v ~,
use various
distinguish three
trees given
may see
dependent words
dynamic process
large corpus
thank david
many items
subcategorized arguments
also increase
effective method
ill tile
first year
matching score
given language
form --
retrieval effectiveness
ambiguous word
examine whether
speaker must
still significant
add words
reducing ambiguity
first thing
task required
); 2
/( p
standard prolog
time processing
captured within
uses several
technique proposed
null tive
projected onto
less computationally
elements ),
really means
every semantic
last class
modifier word
much detail
testing examples
phrase used
evaluate whether
specific parameters
human ).
source documents
event may
many rules
performs slightly
techniques discussed
000 lexical
certain level
different names
subcategorisation frames
person plural
complex language
nominal expressions
automatic pattern
european community
level information
functional words
semantic conditions
without requiring
text parser
28 aoi
knowledge domain
new entities
generation ).
strictly local
conceptual level
recognition phase
training portion
square test
type person
high accuracies
also account
), see
), knowledge
trec questions
easily see
formalism allows
human error
6 lists
remain unresolved
features specified
simply look
sense inventories
single noun
), machine
dialogue annotation
computer memory
model takes
general enough
supervised classification
since \[
general properties
e c
intuitive notion
items ),
starting point
turing machines
usually represent
preceding one
linguistic sources
resulting classifier
better measure
structures cannot
general kind
update rules
pattern specification
z ).
data required
analysis technique
problem due
tense marker
generating sentences
research interests
morphologically rich
represent one
z \]
test data
produce results
quantitative measures
van benthem
one syllable
;~ e
third module
include domain
syntactic unit
r [?]
table 4
indexing terms
following fragment
using transformation
language vocabulary
allow easy
bbn byblos
dependent clauses
combined method
problems concerning
rough measure
information system
boundary locations
optimal solution
joining two
model determines
typographical errors
p may
1983 \].
conversational telephone
%, since
selection phase
like grammar
approach based
noun groups
procedures used
approximate solution
using automatic
together according
3 evaluation
another example
cases would
tables 4
). computational
must start
include words
gram word
system proposed
good chance
three subtypes
test suites
different nodes
rank sentences
derived automatically
-- often
standard dynamic
relatively coarse
word represents
better suited
algorithm outperforms
knowledge without
token ),
information management
another problem
generally found
7 seconds
connects two
class assignment
), thus
generation modules
also performed
evaluation scheme
readable dictionaries
hmm models
structural features
66 %.
two templates
short vowels
also subject
case .)
reliable information
underlying grammar
\] propose
b \[
verbal form
following entries
text pre
sequence p
produce two
one annotation
product ).
mapping rules
local ones
last paragraph
order variation
directly applied
verbs using
technical support
approach uses
shows another
recognizer output
parameters th
likelihood criterion
new perspective
tag assignment
language work
current phase
theoretical linguistics
semantic contribution
particular features
provide background
widely assumed
10 million
existing lexical
word dependency
directly within
recently published
explicitly given
domain using
important enough
key differences
important properties
np rules
whose contents
indexing scheme
earlier studies
three arguments
extremely helpful
appendix contains
every one
certain sense
always realized
recursively defined
resolution algorithms
kaplan 1982
also lead
problem occurs
n )),
german corpus
computational grammars
level rules
plan operator
semantic module
pairwise agreement
several ways
list ).
operational definition
1 reports
representative sample
time without
complex morphological
assist users
one ).
algorithm still
results demonstrate
minimum value
generalized iterative
best represented
system extracts
resulting state
naming conventions
spatial prepositions
word probabilities
also difficult
name tagging
information acquired
various criteria
exploring methods
similar data
distinct word
paper argues
another aspect
form based
grammatical relations
following situations
crossing edges
600 sentences
test showed
arbitrary length
main difference
grammar parser
surprisingly high
literal meanings
sun microsystems
based structure
44th annual
many trees
new part
ranked higher
., words
input sequence
text snippet
gram statistics
huge set
output structures
approximately 70
constraint c
processing techniques
pairs extracted
optimal sequence
1957 ),
theorem proving
humans process
selected based
\[ van
association measures
direct comparisons
language specification
final sentence
assign scores
good place
english language
degree 2
word repetitions
also information
1 since
et a1
original rule
e would
given access
summary based
internal representation
see note
cf .,
optimized using
act information
modular way
identification task
various steps
true probability
xn ).
2002 )'
approaches may
). together
example 2
need special
also determine
effective way
91 ).
line ),
performing systems
rules produced
.... \[
compound nouns
referent may
result would
specific needs
actually make
results provided
intermediate levels
highest priority
91 \]
second interpretation
short ).
different learning
particular element
left part
accurate classifier
3 cases
., without
one 1
robust enough
string z
state approach
exhaustive list
., l
answer pairs
using several
ones based
front end
adding one
information plays
single unit
optimization problem
frequency within
volume 14
selected verbs
). considering
full syntactic
4 thus
75 %.
user satisfaction
training size
suitable candidate
increasing attention
r 2
current feature
lexical item
generated text
learning data
two main
mentioned entity
small portion
target application
formal terms
clustering algorithm
real estate
given sufficient
major classes
\] ....
v e
particular relation
models considered
problem using
case endings
weak points
classifier would
summary statistics
provides several
dictionaries contain
x ~)
), parsing
reported accuracy
relatively high
developing automatic
categories assigned
linguistically justified
driven methods
might represent
reduction step
large lexical
19th century
document matrix
correctly disambiguated
minimally supervised
dialogues collected
main component
manual transcriptions
previous sentences
clear indication
sentence 4
backward looking
clearly shown
different parameter
precision figures
one finite
dekang lin
parsing decisions
recently begun
2 0
another consequence
79 ).
system builds
next round
pos tagged
). obviously
selecting sentences
tag information
construction process
features might
single clause
possible states
another name
first defined
inductive learning
thus provides
individual discourse
process requires
79 \]
briefly consider
knowledge resource
suitable antecedent
straight forward
term whose
objects representing
hypothesis test
princeton wordnet
g '.
one parser
get away
w n
larger dictionary
must still
system offers
greek letters
interactive natural
remaining features
special tag
generally less
usually described
good candidate
1 )-(
major reason
5 0
extraction patterns
contextually appropriate
set also
available corpus
best sentences
functional form
rather weak
different computational
preferred interpretation
language morphology
input phrase
contains lexical
1998 \].
following terms
tile analysis
semantic constituents
ntt communication
czech data
tool used
2 ):
wordnet taxonomy
module consists
cannot generate
would eliminate
semantic understanding
even stronger
ambiguous input
dang et
english morphological
initial semantic
flies like
evaluation methodology
;). however
semantic well
smallest unit
hmm model
time users
defining two
linguistic factors
might serve
phoneme strings
first guess
corresponding string
analysis part
trees associated
90 \].
components --
development purposes
first show
translation equivalents
would miss
tagging text
semantic basis
made without
strategy might
identi cation
robust approach
5 ):
local relations
participants may
language behavior
appropriate content
work best
important elements
correct ),
interactive speech
subsequent use
interesting application
network structure
ordinary language
hahn et
term would
position 3
resulting text
large group
time point
users need
significant work
certain forms
necessary step
type attribute
parameter values
initial evaluation
object relation
examples illustrate
common ground
alignment results
stolcke et
one percent
examples 2
77 \]
three principal
worse performance
proposed system
formal logic
automatic procedures
probable analysis
set represents
quickly becomes
another property
objects described
multiple readings
new path
might play
sufficient context
following scheme
longest possible
conditions ).
person ).
hierarchical relationships
algorithms rely
certain respects
new generation
disambiguate word
\]. since
still needed
average 1
constrained grammar
manner using
several translation
improve word
high degree
certain goal
models presented
1987 ))
become available
possible utterances
one paragraph
occurrence statistics
n (:
word phrase
first criterion
unannotated corpus
query generation
current focus
effort would
best configuration
86 ).
training session
evaluation set
measure proposed
achieved performance
derive new
vp \[
concepts using
sentence segmentation
spontaneously spoken
studied problem
lower error
presents one
phrase pattern
kupiec 1992
large increase
simple mechanisms
predictive models
contain much
distinguishing description
86 \]
important new
2 ),
two search
conceptual framework
control structure
usually used
phonetically similar
cluster members
7 ].
variable name
thank aravind
abstract features
binary relations
requires detailed
argument representation
competitive performance
evaluation workshop
generate words
validation set
database queries
dialog box
hardly surprising
described previously
groups according
normally associated
non -)
measure ),
., 1978
sequence based
objective function
whose content
correct pronunciation
etc .)
\[ 23
regular grammars
verb form
apply across
5 ),
growth rate
syntactical structure
corpora available
although two
avoid data
also handles
underlying hypothesis
construction method
great detail
linked together
michael collins
approaches also
psycholinguistic literature
consistently outperforms
another piece
...& quot
bigram model
mutual belief
smaller units
approach achieves
system stores
initial one
argument type
word collection
%). however
sanity check
closely resembles
original information
limited information
hierarchical relationship
highest degree
tried first
high frequency
allows direct
achieved even
efficient data
labels assigned
tightly coupled
work aimed
stimulating discussions
term p
token sequence
many long
formal device
within linguistics
similar pairs
intended meaning
particular construction
one expert
constraints hold
computational load
parsing task
official test
lower word
research goals
perfect performance
syllable boundary
may exhibit
improved language
techniques use
general discussion
johanna moore
article ).
simple solution
typical user
still available
result reported
particular value
first proposition
given multiple
natural languages
best accuracy
different human
(: ation
), 9
sufficiently expressive
results produced
weighting methods
7 ~,
whose label
target phrase
representation level
main features
words ending
document management
model consisting
significant performance
one expression
based chart
system makes
sentence structures
viterbi search
symbols x
three entities
constituent analysis
maximum likelihood
)= 2
two error
right answer
common meaning
topical structure
wordnet version
online sources
seems also
x2 ,...,
would recognize
personal computer
used method
corresponding features
shows significant
main dictionary
string length
several practical
anaphoric pronoun
node 2
john thinks
standard linguistic
broadcast information
subcategorization patterns
cannot express
high potential
transfer rule
reasoning mechanisms
top levels
attachment site
weights assigned
special set
50 words
understanding natural
object referred
long texts
important area
identifying new
logical structure
challenging task
ne system
corpus information
different machines
performed based
two related
second phase
often lead
certain phonological
highest frequency
retrieval ).
superlative adjectives
thomas j
paper addresses
one person
wiebe et
also considered
poses problems
neighbor algorithm
major types
r .,
different tools
one extra
inner product
two similar
future developments
different architectures
two devices
remaining two
general techniques
classes without
information alone
one source
dictionary based
final values
textual material
control mechanism
inclusion relation
null guage
tag trees
words sharing
resulting output
syntactic grammar
shows sample
value pair
bilingual corpora
various relations
models like
parse selection
obtain good
evaluation process
one block
quite often
functional relationships
relatively fast
constraints could
enabling users
structure without
one normally
sources including
computer languages
35 ),
heuristic search
quot ;/
depends heavily
small amounts
second highest
systems \[
two word
penn chinese
used co
certain case
features allow
1 indicate
general knowledge
sentences often
causes problems
last approach
neural net
original rules
discourse theory
validation process
method allows
would contribute
general machine
related topics
suggested using
simple sequence
alignments produced
third sentence
dagan et
7b ).
6 months
error made
right away
indexing process
statistical language
search system
dozen different
formal definition
database includes
recognition tasks
uses different
could identify
specific terms
-) automatically
technique could
standard models
precision value
treebank grammar
whose application
instances may
becomes important
test question
human annotators
edge e
data comes
1 even
methods described
generalized version
could probably
whose probabilities
relevant data
predict whether
mean average
test time
straightforward extension
tile root
}. let
next week
per se
characteristic feature
dialogue modelling
parse process
students learn
corresponding relation
style grammar
tagger achieves
brief review
new car
syntactic component
symbolic systems
), syntactic
system attempts
tm system
gram counts
train classifiers
first international
\] since
constituent parts
combine together
candidate list
markov chain
specific dictionaries
unified account
count noun
second feature
formalisms like
storing information
2003a ).
clusters using
000 trees
character length
easily extracted
), given
length k
relations \[
b c
smaller set
full tree
increased complexity
lexicon based
lie outside
without imposing
show one
first trained
work needs
preceding np
heavy use
testing purposes
attribute value
grant mcs
technology development
clear preference
contains information
results prove
tags ),
main difficulty
automatic alignment
take part
formula 3
unsupervised wsd
level goal
correct english
develop automatic
minor modifications
following definition
candidate answer
major challenge
scoring algorithms
spelling errors
language rather
important knowledge
text annotated
hierarchical clustering
sense annotation
must relate
[?] 1
realization module
slightly lower
transcribed speech
julia hirschberg
various parameters
specific grammatical
basic definitions
automatic extraction
vocabulary contains
time taken
determining sentence
idiosyncratic properties
two crucial
simple maximum
nlu system
boundary information
particular requirements
potential solution
). specifically
encoding standard
coordinate structure
collins et
main results
weight assigned
standard arabic
multiple lexical
system could
subordinate clauses
klavans et
could begin
phrasal patterns
great many
even lower
project number
nodes dominated
15 %,
complete phrase
utterance units
adding additional
automated analysis
people make
corresponding results
instance ).
discourse participants
0 ...
string similarity
tree must
effort spent
massachusetts institute
sense ).
), modified
incorporating information
many ways
answer would
single grammar
experiments demonstrate
program takes
systems according
8 4
robust techniques
2 suggests
see \[
j ))
latest version
discussed earlier
theoretically sound
bangalore et
based criteria
definition includes
usually defined
method yields
earlier one
grammar allows
systems tested
evaluation method
relative likelihood
usually refer
methods used
exist among
definition given
graph representation
brill tagger
2 english
entire process
ambiguity must
tree path
four independent
,. l
per day
number within
produced summaries
6 summarizes
sophisticated system
rate reduction
verb within
grammar approach
model significantly
main components
generation approach
words among
side conditions
tree grammars
chinese translations
relatively well
propositional attitudes
roman numerals
kappa statistics
better understanding
validation method
actual values
single coherent
specific tag
likelihood ratios
cannot capture
,..., c
inference techniques
). current
using bayesian
5 english
inner structure
40 different
mt ).
user question
7 e
spanish version
correct set
church et
indexing method
one antecedent
pushed onto
bootstrapping methods
)) using
constraint based
using default
sentences might
peking university
independent units
appropriate phrase
says something
e rule
credit card
4 given
slot fills
clearly indicates
sense 1
8 show
contains entries
may modify
companion paper
sag 1987
way using
words belong
let us
pattern appears
sentence fj1
contains rules
features needed
data flow
fits well
); null
parser produced
single component
copestake et
human annotated
000 characters
external information
learning time
randomly generated
sentence often
general parsing
using 5
b would
computing similarity
works better
recognized word
grammatical category
discourse segmentation
writing system
another term
stochastic segment
6b ).
features introduced
processes may
rather similar
adverbs ).
compilation techniques
following sections
segmentation problem
(+ 1
database interface
html markup
salient discourse
de ne
chunking task
null thus
language ),
general tendency
basic feature
clearly demonstrate
processing includes
previous level
might prefer
sidner 1986
experiment reported
least 5
taking two
whose left
l \].
ken church
using different
v ...
acquiring semantic
standard model
lexicon design
mapping procedure
sentences longer
output distributions
dependent ).
agents may
see http
human memory
called topic
la construction
structure corresponds
ner task
stress pattern
called head
cluster together
called discourse
also varied
prior information
edr corpus
achieve accuracy
feature vectors
specific nature
standard form
given feature
case frame
two general
single string
a1 ,...,
previously identified
less complex
current database
generally used
become necessary
apply machine
type inheritance
variables associated
also like
state transitions
one allows
dependencies among
individual pieces
used mainly
precedence relation
implicitly assumes
ending position
3a ),
language terms
sentence expressing
lambda calculus
node marked
academic papers
quite expensive
). depending
verbal nouns
quite significant
one attribute
theoretical approaches
occurrence counts
syntactic features
also select
overall time
corresponding value
one approach
applications would
phrase headed
gratefully acknowledge
also enables
000 unique
generally acknowledged
good knowledge
complex feature
yet complete
one special
tree structured
test would
explicitly models
different role
system gives
base consists
utterances whose
improvement obtained
fully connected
features available
representation framework
considerable time
always need
briefly describe
,...& quot
occurrence matrix
language lexicon
1b ),
sigmoid function
open questions
l .,
translation tasks
full linguistic
paper discusses
pp ).
small initial
done recursively
thorough evaluation
niveau de
representation like
plan inference
language syntax
discourse structures
3 reports
modular structure
segments may
pp \]
restricted vocabulary
arranged according
two input
knowledge ).
based ones
noun type
p ,.
specific objects
matching words
transfer rules
). null
frequent nouns
already included
syntactic descriptions
markov assumption
one direction
final list
impossible without
1992 \]),
case may
word pos
since every
autre part
completely satisfactory
essential feature
algorithm applied
model representation
even use
news report
restricted number
encoded within
results show
dependent word
provided information
p l
two major
sun et
thus expect
81 \],
two document
procedure may
patterns involving
available information
target text
particular pair
expected value
still necessary
major motivation
resolving pronouns
reported experiments
two constructions
linguistic resources
3 since
f --
advanced computer
typical information
wrapped around
linguistic expressions
mechanism allows
reasonably small
first non
boundary symbol
textual data
final answer
information given
might refer
valued function
different verb
question marks
several data
1 0
created based
generated based
implementation described
() ur
intermediate level
several methods
). discourse
described informally
given names
also try
system recall
time ).
another constraint
first reading
months ago
first compute
cases except
cant improvement
learning research
paper outlines
phonological features
system looks
independent set
nlp community
1997 )),
rules require
occurring words
function might
wrong results
). three
verbmobil system
end application
sophisticated techniques
free parameter
testing sets
method using
robust parsing
multiple reference
wsd methods
text using
idf term
root node
global features
new states
decisions made
agreement phenomena
generated directly
geographical locations
approximately 25
figure ),
applied rule
clearly impossible
syntactic relation
specific set
ensure consistency
algorithm applies
used consists
state sequences
roughly equal
phrases ),
average mutual
allow non
better strategy
many relevant
several classifiers
normalized score
6 c
sentence might
1 ):
par un
.)& quot
filtering step
observed word
past tense
), r
simple clauses
following order
interannotator agreement
alternative sentence
20 minutes
different event
developed based
expressive enough
initial consonant
dialogue module
sophisticated knowledge
novel language
sentence alignment
context x
10 \])
trec topic
primary objective
u b
number ).
stru (:
selection algorithm
relative probabilities
automatically create
,/ 3
final language
system assigns
work ),
indefinite article
verb ).
spelling correction
japanese ),
components involved
task --
automatic grammar
positive impact
existing ontologies
adverb ),
automatically generated
verb \]
sentence extraction
us ),
former two
originally defined
user would
make certain
attachment preferences
cannot describe
92 ).
relations need
new tasks
structures called
np -->
include word
string given
two candidate
92 \]
resulting word
single dictionary
containing information
language units
real application
e information
represent relations
third reason
generate hypotheses
much larger
much time
f ,'
two children
singular ).
., 1999a
3 )-(
morphological processes
semantic values
temporally ordered
function ),
phrase alignment
multiple information
interesting question
weakly supervised
longest matching
l ,;
words since
logic grammars
character may
., 2003
us consider
default rule
html ).
ranked candidate
therefore requires
models perform
obj ).
recently attracted
verb v
grammatical units
measures used
task involves
100 pairs
stated otherwise
systematic study
following translation
meaning may
de vue
internal organization
requires substantial
rules specified
every argument
generated sentences
., v
cluster containing
data --
oil tile
phrase extraction
possible variations
number 1
occurrence constraints
semantic classes
weighted according
implementation used
equal number
two properties
chinese named
time bound
international news
), john
data acquisition
hand labeled
1 ~)
would give
manually built
two kind
approach assumes
plan recognition
us compare
initial steps
n --
many practical
opposite case
different depending
results confirm
similar lines
comparatively small
). generally
eu project
important characteristic
., 1
restrict attention
small difference
modeling approach
also section
webber et
process whereby
first constituent
1 ),
japanese dictionaries
entries may
obtained automatically
note however
explicitly model
certain contextual
new utterances
level translation
1995a ).
evaluation efforts
linguistic interest
proper place
brings us
category label
pattern set
simple surface
put differently
would normally
adverse effect
). example
additional condition
tree nodes
one already
whole sequence
existing software
semantic network
topics covered
two variables
90 %)
syntactic choices
usually require
possible constituents
least twice
single element
rather general
based work
description would
formalism provides
text along
first consider
task related
grammar using
generation using
extraction performance
avoid producing
hand coded
per token
provide empirical
provide valuable
somewhat limited
another noun
identify several
discourse cue
find many
several possible
becomes extremely
subject relation
length model
several sub
simple predicate
running time
likely part
yang et
), k
broad sense
bias toward
decision whether
constraints placed
therefore provide
different representation
previous experiments
including context
frame contains
text passages
original definition
rate increases
added automatically
base \[
major part
1 ),(
growing interest
could represent
,: e
order unification
first compare
semantic considerations
single relation
appropriate surface
size would
classification problem
actual answer
information explicitly
adwait ratnaparkhi
build models
engineering point
wsj sentences
complete text
fair comparison
original meaning
details ).
english names
predicate symbol
2 illustrates
alternative derivations
certain phenomena
prototype system
shorter words
n figure
feature convention
speech corpora
several options
label assignments
statistical estimation
e rules
immediate constituents
randomly chose
pragmatic relations
many chinese
parsers based
approach follows
already achieved
binary variable
h ),
component makes
improve system
may affect
multiple entries
complex questions
specific domains
different type
ie ),
missing information
work builds
different topic
must recognize
multiple rules
standard context
understanding research
different discourse
performing one
far less
rules expressing
names within
different analysis
general syntactic
seed word
5 illustrates
explicit temporal
\], however
new insights
task based
possible candidates
second measure
algorithms using
baseline trigram
dialogue turn
u f
parameter optimization
methods discussed
four additional
temporal constraints
much shorter
19 ):
return true
analysis tasks
specific applications
test scores
q ~,
corpus like
baseline algorithm
;). therefore
structure formalism
u r
easily recognized
particular structure
language makes
interpolation weights
written grammar
full model
applying bayes
results vary
features two
-- two
known information
accurate word
7 4
independent systems
actual language
identity function
nouns must
could serve
must bear
computational advantage
sample dialogue
appropriate text
finding one
\], \[
issues related
per category
sophisticated parsing
sentences );
heterogeneous data
proposes two
brackets ).
different times
free rewriting
dictionaries ).
separate classifier
n word
several months
specific systems
since lexical
cannot conclude
tree learning
strict word
demonstration purposes
document may
tag g
evidence suggests
conceptual dependency
rst time
hoc analysis
85 %,
two novel
new entry
often makes
model including
performance measure
data based
one using
0 0
whole phrase
system made
simple pos
used \[
translation algorithm
frame representations
ambiguous case
less often
one well
five possible
baseline approach
method outlined
short discussion
made precise
describe syntactic
80 %.
value among
rules rather
speech rate
explanatory power
order constraints
discourse unit
indicate word
used semantic
widely applicable
easily converted
recently developed
linguistic usage
problem associated
similar behaviour
structural transfer
6 p
key idea
problem remains
top level
systems would
intended application
strung together
empty string
useful data
several pairs
mit press
john bought
viterbi alignments
low inter
resolution system
user asks
shows results
may expect
important syntactic
first determined
possible parts
like verb
three characteristics
algorithm 1
readily accessible
1 second
sometimes difficult
subsumption relation
two elements
appendix 2
kindly provided
1974 \],
identify key
wsj test
optimal search
freer word
). e
also relevant
practical value
produced two
matter relations
adjectival phrases
content structure
compression rate
bipartite graph
independent method
dialog manager
19 ),
input --
xl ,...,
summary length
word stems
specific order
structural similarities
simple extension
sentences 3
incorrect analyses
fundamental idea
dictionary system
total word
new tool
went home
6 %),
process includes
sentence gets
even slightly
rules might
1985 \]).
adjoining languages
complete coverage
table 3
;. moreover
different portions
latent semantic
third dimension
rules become
6 %).
work suggests
infinite sets
might require
., 1999
page ),
3 indicate
shows two
communication system
sense tagging
', j
surface syntactic
features necessary
potential users
grammar notation
800 words
growing need
automatic dictionary
95 ).
lines show
different genres
system consists
english questions
different problem
), words
;. using
remaining one
tile result
predicate logic
using tf
word belongs
three experimental
findings suggest
ra (:
also perform
tagging accuracy
node type
possible different
various grammatical
two families
selected 50
robust syntactic
using specific
syntactic means
structural position
captured using
sentence grammar
also assigns
testing ).
search may
null ments
classes ).
another clause
human languages
adjunction site
interval \[
error occurred
linguistic patterns
structures must
2 note
component could
semantic types
context vector
\[ halliday
alternative values
many application
may simply
experimental settings
study reported
applied rules
text comprehension
grain size
particular genre
may trigger
.. 4
committee members
higher number
previous case
either type
possible transitions
three nouns
detailed results
automatic indexing
50 ).
cannot really
constructions whose
head verb
0 ),
narrow domains
utterances based
single value
texts --
spelling mistakes
new algorithm
word --
algorithm employed
string associated
e considered
corner parsing
standard clustering
5 note
automatically creates
two columns
source file
\] 0
primary data
prohibitively large
larger discourse
find optimal
first item
28 aoot
characters ).
different aspect
phrase grammar
training results
h (~
ill terms
., person
key insight
network structures
many valuable
input devices
statistical measures
v ,~
lexical semantic
hierarchical approach
across speakers
correct boundaries
morphological generation
sufficient conditions
third language
eh8 9lw
dialogue situation
implicit relation
grant 01
totally different
several limitations
lexical resources
practical utility
logical formula
generalized lr
lexical expressions
roles based
feature combinations
logical operators
~, 6
two base
patterns containing
content overlap
discourse history
actual use
final interpretation
first 100
including 1
input word
structure similar
head daughter
intention recognition
introduce new
example dialogue
optimal word
people must
state x
default rules
language applications
\[ 5
detection techniques
common data
structural patterns
sufficiently small
within individual
basic tools
ai ).
still open
three states
compact representations
using appropriate
linguistically sound
). unfortunately
analysis tool
phenomena within
reading ).
nodes whose
constraints \[
current representation
also grateful
one important
(: c
q e
could support
final part
individual concepts
similar lexical
computationally effective
ldoce definitions
tile language
classes derived
modeling techniques
words even
\]. second
language systems
f ~(
possible sentence
sentence si
grained scoring
nlg ),
sentence string
compound sentence
also derive
symbols must
closed vocabulary
certain phrase
test design
logical relations
abstract structure
stop list
method cannot
directly using
system via
give results
one situation
experiments show
departure time
control parameters
4 presents
grammatical systems
reference resolution
two halves
content planner
forme de
language tend
dashed line
enough knowledge
earley algorithm
entity types
translation module
expression patterns
rows 2
first set
wn ).
company ),
clarification question
providing evidence
difference among
~, respectively
three tasks
current analysis
\[ miller
html document
sentential level
first query
\] proposed
turn would
unit ).
whose output
entire data
commercial advantage
one node
proposed solution
less effort
submission received
step procedure
20 ):
working towards
complexity analysis
sentence training
relative clauses
current goals
different name
many methods
capture certain
richer feature
produces many
must produce
information processing
would typically
.) 5
many modern
constituent types
includes part
separate text
11 \].
scoring metric
major feature
also lists
first training
original translation
disambiguation component
document relevance
expected frequency
two essential
human tutor
\] ),
becomes much
clearly defined
next words
generally require
ones obtained
larger portions
clause constructions
systems reported
1 depicts
successful approach
produced automatically
less expensive
less significant
factors involved
learning lexical
type b
analysis algorithm
whether two
full context
parser trained
fully expanded
high rate
similar rule
segment boundaries
values range
texts based
higher performance
pp attachment
strongly connected
comes across
word based
equal weighting
reduce ambiguity
questions asking
processing models
uses language
martin et
boolean query
put forward
two thresholds
two slots
dialogue manager
learning stage
empirical experiments
forms found
phrases could
frequently occurring
1999 ):
arbitrarily set
negative evidence
three utterances
using purely
recognition problem
row gives
little use
position independent
unseen data
various objects
remains unchanged
accuracy rate
different dialogue
acceptable translation
since information
corresponding structure
numbers associated
made us
could employ
approach may
application would
following statements
candidate feature
systematic differences
clearly show
50 percent
four letters
much evidence
computational analysis
allan et
process depends
untagged corpora
results might
make things
still less
approaches mentioned
documents available
system \[
microsoft research
development environment
algorithm shows
reduce parsing
special semantic
text surrounding
r u
different semantics
many good
rules allow
occurrence restrictions
johnson \[
); 4
us call
user group
two knowledge
evaluating different
output file
propose another
classify documents
often gives
newly created
word statistics
node representing
make another
three years
feature mapping
plans include
taking one
microsoft windows
cannot combine
speaker believes
20 ),
two machine
effective communication
strongly affected
common sub
following data
1992 8
hmm approach
largely depends
given priority
correctly determined
ted briscoe
embedded verb
noun senses
concatenative morphology
tighter integration
lisp function
dependent elements
solution based
possible derivations
conditional distribution
retrieval techniques
unless otherwise
correct parse
tree transformation
enabling us
must rely
error would
domain entities
remain within
easily expressed
tags per
would say
therefore important
documents written
specialist lexicon
improve parsing
main differences
linguistic criteria
specific texts
meaningful information
experiments confirm
1996 \].
87 \].
us describe
takes time
certain attributes
topics discussed
automatically determine
methods use
exact answers
adjunction operation
motion verb
manual labor
parentheses ).
respond appropriately
extraction ),
declarative sentences
directly comparable
internal representations
trivial task
models assume
often co
lexical transfer
text editing
rule application
every token
\[ 32
2000b ).
shall focus
text provides
form may
many errors
automatic construction
patterns found
selection strategy
current problem
l ;\]
probabilistic part
examples used
set --
set union
1999 ),
event ).
recent efforts
inflectional endings
every edge
). indeed
vp rule
g ,,
syntactic alternations
true ).
second key
information without
end time
introduces new
whole text
low degree
parsed correctly
third sigdial
extremely unlikely
provide alternative
great improvement
many possible
', c
future applications
required information
obvious differences
e ('
tile use
short sequences
simple search
recall drops
target verbs
let alone
task involved
generative grammars
98 %,
\].& quot
3 0
node name
major focus
distance movement
since rules
language dependent
accurate enough
algorithm performs
two events
walker et
relation types
manually analyzed
line resources
move away
strings containing
add new
con guration
user speaks
tested whether
predefined categories
takes approximately
combinatory categorial
figure illustrates
alternative realizations
tree consisting
', q
boolean features
always refers
framework proposed
also know
identify different
analysis yields
certain state
). 4
experiments indicate
many options
matrix clauses
p ).
resolve word
real text
highly modular
systems attempt
independent acoustic
entropy modeling
overall design
\] es
graphs show
class probabilities
adjective pairs
type r
must give
p \]
performance significantly
exact number
one parent
following \[
system according
x \[
thank three
linguisti c
type associated
exact matches
telephone line
developed system
different pieces
various attempts
computational problem
1960 ),
domain model
real time
final phase
terms rather
low frequency
\], c
cation problem
questions within
path ),
statistical learning
parameters --
relative salience
manually derived
good start
use full
target sentences
brief discussion
every category
annotators ).
largest class
symbol pairs
immediately apparent
case without
special interest
special cases
finds one
another difference
., finding
short queries
length 1
correct transcription
empirically found
thus demonstrating
seem like
concept c
wilks et
false ).
). participants
system evaluations
processing model
every symbol
may miss
using language
average sentence
entries contain
', whose
adjective phrases
approximately 400
easily incorporated
gorin et
results indicated
many incorrect
n matrix
general observation
another version
input document
feature associated
following sub
three subjects
another grammar
issues involved
correctly identifying
theoretically interesting
structures assigned
analysis module
gram probability
new constituents
common phrases
problems posed
grammar provides
greater amount
linguistic communication
15 ].
include new
reflexive closure
event class
1986 ).
16 \],
better result
words rather
commonly assumed
entirely within
done based
resulting system
semantic processes
istituto di
general concept
rates reported
1986 \]
entropy framework
selectional preferences
built automatically
constituents according
adjacent nodes
similar information
already encoded
possible improvements
parsing procedures
linguistic ones
correct speech
morphological characteristics
could refer
nirenburg et
two parsing
analogous way
1 illustrates
also proposes
1993 );
automatically inferred
single pair
sufficiently accurate
automatically generate
many pairs
ie systems
given action
j \[
annotation schema
negative sentences
like e
less coherent
fairly large
3 ),
term goal
visualization tools
inflectional forms
distinct categories
various domains
set based
correct semantic
general linguistic
could potentially
unbounded dependency
works include
data across
\] suggests
1983 )).
use cross
identified manually
several competing
remaining ambiguities
possible values
lexical patterns
therefore include
occurrence may
activation level
new goals
systems group
1 gives
cannot decide
role filler
knowledge base
novel feature
year 2000
knowledge already
frequently asked
possible senses
search decoder
simple word
irrelevant documents
generation conference
property p
overall accuracy
character words
us recall
two restrictions
final feature
various applications
airline guide
lexically specified
knowledge acquired
could develop
acts may
4 %.
referent ).
88 ).
based lexicon
1991 ).
possible classes
automated language
adequate translation
user information
another component
relatively free
included many
every variable
selects one
good compromise
one actually
88 \]
need help
multiple ways
1991 \]
help distinguish
many phrases
third part
world knowledge
speech assignments
processing problems
conjugate gradient
words become
ideas underlying
design phase
quite common
performance increases
important observation
thank eric
given verb
montague semantics
oriented language
e ~:
occurred frequently
list includes
one synset
pang et
probabilistic grammars
results suggest
thesaurus construction
., new
smaller clusters
essential role
c ~)
parser might
8 );
various systems
terms ),
direct relation
zero mean
occurs frequently
language texts
ditransitive verb
along two
lower score
..... c
values indicate
also discovered
easily applied
general technique
constraints apply
second type
'~ c
system returns
important concept
80 percent
atis database
using morphological
algorithm allows
second analysis
domain experts
also tried
occurring word
), taken
normally distributed
precise information
relative improvement
restricted quantification
research contract
\] makes
), within
first two
systems command
also showed
typically involve
another string
certain text
full parser
shall illustrate
relevant lexical
syntactic theories
formalism called
sont pas
knowledge plays
document processing
desired effect
semantic interpretation
parameters based
second step
take several
data .)
general models
syntactic structure
important linguistic
later work
model seems
following pairs
sequence may
usually denote
given event
also worth
simple statistics
better language
incorrect analysis
vector whose
people working
maximum depth
similar rules
collocations ).
n .)
tile initial
words ),
corpus comprises
approximately equal
automatically extracts
), context
relative strength
al .)
multiple modalities
use wordnet
elegant way
(' c
somewhat arbitrary
oov word
good approximation
dependent knowledge
semantic part
example discussed
disambiguation results
significant effect
allow two
complex event
space limitations
two segments
', p
performance analysis
\[ 1980
hypotheses based
us note
decisions regarding
zhou et
ary relation
textual corpus
system need
often made
nearly always
organizing principles
examples 4
already contain
complete sentences
correct one
previously published
languages ),
ideal situation
multiple interpretations
context must
syntactic head
auxiliary trees
strong semantic
6 7
intuitively plausible
text associated
also possible
comparable corpora
relevant sentences
;, meaning
current trend
four separate
rules governing
e g
given task
surface linguistic
arbitrary context
medium size
reasoning based
may bear
always lead
maintenance manuals
acquiring lexical
;;& quot
verb occurs
features one
-- one
primary importance
nodes correspond
stay within
major tasks
cfg ).
sample size
constraints c
component consists
... wj
author ).
.. n
components may
search spaces
tagged correctly
treated specially
33 ).
interesting aspects
major impact
rapid progress
spearman rank
latter would
j j
symbolics lisp
levin classes
require human
syntactic differences
rather restricted
planning approach
partial translation
provides access
., 2002b
recall curve
among words
dictionaries provide
little difference
two lexicons
clear enough
simple experiment
entity identification
single task
chinese words
elsewhere \[
systems performed
implicitly assume
questions ).
might possibly
independent speech
young children
resolution may
confidence levels
also affects
also permit
category c
linguistic work
dice coefficient
first note
tagging experiments
one nonterminal
order may
may contribute
phonological structure
language consists
case corresponds
surface words
better estimate
te (:
newly developed
described later
new products
growing number
small study
independent semantic
1 note
specific speech
first way
different pattern
different term
usually available
new strategy
global structure
2 one
abstracts away
linguistic relations
sur des
first necessary
without constraints
three layers
form solution
words appears
sinica corpus
discourse features
phrase construction
uses multiple
represent objects
2b ).
dictionary may
algorithm yields
specific semantic
research project
table containing
finding answers
second example
case particle
many text
pos features
next higher
additional effort
programming framework
system developer
different phases
material presented
potential word
create new
data base
lingual corpora
provides sufficient
standard trigram
restricted set
multiple choice
output space
application areas
specific resources
question arises
sentences shown
specific expressions
one mapping
word roots
second problem
text coherence
quot ;#&
word counts
choosing appropriate
classifier achieved
basic type
paper begins
initial category
related ones
document without
therefore given
expanded using
greatly improve
power set
similar level
shallow parse
e ',
rather simplistic
statistical tools
). despite
original data
made clear
based summaries
kappa score
\[ 1995
initial symbol
ill order
noun noun
commercial products
processing within
also tested
null l
lb ),
usually written
japanese texts
shows promising
complex representation
remaining ones
constant number
future studies
first appeared
5 cases
coming back
substantially higher
given type
trained classifier
., using
worth pointing
given topic
even relatively
nlg applications
role fillers
*& quot
given discourse
based english
symbol c
class classifier
proceedings \[
statements made
generic summaries
said earlier
category whose
new challenges
detailed evaluation
quot ;...
second strategy
2 nd
existing approaches
dog ).
examples discussed
composition rule
relevant aspects
standard approach
designed using
recognizer may
scores indicate
model language
structural relations
ne pas
preference score
equation 7
word alignments
entities mentioned
another hypothesis
automatic recognition
complex type
even human
second aspect
computational treatment
thus define
name entities
french sentence
step towards
falls outside
two basic
phase one
nlg systems
single head
generate new
tree representation
arbitrarily deep
new root
sentence means
average values
used corpus
2 .....
exactly like
general model
., see
set membership
constraints would
10 %).
testing conditions
quality machine
performance degradation
involved ).
1974 ),
relation called
made explicit
ignored ).
generation capabilities
system architecture
lexical np
previously seen
better use
third phase
possible types
ro (:
without resorting
consistently high
also increased
present analysis
six rules
ordered tree
extra feature
following question
future efforts
expressions within
previous iteration
..... p
linguistic performance
general function
interpretation based
principal component
first automatically
obvious fact
contain several
much deeper
present three
thus give
necessarily imply
semantic grammars
four kinds
determined threshold
source structure
defined two
another element
translation cannot
sidner 1979
context feature
possible implementation
rather vague
previous approaches
results available
lower probability
motivation behind
assign probabilities
operate within
trained without
better quality
possible realizations
plural pronoun
ordered list
logical terms
english news
two syntactic
stochastic tagger
learned decision
good match
input system
information taken
language parsers
essential step
different principles
among researchers
information within
possibly overlapping
individual component
lowercase letters
morphological analyses
tag pairs
another relation
improvement comes
planning module
semantic definitions
powerful features
years since
good models
two given
parseval measures
appropriate input
ask questions
2 ,...,
pas de
specific cases
handle well
larger corpora
kleene closure
underlying theory
w ~,
completely covered
), v
taskar et
extraction process
possible answer
figure 20
concepts must
automatically segmented
experiments ),
morphological description
suffix information
linguistic behavior
theoretic framework
subsequent discourse
strict separation
preceding sections
basic problem
appropriate place
let h
fast algorithm
ii ):
several areas
treated similarly
100 test
language change
preliminary test
one recent
one associated
appear inside
programming procedure
word .)
general problems
), 1
boundary tones
five levels
using normal
across applications
identify specific
formalism must
restricted domains
several lexical
yield significant
entity tags
least part
sophisticated linguistic
strong assumptions
). often
given example
component \[
known whether
situation described
looking center
originally introduced
corpus showed
types may
asr ),
dynamic aspects
process model
first generates
us something
another difficulty
taken together
frames representing
important function
third feature
computer vision
expression refers
research must
five runs
draw inferences
values 0
search engine
final states
using multiple
given problem
cases also
model contains
cases occur
three constituents
8 ],
details see
svm classifiers
model produces
encoded using
de deux
phrasal units
th sentence
produces text
system would
across systems
finer grain
ir tasks
translate sentences
3 depicts
related work
impose constraints
sentence shown
substring table
extraction applications
two probability
complex example
adjectives ).
discourse strategies
mouse clicks
comparative experiments
drawing inferences
\[ ni
precision rate
sri core
language speech
question cannot
\] gives
basic strategy
different formalisms
extensive research
algorithm using
partially due
small degree
past year
known relevant
method identifies
phonetic representation
one tree
previously translated
complex problem
first ranked
time parsing
senten (:
space would
), finite
column corresponds
phrase containing
different weight
voice input
still ambiguous
english discourse
relative error
reaction time
various places
adv ),
nodes corresponding
english analysis
connected components
around 90
success rate
value 0
present example
... c
worst results
automatically select
surface form
two rows
forms used
noun n
cannot apply
whose input
one utterance
frequency values
usually results
huge number
various texts
corpus collection
using hierarchical
closer analysis
achieved via
extraction systems
language resources
several sentences
different nlp
categorial unification
end user
language expressions
patterns among
three goals
robust model
paper briefly
character sets
line 8
nominal heads
define p
{& quot
ambiguous words
scale lexicon
order phenomena
informative words
specific assumptions
wsj section
framework could
two paradigms
definition 1
pilot experiment
large body
different points
first 20
whether one
c /(
patterns produced
phrase describing
recent progress
important problems
domain specificity
difficult case
acquiring information
two proper
rules concerning
ii ),
example contains
approach relies
sophisticated models
aligned text
semantic feature
identify appropriate
entities --
right adjunct
simple kind
10 runs
different noun
requires careful
going back
.'& quot
manual development
600 words
literary works
main theme
,' u
may signal
whose corresponding
dcg rule
model treats
users could
source position
data included
h ()
retrieval conference
presented earlier
one hidden
actually quite
different size
substantially improve
whole word
makes two
evaluation corpus
making sure
gives results
commercially available
verb node
next four
j c
descending order
ambiguity problem
first parse
applications like
often possible
7 );
work aims
)). one
lexicalized models
improved significantly
4 1
relative word
many mistakes
trigram language
r contains
separate experiment
two extensions
represented separately
always considered
experienced users
zero ).
information implicitly
suggest two
word endings
deep understanding
np whose
overall picture
would greatly
many algorithms
formula used
structure plays
hybrid methods
large collection
entire np
common stem
score among
different logical
human linguistic
output structure
individual members
average numbers
usually referred
could always
lexicon also
distance algorithm
larger structures
average f
n ,,
considerably simpler
maxent model
nested structures
would actually
6 characters
syntactic ambiguities
critical role
values ),
also add
runs using
typical application
agent might
dynamic information
two examples
3 could
module extracts
first briefly
rue de
supervised methods
conditional distributions
parser training
match fails
total length
many ideas
definite noun
software environment
linear programming
boosting algorithms
\[ 31
rows represent
manually annotated
possible morphological
connected together
particular concepts
significant changes
could use
user modeling
x would
system therefore
recurrent patterns
facto standard
approach without
01 ),
processing modules
mechanical translation
overwhelming number
currently investigating
considered two
speakers use
mitre corporation
\[ pp
recognition result
saw earlier
performs best
three parts
five sentences
contains five
clearly indicated
first implemented
semantic marker
abstract relations
nouns denoting
individual features
roles played
value ),
7 3
experiments suggest
english example
speech sounds
model leads
given name
reasonable size
several aspects
state grammars
temporal sequence
accurate parsing
including using
resulting score
parsing grammar
atn grammars
relation holding
models p
appear frequently
phrases \[
sentence pair
templates used
level would
3 describe
corpora show
review related
data alone
human rights
best individual
particularly crucial
since wordnet
matching approach
form words
based chunker
contain little
method called
evaluation scores
gram models
sentence recognition
contains around
following ordering
r n
generally available
rule c
terms could
retrieving relevant
learning experiments
easily accessible
levenshtein distance
entropy tagger
significant change
independently annotated
may define
objects referred
never used
specific communicative
next word
rules automatically
\] note
information ).
previously assigned
travel information
relation must
abstract specification
analysis cannot
a2 ,...,
produces output
selected words
almost complete
specific patterns
japanese speech
speech features
user initiative
reliably identify
descriptions based
general context
major role
3 systems
distinct words
attributive adjectives
intervening words
document sets
mass noun
local semantic
w e
f score
subsequent iterations
user goal
v denotes
generating text
experimental results
phrase pair
like structures
singleton sets
additional training
practical implementations
composition rules
given dialogue
three different
form without
15 \],
full advantage
japanese expressions
may give
generative description
c ,)
phrase recognition
often consist
shown ),
previous state
entities based
4 percent
errors due
since g
stochastic methods
much sense
start symbol
linguistically interesting
select different
lexical relations
names using
2005 shared
set .)
parsing component
correlation coefficients
vocabulary task
factors may
new symbols
find similar
significant semantic
2 8
propositional variables
important factor
modal operators
eight sentences
given threshold
easily generalized
nist mt
well known
following lines
entire corpus
(; r
based method
features improves
underlying structures
new values
section showed
learning framework
feature passing
deductive system
ai techniques
one positive
et que
[?] x
general introduction
e },
segments ).
possible representations
backtracking mechanism
available data
system reported
immediate access
input length
relation definitions
5 8
usually appear
also calculated
new classes
active verb
rule whose
best state
also introduce
1 \[
let k
system computes
sophisticated model
different paths
system behaviour
might choose
three dimensions
different reasons
john read
key factors
sentences 6
terms tend
), spanish
potential antecedent
deeply embedded
different n
mapping table
quot ;$&
process within
6 million
l language
expert human
16 ):
whereas others
using labeled
larger size
john carroll
word translations
semantic characteristics
powerful way
body parts
table 6
synthesis process
latter group
place names
3 gives
templat e
followed immediately
particularly challenging
information derived
newswire articles
textual information
), compared
precise meaning
7 %,
overall goal
trees using
usually means
one technique
system scores
extensive corpus
systems presented
limited availability
candidates ).
correct classification
certain word
phrases according
models --
occur independently
overall framework
f may
brute force
first evaluation
de ),
test section
smaller corpus
report accuracy
fourth row
description like
manually reviewed
later ),
empirical approaches
prosodic feature
project began
realized within
whether tile
data include
important roles
rule would
well motivated
partially matched
strings produced
simple system
given piece
also would
achieve optimal
using feature
idf value
work lies
next paragraph
words tend
software package
adapted version
~) l
results concerning
associated lexical
two actions
desired one
recent research
knowledge includes
first instance
original question
83 \],
made earlier
knowledge extracted
research focus
three sentence
individual results
utterance would
typically contains
test suite
performance improvements
limited time
second baseline
syntactic combination
additional features
existing text
particular kinds
english name
sentences respectively
04 ).
semantic preferences
june 2006
principal components
similar procedure
many mappings
approaches provide
domain expert
different heuristics
van santen
various natural
largely unexplored
paper describes
still exist
score reports
acl reviewers
relevant sense
database entry
include syntactic
takes place
unlabeled examples
concept accuracy
parsers may
current language
rule ordering
sample grammar
extraneous information
several tens
compared two
one simply
sentences provided
previously created
frequency list
four english
already mentioned
synthesis systems
mark johnson
best alternative
small example
word recognizer
target translations
agglomerative clustering
certain verbs
different alignment
right type
scenario template
16 ),
penn wall
task may
one motivation
use data
b ',
viterbi decoder
become even
consider examples
along one
gram model
model estimation
whole input
complete picture
many levels
2001 )).
identify noun
corpus text
4 shows
different extraction
formula 4
without relying
minimal length
information filtering
command language
clause may
knowledge embodied
across texts
search fails
necessarily lead
terminal yield
find sentences
various speech
character ).
effort towards
ibm 360
always select
chunk labels
tilburg university
multiple classifiers
two hundred
human translators
first one
typical errors
string parser
standard notation
immediately previous
,, 2
cannot therefore
would map
concept corresponding
case marking
underlying concepts
), preposition
language retrieval
information inherent
), showing
briefly discuss
kept separate
unigram model
structure parsing
total score
meaning ).
previous text
future evaluations
4 concludes
possible interpretation
provide general
synonym sets
various semantic
whole document
typically use
9 \]).
errors using
disambiguate words
techniques described
sentence structure
np --
two regions
two original
1973 ).
charles university
\]. however
li \[
also conducted
formal proof
character error
1973 \]
various expressions
summaries using
reuters newswire
less plausible
data may
exact nature
structured language
language patterns
sample sizes
sentence describes
rule p
richer representation
common lexical
features since
computational language
less easily
-- since
dcg rules
n may
syntactically incorrect
also get
term expansion
supervised setting
allow one
node n
longer word
words co
outside probability
linguistic terms
ms ),
7 ],
larger unit
blind test
\]. \[
also count
ide et
9 2
formed input
relative scope
log files
deciding whether
different versions
main idea
systems like
%. table
scheme described
predicative adjective
interpretation requires
test whether
also implement
almost certainly
might appear
scientific literature
yields good
used information
second term
official run
proven useful
dynamic programming
provide input
interactive applications
large text
algorithm goes
models based
1992 ),
little human
contain different
viterbi approximation
either n
word alone
result showed
simpler case
morphological variations
modified according
trec qa
limited coverage
smaller number
sentence 6
barzilay et
may move
since none
,,& quot
syntactic annotations
larger n
method must
obtained even
identified using
stack symbols
1990 ).
relation names
mentioned earlier
many uses
using textual
interactive systems
cross references
1990 \]
different initial
propose two
structures onto
major step
), yielding
make two
chinese treebank
corpus c
sufficiently similar
correct answers
impose restrictions
logical systems
give additional
including sentence
textual entailment
question types
1 .....
let l
phrase head
modeling technique
incorporate semantic
new cluster
web page
higher order
mechanism called
numbers 1
entropy classifiers
time warping
assigns probabilities
potential new
good predictor
particular constituent
known methods
role classification
space limitation
construction rules
accurate syntactic
ill \[
atalay et
text generation
attributes may
new facts
less space
different internal
positive result
matching using
techniques used
column ).
send messages
also looked
hierarchical tree
data likelihood
based statistical
object complement
multiple possible
victor zue
engineering techniques
special rule
~, 0
since chinese
high school
section summarizes
produce summaries
18 \],
using large
); 3
previously known
burch et
matching techniques
us look
new types
cultural differences
prosodic characteristics
formalisms based
language side
terminal categories
bar level
dictionary words
document clustering
exact definition
embedded structures
word associations
bold face
two linguistic
many others
value set
parsing technologies
une telle
chinese ).
human experts
full understanding
critical problem
sentiment analysis
looking ahead
primarily intended
certain general
weather forecast
speaker adaptation
system creates
utterances may
new implementation
document ).
one part
still another
represented directly
based mainly
many sentences
performed without
significantly increases
nouns occurring
two domain
scheme used
), allows
current use
also cause
annotation system
keyword extraction
documents provided
question answer
yet clear
earlier attempts
null ).
correct argument
used rules
1 ,...,
etc ).
still needs
processing issues
project \[
nearly 10
another pair
sentence position
paper concentrates
correct number
entire group
corpus whose
two words
upon completion
contains approximately
verbal arguments
minimum set
highly correlated
prefer one
descriptive work
pair grammar
following areas
achieve state
first position
research groups
syntactic subcategorization
., b
measure scores
3 texts
scale systems
interpretation system
following lexical
semantic relatedness
thus giving
important clues
force us
root nodes
earlier sections
roles may
collocation extraction
processing environment
operations described
;. hence
performance seems
determines whether
indirectly related
words per
(; l
base level
second constraint
structure underlying
research done
detecting errors
50 queries
higher recall
approach fails
certain input
newswire stories
sentence planner
events ),
object ),
also ran
positive weight
part consists
something new
element within
linguistic units
german text
unseen sentences
large coverage
reading comprehension
pattern classification
resolve ambiguity
representation --
lexicalized context
sentence splitting
second result
coreference links
specific corpus
entirely new
previous experience
interactive translation
choice among
common test
much later
assign pos
sentences produced
;) would
often cited
foot node
decision based
vp -->
could never
different tags
two models
also investigate
irrelevant features
appropriate translations
major problem
french language
null 1
parsing results
clauses may
efficient procedure
traditional grammar
distinct sets
set included
input may
first manner
main functions
class --
semantic database
joint effort
approach taken
among word
best output
also extends
cannot distinguish
chinese word
three stages
certain constructions
definition allows
parse failures
computational use
). 3
problems could
salient features
given pattern
transition networks
given term
also maintains
generation grammars
preliminary experiments
discriminative training
semantic analyses
one central
higher overall
algorithm provides
computational reasons
separate system
associated information
existing algorithms
context consisting
course ).
one smoothing
becomes smaller
values could
binary valued
type 1
general motors
explicit lexical
new language
\[ appelt
explore several
would ideally
central concept
parsing strategies
discuss three
text editors
aligning words
independent fashion
semantic description
following rule
query language
\] \[
one single
total probability
annotated corpora
search ).
deeper understanding
also runs
researchers working
three values
l )~
volume 25
;, however
related terms
news domain
\[ 1977
matching operations
thus preventing
three senses
structural units
automated learning
two samples
could extend
different grammar
representation levels
several characters
also represented
entities involved
cambridge university
three classes
deeper analysis
remaining terms
91 %.
query vector
search error
feature extractor
regular language
system performed
words automatically
robust parser
even impossible
phonetic symbols
85 \],
generation phase
two persons
lexical entries
corresponds exactly
operations used
larger sample
whose translations
added information
acoustic observations
single best
position 0
performance degrades
time associated
active voice
texts may
alshawi et
previous algorithms
distinctions within
order predicate
word may
oviatt et
parse information
formal aspects
linguistic objects
agreement across
using 2
quite easy
previously established
enclosed within
fellbaum et
two purposes
frame representation
random baseline
type parser
first successful
flat list
must actually
conditional random
especially designed
active chart
processing approach
many correct
), due
best previously
unseen test
see sections
one site
journal article
learner uses
would help
ontology ).
monosyllabic words
wherever possible
least 2
anchor points
large databases
final representation
major challenges
may say
noun dictionary
initial capital
25 \].
second noun
post hoc
one path
left bracket
riezler et
infinitive form
context dependencies
processes involved
another task
1963 ).
., f
previously developed
figure 8
subject codes
careful selection
also correspond
common information
gender information
rule uses
also assumed
symbolic rules
linguistically meaningful
simple parsing
small pieces
another potential
typical text
phrase modifiers
also uses
different ranking
using em
question contains
different sizes
questions using
certain kind
., r
upon encountering
thus different
interlingual machine
representation based
legal documents
translation quality
), z
still rather
32 %,
hopkins university
algorithm operates
structure given
similar content
semantic cues
). features
basic principles
experiment showed
vocabulary used
discourse structure
final type
differ considerably
boundaries ),
following aspects
single proposition
entities across
\[ hobbs
thematic structure
likely sentence
final analysis
method obtains
)). since
given constraint
solution may
class based
features indicate
future research
causal relations
participating systems
collaborative effort
directly expressed
numeric expressions
value greater
whole system
without explicitly
2000 )).
particular english
discourse referents
final step
translated english
appropriate case
representation uses
communicative functions
possible feature
usually less
actual event
fully supervised
per question
parser generates
mechanism must
2pr /(
select appropriate
five characters
b },
constantly changing
environment ),
abstract terms
makes one
lend support
dod ),
hard copy
et non
token ratio
lexical category
figures 7
-- even
highly accurate
sentence produced
litman et
morphological root
\[ lo
english term
93 %,
foreign language
preprocessing module
really necessary
instructions given
lob corpus
aligned sentences
top ten
following segment
agreement constraints
useful level
unification formalisms
information allows
complement structures
meaningful comparison
language communication
without referring
otherwise would
ac (:
several experiments
unique id
journal articles
based parsers
position ),
head constituent
77 %.
extract two
several tests
k values
unannotated text
tagging program
reduced significantly
parallel texts
yet included
location information
language generated
represent semantic
word categories
single documents
top line
mainly used
section 00
30 sentences
compositional semantics
increase precision
5b ).
several million
e text
particularly clear
several forms
enable users
minimal syntactic
1 8
would trigger
using human
retrieval precision
vector quantization
corpora ),
user also
). first
system analyzes
avoid misleading
popular approach
tree induction
fourth type
initial efforts
training criterion
abstract notion
specific predicates
highest value
easily explained
based natural
threshold values
make matters
irregular forms
shows performance
unrealistic assumption
important source
framework \[
prior experience
immediately follow
considered one
surface expression
retrieval model
full sentence
giza ++.
br example
86 %.
database using
special rules
approach ).
verb arguments
motivated constraints
several respects
involved using
annotated documents
corpus --
detailed specification
ambiguous parses
rules fail
chosen using
pattern matches
use larger
rewrite rule
four factors
scoring metrics
differ according
translations given
particular query
cannot account
recall performance
grained level
online dictionaries
ranking task
performance improved
modest amount
incorporate various
large hand
corpus evidence
finite form
possible names
systems perform
semantic dependencies
using tools
path expressions
flexible way
\] .....
would explain
explicit reference
potential ambiguity
design goals
based speech
predicate whose
general issues
), using
particular training
compact representation
maximum length
final example
following information
immediately followed
embedded clause
two problems
3 however
often cause
greater range
toy grammar
roughly classified
processing module
labelled bracketing
translation project
traditional dictionary
directly reflect
best search
based semantic
containing approximately
must share
four days
parsed corpora
given noun
000 verbs
second use
78 %,
speaker knows
report experiments
alternative analyses
given size
approach similar
learning mechanisms
specific rule
example involving
various research
also partially
speech recognition
procedure requires
algorithm remains
bigram language
dependent models
reyle 1993
semitic languages
3 \[
mutually independent
central point
information overload
slight change
year ago
short introduction
). results
k x
montague grammar
also test
every possible
semantic role
form ).
complex noun
words via
every sub
developed recently
example 10
lisp machine
based evaluation
sparseness problems
observed words
english constraint
paper reports
syntactic functions
three types
formal rules
first steps
fairly straightforward
first find
parse trees
small subset
becoming available
set may
two experts
authors wish
detailed comments
model parameter
cfg parsing
stage 4
linguistic applications
novel aspect
explicit information
many alignments
sur les
e vn
automatic segmentation
main advantages
binary classification
summary ).
method takes
sag 1994
numerical value
operations defined
important structural
learning programs
e q
certain constraints
tile verb
resolve syntactic
also benefit
000 sentences
thus leading
previous related
small corpora
grammatical frameworks
adding semantic
important issues
form v
columns show
following items
make changes
approaches include
one operation
special symbol
node 3
case elements
unification failure
project funded
also correct
readers may
human perception
two sets
linguistic systems
carefully designed
grained sense
group would
c \[
viterbi decoding
training documents
system handles
prosodic cues
connect two
two exceptions
different descriptions
manually extracted
currently employed
two scenarios
smoothing methods
original lexical
thus cannot
japanese corpus
hierarchical model
general algorithm
tile current
work since
tuning parameters
following rules
one obtains
sequence labeling
obvious advantages
crucial issue
4 using
could write
human coders
table show
linear kernel
previous word
finer granularity
building new
broadly applicable
partial matching
1980 );
could argue
une seule
corpus based
rule corresponding
), along
model 2
stack contains
context representation
1996 )).
tile event
enough context
accuracy drops
linguistically motivated
least likely
greater number
quite helpful
new interpretation
chunk information
cannot determine
b )),
resulting models
described next
tables show
new project
lexical units
words \[
partial ordering
based one
data contain
bold font
parameters may
substantially reduce
1979 ),
always made
step involves
easily added
information already
developing tools
selected word
data processing
alignment process
simply add
(~ j
john loves
minimum amount
modal logic
adequately represent
case system
world model
class membership
36 ).
similar methods
\[ 19
general approach
processing using
may wonder
possible questions
finite sets
distinguished two
program must
corpus uses
allow certain
extract automatically
kay 1980
language community
6 \])
fixed point
appropriate target
retrieve information
could choose
young man
written corpora
similar approaches
definite descriptions
original sentences
58 %.
performance gain
different relation
single global
parser capable
primitive concepts
context analysis
intended sense
abstract objects
initial data
score 0
queries like
one usually
f l
de ning
capitalized word
specifies whether
examples involving
task domain
three language
among annotators
explicit rules
model correctly
morphological analyzers
without spaces
actual example
also indebted
model construction
given unit
sentence breaks
coded knowledge
could either
intended effects
mnemonic names
major improvements
trivial matter
complexity measures
become apparent
english noun
specific subject
common sense
steps towards
third party
sound way
many constraints
tree contains
translation must
detected using
ure 1
use would
grammar takes
french newspaper
test collection
acceptable sentences
human labor
tag corresponding
parser evaluation
features rather
thus identified
-- rather
boolean combination
-- 0
nominal phrases
ea (:
wordnet ),
cumulative effect
seed words
major senses
original design
necessarily hold
given n
comparable results
alignment systems
acts ).
computed automatically
learning methods
model complexity
two context
corresponding english
corpus sentences
expressions using
far greater
system prompts
two hypotheses
sri language
total processing
closely resemble
dependent model
project whose
frame semantics
learning approaches
third type
science laboratories
language interaction
task without
several interpretations
different results
investigated whether
based transfer
ranking problem
graph containing
grammars using
first fashion
computer manual
;!& quot
training procedures
manually corrected
three hours
world problems
word sequence
lemma 1
driven approaches
process allows
states ),
assigned labels
across various
excellent performance
combine features
standard cfg
clause without
third step
one issue
independent information
little overlap
closely follows
n words
standard tree
model human
), w
pruning methods
line dictionaries
desired number
different perspectives
dialogue turns
syntactic constituent
action part
collection process
initially empty
last years
word meanings
clearly stated
would reflect
information using
maximum score
text source
previous experiment
specific information
accurately reflect
technical fields
grammatical rule
already selected
np chunks
related domains
semantic representation
words one
eg .,
different choices
single non
thus facilitating
evaluation tests
tile type
cognitive process
statistical pattern
features seem
corpus design
different components
;, c
morphological classes
considered successful
based mechanism
important direction
accurate models
might hope
tile discourse
data without
major drawbacks
richard power
step uses
used sections
1 plots
widely believed
certain slots
correct category
translation phase
complement clause
easy access
individual differences
paper defines
manual intervention
two authors
score ),
e p
wsj text
significant number
related applications
many characters
explore different
recognition hypothesis
\]. therefore
alignment method
tile three
question must
preceding question
newswire documents
require complex
function calls
answering process
columns indicate
n l
investigate ways
two indices
coordinate conjunctions
representational scheme
make one
passive construction
tagged corpora
), wh
lexical meaning
al l
semantic type
induction algorithm
set h
good enough
human expert
variable representing
two fragments
multilingual dictionary
given alignment
features proposed
generation tasks
distinguish among
segmented word
one takes
specific permission
corresponding set
appear together
making use
semantic analysis
algorithm identifies
system presented
verbs occurring
different predictions
words outside
breadth first
classifiers trained
syntactic lexicon
mutual translations
memory usage
forward application
g --
anonymous acl
entity task
first argument
total words
rewrite rules
computational costs
morphological processor
sentence matches
motivates us
driven phrase
represent various
also closely
target sequence
somewhat surprisingly
produce texts
certain knowledge
first number
learning program
vertical lines
features ),
also listed
annotation may
9 times
types within
proposed new
nearly perfect
resulting form
derived via
conceptual knowledge
corresponding training
actually present
\] 8
contains sentences
could appear
provided evidence
case slots
constraints like
components ).
string using
available evidence
input processing
mainly interested
w ,~
logical expressions
classification accuracies
specific items
two datasets
nigel grammar
among elements
could understand
syntactically related
used wordnet
study showed
modify nouns
prolog terms
heavily depends
without reducing
let v
chomsky \[
probability score
wer e
strategy proposed
;, whose
interpretation may
feature allows
first work
syntactically similar
independent rules
linguistic material
resolution ).
working system
log n
expectation maximization
7 describes
actual practice
nlp research
boolean operators
tasks may
scale natural
answering systems
also take
table 17
models differ
process yields
uses three
e '.
gb theory
candidates using
sent back
information pertaining
theorem 4
simple examples
primary goal
l [?]
62 ).
specific rules
different value
random access
based retrieval
null elements
third problem
current hypothesis
g ..
every question
absolutely necessary
common forms
german verbs
using current
\[ 2
known algorithms
information along
e en
whatever information
), whereby
dictionary ).
standard vector
domain independence
lexical selection
also estimated
use corpus
factors determining
items like
additional structure
candidate term
first option
two annotation
based patterns
small size
correlations among
linear classifiers
commonsense knowledge
single constituent
important contribution
generated output
document object
performance slightly
actual dialogue
antal van
algorithm simply
25 times
certain length
free grammar
7 6
precision results
quantitative results
model works
dialogue utterances
certain events
tile example
;, would
communication ).
phrase segmentation
convergence criterion
large test
common goal
gene ontology
texts contain
since text
second difference
complete solution
equal importance
make assumptions
binary predicates
common framework
e --
entity type
sutton et
local rule
tile problem
., part
action must
scale semantic
augmented context
utterance level
system initiative
efficient methods
many experiments
results among
word processing
singular form
linguistic studies
grammatical subject
two ).
gure 2
achieve similar
lexicon --
normal speech
largely due
spanish ),
german sentences
input words
head features
28 aotrr
lexicon acquisition
many forms
\] np
rules generated
specified types
word contexts
constituent type
phone recognition
representative set
), relative
pour les
three processes
possible analyses
lower bound
lowest frequency
object control
9 %)
practical advantages
particular way
context window
fairly complex
.) 2
\[ wl
appropriate referring
brown corpus
major portion
simply count
system like
single lexicon
scale evaluation
highly informative
recall ),
matrix notation
data point
conceptual hierarchy
algorithm processes
false start
fundamental research
(; 1
performance difference
algorithm called
analysed using
analysis must
(~ c
research literature
default ordering
special meaning
main sources
average duration
typically done
words need
complex internal
many translations
grammar description
many respects
early versions
system enables
accurate model
actually use
dictionary word
studies show
segmentation task
air compressor
extensive experiments
kappa coefficient
object positions
specific category
desired behavior
greedy search
input without
solving process
recognition components
representing natural
different orders
e ..
correspond directly
scored based
p r
correct tag
muc coreference
bias introduced
correctly generated
model developed
iterative algorithm
two parses
assumptions concerning
may involve
relational grammar
problem arises
upon request
experiment would
subject matter
greatly simplified
improves performance
functional grammars
two parallel
systems extract
., 1989
often difficult
resolved correctly
dependency information
always possible
evaluation procedure
brief introduction
phrase structures
contrast relation
binary rule
9th conference
two modules
second component
six possible
vocabulary size
grammatical information
new feature
develop tools
extract semantic
null able
significant increase
complete sequence
yi ).
may actually
less ambiguous
multiple levels
validation procedure
middle ground
could easily
provides useful
obtaining better
modifier attachment
formal framework
learning setting
92 %.
test run
standard technique
surprisingly little
maximum probability
seven features
statistical nlp
l 2
available lexical
2nd workshop
since c
25 %)
may come
substantially reduces
consistency check
sophisticated approach
systems assume
constituents like
1993 \],
null although
minimal linguistic
concept may
). consequently
trained system
techniques might
sentence even
evenly distributed
two finite
40 times
syntactic configurations
strictly speaking
representing semantic
test text
mixture components
proposed techniques
distribution pr
language speakers
coreference annotation
experience shows
application programs
achieving better
certain restrictions
prosodic analysis
extracted using
mentioned entities
whole may
using discourse
alternative representation
textual content
different requirements
analysis phase
v \],
probability given
res de
terrorist incidents
already acquired
tried two
easily identified
full paper
independent variable
nothing else
best candidate
texts without
approach avoids
research problem
word without
closed world
white spaces
using three
word l
pustejovsky et
explicit modeling
programming algorithms
text materials
knowledge within
appropriate sequence
x ))
verbs cannot
current computational
larger value
types ).
considerably smaller
string languages
adequate account
total function
inflected language
experiment conducted
one description
8 \],
must explain
target domain
process using
performance measures
classification decisions
would constitute
spelling variants
constraints upon
du texte
alternative analysis
six months
provides three
full implementation
without taking
users would
\[ 1984
one entry
performs two
string w
relatively little
quot ;,
suppose one
syntagmatic relations
dependency rules
grammar consisting
functional roles
th feature
relation type
new names
approach requires
spoken data
major points
feature occurs
summarization task
step 7
took part
), provides
full list
noun verb
examples containing
canonical representation
grammatical rules
like tense
treated like
resolving anaphora
sample task
capture semantic
generating system
also presented
given grammar
time within
thus make
pl ),
directly accessible
generation algorithm
training sample
variables x
realized using
classifier based
chapter 5
force office
shirai et
also discarded
types 1
rules work
well defined
similar text
acquire lexical
explicit goal
contain sufficient
detailed grammatical
gram approach
incorporate lexical
section 22
use lexical
examples shown
create problems
might find
famous example
sentences rather
korean language
various possibilities
potential solutions
shall present
combining lexical
lexical translation
descriptions may
complex grammar
would usually
varying levels
1978 \],
verb definitions
shall return
sets ),
could hardly
features capture
increasingly large
cowie et
might prove
lexical co
data sets
discussed elsewhere
grammar writer
classifier uses
40 %)
new focus
better representation
basic component
initial question
fundamentally different
centering framework
establish links
several mechanisms
pieces together
four cases
local information
representing one
present grammar
essential difference
dialogue move
output ),
similarity among
lexica l
provide details
entities may
whose meanings
acoustic training
document vectors
world ).
without loss
new nodes
proper word
us information
annotated version
--& quot
values according
could arise
single english
synonymy relations
technical documents
syntactically correct
correct meaning
optimal model
corresponding two
thus avoid
multiple sentences
letting us
somewhat less
development phase
turning point
lfg formalism
sources used
200 times
entropy based
input form
inner loop
problem cannot
acceptable level
case markers
meet certain
several small
taxonomic relations
interactive question
related texts
method significantly
inherent properties
full language
correctly answered
arc labeled
expected results
found many
fast access
results since
possible dependencies
2 ))
four distinct
digital libraries
early version
avoid confusion
r -~
also assumes
subtle distinctions
(: lasses
c would
paper proceeds
also forms
formed syntactic
may share
information relative
body part
engine uses
discourse relations
,, u
cannot select
2 makes
corresponding positions
structure built
main factors
), b
general phenomenon
strategy makes
also contributed
standard precision
one fifth
4 provides
\] calls
slightly improved
relative probability
de \[
systematic way
main topics
5 ))
less sparse
among events
section 7
whose purpose
place name
given string
one algorithm
disagreement among
following ways
problems raised
entity name
including part
document length
programming search
many local
mentioned problems
incremental way
second hypothesis
dependency trees
best understood
existing system
search techniques
following procedure
node containing
another criterion
nlg community
particular parse
possible role
mechanism works
buchholz et
predicate corresponding
system responds
grouping words
data taken
essential component
category may
nearly impossible
statistical n
integrated systems
describe different
usually work
), x
1985 );
algorithm must
rule like
1988 )).
precision drops
many named
significantly lower
original sense
system predicts
sentences ),
shown promising
empty ),
clustering problem
ontological knowledge
foreign names
since p
successive steps
possible action
1975 \].
classification task
act like
local rules
standard part
word form
new questions
performance figures
functionally similar
computational complexity
single query
simple linguistic
spontaneous utterances
require considerable
link words
two person
appropriate contexts
2 depicts
may carry
distinct elements
need arises
formal statement
morphosyntactic information
shall use
two extra
recent experiments
fashion using
fixed word
alternative strategy
highly related
new learning
applied directly
utterance like
passive verb
nodes contain
also follows
especially relevant
canonical example
\[ wilks
500 words
made within
two phonemes
second element
natural spoken
notational variant
may map
also influence
without ever
using voice
highly similar
bigram feature
final result
performed better
lexical material
\[ ill
done without
english grammar
may sometimes
axis represents
simple domain
single training
improving recognition
acoustic features
95 %.
model increases
one advantage
semantic treatment
different granularities
1998 ].
certain basic
following section
language dictionaries
target predicate
application program
estimates p
two compared
word corresponds
raw frequency
utterance meaning
also holds
previous system
($ 2
small sample
initial research
shared across
produce one
character sequences
form new
simple words
would limit
structural changes
set without
potential arguments
automatically discovering
set l
improve tagging
reliably estimate
also differs
obvious way
units called
major reasons
baseline measure
zero probability
h (;
without needing
latter also
various lengths
examples found
complete match
accurate prediction
lr parsing
calder et
word triphone
help guide
annotated training
generated word
logically possible
former method
matching technique
50 %.
often considered
new parameter
growing body
appropriate domain
constitutes one
thus contains
window size
following statement
.~& quot
sentence rather
speaking rate
en effet
3 corpus
angeles times
temporal properties
determiners like
best system
make inferences
information includes
documents rather
tree transformations
1995b ).
linguistics students
phonetic realization
1 rules
binary rules
grained distinction
percentage agreement
information extracted
consonant cluster
grammar development
approximately 5
speech synthesizers
insertion errors
statistical alignment
past two
daunting task
-- could
partial information
including single
also applicable
generation module
contain multiple
must reflect
sun ultra
last summer
appropriate words
tagging error
implemented systems
smallest set
necessary condition
structural change
lattice parsing
computational tools
good indicator
simple models
semantic analyzers
using web
31 ),
!& quot
rich morphology
unique name
various information
interesting alternative
become active
discrimination task
independent meaning
optimization criterion
several approaches
match score
ltd .,
method employs
linear time
treebank style
remaining 20
hash table
output units
never observed
4 x
). 6
exponential models
extracted sentence
information provides
management systems
specific meaning
user experience
text understanding
\[ le
could operate
significantly improved
single integrated
smoothing algorithms
natural classes
vector represents
), f
worst performance
model indicates
existing plan
f ).
paper ),
one anchor
known beforehand
contrastive analysis
500 samples
given features
f \]
additional criteria
2 could
enable us
either end
class hierarchy
tense verb
models may
organization names
first generate
certain categories
explicit statement
parameters l
large sense
curly brackets
second parameter
qa system
detection algorithm
given dictionary
current study
score greater
different frequency
ti (:
oov words
conceptual categories
local trees
), adj
b '.
using dialogue
currently using
output device
require significant
whose domain
paris et
tests using
document similarity
people use
much text
nearest neighbours
corpora automatically
received considerable
different disciplines
given hypothesis
mental state
high agreement
\[ verb
reasonable degree
f v
may make
message understanding
parsed version
top candidate
function symbol
planning phase
trivial way
using support
representing various
graph representing
phrase might
x ',
three feature
node pair
certain probability
news agencies
least 30
e 7
difference lies
forward composition
used independently
word sets
basic element
expression \[
f 1
words corpus
procedure given
parameter n
different task
), tree
standard deviation
lexical conceptual
might work
first proposed
words generated
translation tools
supplementary information
considerable attention
acquire information
3 plots
linguistics literature
example indicates
task ).
incorporate information
use information
related via
:// ftp
like previous
combining information
provides support
internal format
every user
complex descriptions
similar ideas
sentence ),
5 describe
analysis takes
overall improvement
one module
b --
among segments
rows 3
\[ 1998
might best
particular evaluation
data annotation
artificially constructed
often presented
correct order
parser since
documents ),
given element
clause ).
correspond closely
nlp group
sentence constituent
names must
two annotators
c .).
first ),
various rules
frequently found
languages generated
mildly context
considerable improvement
r indicates
classes within
scientific papers
ranked high
g .~
likely interpretation
empirical investigation
word occurrence
9 4
arbitrary tree
methods show
study would
ibm models
binary trees
maintain consistency
particular instance
human understanding
section .)
specifi c
2006 student
original goal
structure within
existing technology
also obtained
incorrect predictions
matrix b
new measure
activation levels
running text
consecutive utterances
small grammar
language generators
given relation
basic questions
base phrase
data ).
identifying sentence
specific ways
learning algorithms
different theories
np may
semantic processor
used data
standard definitions
word relationships
nine different
first extracts
conduct experiments
better described
third term
paper deals
oriented dialogue
event expressed
via one
used methods
indefinite articles
addresses two
n ).
accuracy improves
extremely rare
j ',
r ~,
follows immediately
explicitly expressed
.' l
alone cannot
implemented two
investigate methods
various situations
rules designed
program works
possible type
l -~
process goes
7 \],
n \]
possible discourse
default assumption
many categories
), much
possible analysis
acceptable translations
act upon
correct translation
c .),
also explore
might exist
trees must
sentence syntax
fully general
performed worse
also offer
texts taken
representations based
positive integer
approach thus
low value
procedural semantics
previous knowledge
automatic query
reduction rate
subject headings
al ..
relevant text
considerable variation
previous researches
paper offers
results indicate
previously discussed
hmm states
technical documentation
involve either
negra corpus
syntactic factors
dialogue understanding
also achieved
every lexical
n v
sentence appears
2 gives
different head
particular one
earlier research
), japanese
new database
seems worthwhile
semantically meaningful
automatic training
highly general
perform experiments
first letter
use rules
simple form
definite article
1977 ).
japanese writing
label ),
n parses
kl divergence
every stage
exponential model
better support
dialogue analysis
1977 \]
newly introduced
cues used
many directions
false alarm
interesting future
verb takes
two special
results even
new tools
simple function
via simple
full interpretation
., nouns
5 gives
linguistic assumptions
systems whose
total ).
english dictionary
exponential form
particular interest
semantic pattern
particular cases
wasted effort
), de
processing capabilities
system ranks
probabilistic language
initial corpus
refer directly
basic components
3 lexical
g .)
assigning probabilities
corresponding parse
much faster
natural numbers
tagger using
attributes ).
system interprets
entre les
language definition
exponential function
parallel corpus
r example
program developed
conceptually related
much attention
since verbs
1995 )).
partial trees
three nodes
singular nouns
novel use
best translations
relatively straightforward
model representing
consistent improvements
recent advances
sigdial workshop
data derived
without error
last node
might indicate
conceptually similar
total error
string x
whose surface
add extra
list using
largely ignored
discourse functions
although one
con dent
subjects could
lexically ambiguous
may help
declarative formalism
11 \]).
words connected
optimize performance
tags could
separate domain
~. however
also whether
units must
models also
internal structures
referring expressions
much improvement
linguistic intuition
language workshop
\[ type
every rule
test runs
examples demonstrate
learner may
word (&
linear ordering
7 show
module would
retrieve relevant
could play
system comprises
might even
given either
detection module
research council
nominal group
separate words
made manually
let z
). combining
lexical data
first tag
answer sentences
empty set
scope ambiguity
two conditions
used five
1967 ),
relations whose
cannot perform
two equivalent
lexical identity
modular architecture
every day
comprehensive coverage
recognition task
~,& quot
time linear
four criteria
conventional dictionary
search queries
based morphology
sections 15
resulting expression
terminology ),
many phenomena
ordering problem
early results
quite frequent
paper suggests
manually constructed
.) null
statistical grammar
parallel fashion
see shieber
without word
information associated
hereafter called
exhibit similar
higher scores
practical way
roles ).
knowledge source
anaphoric link
second row
observation sequences
additional annotations
time expression
stanley peters
vocabulary sizes
text structuring
x1 ...
shared knowledge
rules generate
1 >,
specific choice
stage 3
uniform prior
successfully processed
years \[
cs ),
ibm model
model described
examples provided
independent phone
absolute values
error correction
always get
parts ).
last sentence
made extensive
essential components
high entropy
shows recall
domain language
partial credit
reasonably accurate
abstract semantic
four steps
easily identifiable
). among
john hit
action description
representation may
e .)
w )=
structure annotation
plan structure
following lists
different sites
clauses ).
likely path
input ).
many small
similar levels
evaluation data
relations would
text sentence
one drawback
process provides
rule becomes
widely accepted
evaluation methods
never appear
syntactic boundaries
features per
also becomes
individual documents
defined ),
efficient algorithms
two new
giza ++,
combines several
larger structure
heuristic approach
auxiliary verbs
major difference
given value
nested structure
tree 2
corresponding natural
second possibility
tagged version
class may
probability models
often omitted
long time
golden standard
null given
analysis tools
specific translation
predicate calculus
1993 ):
making choices
validation data
rules proposed
1996 \]).
context size
). additional
target slot
passive voice
shen et
constraint requires
ordinary dictionary
assigned sense
per sense
statistical component
(\[ 2
prepositions ).
obtained via
paper makes
functional description
classifying documents
structure ).
multiple constraints
exact matching
remaining cases
lookup table
overall search
parameter estimation
previous parsing
phrases like
bilingual lexicon
translation speed
additional work
particular position
theoretical point
state network
following standard
systematic evaluation
reversible grammar
relation symbols
current task
retrieving information
description --
computer implementation
statements like
editing operations
conveying information
system comparison
-- unlike
purely statistical
8 0
native english
rules ),
name classes
using 4
argamon et
mean length
highest entropy
near future
left untouched
work makes
language could
well beyond
properly contained
applies equally
descriptive texts
algorithm takes
become increasingly
r e
corresponding speech
linear svm
dialogue strategy
term unification
wu 1994
already given
presently available
slight increase
making predictions
first segmented
6 --
word clustering
several alternative
different orderings
phoneme sequence
best case
groups based
cannot derive
second study
88 %.
), jackendoff
relatively insensitive
methods require
whole words
works well
8 ):
level phenomena
ef ciency
algorithm determines
speaker independent
test sentences
texts ).
every segment
kernel functions
provide accurate
parser performance
word ).
may explain
model used
core technology
simply take
corpus linguistics
al 1999
discriminant analysis
represent information
10 words
operational setting
two notions
coverage parsing
important reason
minnen et
webber \[
thereby allowing
early days
log 2
unified framework
evaluation showed
\[ 1973
corpus obtained
determine exactly
conditional entropy
like structure
following dependency
retrieval module
common approaches
;:& quot
1 ))
vice president
word error
features automatically
modeling using
protein interaction
human sentence
1993 ),
initial attempt
similar verbs
creative use
discourse ),
computational task
end state
extensive empirical
sighan workshop
approach include
process operates
predict words
semantic weight
cation task
international phonetic
argument structures
pos ),
temporal knowledge
aro grant
graph theory
selection step
gram sequences
one strategy
l ~,
projective dependency
applying machine
using viterbi
three groups
collier et
irrelevant ones
within words
linguistic side
v ~)
potential problems
competing hypotheses
iff either
using pattern
possible name
underlying structure
2 lists
using term
system proposes
k [?]
specified feature
context contains
first definition
relies solely
structure derived
initial lexical
resolution using
transitive closure
studies suggest
inherent ambiguity
object position
tile noun
sentence set
semantically motivated
unstressed syllables
), called
source material
several text
learning based
interpretation function
v ),
(! l
computer language
formed words
two proposals
collective nouns
data shows
class categories
current topic
tl ,...,
situational context
paper attempts
current discourse
beam width
initial stage
example used
translations ).
extraction rules
5 lists
linguistic research
null b
additional parameters
feature values
become one
identify discourse
constraint system
without applying
4 proc
another constituent
words account
using complex
adjective phrase
verb entry
basic requirements
also helped
probably require
1 otherwise
sentences parsed
8 ),
1 /(
equivalence classes
work attempts
human reader
++ toolkit
many approaches
sequence length
identify three
processing technique
thereby making
similar sentences
good correlation
complements ).
immediate consequence
tree rewriting
rule also
closely match
logical predicates
). interestingly
systems --
published results
structured representation
class probability
proposed solutions
parses ).
also occur
another significant
possible dialogue
limited ability
): 2
give two
various tests
bootstrap resampling
one minute
extract answers
fully understood
parse errors
occurring within
l ~'
likelihood method
length feature
considerably different
;. 5
available software
new action
generic information
different strengths
whose occurrence
n /,
redundancy rule
la structure
33 %.
formal methods
berry et
constant c
line data
quot ;),
originally designed
application developers
pragmatic information
syntactic parse
word class
friedman et
computationally useful
grammar checker
absolute numbers
main loop
two well
lines indicate
contextual clues
main interest
probability model
). certain
certain contexts
would significantly
per million
system showed
two techniques
), hence
rightmost one
representation also
totally ordered
severe problems
almost 50
), precision
ranked using
entry ),
compte de
distance feature
smaller data
feature equations
computational discourse
obvious one
type x
morphological feature
ef cient
work could
common errors
certain concept
significant ways
1978 ),
atomic categories
one sort
finite alphabet
also many
first attempts
probability function
j },
statistical features
still one
subcategorization information
adding information
one sentence
proposals made
development tool
wide web
also obtain
planning tasks
label set
could prove
frequency analysis
-- something
also represents
language translation
source texts
results seem
permits us
node dominating
tree fl
end users
selection problem
., high
type coercion
system errors
obvious cases
time algorithm
treebank parse
find words
three human
target word
parser needs
following methods
retrieval using
extreme case
two vectors
bilingual english
used widely
full feature
one starts
function could
probabilistic interpretation
mellish et
learning experiment
grammar cannot
researchers interested
using constraint
john ).
function call
), part
certain domain
ahrenberg et
ted pedersen
change ).
appropriate slot
different implementations
using leave
categories described
john \]
different dialects
generate utterances
many variants
regular verb
without major
current example
conceptual domain
whose main
c (;
get confused
new event
near synonyms
derived lexical
also covers
theorem 3
first process
cannot know
target class
certain words
\[ 30
model theory
paragraph ).
short sentence
always correspond
high performance
manually examined
i1 est
currently includes
first call
training statistical
rule probabilities
present study
entire collection
might become
perhaps less
variables must
ensemble de
,, n
1 could
popular technique
window sizes
syntactic parsers
active features
results ),
confusion matrices
)/ p
good examples
might seem
translingual information
spelling error
multiple translations
without creating
name types
pragmatic rules
market reports
pronoun occurs
21 \].
input expression
problems connected
3 rules
mean score
extract relations
models contain
real users
discourse units
automatic way
link clustering
looking back
per character
two aligned
advanced natural
)). note
must define
text planners
system searches
following vowel
modular fashion
assumptions regarding
one author
experiments aimed
algorithms cannot
systems based
translation step
either e
first describe
participle forms
many contexts
), originally
set ).
parser designed
)( 1
someone else
(: ase
template contains
similarity functions
let w
simply discarded
asr performance
ones may
case words
\[ brachman
types using
express different
paper illustrates
provide users
adding rules
primarily due
later ones
name list
hansard corpus
phase consists
called functional
key terms
easily handled
single parse
physical target
v (~
may reflect
close together
classification decision
separate sets
second event
results compare
rather low
feature subsets
head position
expression would
translation candidates
relative effectiveness
full names
model 4
effect relations
many people
lower node
shall first
two operations
sensitive grammar
initial segment
model defined
flexible interaction
paper gives
0 ))
rule expansion
removed ).
\[ 21
understanding conferences
plausible explanation
actually appear
slight modification
similar constructions
subset relation
first parsed
lexical look
bilingual training
accuracy even
corresponding position
one cannot
semantic representations
little effort
six sentences
standard generalized
methods might
typical case
using nlp
grammars called
must resolve
verb prefixes
result holds
categories used
side symbols
without changing
correctly segmented
also assign
user input
defined set
expression matching
carroll et
related research
german ministry
meaning preserving
top 50
pentium iii
2 data
practical natural
minimum number
different fields
small value
feature f
2003b ).
categorized according
rules makes
grant iri
might consider
whi ('
yu et
001 ).
p v
following five
information attached
earlier stage
repeat step
state language
parser proceeds
text processor
step may
final ranking
task requires
normal circumstances
language material
useful resource
formulation allows
various ways
form given
syntactic expressions
using xml
graph structures
every level
collecting information
since unification
transformational rule
second task
two important
\[ u
new representation
may attach
recognized using
40 hours
full nps
question type
distinct syntactic
e ,,
originally proposed
p 1
scoring system
tagging system
straightforward approach
communication situation
phrase chunker
given pair
statistical results
scores reported
linguistically valid
question analysis
logical subject
generating referring
typically used
recent empirical
four parameters
parameters ).
memory ),
allows arbitrary
first seven
currently exploring
automatic means
relations directly
japanese translation
longer words
example 4
first order
wrong one
0 otherwise
completely independent
coverage syntactic
u ~.
hypothesis h
prolog \[
clearly indicate
structures involving
two languages
still need
first method
new type
purely semantic
output texts
78 \],
several algorithms
new topic
facilities provided
frequently used
new discourse
connected component
discourse plans
paper looks
complete word
links may
sentences per
new analysis
97 %,
natural interpretation
translation example
using logical
two node
consider using
shall write
operations necessary
features \[
grammar symbols
-- \[
class sequence
sentential forms
). work
shall argue
earley deduction
fully specify
left recursive
wsd algorithms
single tokens
require either
models without
link parser
mounted microphone
translation problem
basic verb
subsequent references
2 however
introducing new
1 ',
j )),
used instead
current practice
element x
;-& quot
tag sequence
small fraction
sample documents
associated text
7 0
feature list
chart contains
partial interpretations
simple finite
monotonically decreasing
interactive information
match words
corpus also
search tree
uses heuristics
grants iis
research labs
nps may
probable alignment
exhaustive listing
also required
times faster
rich set
overall average
linguistic behaviour
1 translation
components include
terminal elements
22 \],
introduce different
2 \[
point towards
raw corpora
27 ),
used separately
collected corpus
rst word
sharing among
john likes
appropriate word
two similarity
data annotated
pattern consists
since constraints
independent data
combination techniques
status information
translation strategy
actual grammar
linguistically oriented
hong kong
speech ),
2002 )).
current dialogue
syntactic one
[?] 5
cannot recover
7 ):
certain point
making decisions
complex verb
data provide
also extract
wl ...
every sense
based grammatical
background knowledge
data elements
based scheme
7 discusses
null mation
particular combination
specific data
present perfect
5 \[
x j
disambiguation method
readable text
predicted correctly
recent approaches
state contains
additional non
authors thank
often occur
n elements
major components
consistent use
improves recall
get stuck
). apart
relatively lower
easily seen
information expressed
b .)
always correct
appropriate class
successful analysis
2 possible
research fellowship
map directly
redundancy rules
took several
1997 ).
l u
usually set
verbs must
sentence positions
distance calculation
also assume
type de
including automatic
presented using
yet ).
interrogative sentence
rappaport hovav
whether something
might allow
might suggest
proposed technique
expression corpus
positive example
include explicit
1997 \]
test succeeds
approach considers
data object
significant practical
times higher
two sequences
k .,
following short
category corresponds
disambiguating word
need access
idiomatic expressions
parse time
basic task
great britain
output consists
data track
method seems
around 300
onr muri
also planning
less training
five main
71 %.
took advantage
input variables
linguistic frameworks
45 minutes
granularity level
domain must
rules could
result obtained
xml schema
correct according
practical one
volume 23
would entail
parser ),
nn ).
developed techniques
less frequent
;...& quot
labels used
target document
many alternative
new example
given partial
bound morphemes
experiments designed
approach depends
successfully applied
proposed models
quite close
already processed
broad categories
metaphorical uses
ill section
hanks 1990
based tag
knowledge using
suggests using
whose word
move beyond
word candidate
z x
program described
preferences may
use local
preprocessing phase
() l
single instance
fixed length
many researchers
new problem
lexical research
selected examples
different participants
boost performance
feature vector
fully lexicalized
equivalent words
entity extraction
reader might
quot ;}
\[ pollard
practical interest
practical cases
main difficulties
per frame
compound term
different corpora
contains also
directed towards
derived information
computational semantics
problems facing
analysis step
correct pairs
many users
rules describe
complex task
would lie
using n
syntactic transfer
annotation ).
grammatical dependency
forthcoming ),
varies across
automatic parse
include using
text types
words considered
phrasal category
\], p
cases may
various translation
one network
paper studies
l ,\]
weights used
examples show
occurrence probability
last utterance
wider coverage
frequency threshold
dependency probabilities
could exist
\[ 4
becomes part
two queries
1999 ))
immediately following
least n
present task
final hypothesis
time using
spoken sentences
lexicalized tree
either accept
noted previously
packed parse
one reason
predicative adjectives
thus created
2002b ).
sequence given
sentence per
interpretation ).
next generation
). even
perfectly good
methods allow
two occurrences
documents per
test system
highly predictive
controlled experiments
inc .,
using bigram
discuss results
contains many
single one
jj nn
produced according
linguistic community
simply adding
three key
frame element
structure allows
standard algorithm
expressions must
13 \].
best solution
alternative representations
english verb
1988 \]).
generator could
problem must
even necessary
detailed semantic
linguistics volume
much better
highly restricted
including several
simple linear
l ,:
franz lisp
different segmentation
ontological concept
specific sub
el .,
single structure
noun concepts
randomly select
expected agreement
processing phase
particular argument
researchers use
may constitute
using heuristics
per target
correctly parsed
present article
hacioglu et
also notes
program used
11 ].
elements whose
different subcategorization
given head
lexical matching
therefore provides
syntactic position
.) 4
hierarchical structuring
100 sentences
unseen text
initial trees
\] could
larger range
research papers
30 words
shows one
must encode
traditional ir
web crawler
tasks related
semantic tags
main criteria
two daughters
approach outperforms
nominal forms
algorithm development
represent meaning
complement structure
distance measure
resulting transducer
several levels
underspecified semantic
based methodology
knowledge discovery
derived rules
17 ).
netherlands organization
morphological complexity
parser builds
focus shifts
noun modifier
cfg parser
several related
intrinsic properties
steps involved
sentence boundaries
actually found
e may
easily captured
preliminary experiment
complex discourse
used hand
general perspective
default case
17 \]
perfectly possible
slavic languages
experimental set
minimal attachment
superficial analysis
several similar
english gigaword
approaches rely
general interpretation
would proceed
c1 ...
direct way
order given
ordering constraint
recently used
applied linguistics
experimental paradigm
items whose
evaluation would
process called
attachment sites
result may
iterative scaling
analysis problem
lexicon may
attachment problem
recognized sentence
particular relations
model size
probability ).
tit (:
section also
1 lists
'. n
class words
following text
useful discussions
,' e
ocr errors
sentence begins
main steps
hidden states
). another
event description
ordering principles
points higher
pipeline architecture
tile grammar
dictionary includes
results hold
long sentences
natural sciences
language data
1980 \],
parameter value
grammar induction
training instance
semantic difference
would violate
system coverage
new name
linguistic rule
two instances
transformational rules
immediate right
search technique
best sequence
retrieved documents
approach still
representation without
different structures
indirect way
english task
shallow parsers
original phrase
phrase boundaries
general applicability
broadly similar
l 4
found similar
separate modules
level text
technique would
two mt
intuitive appeal
trained models
current theory
could require
proposed model
iii ),
3 ))
consider sentence
syntactic patterns
analysis strategy
paper consists
name identification
first made
personal communication
object argument
string must
\] g
conducted experiments
\[ kay
system instead
conceptual relationships
separate word
parse would
additional restriction
multimodal interaction
without explicit
employ different
et un
), usually
000 people
symmetric relation
unsupervised training
processor must
features need
probability 1
extraction procedure
system whose
points within
links connecting
spontaneous spoken
additional advantages
speaking style
made using
relation identification
similar effects
collins cobuild
maintenance system
plan operators
decision point
statistical inference
accuracy ),
new dialogue
graphical user
amongst others
different documents
extensively tested
particle constructions
let b
forms produced
effectively use
distributed according
inference processes
44 ),
considerations lead
~) r
percent correct
leave open
also computed
two alternatives
functions used
infinitival clauses
subject position
several verbs
always take
separate class
driven system
cornell university
effective information
\[ st
either singular
thrown away
cohen et
people understand
reliably predict
8 illustrates
automatic evaluation
often left
joint distribution
quite natural
binding theory
object relations
empirical probability
instances ).
know anything
necessary component
manually annotating
irregular verbs
constraint may
contextual conditions
c ))
multilingual corpora
6 .)
decision function
coherent discourse
specific phrases
called non
salim roukos
many templates
important part
automatically analyze
tit (',
tree represent
full description
idiomatic usage
ran two
algorithms make
feature called
better alignment
joint work
shallow semantic
x c
best approximation
computational theory
human language
derived trees
question asked
segments within
10 ).
algorithm employs
work independently
language \[
linguistic aspects
unigram language
term may
attentional state
multiple categories
72 %,
10 \]
one result
g h
constituents whose
number 5
several others
precise sense
etc .--
complement position
pronoun cannot
right end
overall system
must say
method produces
based selection
\[ 1979
research focuses
text contains
x q
low agreement
., 5
alignment performance
prior probabilities
structures containing
possible noun
linguistically significant
compound noun
structure already
preceding section
one dimension
knowing whether
incremental fashion
6 system
systems take
different degrees
complete enough
semantic distance
without knowledge
one list
): null
short paper
grammatical errors
agreement ).
provided many
also run
give one
illustrated using
lexicon probability
prevents us
9 3
promising avenue
whose average
., different
operation ).
new piece
built according
estimation procedure
simple enough
plus 1
vocabulary recognition
gaussian distribution
three times
paper aims
totally unrelated
another two
three candidates
briefly introduce
foreign name
speakers often
different concepts
parser makes
much worse
relations listed
pairs consisting
one answer
sentence parse
consuming process
sort hierarchy
potential source
underlying principle
1988 \].
figure lb
final value
japanese data
tile top
following output
target object
two companies
usually two
existing domain
....& quot
presents experiments
two equations
also mark
descriptions ).
already built
depend crucially
knows nothing
model adaptation
powerful method
expected since
syntactic agreement
times word
distinguish two
achieved good
types given
would involve
input alphabet
equally acceptable
e also
seems better
h '~
symbol p
bilingual texts
templates produced
). rather
avec le
existing words
within 1
short list
). non
different settings
second year
corresponding answers
appear often
different procedures
b ,,
refers back
use contextual
similar experiments
shows tile
queries based
whose position
used directly
would remain
presumably due
entities ).
result also
less dependent
hybrid approaches
rule corresponds
closely related
topic may
fill rules
actual results
fairly easily
make full
whole class
also corresponds
available online
review ).
like relations
), suggesting
case slot
many discussions
2000 shared
+& quot
domains like
synthesized speech
e h
trained model
deep analysis
following axiom
good precision
real word
flow chart
matching algorithm
phonemic transcription
syntax based
let f
state must
would come
provide similar
constraints --
gives two
psychological plausibility
used lexical
miller 1990
input using
based applications
initial tests
one language
grows exponentially
treated separately
supervised approaches
two native
flag indicating
2 c
identified several
english gloss
let r
hendrix et
states would
various application
12 sentences
based entirely
study presented
system encounters
thank anonymous
software tool
elements belonging
good model
explicitly encode
given e
negative constraints
provided valuable
classes using
atis task
linguistic information
containing many
borthwick et
successfully employed
tree representing
use standard
quite promising
68 %.
grammar contains
meaning within
)-( 8
1995 \]).
probabilities could
acquired automatically
handle different
semantically tagged
based systems
grammar productions
structure using
whose children
statistically indistinguishable
previous words
word disambiguation
proper probability
arguments must
main argument
empty intersection
engineering approach
classic example
per la
chunks ).
best viewed
use many
specific question
simple approach
., sentence
al 1990
selecting words
another way
particular needs
net effect
using grammar
distributional information
est de
pour des
specific time
bbn systems
identification process
initial goal
binary classifiers
contexts ),
language pairs
ten minutes
additional set
corresponding number
input vectors
updated version
sentences one
easily done
tile dictionary
two curves
work \[
method leads
syntactic criteria
parser attempts
9 %,
points ).
entity introduced
correlation coefficient
distance dependencies
u [?]
textual input
agent ),
parsing english
highly simplified
extremely powerful
organization name
enabled us
multilingual documents
third element
quot ;(
considerably lower
need never
previous models
preliminary investigation
two solutions
nice properties
head relations
little difficulty
prediction based
mandarin chinese
gale et
semantic dictionary
right daughter
capitalized words
two subsets
use dependency
driven machine
possible use
different functions
second condition
language phrases
function \[
estimates obtained
node dominates
structural information
1972 \],
words could
system asks
negative effects
immediately prior
called grammatical
best match
example might
1 however
k times
correctly classifies
inflected verb
stochastic approaches
highly irregular
corresponding relations
possible reasons
test case
two preceding
linguistic items
quite simple
use probabilities
words following
quite distinct
entropy h
software developed
declarative knowledge
processing takes
1 showed
combinations thereof
semantic descriptions
computationally complex
document generation
surface generation
relative lack
would clearly
called elementary
parser could
structures found
system --
feature weights
two f
x p
another common
grams ).
frequent senses
manual analysis
one morphological
semantic indexing
uses features
languages could
less common
following schema
r ,~
error metric
1 gb
). consider
small differences
inflectional paradigms
actual value
type must
find word
linguistic rules
speech systems
--> \[
tile first
implicit confirmation
therefore much
separate document
rule templates
noun taxonomy
algorithm outlined
case ).
one contains
source side
two recent
two associated
word vectors
different kinds
2 would
following table
evaluate system
c g
various data
45 ),
ten documents
many search
text interpretation
contain three
various methods
tile relation
1961 ).
march 31
based translation
translations using
section compares
finding information
constraints based
aone et
generate different
interface structure
pairwise comparison
algorithms use
could build
relevant constraints
per iteration
measure would
necessary first
last part
using string
system involves
level formalism
correct referent
3 ',
rules combine
complete system
semantic link
low entropy
feature must
generation processes
different non
quantified variables
verb cannot
require less
l !'
systems differ
define word
whole expression
constraints directly
mostly due
predicate phrase
two contrasting
broad domain
tight coupling
potential attachment
two tree
original text
already exists
). equation
alphabetical order
considerably improved
using 3
completely defined
case 1
feature path
system maintains
type definitions
similar case
specific research
first requirement
intended word
boolean expressions
possible alignment
h \[
dataset contains
lafferty et
j p
x '.
suf cient
instance x
current nlp
alternative view
9 ],
certain class
becomes large
logical equivalence
structural rules
event occurs
tree used
word length
wide use
template element
ou r
sometimes also
previous researchers
wrong answers
two subtypes
representations may
distributed architecture
lower bounds
multiple meanings
different status
null context
lexical rule
answer may
material may
\[ n
perhaps best
significant according
syntactic ones
definite clause
illustrates two
system runs
gram precision
resulting syntactic
already taken
tile results
processing tasks
.) let
technique uses
others use
system user
issues include
segmented words
ranked list
c ',
using top
major task
baseline result
additional sources
also mentioned
go beyond
resource limitations
linguistic category
theoretically motivated
underlying idea
stored together
algorithm starts
cannot get
interlingual approach
actually said
skewed towards
use methods
time spoken
artificial intelligence
whose elements
including different
theoretical approach
gis algorithm
tasks involved
new pattern
top k
management corpus
den bosch
new term
single template
related information
sentence one
past work
would carry
indirect speech
x --
nodes linked
human interpretation
two arguments
substantial part
sophisticated smoothing
sentences need
oriented view
q ),
usual sense
slightly less
many similar
query term
,' igure
clearly distinguished
1 according
propositions expressed
input w
cost function
purpose natural
generation perspective
null introduction
language question
machine dialogues
direct evaluation
lists ),
). assuming
translation result
manually annotate
system based
crucial part
match ).
f given
unsupervised way
4 years
generation project
expected performance
first glance
types associated
field contains
level --
whose object
experiment also
reasoning system
even less
1995 \].
darpa grant
given ambiguous
1 j
much knowledge
anonymous reviewers
b may
2 %),
many word
argument relation
common verbs
full generality
specified type
1968 ),
two japanese
entries ).
integrate different
use maximum
7 illustrates
space based
highly efficient
), transformation
place relation
semantic components
pay attention
features generated
,( l
c /)
formal devices
rule schema
various possible
therefore expect
common lisp
three paragraphs
various sub
without loosing
different constituent
possible outcome
system uses
special lexical
precedence relations
phrase attachments
hand tagged
perform text
research supported
complete discussion
weakly equivalent
parse sentences
5 %),
words relevant
type feature
software development
hard constraints
phrase structure
research questions
directly estimate
method adopted
15 minutes
unique features
two equal
locally optimal
total occurrences
two links
problems like
srl system
sense numbers
sequence models
clear tendency
output side
likely analysis
basic travel
using dictionary
related items
major topic
provide us
empirical observation
task includes
2 corpus
ideas behind
first appearance
entry consists
multiple agents
following abbreviations
rule learner
following topics
encouraging given
three fundamental
text must
second criterion
greater importance
representation formalism
bilingual resources
general characteristics
nlp application
prediction accuracy
research activities
simplest type
accuracy could
\[ 1993
algorithm cannot
pose problems
input sequences
many verbs
parser looks
model combines
l n
rules relating
). fig
significant progress
quite successful
would leave
tile value
provide better
4 \].
core engine
syntactic argument
language c
need large
would result
several conclusions
worth considering
river bank
fifth message
could allow
likely candidates
e processing
based studies
taking account
verb senses
disk space
includin g
defined semantic
substantially smaller
function returns
60 words
agglutinative language
results reveal
chief executive
integral part
evaluation test
wordnet relations
remaining rules
abstract information
good introduction
1998 );
pattern would
relations like
personal information
get used
semantic construction
one interpretation
incorrect segmentation
data includes
13 %)
lexicalized grammars
treebank annotations
verb classes
word similarity
systems center
new constraint
abelson 1977
f1 ...
among four
data extracted
phrasal translation
general issue
alternative hypothesis
simple techniques
data given
tag formalism
1 summarizes
morphological analysis
extraction application
seems clear
manually aligned
carreras et
line lexical
potential contribution
elementary operations
interlingua representation
rules \[
validation test
complete knowledge
rule fires
frequent types
label bias
perl script
following regular
would make
multiple contexts
issues surrounding
pattern matching
role labels
g l
noisy data
using first
exact meaning
level word
entire sentences
cases without
f b
seldom used
information source
negationslash [?]
last phase
order models
1985 \],
writing grammars
actually used
data provides
manually added
scale applications
syntactic relations
grammatical coverage
nondeterministic finite
language research
rl ),
1994a ),
word encountered
input queries
also permits
may begin
full analysis
model component
filling task
example grammar
\] proposes
system design
using 10
directly use
), several
3 lists
involves computing
online information
could possibly
important characteristics
level based
column 1
l ,~
fully instantiated
source corpus
dynamically changing
one shortcoming
mode ).
xerox research
polynomial complexity
groups may
content ),
strings generated
adjoining grammars
first look
). fortunately
common assumption
modifiers ),
possible tags
-~ x
language whose
set empirically
2 plots
returned documents
mode \]
much ambiguity
least 10
reference relations
concise description
nonterminal symbol
current clause
asr systems
entity referred
random fields
reasons given
clausal arguments
currently applied
without syntactic
automatically derive
lexical information
several constraints
third row
feature names
cross product
appropriate knowledge
23 \],
category membership
two principal
external resource
first need
selection heuristics
theorem 6
high order
system configuration
processing tool
may emerge
effort required
would avoid
find better
rule involves
particular grammatical
., ltd
individual texts
also involves
approaches using
article presents
grammar must
less natural
strong evidence
precision ),
important issue
focus space
per state
provide enough
component would
errors occurring
different views
phrase followed
decides whether
building block
hpsg ).
parse accuracy
program produces
incoming information
additional factors
rich semantic
analysis seems
2003 ).
reliable word
., x
lexical correspondences
manually classified
official airline
uk epsrc
long strings
., 1995
speaker might
reasoning techniques
30 ).
user may
nlp methods
automatically classify
iv 101
disambiguation mechanism
e ,.
subject argument
wsd system
improve precision
linguistic background
simply assume
generator \[
relational structures
crf model
interesting case
\] however
person pronoun
(\[ 4
also figure
rule introduces
). training
third possibility
language would
larger constituent
also introduces
since many
model introduced
model 3
expressed either
first question
following sentences
un syst
based exclusively
successful implementation
table 8
effect could
improves retrieval
single argument
align words
examples suggest
last feature
\] showed
e l
asks questions
learning may
automatically label
modal information
accessibility relation
discusses two
completely unrelated
every application
recall scores
knowledge associated
sentence patterns
automatic detection
first apply
np ->
100 million
also related
null formation
prolog predicates
length 5
independent evidence
correct lexical
large improvement
system got
final task
relatively large
sequence model
high frequencies
word occurrences
class ).
chinese parser
quadratic programming
also similar
lynette hirschman
segmentation results
approach applies
rules according
), yet
entire test
new noun
theoretical discussion
extraction methods
set notation
four systems
k u
rhetorical structures
particular tag
seed examples
resulting f
sur une
may significantly
commonly accepted
giving us
lexical items
based probabilistic
n b
summary generation
system accuracy
processing algorithm
analysis result
find candidate
collocations using
acoustic parameters
entropy classifier
especially difficult
cannot directly
value could
surrounding text
process takes
would return
aligned word
1964 ).
phenomena occurring
reliable statistics
three specific
co ),
1980 ),
another evaluation
must involve
larger lexicon
multiple clauses
high recall
dow jones
per speaker
context dependence
24 \],
formally equivalent
markov models
using either
section ii
dans un
segmentation bakeoff
model gave
lexical rules
punctuation tokens
description may
parameter tuning
automatically obtain
formal test
lower precision
example 3
state 2
rules consist
)). therefore
must remain
f f
n examples
alternative choices
strategic component
sentential contexts
cannot reliably
pitch accents
accurately estimate
without degrading
see later
rule based
two entities
another instance
one must
also based
e (:
standard treatment
different english
also specifies
different configurations
initially assigned
2 lexical
evaluation metric
carefully controlled
f r
one interesting
classified according
act based
large variation
like ),
phrases instead
continuous density
input given
must come
extraposition grammars
complete parsing
size may
syntactic behavior
briefly outline
may impose
systems achieved
higher cost
correct rule
smaller test
resulting tree
hpsg parser
phrases whose
basic structures
following assumption
others must
interrogative pronoun
smallest number
90 %,
structures representing
org ).
\] ar
empty np
used throughout
available text
early work
last names
order model
certain properties
1 c
concerns whether
european union
class word
require additional
annotation using
many constructions
completely specified
stanford university
dictionary lookup
allows two
whose length
wide acceptance
specific user
time implementation
end system
crucial feature
token sequences
following test
us introduce
human translator
mexico state
except perhaps
move toward
linear algebra
model features
would let
linguistic meaning
ambiguity rate
output generated
chosen randomly
random subset
improvement compared
subsequent work
unique value
english spelling
independence among
better results
important tasks
xi ),
interpretation using
english corpora
89 ).
estimation methods
every iteration
different styles
greatly reduce
resulting graph
another one
candidate antecedents
per text
particular item
verb types
structure provides
89 \]
complex structures
strong lexical
based data
1992 ))
becomes empty
fairly limited
nouns used
allowed ).
results \[
phrases would
flexible control
accomplished via
extended beyond
also crucial
networks ).
pick one
extract translation
attachment points
de les
see section
best predictor
query contains
semantically valid
\[ ci
usually one
tag based
computational effort
like syntax
problem solver
word combinations
feature takes
rows show
vi ),
later sections
highest overall
another structure
possible transformations
particular objects
sample results
distinguish one
corresponding category
s2 ),
novice user
might change
word given
work would
different query
specified threshold
must deal
different viewpoints
adjoining operation
per line
head ),
1965 ).
would use
structures described
using relatively
\] j
simply state
8 cases
also checked
3 data
lexical level
sentence 8
personal pronouns
three phases
distinguishing features
real system
parser consists
might decide
tree structures
network consists
even among
derivations involving
vastly different
language corpus
volume 30
generating word
numerical results
hypothesis ).
different training
includes three
independent lexical
approximately 2
diverse languages
many candidate
tile end
systems may
applied recursively
great care
following heuristic
automatically assign
data associated
also experiment
two part
closely linked
gives one
n r
function would
previously noted
la recherche
line information
symbols representing
string consisting
exist independently
similar phenomena
could solve
individual lexemes
included ).
specific lexical
000 word
highly domain
provide new
factors affect
seems important
better predictions
model first
spatial relationships
ratio test
frequent use
use text
column shows
cardinal number
without altering
general description
mother category
step forward
speech recogniser
represent possible
finer distinctions
occurrence within
might go
one labeled
1966 ).
)) 2
word probability
previously found
partially completed
sentences generated
many properties
straightforward implementation
probability using
lower weights
key requirement
expression like
rules need
particular set
overall probability
separate knowledge
62 %.
problem noted
every phrase
standard tf
nominal phrase
new clause
problems caused
significant gains
one view
state machines
first consonant
1 would
experimental evaluation
semantic generalizations
exact translation
1 ])
stated directly
semantically oriented
biomedical texts
also report
basic concepts
point scale
recognition grammar
cause relation
markov model
parsing begins
base forms
topically related
average difference
individual tokens
highly sensitive
either english
van noord
one feature
occur anywhere
following instructions
\[ 3
serious problems
existing ontology
x 7
focus structure
lowest common
via cross
property holds
study also
government agencies
difference could
procedure allows
accurate estimates
supervised algorithms
based scoring
15 ):
applications requiring
edinburgh eh8
different facets
defined similarly
make statements
rates obtained
car ).
strategies could
major concern
provide clear
structures used
particularly surprising
next module
like units
model building
generated without
rules apply
large search
analysis produces
another mechanism
two central
certain elements
general tool
see whether
expressions denoting
temporal relationships
simple queries
existing modules
12 million
), giving
complex concepts
successful use
first np
oriented toward
model results
implicit relations
1969 ),
second clause
tools developed
found empirically
speech transcription
frequency estimation
--> np
often involves
specific circumstances
may represent
hurt performance
world entities
find new
two single
existing word
recognition component
different features
new alignment
intuitive sense
small letters
also handle
three principles
classification results
produces better
training parameters
use c
variables occurring
increasing function
n k
product names
attitude toward
sufficiently high
performed poorly
learn patterns
., 2005
particularly well
instances using
tides program
appear first
ct al
n arguments
correct information
current version
new extraction
tagging words
.) 3
automatically extract
current grammar
second reading
system understood
memory requirement
different sources
must carry
thus used
biased towards
conceptual object
english documents
technical difficulties
language user
). two
errors come
unlike standard
achieve higher
best error
together words
must map
takes care
model components
optimal solutions
ones ).
constructions may
thus allowing
little less
always occur
final summary
notational convenience
form 1
preprocessing step
phrase detection
temporal context
past ),
different style
collegiate dictionary
unlike many
semantic verb
components within
larger set
15 ),
quite complicated
better understand
roles associated
entirely different
like english
independent test
simplified form
tagging models
important tool
1972 ),
independent aspects
complete details
may serve
actually exist
whose score
collecting data
using inference
hierarchically structured
user interactions
every arc
best parses
negative log
user interfaces
resolution within
third task
finding ways
possible relation
linguistics computational
requires knowledge
). overall
1 %),
order within
equally distributed
specific aspects
larger training
binary branching
symbols used
often represented
forms ),
performed within
powerful mechanism
good source
since people
like objects
new versions
exactly match
component uses
similar meanings
computational lexicography
average distance
dependency based
use instead
content selection
difficult words
role assignment
bracketed corpora
two grammars
inherently ambiguous
particular domains
formed using
without much
step ).
lexical strings
procedure yields
central problems
natural deduction
l 3
sequent calculus
multimodal systems
method 2
detailed account
full treatment
best word
computational grammar
one focus
also achieves
large scale
basic unit
proposed algorithm
pour une
many elements
structure f
parser \[
set includes
work reported
turn may
entity recognition
network \[
models provide
module also
null rithm
based evaluations
classes associated
systems also
conll ),
particular applications
much richer
languages \[
agirre et
interactive poster
preferred reading
word depends
letters represent
shorter sentence
set given
performance might
predicate p
often based
free constituent
illustrates one
sentence generated
grammar learning
1994 \],
first create
3 months
thus achieving
may lie
complete syntactic
first corpus
mt project
large word
newspaper corpora
nodes may
corpus statistics
three points
tile task
structure associated
statistical decision
4 times
thus making
parallel data
space constraints
internal structure
rule 7
feature description
maximum similarity
calculated according
air travel
). note
trec topics
level alignments
), 5
easily extensible
simple syntactic
basic kinds
user inputs
input tree
two formulae
00 computational
particular systems
three noun
null tional
base case
next turn
often correspond
different random
often assumed
hyponymy relation
scheduling dialogues
evaluation presented
often uses
confidence score
des tombe
must result
probabilities according
slot filled
specific reference
anaphoric noun
one page
traitement automatique
produce reasonable
highest ranked
proposed approach
production process
one large
following conclusions
universal quantifier
method correctly
write e
less successful
), different
go wrong
whose yield
attribute selection
coverage parser
contains word
concept hierarchy
work extends
particular location
linguistically plausible
83 ),
describe several
terms according
sense inventory
structure tree
consider multiple
first reported
individual cases
patterns ),
bolt beranek
logical connectives
appear either
application may
syntactic category
since without
appropriate syntactic
text blocks
perfectly correct
previously unknown
abstract meaning
49 %,
one link
relations also
must make
tacitus system
relations occur
quot ;)
across time
extended version
technique presented
/( 1
art results
nps ).
remaining sections
source information
easily understandable
sufficiently rich
possible choices
special characters
shall refer
\[ 10
0 would
h ('.
first discuss
sproat et
efficiently using
\] c
could equally
). unlike
ranked highest
would necessarily
interesting phenomena
grammatical sentences
previous paragraph
c j
additional semantic
give rise
particular order
referential expressions
ne tagging
two roles
whole family
dempster et
strongly correlated
important advantage
different responses
maximize performance
template task
incorporating word
definition 5
additional resources
people prefer
particular method
particularly important
words according
tasks include
beam search
directly translated
different strategies
det n
evaluation experiments
substantial impact
matrix using
greatly reduces
\] q
larger domains
also selected
also involved
strictly syntactic
subjects performed
role information
production systems
used later
error may
user friendly
frames ),
without linguistic
tile head
far fewer
key question
allowed us
human analyst
9 ].
sequence ).
)( x
replacing words
may proceed
preceding text
additional evaluation
two complementary
following constraints
might fail
validation experiments
test domain
frequently observed
section introduces
mutual knowledge
4 %)
syntactic terms
inference algorithm
must avoid
3 summarizes
see also
parsing natural
improvement due
input japanese
java implementation
efficient solution
4 5
treebank ),
set b
little hope
left unspecified
drastically reduced
scale data
structures defined
new tags
evaluation software
np object
technical reports
pronoun must
whose syntactic
less agreement
person names
words consist
valuable comments
translation component
test words
use p
tagging model
approach shows
research task
cannot take
fundamental question
text database
18 ):
german word
reasonable results
level constraints
memory limitations
sentence number
lead one
corresponding full
working memory
broad class
4 different
mixed results
point average
knowledge expressed
back onto
becomes quite
similar contexts
), sentence
original input
ferro et
segments using
resulting model
one parameter
traditional way
per month
supervised word
evaluation framework
different factors
including language
8 8
w ):
syntax alone
scoring function
different topics
level system
linear sequence
together form
word k
daughter category
2006 main
possible value
case using
schabes et
may support
r denotes
following sense
noise reduction
generator would
also requires
local lexical
practical terms
varied greatly
using verb
include part
reduce parser
p x
statistics based
7 cases
latter case
used section
syntactically analyzed
search algorithm
semantically significant
simple mapping
procedure using
english compounds
g ).
elements like
one tenth
case basis
human assistance
factor affecting
processing strategy
cases ).
inheritance mechanisms
standard language
third example
subtle differences
g \]
initially developed
dramatically improve
order ).
60 ).
every text
new grammar
helpful comments
existing document
given constituent
argument structure
automatically computed
methodology based
proper context
facial expressions
subject ),
\] would
since users
relative similarity
based question
whose probability
high semantic
recently introduced
basic premise
particular positions
sense definitions
three reasons
limited scope
verb interpretation
slight modifications
every time
use hand
driven processing
best fits
0 %),
four data
sample space
two subtrees
would never
extract data
accuracy \[
apply several
different way
another thing
points along
special forms
described within
first attempt
algorithm treats
union operation
whether information
14 words
must estimate
system assumes
g v
features considered
steps required
tagging problems
brought together
incremental process
information added
pronouns must
additional cases
dependency accuracy
whose rows
already annotated
18 ),
76 ).
lexical heads
first half
model containing
approach performs
orthographic transcription
development project
several target
using frequency
many slots
large general
japanese sentences
v np
regular grammar
general news
est plus
clustered using
linguistic content
go together
childes database
show good
research topic
still hold
correct sense
next node
false alarms
w ),
1 lexical
elementary tree
global variable
substantially different
syntactic tag
1997b ),
better recognition
complement ),
several weeks
understood without
partly funded
work concerns
learning classifier
top ).
task model
examples per
pronunciation dictionary
model shown
given lexicon
overall processing
first stage
phenomena discussed
briefly presented
), leading
various levels
~. g
parser returns
make progress
given sample
second version
new component
single terms
method works
6 compares
dependency constructions
research indicates
tutoring system
using partial
black box
two nps
base may
recognition results
sentence refers
opposite polarity
difference lists
including morphological
knowledge incorporated
certain operations
many knowledge
opposite direction
may employ
following strings
news broadcasts
two abstract
many researches
errors result
syntactic units
many machine
precision recall
program written
mean reciprocal
three heuristics
reasonable amount
typical word
), cannot
corpora could
combining evidence
interpretation given
factual information
tactical generation
). previous
one region
computed via
capital letters
reasonable way
set r
studied extensively
e \]
improvement using
\[ vp
event types
combine two
level plan
parsing one
strings ),
longest match
system follows
set associated
also linked
;. 2
leave aside
lexicon ).
data model
rule system
using less
factors influencing
text consisting
could infer
new string
relations encoded
fixed context
vary considerably
expressions including
~( c
one measure
est -~-
past decade
top 1
large document
single letter
mouse click
certain important
general understanding
empirically determined
\] p
extremely difficult
two contiguous
city names
first rule
coordinating conjunction
language information
ii \[
new england
different means
factoid question
method suggested
special character
hundred sentences
example shown
additional assumptions
speech patterns
(: h
specific level
words occurred
transcribed data
next sentence
based architecture
syntactic nature
investigate whether
two characteristics
notation used
statistically reliable
less certain
separate syntactic
\[ 1981
may remain
must use
sufficiently well
four semantic
w appears
system differs
~( q
3 c
first day
2 characters
several categories
clustering task
dotted lines
method developed
open set
assigned based
following experiments
large pool
understanding program
language processors
additional mechanism
user preference
representation system
certain languages
bikel et
one human
highly context
1991b ).
single elementary
surrounding sentences
interpolation weight
checked whether
deterministic automaton
five basic
last several
current results
10 occurrences
participle ).
). briefly
domain semantics
translation dictionary
two coders
based research
proposed using
previously described
time must
argument position
following forms
verify whether
target audience
little additional
ill figure
complexity results
final decision
take longer
original matrix
noun pair
5 characters
,, e
neutral way
various areas
ary relations
lower limit
uniform framework
effective methods
average 2
reference summaries
target concept
translation hypothesis
6 words
humphreys et
various lexical
one database
verbs used
strong sense
developed within
cross entropy
containing word
new top
cas de
human subjects
also clear
k n
e word
substantial reduction
using head
read aloud
easily integrate
training ),
queries may
parameters associated
significant aspects
particularly acute
english versions
multiple word
handle multiple
syntactic objects
problem described
interestingly enough
atomic elements
called semantic
see footnote
simple tree
research sites
previously tagged
following respects
project involves
level knowledge
translated texts
second query
word models
single tag
several directions
ney et
data problems
could parse
covariance matrix
either way
h (!
disambiguation performance
currently focused
defined inductively
framework would
figure 2a
formation rules
minimum edit
corpus comprising
c c
simpler models
common case
context used
larger amount
unification grammar
difficulties encountered
path would
24 ),
analysis component
lexicon derived
linear classifier
system accepts
verb must
constraint ).
causative verb
term clustering
least three
work performed
get longer
basic version
formal power
set k
two nouns
approach allows
basic grammar
horizontal lines
matrix sentence
exponential growth
vary according
conceptual similarity
lexical choice
single units
20 %),
\]) e
typically appear
new members
requirements imposed
complete derivation
procedure outlined
term ).
kim sang
c q
latter may
figures 11
physical properties
target words
structure called
three papers
arbitrarily far
may indeed
joint venture
specific sense
pedersen et
semantic web
mary saw
therefore define
cd ),
containing verbs
acl interactive
generating new
semantically coherent
internal consistency
measuring similarity
document using
parsing mechanism
simply involves
translation relation
temporal expression
classical logic
similar point
also plan
frequently appear
lexical cohesion
french dictionary
five categories
rarely used
random sample
--* c
also differ
2000a ).
following holds
broader coverage
may sound
null using
w (~
one possibility
draw upon
based matching
previously used
assumptions made
performed simultaneously
short description
'. null
certain sequences
footnote 1
improved iterative
sensitive spelling
many parsing
use statistics
average complexity
grammar developed
first segment
terms whose
whole syntactic
far easier
also essential
[?] [?]
much linguistic
3 would
91 \].
que les
space restrictions
first performed
note also
several phenomena
also specify
given english
graph structure
new dictionary
reference set
vectors ).
annotators agreed
estimation algorithms
computer dialogue
desired information
achieving high
word association
overtly marked
present version
network architecture
), neither
robust analysis
une analyse
1998b ),
missing word
previous approach
). semantic
mark liberman
current value
sur la
gratefully acknowledged
negative words
new hypothesis
extremely poor
u u
x ,,
2005 );
possible histories
fine distinctions
web sites
derived semi
already well
semantic distinctions
average human
single item
central issue
desirable properties
sentence lengths
based oll
prohibitively expensive
system obtained
clearly distinct
generate multiple
different parse
long sequence
words instead
feature representation
rather abstract
tree grammar
previously learned
manner described
level hierarchy
either automatically
empirically set
computed according
et hi
significant information
unambiguous sentences
words whose
creating new
approximately three
general strategy
use sentences
characteristic features
known word
based classification
systems without
terms would
phoneme sequences
grammars cannot
). evaluation
best approach
semantic property
ivan sag
another argument
algorithms developed
really need
), whereas
linguistic data
eight categories
two tasks
true positive
linguistic methods
represents either
important parts
corresponding french
task within
list consisting
interpretation depends
full noun
formalism described
often take
oxford university
new element
sorted list
approach yields
little doubt
entity e
thematic relations
trained using
original system
crucial role
information cannot
test sets
1986 )).
three tags
broad enough
factors contribute
nouns n
feature analysis
various aspects
~( p
research described
constraints may
following case
one suggested
collection efforts
search using
distributional data
translation components
new technologies
59 ).
index ).
conversational partners
possible pair
tile fact
corpus .)
grammatical level
first determine
patterns could
empty category
system achieved
generated ).
categorization tasks
showed good
spell checker
articles may
less work
proper nouns
given query
new relation
trec evaluation
individual steps
formal linguistics
frame structures
words would
results reported
could go
value pairs
one developed
used also
higher dimensional
complete linguistic
3 p
immediately precede
learned automatically
retrieval tasks
data within
number feature
running words
7 8
declarative form
word model
good basis
simply uses
two utterances
step back
processing stages
state automaton
given training
syntactic process
require non
single set
wsj treebank
simpler model
bee n
native speakers
consistency constraints
longer sequences
4 senses
words matching
goes beyond
con rmed
languages would
1991 )).
markedly different
choice ),
collected data
parameter settings
main objective
table ),
3 table
higher coverage
third condition
another option
sentence fragment
per input
relevant domain
already aligned
expansion based
systemic linguistics
english version
two source
). one
performance gap
long vowel
help people
would argue
moving away
appropriate concept
3 %),
models include
index 1
useful suggestions
using limited
two smaller
[?] 2
positive results
contextual interpretation
achieve better
false positive
narrow sense
specific forms
explicitly use
dif culty
grammar writers
yet still
two years
slot may
concepts used
vary widely
relevant words
domain specific
small lexicon
weighted voting
complex reasoning
used many
use general
reasonably efficient
strong influence
appropriate linguistic
full morphological
th element
possible verb
wsj ).
second relation
noun head
readily available
author thanks
\] lexical
c p
new results
formalism used
also included
model structure
dative shift
reported using
86 \].
highly precise
sequence tagging
independent framework
physical object
tree 3
collection consists
tagging ).
possibilities exist
simple technique
student may
certain instances
ill general
english bilingual
equally well
semantic primitives
specific goal
modeled using
among languages
must necessarily
model describes
several meanings
manual encoding
also restricted
semantic fields
without worrying
build language
), sometimes
\[ 1986
people often
(\[ 3
dictionary entry
lexical content
together ).
yarowsky 1995
figures 9
features could
using e
ambiguity introduced
linear text
first token
modeling tool
certain syntactic
large system
words get
text representation
complicated sentences
paper proposed
inui et
new components
certain combinations
rather ad
one role
state described
underlying conceptual
without context
church 1988
basic features
us take
space may
error reductions
shorthand notation
practical applications
probability associated
particular natural
(- 1
last type
weight value
real examples
travel planning
even non
par exemple
appropriateness conditions
idea would
phrase rather
strzalkowski et
one action
extraction based
qa track
sense 2
utterances within
un certain
detailed definition
--~ b
text type
speech communication
may result
became clear
severely limits
terrorist act
three measures
practical benefits
product reviews
last step
local level
full use
based scores
strategy could
system becomes
different evaluation
quot ;'
c '.
practical systems
major obstacles
text analysis
est le
correct interpretations
5a ),
pollack \[
e ~(
thus use
corresponding texts
news source
two structural
rhetorical relation
provide efficient
appear less
goal ),
design principles
provide examples
c2 ).
piecemeal fashion
single process
incorrect results
finding good
arbitrary numbers
previously defined
two issues
information values
acquisition process
9 \],
particular speech
complex features
form occurs
would correctly
experiment uses
relation name
subject domains
automatic summaries
many basic
english query
interesting point
ltag formalism
computational requirements
application domain
one key
empty stack
traditional one
adjectival phrase
answers based
representations ).
several hundred
expensive operation
apply different
algorithm learns
many details
training set
particular semantic
nodes without
semantic theory
unlike traditional
branching structures
initial prototype
ne w
experimental work
chinese sentence
specialized knowledge
meaning cannot
process starts
performance results
structure grammars
whose words
approach using
second two
related data
(' h
positive value
~' e
based frameworks
previous proposals
making available
related methods
language strings
structures might
english training
first category
algorithm iterates
tags like
surface sentence
used interchangeably
typically found
incorporating new
example 6
error recovery
existing techniques
probability estimates
previous discussion
corresponding conceptual
holds promise
evaluation also
weighting function
resolution algorithm
across sentences
paper dictionaries
remaining data
first symbol
universal quantifiers
flexible architecture
slightly longer
limited knowledge
lisp implementation
tree features
final testing
logic grammar
architecture based
decoding time
clauses within
easily verified
new value
1000 sentences
pattern like
also think
palmer et
query construction
essential features
fourth international
unbounded length
two high
two subjects
segmentation ambiguities
interactions among
well trained
history h
say nothing
model provided
resulting parser
darts le
several variants
convenient way
basic relation
multiple candidate
document manager
3 levels
practical method
active ),
represent lexical
), make
conflicting information
one event
theoretical linguistic
often differ
existing knowledge
major effort
almost equivalent
testing phase
(', l
correct choice
existing machine
grammar representation
discourse segment
words belonging
statistical distributions
word generation
d2 ),
object denoted
wordnet glosses
(! r
acquisition systems
language modeling
des formes
using prosodic
possible referents
general treatment
help disambiguate
hybrid system
magnini et
): 3
contract n00039
syntactically annotated
phrase ),
fixed expression
traitement des
quite rare
analysis components
problems arising
task 1
level may
find examples
could follow
initial seed
different structure
form using
crucial step
previous techniques
b \]
dimensionality reduction
(: l
model captures
rules determine
people might
transfer system
accurately predict
prepositional attachments
qui permet
supervised system
possible head
whole tree
actually needed
preceding sentences
quot ;[
basic building
perfect sense
clause 1
see text
unit must
role labelling
selected using
al \]
thereby increasing
uses decision
aligned segments
relation used
since word
involve two
equally important
9 show
express generalizations
represent sentences
gives rise
complete model
word immediately
syntactic positions
engineering perspective
five hundred
help users
methods work
core system
per cent
boundary tone
e ')
generation step
p ~.
text genres
grammatical forms
approximately 30
texts within
). tile
second way
word within
multiword expressions
4 types
japanese characters
totally independent
2004 )).
process cannot
taxonomic information
based lexical
improved version
data driven
precision curve
empirical approach
several people
simple model
also include
central issues
time limit
correctly determine
start time
identification number
), language
theoretical models
used maximum
makes reference
domains --
single method
features relevant
associated probabilities
case b
abstract data
acquisition method
draws conclusions
one useful
could hope
n 1
test ).
quot ;\[
noun modifiers
system also
good sentence
il faut
generation literature
recently shown
training process
word context
jr .,
translate words
), pp
quantitative evaluation
different sorts
word hypotheses
disjoint sets
may return
3 uses
?& quot
). similarly
handle large
derivational processes
parsed training
language like
official endorsement
1 sense
grammatical framework
e .~
written summaries
standard measures
estimates based
5 ',
york state
). finally
expository texts
large knowledge
tile best
allows easy
spoken text
appropriate model
list representation
flight number
contains slots
intelligence laboratory
one representation
public domain
phonetic features
evans et
brief description
experiments conducted
like natural
subjective judgments
mental model
truly ambiguous
1987 \])
appropriate form
different articles
several words
foltz et
wide variety
work presented
automatically based
written english
gather information
following categories
problem seems
would tell
le probl
demonstration sessions
tree building
word prediction
present results
pushdown stack
given two
universal principles
rule 9
hand written
small set
automatic selection
1997 association
adding lexical
basili et
whether particular
standard feature
involves adding
based rule
utterances ).
agent architecture
one type
ef ciently
one topic
speech community
key aspects
system integration
one discourse
different mechanism
subject control
temporal precedence
definition language
contains knowledge
next utterance
perhaps surprisingly
system represents
one analysis
national language
crucial problem
system engineering
level categories
average frequency
top elements
adjacency pair
3 sentences
small training
corresponding data
\[ 6
like speech
problems would
similarity relations
marcinkiewicz 1993
third constraint
successfully recognized
number 2
computational linguists
extraction accuracy
anaphoric element
one step
also yield
introduce several
verb 2
rule may
target context
test word
second member
obtain higher
much higher
14 ).
also may
database contains
word fragments
viz .,
like semantic
turn taking
sufficiently reliable
verb groups
general classes
wide margin
verb plus
values would
., 2
different usage
heuristic algorithm
would likely
tile right
act may
14 \]
present two
valid parse
different genre
1971 ).
definite articles
generation problem
many examples
linearly ordered
tnt tagger
different position
either one
event must
expect better
plus de
words correspond
existing parsing
applications based
greedy approach
automatic process
), morphological
par des
analyses produced
4 )).
separate models
systems mentioned
anaphoric relation
contextual feature
recursive definitions
may always
allows certain
technical terms
resolution module
long run
underlying semantic
several clauses
effectively used
larger one
output could
including names
), contains
segmented using
weighting factor
bottom half
text window
two layers
simply counts
increased interest
selection task
keeping track
cover many
crucial aspect
whose function
value would
th row
symbolic approaches
additional argument
three questions
information types
patterns automatically
thus obtaining
rather high
ungrammatical sentences
user ).
also fall
higher levels
method described
present tense
valued feature
database access
may use
semantic structures
ambiguous sentence
de c
checking whether
generation strategy
also characterized
processing language
final alignment
given amount
five words
knowledge database
): first
throw away
among alternatives
effort involved
', l
morphological tags
language particular
would identify
scale corpus
semantic constraint
thompson et
model produced
highly competitive
56 %,
training method
critical component
compile time
discourse situations
final remark
using decision
p must
accurately identify
). regarding
based account
). 8
would probably
future experiments
every test
learning ).
1983 \],
possible orderings
der sluis
resolving ambiguity
output modalities
actually observed
objective measure
remaining pairs
indexing methods
without extensive
deletion operations
million documents
morphology system
either simple
grammatical case
4 hours
bilingual dictionaries
e system
whose aim
improving retrieval
decoding algorithms
state 4
english results
similar word
reasonably expect
accuracy reported
automatic systems
complex way
sequence using
represented explicitly
additional relations
allowing multiple
built upon
already includes
use bayes
assisted instruction
theoretical model
), pr
experiments reported
external knowledge
detail elsewhere
distinguishing among
supervised machine
main semantic
identify potential
genetic algorithm
rr 1992
revised version
conflict resolution
mi ),
become possible
3 test
least 4
pragmatic aspects
one example
key information
tree construction
crossing branches
clause boundary
natural language
corresponding semantic
x represents
would end
symbol sequence
core knowledge
1 million
based largely
done within
1994 );
depends mainly
\[ hovy
interactive user
without pre
spoken output
two tables
might lead
standard deviations
one problem
several researchers
possible antecedents
made accessible
general term
legal texts
like one
correct label
three choices
without disturbing
sentences could
fully unsupervised
small domains
three components
already provides
accuracy scores
foot feature
prior knowledge
upper right
function like
longest word
require training
hours ).
missing elements
structure assigned
stage process
conceptual graph
current theories
., human
manually labeled
using corpora
applied across
major component
representation given
corresponding sub
official languages
4 contains
would begin
generative phonology
current structure
parser creates
system detects
traffic control
constant time
6 ).
japanese character
relation defined
certain arguments
arbitrarily long
work differs
size ).
evaluation program
statistical training
must face
method used
particular communicative
overall error
class includes
sun workstations
program checks
6 \]
learning word
intended purpose
far larger
user specifies
consider several
asked questions
\[ wi
class given
real words
filtered using
also modified
domain contains
cannot occur
entire sequence
noun compounding
4 let
following probabilities
crafted knowledge
automatic method
take either
word v
xia et
borderline cases
spoken dialog
state pattern
ten thousand
close relation
first select
every student
sentence meaning
algorithm 2
high number
grammar described
classification criteria
communicative acts
text files
user profile
manually coded
term memory
one aspect
linguistic coverage
model learned
may raise
parsing work
unexpected results
directly onto
left end
word 1
particular importance
performed using
help alleviate
experiment involved
adverbial modifier
theoretical problems
referent must
existing lexicons
festival speech
performed well
igure 2
,& quot
fully operational
development tools
morphological component
selected sentence
constituent structures
would arise
another ),
data source
dictionary using
get two
strongly related
new pair
6 1
every woman
simple context
two variations
semantic concepts
also demonstrates
identify unknown
tagged training
based information
scores would
target ).
copyright 1988
computational theories
point may
chance agreement
approximately 4
overall word
reduce redundancy
separate model
direct objects
correct dependency
pruning based
word formation
sentences tend
., 1984
efficient search
various experiments
approximately 200
higher bleu
primarily focused
distance measures
cannot obtain
92 \].
analysis suggested
algorithm correctly
arabic morphology
presenting information
h ('
ap newswire
remain unsolved
lexica \]
reports ).
highest number
similar experiment
cannot detect
parallel sentences
algorithms described
various forms
annotated texts
two constituents
using google
space required
sentenc e
tag probability
computationally less
entries associated
2 represents
extracts information
see table
closer together
appropriate context
empirical basis
string language
3 7
main innovation
input signal
forward search
small random
paper could
french verb
translation task
good deal
general facts
indirect objects
fully annotated
'~ l
nodes within
first try
partial translations
one root
presented several
evaluation forum
training material
following sequence
best score
sentence selection
general types
recognizer using
might ask
specific content
\]. furthermore
\[ e
syntactic correctness
without extra
important consequences
difference would
difficult enough
levels ).
conditioning context
answer candidate
could find
underlying assumptions
related question
negative ).
second pair
concrete object
1982 ).
functionally equivalent
interface systems
() r
new verb
one consequence
1998 \],
predicates like
mann et
resulting algorithm
indefinite np
accurate description
two goals
grammar used
derived based
objects ),
also explicitly
table illustrates
1982 \]
shortest path
one name
linear chain
initial design
choosing among
entire phrase
relations --
frequent verb
2 \])
outside algorithm
tile latter
;) may
lexicon using
true iff
certain linguistic
detection problem
strictly context
mary .&
words corresponding
(' l
clearly distinguish
u n
recursively enumerable
foot nodes
class b
data entry
large lexicons
functional dependencies
table gives
16 khz
syntactic data
corpus may
participants must
make reference
derived directly
model combination
). since
parsing performance
scale lexical
right frontier
punctuation ),
good reason
wer ),
5 \])
tagging approach
demonstrative pronoun
finely tuned
theoretical foundations
rank correlation
test document
boundary markers
conventional way
based rules
first procedure
isa relation
end point
initial configuration
individual speakers
left open
variable ),
sentence could
k 2
web documents
include several
multiple knowledge
initial assumption
phone number
whose context
another elementary
level actions
role label
2 },
questions must
bateman et
also put
documents could
explicit indication
might generate
relying solely
also considers
important facts
implemented via
constraints without
pos tag
one dialogue
attachment decisions
alternative structures
context might
serious drawback
tree containing
equation 1
grammars use
14 shows
parameter training
relatively complex
algorithms used
third clause
expression whose
principles used
improved results
major features
important types
different criteria
17 %.
initial test
following approximation
sequence w
idf ),
functional role
submitted ).
existing syntactic
important result
sections 02
ambiguity resolution
differ substantially
tree shown
several sets
given parse
translation engines
mechanism described
final version
volume 19
iv ),
zero pronoun
training texts
local optima
common domain
module based
lodhi et
semantic unit
one property
final grammar
left daughter
wordnet \[
statistics obtained
occur rarely
first section
individual entries
significant advances
new task
input utterances
string contains
still remaining
also restrict
action verb
manually assigned
le lexique
define l
higher accuracy
following phrase
stochastic parsing
wordnet semantic
different steps
contain either
help improve
latter point
general language
common words
two parts
function g
posterior distribution
extra effort
angled brackets
systematic manner
approach provides
extract lexical
negative class
candidate features
highest performance
ti ).
greater detail
gain ratio
chinese tree
input source
module uses
undirected graphical
language competence
general noun
alignment would
.~ h
become widely
first statement
l e
small effect
multiple events
also supports
order languages
industrial partners
disambiguation process
many linguistic
programming system
constructions ).
rules defining
automatically induce
set v
second instance
refer back
exactly equivalent
b ')
see figure
topic area
hierarchical representation
representing lexical
buccleuch place
specific application
interesting class
1 g
word boundary
organizing principle
values correspond
x )),
substantial improvements
control information
forms one
surface realizations
many domain
small amount
rule sequence
quite reasonable
acoustic speech
set 1
denotational semantics
rules like
one piece
word expression
hindle 1983
methods seem
relations based
manual construction
theoretic approach
interpretation within
must obviously
morphological descriptions
rare words
ix \]
described using
around 50
following metrics
main disadvantage
anaphoric resolution
become larger
standard measure
striking result
many words
correct phrase
two different
sparseness problem
intensive task
complete parses
inside probabilities
syntactical information
dataset used
sentence order
closed test
1990 )).
us whether
supports two
weak generative
even greater
involves many
algorithm works
news wire
term vectors
done manually
corresponding roughly
pairs involving
handled via
vectors using
probability path
mechanism used
instructional texts
system state
(( c
object would
five sets
analyzed sentences
level processing
pragmatic level
text organization
using another
first choice
angle brackets
text format
research unit
1993 ))
seen whether
feature types
support research
another item
lincoln laboratory
conceptual entities
require two
exhibit high
rather well
frequent type
corresponds roughly
disambiguation systems
real problems
style semantic
four possibilities
process language
6 shows
10 %.
person name
nodes ).
spanish data
following people
methods ),
different combination
last term
major difficulty
relevant word
many models
correct errors
phase iii
hidden markov
final query
cannot extract
words listed
capture lexical
unseen words
made several
two synsets
two dimensions
provides another
different ones
found within
component also
relative weights
obvious question
v ))
might express
topical information
standard database
structure description
sentence mary
sentence translation
request information
already pointed
section 9
final training
chang et
analysis task
target expression
mainly two
may never
tree pair
model defines
sous forme
2 showed
speech category
rules applicable
100 ).
jensen et
first translation
discourse meaning
technical domains
use case
experiments performed
source data
highly skewed
generative capacity
cannot assume
similar enough
distinct values
improves translation
similar properties
level linguistic
2 gb
following attributes
null language
language also
overall document
systems participating
linguistic evidence
gave rise
would represent
varying length
8 ))
english parse
procedure called
simple parser
object np
n3 ).
algorithm developed
robust language
accurately represent
plural noun
application ).
spoken word
probabilistic grammar
document vector
within single
individual terms
distinct classes
might wish
created using
generated using
latin alphabet
incorporating syntactic
xml markup
publication date
relational structure
2 )),
last night
without special
newswire texts
work efficiently
indefinite number
certain percentage
often also
particular corpus
would generally
using global
two sentence
single semantic
another set
particular field
could work
high information
h ',
therefore cannot
actual english
conceptual model
right way
corpus examples
score according
20 million
relied upon
different answers
functional representation
word entry
5 )),
., 1998
two propositions
may extend
general use
rough idea
quot ;.&
named entity
may view
many purposes
already recognized
bottom level
tile lexicon
nlp technology
row represents
error ).
rhetorical structure
2000 ].
grammatical phenomena
nonterminal labels
developed using
). currently
given instance
would serve
diagonal covariance
previous examples
linguistic levels
tag derivation
le l
say n
np without
providing us
75 %)
system users
transitive verbs
current implementations
des cas
canadian parliament
100 word
first assigns
lexical coverage
level without
used previously
also contain
framework presented
level models
form x
... l
speaking styles
confidence ).
parsing proceeds
raising verb
annotated resources
functional type
directly used
length 2
historical linguistics
hapax legomena
programming techniques
main research
following clauses
features according
scale information
functional analysis
theoretical viewpoint
present evaluation
). performance
generate several
specified value
3 characters
choice made
assign probability
performance differences
reduced version
english pair
string project
human listener
predicate may
processing must
morphological variant
means something
original penn
may believe
also investigated
one specific
written texts
novel features
data due
synonym list
easily extend
different vocabulary
extraction technology
penn treebank
7 ),
tagging using
inverted index
nouns appear
analysis indicates
recall errors
build large
classification performance
later stage
language application
state n
extremely small
similar across
theories like
translation e
speaker says
must identify
different argument
dialogue corpora
modular design
synchronous context
known parsing
normalization process
also display
value assignment
certain problems
), feature
ai .,
basic structure
orthographic information
compositional way
number increases
features consist
given one
research effort
second position
j l
like np
phonetic recognition
argument ),
group may
broader sense
city name
word tagging
phrasal level
stable across
different distribution
algorithm constructs
left recursion
possible segmentation
entire text
identified correctly
also extended
alternative paths
computational implementations
authors would
also relies
inflectional languages
smallest possible
avoid creating
experimental methodology
given structure
phrases occur
communicate via
research foundation
applying syntactic
one subset
universal grammar
arbitrary feature
categories defined
consider different
formal run
local contextual
substantial portion
roughly 10
actually implemented
contextual cues
following alternative
relatively inexpensive
larger ones
formal point
values corresponding
different relations
obtain better
two derivations
iwpt ),
sections 1
higher level
extract information
word trigram
models built
individual units
time step
term similarity
cost associated
extremely valuable
based modeling
statistical means
complex structure
grammars must
must remember
aligned words
ng et
future directions
lc ).
training speech
us also
also words
1991 \]).
corpora would
output data
speech tag
software components
driven approach
microsoft word
best solutions
line text
tagged brown
system comes
wsd ).
bilingual translation
suggest ways
verbal root
mr .,
easy task
could indicate
incorporates two
lexical variation
unstructured data
verb type
must begin
form f
l \],
ways depending
lexical databases
model trained
prior probability
information must
hajivc et
representing information
next phase
provide context
discuss two
political entity
derivation tree
training resources
\[ sidner
draw attention
goals may
found ).
parsers using
one pattern
also match
one term
surface representations
algorithms discussed
functions described
nsf graduate
entities within
though none
allows non
terminal alphabet
cepstral coefficients
tree structure
lexical chains
1998a ),
typical syntactic
knowledg e
gold standards
earlier drafts
also l
sophisticated language
possible structures
throwing away
task using
could even
shallow methods
green et
overall approach
via lexical
segment pairs
unification mechanism
simpler approach
work represents
next highest
automatic transcription
see appendix
use may
clear advantage
automatic natural
3 million
quot ;;
similar system
1 also
human intervention
correctly predicts
le point
chance performance
also investigating
different origins
one complex
full complexity
advanced question
might otherwise
among entities
parsing ),
exactly matched
four items
several word
past research
exceptional case
2 summarizes
katz 1987
one discussed
competing analyses
third component
contains six
)). however
segments must
mapping function
model needs
comparable performance
short term
efficient enough
large proportion
special attention
first traversal
semantic grammar
work towards
data using
different parameters
two mentions
e ~-
lower values
described earlier
relevant document
japanese dependency
np form
used --
tile english
dictionary provides
', v
relation extraction
individual objects
capture information
data well
tease apart
four rules
final results
implicit semantic
naive method
illustration purposes
little training
5 summarizes
automatic speech
level form
different performance
n using
articles taken
wordnet hypernym
million english
would support
fairly common
small changes
system output
optimization process
loosely based
gazdar et
(\[ 6
specification may
feature representations
interesting properties
pointing device
improving performance
procedure must
fewer features
automatic semantic
contextual dependencies
information sciences
help us
larger vocabulary
probability assigned
spoken input
paradigm used
naval warfare
good indicators
increased number
high reliability
physical actions
learning new
system improvement
common occurrence
including zero
1 represents
every character
learning system
without manual
source phrases
correct text
given position
take less
two copies
translation memories
tile resulting
concept ).
term occurrences
offers two
end ).
provides significant
must incorporate
entirely satisfactory
null tic
function application
two stages
larger number
simple description
results could
overlap metric
describes several
one cluster
english structure
another method
like fashion
grammar formalisms
previously introduced
words long
final two
system developers
small change
one .&
three independent
strongly equivalent
built around
context available
constraint solving
lexicon includes
examples indicate
gram language
language phrase
later reference
semantic component
often required
result given
th e
many exceptions
good quality
queries ).
certain context
logical sense
000 words
different behavior
training cases
noun compounds
linking two
potential impact
let x
>& quot
terms like
one constraint
times ).
arbitrarily many
first application
particular rule
without using
\[ exical
1 \])
planning task
sentence consists
language program
using equation
standard bottom
third quarter
whose source
relations involved
practical importance
action described
), [?]
explicitly marked
parsing scheme
randomly drawn
system fails
informally speaking
syntax ).
improved recognition
various contextual
two values
systems requires
reasonable performance
lexicon provides
two senses
three discourse
sequences like
systems across
systematic fashion
pairs containing
p .,
frequently appearing
suf ciently
may write
two classes
), working
extra features
.~ l
specialized domain
conceptual roles
1986 \].
though many
may argue
information coded
l ...
recognition rate
spurious ambiguity
might call
corpus processing
sentences \[
grammar size
96 %,
1 },
tile training
without giving
performance among
software tools
context features
many relationships
phrase consists
data along
words like
related areas
analysis described
insightful comments
advanced research
relevant properties
hearer must
every case
utterances using
also point
sets according
unless explicitly
one valid
limited domain
used word
existing linguistic
structures shown
related lexical
two neighboring
de col1ng
typical examples
logical calculus
paper proposes
selection criterion
graph ),
10 1
use log
stative verb
meaning must
questions correctly
technical texts
like number
balanced corpus
based procedure
quite possible
actions ),
noun reading
different things
often contain
gathered together
original words
various mechanisms
languages like
case roles
mouse button
\[ 17
valuable resources
also form
also built
l :~
word segmentations
structural aspects
1998 ):
systems provide
design choices
weighted average
-- c
planner must
88 \].
necessarily correspond
tts system
assess whether
1991 \].
means one
c ,,
linear constraints
optimization method
dirichlet distribution
experimental systems
result demonstrates
also function
essentially two
23 28
similar techniques
kappa values
allow comparison
already part
resulting sentence
semantically compatible
sets consist
may either
shall briefly
like agent
noun complement
since parsing
different needs
;, may
., including
model rather
\[ 24
widely studied
representation model
known syntactic
use also
sense granularity
substantial amounts
user sees
increased performance
output pairs
currently running
also taking
definition questions
related sentences
current best
many hypotheses
word occurs
previous linguistic
initial experiments
phrasal translations
representation built
text representations
simple algorithm
serves two
kh2 test
attachment ambiguities
considered relevant
quot ;'&
comparison would
short answer
logistic regression
local maximum
01 level
st ),
class model
processing large
partial solution
promising area
idf method
following excerpt
unsupervised systems
information captured
every production
ca ).
verb root
first data
confusion matrix
also aim
existing models
independent text
absolute difference
ca \]
elements --
one noun
prototype implementation
english counterparts
correct output
terminal node
empirical grounds
would prefer
different individuals
janin et
understanding system
:~ l
grammar produces
provide clues
level higher
collaborative work
labeled instances
specific text
antecedent ).
technology foundation
small one
analysis used
model might
representative corpus
features instead
robust estimation
evaluations using
), 2
decisions based
paper \[
latter class
experiments presented
japanese phrase
learning techniques
features whose
briefly mentioned
efficient system
particular subject
dialogue moves
novice users
user knowledge
experimental method
mean vector
), plus
state grammar
large linguistic
atn grammar
sentences according
previously observed
c ~:
indicate different
additional meaning
1998 ),
svm classifier
obtain new
lexical decomposition
including discourse
process must
corpus without
plupart des
different behaviors
driven techniques
g r
score would
small clusters
logical forms
noun ),
articles ).
many authors
contain less
quite powerful
mapped onto
typically require
crucial use
pruning thresholds
1984 ).
[?] 4
brants et
large domain
first left
interesting new
category ).
bound variable
10 percent
vertices v
90 ),
1984 \]
n w
semantic features
define four
whose antecedent
knowledge cannot
us assume
human use
following main
journal texts
unsupervised method
texts using
clear understanding
would indeed
30 %.
similar events
word using
document must
32 ),
using bilingual
positional features
theoretical issues
individual systems
standard analysis
expression corresponding
current formulation
methods tend
class problems
new theory
features would
-- would
possible paths
np chunking
50 times
several major
answers may
ner ),
tipster evaluation
logical framework
easily detected
mutually dependent
speech applications
section iv
constraint equations
word basis
candidate pair
times slower
syntactic classification
devices used
one moves
category v
using 100
approach called
4 \]).
working within
verb semantics
turn requires
r ):
nearly 100
best models
definition 2
pairs found
sentence \[
c /;
relevant elements
.) another
parameter space
discuss previous
work around
expected number
two numbers
could become
arc used
general view
automatically obtained
particular information
must represent
assume familiarity
associated word
rule sets
correct mistakes
;). thus
tree bank
functions defined
sense baseline
least 100
contains words
various meanings
three iterations
might conclude
may appear
must examine
list contains
compact way
easily confused
25 %,
difficulty lies
general feature
newspaper article
n sentence
broad spectrum
therefore decided
university students
02 ).
contain unknown
tree produced
corresponding lexical
also worked
comprehensive system
93 ),
philip resnik
simple yet
stem form
quot ;*
currently studying
strategy would
left implicit
contextual analysis
become aware
alignment information
4 2
almost equal
possible reading
already constructed
distinguished symbol
first five
1 )),
evaluation based
various approaches
two facts
); second
zelenko et
appropriate conditions
state si
annotation tool
grice 1975
common error
really like
e r
summary quality
quot ;:&
narrative texts
first possible
derived forms
\] also
users may
bootstrapping approach
unique set
first sub
leads directly
xml document
single consonant
asking whether
1983 );
interesting area
result supports
cohen \[
two types
world wide
example ),
par les
exchange information
normal forms
known examples
reading times
incorrect ones
1981 \],
problems presented
semantic relation
application requires
also higher
hpsg grammars
space ).
require one
95 %)
parses using
defined independently
system error
subcategorization frame
average size
speakers ),
patterns generated
hierarchical organization
2c ),
obtained directly
highly reliable
also exists
state 3
partition function
technique based
rule fails
large samples
several candidate
using training
easily understand
also fails
matches exactly
performed much
content information
much discussion
r ),
original form
machine communication
word aligned
70 %.
1953 ).
search must
limited form
example consider
description length
one use
ellipsis ).
tag sets
usually made
time interval
-- p
semantic specifications
foundation grant
could consider
models ).
five major
least 3
13 ].
existing model
help system
subject np
bilingual data
takes us
martha palmer
., u
text encoding
new corpora
first searches
grammar extraction
50 %)
u 2
corresponding rule
system similar
direct speech
important feature
algorithm described
common word
posteriori probability
also taken
search path
der mann
10 shows
kathy mckeown
several properties
particular rules
increasingly complex
passive ),
recall would
cpu time
must play
combined system
single np
everything else
templates generated
prepositional complement
binary classifier
medical language
german words
retrieval result
must lie
\] represents
little effect
could compare
discourse consists
told us
possible assignments
surrounding contexts
template relation
open vocabulary
earlier experiments
possible outputs
89 %.
newspaper articles
concrete examples
certain nouns
great promise
particularly suited
sentence according
labeled examples
applications include
;. let
prior linguistic
input node
potential advantage
great success
verbal complex
que des
pronouns refer
social interaction
documents according
speech translation
semantically ambiguous
78 ),
2 p
figures 1
table look
linguistic level
): e
using available
language text
easily adaptable
still incomplete
phoneme conversion
maximum mutual
common class
many parses
interpretation algorithm
first tested
structurally similar
di cult
). second
formed utterances
defined via
models 1
also observe
representation within
possible constituent
40 %,
tile two
one response
rely upon
requires us
direct correspondence
syntactic behaviour
finite list
2 table
top 1000
5 p
corresponding increase
given certain
sparsity problem
driven parsing
rare word
evaluation experiment
e k
entire word
event representation
sentence pairs
length increases
information states
matter whether
level annotation
best precision
attachment decision
supervised models
serious challenge
semantic choices
even one
usually consists
automated reasoning
defined formally
test queries
knowledge specific
verbs appear
two language
parsing process
format used
paper summarizes
validation experiment
based taggers
static information
communicative act
across many
parsing could
traditional dictionaries
extremely time
information 2
tags instead
always included
recorded speech
2 %).
current address
acted upon
best model
22 ),
rule formalism
basic ones
experimental studies
main aspects
models derived
7 gives
le ).
may easily
form part
one alignment
ranked lower
prosodic features
application system
np ).
done using
low number
event type
dead end
fragment ).
scientific articles
immediate constituent
examples ),
since syntactic
approximately 3
procedure takes
)= 0
sufficiently constrained
using features
answer given
level ).
following figure
still far
np \]
... ).
larger vocabularies
2001a ).
reference answers
user chooses
traditional notion
capture long
5 %).
algorithm used
input sentence
structural level
word node
without recourse
subsequent steps
rate ),
automatically learn
immediately dominated
specific algorithms
particular category
based chinese
different terms
official muc
better training
realization component
set using
unseen word
verbmobil project
may identify
sample set
plausible candidate
subject field
word w
nouns ending
uses two
applications may
)) 3
new proposition
actually produced
resulting network
quite consistent
either 0
thus less
k e
le premier
np v
condition c
level ontology
second best
single corpus
even simple
11 \],
solid arrows
complex ones
subjects would
sn ),
independence assumptions
state descriptions
research results
... v
estimating parameters
two hours
different dictionaries
programming interface
major differences
based tagger
), consisting
motivated us
therefore use
length three
quot ;@&
linguistic sense
discourse connectives
alphabet e
often comes
model summaries
world war
np 1
parsing systems
processing tools
different distributions
corresponding segment
new structures
., 1980
j ).
identified three
element must
show later
tile way
second option
level 1
... 1
represent many
following word
changes made
respective sentences
natural human
reasonably high
group words
mathematical sense
j \]
method combines
partial words
stochastic models
2 --
domains may
also implicitly
perhaps even
come first
overall similarity
users also
different notion
annotated corpus
corresponding reference
particularly high
making certain
() 1
may end
semantic value
easily identify
singular values
parser assigns
speech taggers
ou de
good feature
information measure
different grammatical
soviet union
speech material
reference number
target languages
primarily interested
competing candidates
general questions
), developed
textual elements
constant throughout
still largely
three pieces
relevant knowledge
citation form
5 --
explicitly represent
must support
standard practice
several elements
also improve
r (~
word sentence
general learning
several equivalent
target node
significantly less
partial match
avoid ambiguity
linear parsing
lingual dictionaries
application area
rapid prototyping
sets would
trees representing
wordnet lexical
derivation must
parameters using
required syntactic
new documents
quot ;~
free parameters
behaves like
identifying names
use supervised
van der
four sections
three fields
consistent results
one initial
'& quot
formedness constraints
labeled tree
states correspond
japanese text
four ways
existing generation
output would
3 also
highly associated
multiple linguistic
statistically significantly
word sequences
discuss one
eugenio et
process large
lower quality
language output
argument positions
explicit word
utterance --
position information
without actually
main information
substantial difference
parsing method
2 ..
e domain
example suggests
latter kind
best single
object level
relative success
node ),
design decisions
provide feedback
using random
2 figure
labeling task
hypotheses using
temporal entities
whole algorithm
corresponding information
information database
types de
research topics
crucial difference
added benefit
), cf
discourse \[
87 \],
rule ).
binary predicate
current trends
containing words
different tag
94 ).
wider scope
intelligence research
aberdeen et
efficient parsing
wit h
several new
function composition
still difficult
included within
speech synthesis
low performance
provide high
act ).
overall score
may arise
identify non
rather free
significant time
connected speech
parse input
since h
chapter 2
form must
multiple words
hence cannot
hierarchy used
xml elements
compositional manner
elements corresponding
given argument
transfer model
using distributional
forward pass
phone call
explicit marking
voyager system
ir task
makes good
). otherwise
achieve acceptable
important questions
useful feedback
widely applied
muc task
pattern p
defined according
right modifiers
terms occur
v \[
interpretation module
language learners
relevant events
verbal noun
structure cannot
general computational
book .&
used across
set operations
induction hypothesis
(: ally
limited context
threshold used
could capture
based dependency
relies heavily
3 represents
specific search
large sets
secondary importance
80 %)
current status
representation ).
comparatively high
useful properties
annotating corpora
optionally followed
rather crude
english ones
every man
model training
purely lexical
lr parser
bootstrapping techniques
machine interaction
first time
automatic classification
based application
single number
generation grammar
whether p
training corpus
varies greatly
computationally tractable
multiple models
features correspond
high translation
result without
rules arc
also analyzed
another semantic
occur without
language variation
object pronouns
aligned parallel
good starting
measure word
base system
following experiment
three anonymous
li (:
computer scientist
method introduced
new concepts
already assigned
use l
text collections
method 3
easily found
dynamic knowledge
inheritance hierarchy
sentences would
similar syntactic
syntactic rule
informational content
names used
extraction approaches
simple default
particularly true
1976 ).
words also
uses statistical
system input
like german
words occur
newspaper stories
verb complex
additional material
terminal x
often fails
higher likelihood
different nature
matching problem
makes errors
using two
string rewriting
would present
1976 \]
may incorporate
3 \])
longer utterances
nearly two
possible configurations
previously shown
creates new
tag ).
example set
paths may
considered good
separate module
measure based
base management
requires finding
latter approach
certain structural
second international
recall levels
dimensional array
approximately 10
linguistic forms
various alternative
coverage grammars
different summaries
output labels
explicitly representing
speaker ),
next move
complex relations
phrase patterns
around 15
1997a ).
corresponding entries
c represents
makes possible
present work
values obtained
clustering results
translation unit
corresponding rules
frame structure
empirical validation
alternative strategies
2 uses
information contained
also derived
answer depends
annotation manual
two processes
performance even
entire input
provides two
nodes along
function c
n x
known linguistic
completely separate
used .)
time .&
often mentioned
correct sentences
), direct
rule specifies
exist many
automatically detect
generation component
previous context
class 1
missing words
suggest possible
3 },
final structure
q ))
experimental data
matching strategy
radio broadcasts
indicator functions
thus increasing
current constituent
past participle
theoretical basis
successive application
easily incorporate
deverbal nouns
set w
show whether
one knows
ikehara et
linguistic framework
systems include
entire document
trees used
discussion .)
different objects
70 ).
one difference
illustrate two
system achieves
operational system
probability tables
would often
dependent language
abstract level
significant contributions
argument boundaries
useful approach
known words
contain pointers
every sequence
negative score
lower part
prepositional objects
much earlier
automatic corpus
best hypotheses
following general
verb cluster
highest recall
english letters
also produce
phonological rule
paper explores
resulting list
potential candidates
also employed
entropy models
one grammar
usually implies
come together
000 ).
various text
). following
filtering process
[?] n
test examples
numerous examples
always yield
word instances
extraction program
results showed
semantic inference
classification process
proven successful
nlp processing
capture local
syntactic aspects
english vocabulary
2 sentences
interpreter must
three corpora
grammatical formalism
three primary
verbs found
grammars developed
takes precedence
usual case
x ,&
consistently used
constituent boundaries
universally quantified
method selects
c },
tasks using
unique way
current lexicon
handle complex
based methods
whi (:
tile previous
paper would
head category
phase uses
determine possible
fold increase
function whose
stem ).
would always
88 %)
processing speed
language learner
also gratefully
significant improvement
appropriate node
least amount
sampling rate
local minimum
sparse training
5 sentences
optimal values
original treebank
items involved
nihon keizai
multiword units
maximal value
relations include
decision boundary
common properties
different set
interactive mode
semantics ),
remains constant
following input
description given
nonempty set
tile parse
extract relevant
several techniques
single rule
relative contribution
control mechanisms
selection restriction
la ),
kim et
de textes
h c
first introduce
based classifiers
anonymous ftp
point ).
phenomena described
units used
reduce computation
close coupling
recently become
distinct feature
noun complements
%, 80
normally used
time adverbial
\], although
). systems
electronic versions
;,& quot
alternative way
noun compound
ner systems
window broke
one component
properties like
little change
particularly effective
), little
pronoun use
applied without
2006 association
rank 1
larger units
following major
high cost
usually consist
existing annotation
syntactic subject
random distribution
n f
type 2
label c
module may
simple sentence
b r
complex morphology
called translation
several knowledge
include language
lexicon lookup
number iri
found using
topic words
every entity
analysis procedures
labeling problem
grid search
know exactly
specific verbs
line 7
several machine
classification systems
newswire corpus
text indexing
links within
literature \[
generate appropriate
learning paradigms
e form
last element
evaluation campaign
time points
lexicon model
similar examples
6 provides
structures produced
certain special
technique requires
computational lexicon
new kinds
using common
sentence would
increases significantly
basic idea
system selected
60 %.
semantic space
full name
particular meaning
rate training
must refer
annotator 1
translation outputs
probable parse
flickinger et
;>& quot
documents would
later section
algorithm discussed
resulting vector
computationally expensive
telephone corporation
mainly concerned
worth noticing
first would
automatically detects
one string
corpus annotation
important consideration
lexical constraints
source sentences
small range
occur next
appropriate sentence
language --
therefore must
false starts
dependency treebank
could describe
explicitly used
bleu scores
;, l
property inheritance
method first
final position
marcus et
given rise
least common
may request
seen previously
distinction among
speech data
issues associated
grammatically correct
branching structure
reasonable set
philosophy behind
vocabulary continuous
hypothesis 1
f )).
draw conclusions
tags based
2 test
darpa communicator
large datasets
explicitly asked
back end
total numbers
cannot handle
actual structure
advanced information
formal properties
input x
must develop
distance threshold
76 %.
distribution defined
volume 21
underlying logic
relations may
scores obtained
grammar written
user interface
japanese thesaurus
murray hill
user explicitly
applicability conditions
precedence rules
specific sentences
pattern based
syntactic information
corpus ).
algorithm seems
process known
first prototype
six categories
question might
likelihood scores
), type
filtering method
thus produced
pairs used
participle form
5 test
results according
speech acts
related forms
one quarter
possible features
conditions must
use pos
classification method
common across
verb noun
vertical line
original intent
000 nouns
higher probabilities
computer screen
million tokens
certain conditions
acoustic information
better way
attributes associated
whose similarity
models outperform
sharp distinction
), three
., noun
original word
). alternatively
external context
strong assumption
features corresponding
high dimensionality
initial probabilities
1990 \].
system requires
one top
). almost
computational advantages
draws attention
cannot translate
events like
combined score
descriptive information
word b
also shows
built based
may overlap
aoot 1992
opinions expressed
logic formula
severe problem
documents retrieved
based around
speci cation
compares favourably
3 )),
function p
evaluation system
entropy model
). co
computed efficiently
collection initiative
instead use
following properties
reasonably well
modification relation
statistically different
currently working
essential idea
published work
changes according
extracted fl
following kind
thus generating
scores achieved
would build
standard format
corpus 1
\[ 1967
example illustrates
single segment
language related
anderson et
null a0
limited class
method results
different domains
l );
); note
intransitive verb
also compared
manually identified
towards achieving
small vocabulary
7 data
component word
marginally better
words left
extended notion
central part
freund et
). recall
dependency parses
sql database
word x
right ones
provide explicit
present another
svm ).
upon previous
distinct nodes
quot ;#
common elements
indexing system
fine tune
english morphology
particular ways
output directly
bottom left
existing parser
1 p
see kaplan
select documents
user intended
different applications
speech \[
proper part
;. 3
phonological information
define new
small number
1999a ).
standard term
often related
match algorithm
several important
simplified model
1975 );
something different
0 c
key phrases
28 ao
every individual
first table
fairly easy
current english
global information
corpus derived
contains four
two interesting
extraction engine
senses used
variables representing
1 table
necessary semantic
n )).
c )),
good measure
syntactic rules
ordering relation
close proximity
context independent
one selects
different systems
art systems
tutorial dialogue
order formulae
might consist
way toward
comparisons across
make good
new constituent
man loves
natural science
additional data
also better
system .)
dictionary definitions
language based
deep structures
phrases --
understand natural
little interest
alignment procedure
traditional method
acres de
approach takes
2 7
local search
knowledge representations
sentential form
concepts found
basic functions
clusters containing
th ,~
labeled precision
conclusions contained
10th conference
1958 ).
direct commercial
traduction automatique
1 %).
large parallel
software used
text passage
simply taking
also performs
uniform across
nlp researchers
one dictionary
heavily influenced
srilm toolkit
process works
boundary ).
landauer et
actual usage
,..., k
bring us
electronic version
sentence corpus
5 7
perform word
vector machine
example two
reliable enough
rather unusual
descriptions using
average 3
language uses
may play
one hypothesis
various algorithms
digit strings
output sentences
(~ l
people find
providing examples
produces high
two definitions
sample texts
problems occur
less importance
1 '.
specific input
prepositional complements
stress patterns
also keep
derose 1988
parser errors
automatic rule
relevant syntactic
analyses per
known problems
style rules
problem faced
contains 1
make errors
representational level
etc .,
2005a ),
unique parse
since words
different order
minimum cost
analysis might
strongly depends
7 \[
entities using
quot ;.)
set provides
three concepts
surface cues
must leave
words around
paper extends
processing step
links ).
word preceding
models typically
work involves
rules whose
second constituent
every state
sri international
around 95
1981 ),
level approach
step 1
different method
computer science
less consistent
w (;
user actions
lambda expression
learning capabilities
current set
possible results
intentions underlying
/& quot
input utterance
otherwise ).
make possible
maximum sentence
analyzer developed
large class
specific constructions
viterbi path
word f
phonological rules
v c
currently done
entire system
one element
binary search
around 80
semantically non
\[ 1985
briefly sketch
garside et
tile one
familiar words
subtree rooted
many new
considering two
input list
processing times
., use
number n
smaller chunks
practical situations
one first
underlying meaning
useful knowledge
common base
hmm alignment
possible responses
lexical forms
1 --
current training
time response
sls system
since l
one involving
german ).
correct alignments
text together
negative examples
take another
theoretical significance
variable binding
., n
possible strategies
language sentences
j .~
tile structure
one relation
strongly supports
h '.
following elements
features listed
also served
generation components
section four
meaningful words
five languages
different frequencies
formal system
rules would
types must
case occurs
first ran
widely distributed
u ~,
hierarchical agglomerative
recognition process
york university
textual segments
john told
semantic theories
efficient dynamic
syntactic trees
2005 ):
section ).
), including
interesting examples
separate sentence
context provided
x .)
particular choice
graph consists
semantic structure
multiple hypotheses
articles using
templates ),
might look
proposition p
relatively easily
probable one
among several
human supervision
somewhat easier
single token
good predictors
local word
automatically without
length 4
shall show
inc .&
certain constituents
one difficulty
statistical distribution
phrases based
different temporal
semantic hierarchies
fit well
might need
single edge
system keeps
using cosine
1 ..
11 ):
adverse effects
detailed information
used alone
infinitive marker
uses one
two feature
corpora like
concept node
certain well
1 figure
directly available
sense tags
z 2
end node
probabilistic chart
word list
input language
possible topics
variables used
id number
capture many
also avoids
general verb
kevin knight
work based
semantic head
questions asked
\[ il
particular translation
relating two
within 5
additional requirement
level object
completely determined
interactive program
might reasonably
use shallow
section 1
different positions
sophisticated tools
sl ),
statistically signi
discourse would
1 ,'
distributions using
word graph
nouns appearing
algorithm produces
important role
larger systems
smadja 1993
first user
made use
cognitive linguistics
template structure
certain goals
input vector
without saying
automatically inducing
also want
linguistic competence
work directly
every predicate
multilingual applications
quite far
easily described
within reach
within different
grammar construction
french version
predicate ).
set x
last row
parsing \[
semantic differences
unsupervised manner
currently known
work uses
simple reason
similar concept
french grammar
rule approach
distinct tasks
probabilities computed
function based
would possibly
might apply
reasons behind
several occurrences
leacock et
global level
rule requires
methods achieve
c (:
~, g
2005 ),
publicly available
provide important
many proposals
japanese verbs
original document
corpus shows
target tree
possible way
one tries
intuitively correct
primary stress
even number
every point
extended set
thus defined
uses simple
similar linguistic
expression may
previous utterances
size k
one variant
new version
final template
relevant examples
either order
bilingual corpus
single category
representation allows
available ).
based text
might add
method exploits
lower nodes
semantic similarities
get rid
jerry hobbs
long period
f ~.
methods need
group ).
11 ),
1 based
experiments showed
communicate information
like wordnet
single symbol
common knowledge
domains without
files containing
high scoring
include multiple
lm ),
highest posterior
three kinds
much recent
h ,'
match occurs
four methods
development effort
much remains
senses defined
two names
scores also
princeton university
2 .)
give information
), support
could automatically
lack thereof
using binary
language input
mohri et
raw texts
groups words
2004 shared
whole sentence
general task
discourse referent
similar words
absolute frequency
model generates
0 %).
first uses
prolog code
recognition method
database used
basic lexicon
termination problems
also annotated
system knowledge
considered separately
|; ion
paper concerns
traditional statistical
constituent structure
matching pattern
w ~)
l .'
1 uses
word vector
questions remain
english adjectives
extensive discussion
effective solution
grammatical input
g 1
several basic
models using
test samples
5 .)
[?] 3
also depend
automatic information
also provide
statistical parameters
main ways
fine tuning
standard logical
communicative goal
earlier approaches
many techniques
joined together
predicate word
entry would
commonly associated
one phoneme
semantics could
zue et
another np
adjunct relations
lemma 2
language constructions
used include
1985 ),
greater accuracy
annotated trees
text containing
hoc way
sentential context
asr errors
basic notion
partial parses
easily used
functional tags
shared tasks
current location
frequency features
morphological segmentation
surprising given
similar models
stochastic finite
one construction
gaizauskas et
pos labels
highest scoring
basic grammatical
electronic dictionary
grammatical constructions
within sentence
possible orders
), allowing
following discussion
empty subject
two textual
recall level
large compared
one value
x1 ,...,
ambiguity ),
meaning representation
two nodes
least partial
generate n
increased recall
subcategorization frames
set f
structure contains
kay 1979
e ).
formed sentence
single variable
examined several
tokens ),
difficult problems
partial word
due primarily
system thus
text describing
average results
left sibling
assumption made
general representation
treebank tag
different statistical
art statistical
first sentences
second item
may prove
given grammatical
systems still
23 ),
initial state
several syntactic
result ).
journal corpus
type verb
construction may
higher preference
several combinations
important task
quite difficult
message containing
must always
knowledge must
phrase refers
performed manually
passed back
noted earlier
learning technique
user models
strong constraints
always contain
factors must
automatically built
two large
), u
.' b
first concerns
perform better
different places
de la
per word
u e
positive training
first group
ei ).
new query
new paradigm
linguistic expertise
extensive knowledge
complex grammatical
different texts
byblos system
although none
generalization performance
n ~.
general type
prosodic structure
general topic
could combine
results would
e v
r ()
abstraction hierarchy
linguistic phenomena
simple counting
et les
sense 3
easier task
general discourse
functional grammar
interchange format
must conform
learning models
use external
link types
pair consists
turn depends
several alternatives
analysis grammar
2001 ],
kittredge et
selection strategies
8 %),
2002a ),
new training
containing 1
e 1
rare events
\[ brown
detailed level
information including
phrase rule
little agreement
morphological structures
little bit
representational framework
), making
multiple parses
per class
certain node
criteria described
maximum values
natural consequence
euclidean distance
th component
acoustic modeling
top 500
algorithm outputs
used may
functions available
ftp ://
like mary
therefore created
brief overview
editing tools
continuous speech
four possible
given tag
use similar
better evaluation
user interacts
four sub
english ),
basic units
). data
complete tree
., 2000
extraction algorithms
different conceptual
well since
., 1977
discriminative methods
,..., sn
rule base
simple types
may prefer
specific constraints
theoretical considerations
bottom line
essentially identical
another corpus
2006 ).
also able
using simple
proposed analysis
complete utterance
independent system
structure representations
three versions
also notice
corpus tagging
describing objects
tag new
grammar containing
f must
taken seriously
46 ),
allows one
second set
led us
globally optimal
kingsbury et
example parse
summarization evaluation
%. although
project ).
two learning
small fragment
unambiguous word
many operations
use word
first outline
2000 );
specific system
6 presents
becomes impossible
simple implementation
japanese input
combining word
numerical expressions
efficient processing
standard n
describe three
immediately adjacent
modern english
techniques proposed
unseen events
bringing together
identify two
original approach
important type
grammar shown
). formally
qualitative analysis
speaker dependent
important topic
finally section
simple list
special case
natural dialogue
least 6
volume 24
accuracy increased
one row
first test
different acoustic
immediate dominance
key ideas
string representing
6 proc
depend directly
also reduce
works correctly
rst tree
bayesian framework
like french
among constituents
section shows
explicit knowledge
generalize across
document representation
distributed throughout
six words
surface representation
case arises
system alone
poor precision
appropriately chosen
word bigrams
english lexicon
correct forms
important step
many important
together within
75 %,
word ends
average value
known ).
pruned away
large enough
manually transcribed
best tagging
hearer believes
artificial languages
feel confident
decisions may
bilingual lexical
build complex
\[ grosz
english syntax
interesting observation
relatively less
numbers represent
often involved
also plays
4a ),
\] --
previous algorithm
english pronunciation
standard bigram
document understanding
scheme proposed
theoretical grounds
). however
probability values
basic english
1992 \])
two decades
better one
(', r
speech research
completed parse
four months
obvious choice
seconds per
several extensions
successfully identified
english letter
correct translations
whose associated
user supplies
criteria used
similar model
produce good
figure 7
emission probabilities
diverse types
improves accuracy
complex nature
first correct
basic methodology
many languages
new features
reasoning capabilities
%. moreover
document analysis
table entries
slight improvement
., hobbs
corresponding translation
similar form
baseline results
possible parse
certainly possible
forms could
general problem
given item
design decision
bigram grammar
). \[
relatively independent
contains enough
literal interpretation
contain non
best algorithm
must build
parallel text
events occur
t2 ),
p )).
bnc ),
similar function
expressions used
two states
linear models
interesting linguistic
several examples
explicitly present
accusative case
(: r
quite poor
rules directly
may refer
2 million
although many
suitable representation
nonterminal label
based algorithms
one containing
among candidates
null step
model capable
aligned using
particular application
rules correspond
standard alignment
good news
data must
simple language
clause grammars
grammars used
information shared
may potentially
features like
-- like
user model
open ended
limited memory
verbal element
specifically tailored
aide de
intuitively obvious
two groups
built incrementally
oflazer et
n must
darpa speech
isolated sentences
path p
perfectly clear
label assignment
semantic ambiguities
task definitions
\] figure
5 million
one study
di erences
brief descriptions
translation results
often requires
without performing
thus require
identifying discourse
equally effective
might therefore
could change
approximately 6
larger texts
repeated application
produce errors
would first
inference engines
classified based
assign high
topic ).
one describing
resulting feature
uniquely identifies
make heavy
learning model
semantic consistency
speech would
corpus annotated
representation using
objective measures
expected frequencies
could decide
slight variation
also see
reliable indicator
sufficiently different
quite robust
default ).
basic set
parser whose
c ++.
3 main
conversational implicature
linguistic approaches
like chinese
modeling human
network whose
feature type
agence france
new computer
paper analyzes
languages whose
possible pairings
representation techniques
syntactic heads
cannot guarantee
using existing
lexical insertion
similar problems
\[ 20
good representation
generation decisions
nlp ).
evaluation may
since context
variables appearing
tagger developed
los angeles
quality measures
ordered pair
important problem
successfully completed
perform worse
1 7
tools described
given set
occurring noun
published dictionaries
mccarthy et
another agent
probably would
whole classes
overall precision
system include
longer sentence
structured way
several solutions
common characteristic
elements may
w ))
quot ;!
highly abstract
two blocks
highly constrained
conducted within
whose arguments
14 %.
provided us
inverse relation
tree resulting
direct reference
rich lexical
following axioms
human action
approach significantly
level languages
many sequences
manual effort
greater level
exact match
quantifier scoping
following restrictions
whose leaves
often provide
current statistical
common underlying
1 )(
single meaning
relations without
overall model
complex set
%. thus
canonical position
identify common
prager et
relevant linguistic
technique may
gaussian prior
fold cross
based learner
cky parsing
column 5
parser would
machine could
first check
additional research
concept b
network would
generator uses
experiment ).
two successive
language constraints
step approach
node consists
different parsers
art parsers
seems much
linked via
broadly speaking
randomly picked
interior nodes
p contains
information helps
large annotated
scale text
). according
graduate student
items may
definition 4
already demonstrated
less affected
interpretations may
... w
source software
parse may
human writers
37 ).
simple morphological
edge labels
probabilities p
final performance
one end
word part
important aspect
grammatical constraints
1970 ),
involving word
tensed verbs
learning problems
potential answer
lexical categories
state approximations
telephone network
provide reliable
algorithm combines
widely held
technology transfer
anaphora resolution
\] uses
particular data
different natural
3 %).
free order
good candidates
n highest
international natural
normal text
1996 );
better position
two human
per document
utterance length
associated syntactic
grammatical system
disambiguation information
q \[
answers ).
sidner \[
among different
large memory
similar sets
quot ;>&
striking example
make mistakes
null 5
active edges
result shows
functional descriptions
values computed
4 4
strings like
actual distribution
control flow
role played
across word
corresponding chinese
would choose
many queries
speech labels
query results
better fit
cost effective
wrong choice
zero values
train models
1000 word
see l
variable number
constraints given
inference problem
physical targets
), especially
subj ),
module receives
alignment methods
actual work
main content
vast amounts
unification process
merge operation
tools used
length n
state transition
therefore likely
many occurrences
system may
sentence final
1997 )).
may exist
different speech
art speech
second method
unsupervised fashion
new strategies
two analyses
would either
category includes
high dimensional
synsets ),
kwok et
created manually
graphical interfaces
1994 ):
separate paper
two tools
test hypotheses
spoken words
xtag grammar
possible binary
algorithm continues
trec evaluations
account would
improving speech
rule notation
different semantic
active edge
start state
authors also
yorick wilks
example one
linear form
n feature
nonzero probability
extract text
better retrieval
phrase rules
parsing research
clauses must
input must
also important
single morpheme
another rule
transfer approach
different resources
already classified
different expressions
output strings
etzioni et
supervised approach
linear function
would construct
processed text
wrong translation
one found
right ),
unified approach
associated probability
3 --
structures rather
new file
last three
domain corpora
1 alignments
semantic categorization
analyzed text
important consequence
desired level
preceding word
become standard
general property
search problem
les deux
extra argument
1992 )),
hypothesis using
optimal path
dictionary contained
speech corpus
possible partial
combined models
6 %.
structure must
unary predicates
parser encounters
say anything
systems built
estimation error
complex process
yields better
separate list
low frequencies
achieve high
'. finally
paradigm shift
receive different
necessary since
generation task
set called
2000 ],
applied simultaneously
common syntactic
quantitative data
argument classification
la fonction
syntactic choice
therefore developed
positive effect
constituents may
early attempts
process described
using additional
new topics
100 words
well even
approach suffers
disambiguation rules
related meanings
one degree
several arguments
bigram features
(~ v
one verb
entity detection
successful completion
answer ).
noun followed
recognized correctly
relatively limited
complex domains
lexicon entry
lexical phenomena
general trend
larger data
extra work
nous nous
~: c
expressions refer
information specific
renewed interest
three training
certain values
(~ 1
low recall
(> f
search strategy
representation languages
driven generation
somewhat larger
relation among
held constant
proof procedure
related approaches
,..., cn
similar context
1994 ),
20 sentences
closure properties
provides additional
press ),
local constraints
becoming increasingly
concept associated
algorithm 3
planning component
rules corresponding
c --
sentence level
reasoning behind
may vary
good strategy
many instances
individual scores
word must
based user
important distinctions
certain classes
particular sub
output forms
first level
correlation values
machine translation
analysis results
3 introduces
systems aim
processing phases
little work
going project
etc .;
human raters
igure 3
possible attachments
framework based
alignment errors
actual performance
lfg parser
journal reference
conceptual classes
w ,)
ordinary english
also causes
subsequent sections
japanese system
fewer parameters
finer grained
travel domain
new way
internal argument
stochastic approach
phrase category
telephone numbers
current parse
verbs appearing
shall consider
corpus using
f [?]
unary rule
one daughter
many syntactic
every word
avoid problems
might attempt
transcribed text
x may
complex systems
since v
important property
many combinations
standard grammar
best explained
major constituent
different one
also reduces
derivation step
correct classifications
39 %.
perform disambiguation
type name
words --
paper investigates
type described
reference translation
wl ,...,
form representation
1 .)
0 7
making process
a1 a2
specific case
corresponding dependency
focal point
category b
main application
mental processes
xinhua newswire
2004a ),
functional components
may require
terminal nodes
still obtain
accuracy would
conceptual network
marginal probabilities
b ).
verb relation
human discourse
basic method
node \[
)) e
performed several
multiple annotators
would predict
would appear
last example
following basic
synchronous tree
different clusters
lexical probabilities
different evaluations
c ..
structures ),
3b ).
different cases
future system
., 10
partly based
languages --
many alternatives
go one
many false
given order
word definitions
). roughly
text portions
sending messages
one task
solution must
time consuming
process used
scores computed
show improvement
performance overall
determined automatically
start point
noisy input
steedman 1985
interesting problems
quite long
based directly
helps us
-- whether
feature name
measure score
terms related
similarity scores
following simplified
whole utterance
like data
people seem
third criterion
way described
n nodes
semantic annotations
1996a ).
obvious application
great help
could achieve
must denote
choosing one
language technology
would succeed
accuracy gains
c ,'
grammatical knowledge
closely associated
sentences long
multilingual natural
required ).
must extend
1955 ),
reasoning processes
produce text
see next
usually performed
matches one
takes much
pour la
english adjective
calculated based
could look
careful examination
). various
potential use
system demonstrates
significant effects
one less
input feature
underlying data
(: ril
'~ r
training procedure
b 1
holds true
always higher
second per
improve significantly
optimal feature
several linguistic
three features
algorithm selects
masculine plural
virtual memory
available annotated
may present
following syntactic
miles osborne
discourse information
rich information
target grammars
always exists
defined rules
semantic networks
), noun
correct target
probabilities based
first sense
overall context
speaker change
select features
pos information
thematic information
present method
implemented efficiently
syllable structure
one article
words related
simple part
search starts
concepts defined
sur l
works bottom
beyond simple
acquired rules
remaining errors
paper presented
quot ;..
.... ).
multilingual speech
less difficult
european languages
negra treebank
must believe
terms based
tested using
z ~,
.... \]
remaining five
output layer
probabilities directly
type used
level syntactic
automatically derived
usually difficult
current natural
domains ).
captures information
section examines
viterbi algorithm
considered correct
selectional restriction
(' r
also goes
recognition evaluation
tag sequences
many proper
tagging tasks
single translation
hold across
using certain
uniform probability
organized according
individual lexical
condition holds
different assumptions
additional evidence
sicstus prolog
per hour
requires much
multilingual resources
97 ),
ie system
2 may
sentences like
well designed
graphical display
less information
arbitrary size
specific named
acyclic graph
initial search
far short
later time
generally represented
appropriate part
one leaf
free texts
techniques could
major advantages
test 1
-~ 2
lexical meanings
considerably less
categorial grammars
nearly identical
greedy algorithm
general theory
linguistic surface
measure may
one head
typically one
system interface
whole language
\] 7
fundamental assumption
also responsible
.) one
may often
substituting one
explicit syntactic
vocabulary item
might increase
shows whether
semantic proximity
one unknown
feature used
false negative
like \[
maps every
words based
phrase ellipsis
following probability
learn rules
computed separately
cannot learn
several models
time delay
various constraints
sampling method
sphinx system
chapter 4
original parsing
., either
many mapping
relate two
speech act
onr grant
semantic specification
morpheme boundaries
seeking dialogues
design philosophy
relatively easy
morphologically complex
method calculates
extraneous words
effect would
similar ).
systems applied
identical results
also exhibit
initially used
verbs require
automatic procedure
performed significantly
scheme could
precisely defined
algorithm first
words directly
smt systems
linguistic resource
4 points
), adverb
languages based
simplifying assumptions
provide much
de traduction
first presented
utterance may
english compound
computational natural
would agree
category associated
distinct parts
simple smoothing
gives information
might account
semantic codes
approach outlined
system considers
;. null
object pairs
give detailed
individual rule
last name
unique identifiers
). research
successive sentences
stated earlier
tural language
problems mentioned
lagrange multipliers
address two
current evaluation
certain extent
detection task
rules take
parser uses
relationship among
reported elsewhere
descriptive content
whose part
time complexity
focused mainly
theory proposed
c1 ,...,
domain concepts
based summarization
12 words
), let
conventional systems
certain facts
verb construction
set must
different manner
73 %.
interpretation processes
compositional nature
line represents
research work
two key
entropy approach
~, j
main body
syntactic label
modeling task
supervised techniques
different shapes
certain types
input nodes
semantics \[
probability pr
words w1
kronecker delta
theory ),
verb together
f measure
dependency parsing
therefore useful
different retrieval
argument relations
white house
japanese translations
roles assigned
speech group
desired goal
cause difficulties
produce output
translation pair
already identified
modern statistical
much context
next lower
information collected
n states
works quite
language accepted
relative ease
several purposes
still left
computational resources
third international
theoretical perspective
different communicative
method produced
first experiments
general format
capital letter
also syntactic
appelt 1985
tests described
higher word
dans les
depends crucially
careful attention
... b
different patterns
correctly understood
acquire new
structure nodes
many solutions
used words
also includes
frame ),
error function
quality measure
lexical variants
algorithm results
right set
onaizan et
), use
following examples
hoc manner
rule given
also extracted
system supports
initial word
also given
thus allow
three subtasks
wi ),
high scores
study ).
among senses
vp ).
experimentally evaluated
specific target
control verbs
\[ 28
standard dictionary
nl ),
every input
), n
upon work
vp \]
hoc retrieval
substantially lower
three strategies
syntactically well
determined via
marker ),
versa ),
100 %.
among classes
general pattern
description ).
current one
grishman et
phrasal verbs
specific details
'. since
untagged corpus
paper differs
model improves
main sub
resulting description
collecting statistics
hirschman et
single chinese
e z
also provides
features also
must first
performance drops
parameters must
additional lexical
log file
grant n66001
identify one
-- also
annotation project
representation includes
immediately obvious
central role
); therefore
used without
hard problems
q c
two conflicting
also influences
always improve
defense advanced
order variations
primary purpose
algorithm tries
idf weighting
every document
particular question
new space
first goal
technical writing
language production
programming environment
select two
example 8
principled way
'.& quot
discourse level
type theory
particular time
sentences drawn
based heuristic
significant influence
using formula
take two
value p
oriented dialogues
systems employing
current interest
sentence like
belong together
many recent
syntactic dependency
lexical contexts
e able
text used
full version
tripartite structure
would remove
unique number
dramatically reduce
modified kneser
observations suggest
discourse trees
similar nouns
full grammar
state networks
short example
english glosses
2 demonstrates
pages 9
knowledge contained
previous sentence
correspond well
potentially important
representation provides
management succession
highest scores
unary rules
another category
speech assignment
accuracy levels
tag combinations
easy way
cases using
given texts
2 also
tags may
mean different
feature fi
heuristics described
simply replacing
strategy also
exclamation mark
still possible
tag given
tokens per
exponential explosion
valid semantic
scored using
standard first
useful linguistic
great part
well ),
pcfg rules
certain language
ordering among
tagging tool
richer set
textual form
also adds
using functional
virtual machine
categorization task
pattern may
large population
tests used
40 minutes
special purpose
2001 ].
relevant context
evaluation metrics
semantically close
5 also
best sentence
ie technology
following six
one summary
state hmm
a1 ).
incorrect parse
frequency less
one representing
driven parser
ungrammatical input
using relations
size 1
human dialogue
retrieval task
additional rule
first character
coherent way
randomly selected
automatic translation
application scenarios
approximately 250
relative quality
type 4
parsing community
longman group
two quite
building language
closest one
c /,
manually translated
), divided
available knowledge
specific categories
three days
difficult issues
take effect
also exploring
000 instances
partially supported
selection mechanism
mapping f
practical application
available machine
system still
crucial property
partial knowledge
problems --
quadratic time
\[ ....
value matrices
nlg system
following diagram
39 ).
two auxiliary
substantially improved
processing unit
thus resulting
task description
length 3
correct solution
thompson 1988
rule b
base model
ungrammatical constructions
approach cannot
position j
first layer
character types
often include
represents one
predictive value
useful technique
possible ambiguities
). therefore
experiments based
1992a ).
candidate set
three consecutive
steps 1
relaxation techniques
f .,
base form
show similar
parsing stage
initial experiment
test shows
adds information
... f
bos et
le r
less pronounced
failure occurs
derived word
systems ).
general constraint
two type
related text
calculating similarity
must choose
scoring algorithm
tagging algorithm
... r
prediction models
one transition
developed several
() f
another variable
relative pronoun
graphical form
last condition
every kind
automatically creating
many arguments
text fields
c ~(
parser accuracy
general principles
several problems
). every
simple feature
whose definitions
programming approach
.) finally
based metric
inside probability
1 indicating
four levels
usually considered
remaining text
\] system
\[ 1997
two step
make correct
data description
heuristics used
context rather
associated words
component may
one described
combine various
must either
result using
rules presented
based anaphora
., etc
past five
commercial product
il \]
1988 );
word takes
bresnan 1982
following actions
focus ),
annotations provided
linguistic variation
system compares
using real
condition may
seem surprising
special processing
coreference task
sgml format
current position
syntactic methods
hirst 1991
speech information
two times
many japanese
lexical preference
two candidates
rule compiler
pour l
1 case
language may
yangarber et
n \].
system processing
communicative intentions
parameter set
without modifying
makes sense
model since
modeling problem
level tree
phrase heads
operation called
h ,,
problems related
model generated
tag b
term occurs
acoustic data
analysis revealed
retrieval engines
classifier using
could therefore
generally difficult
new evaluation
positive instance
relations ).
applying different
acquire knowledge
correct sequence
incorporate knowledge
reference data
use knowledge
satisfactory solution
governing category
application context
effective search
class x
based multi
three orders
approximately 60
\[ li
many links
l \['
combining knowledge
flight information
translation services
achieve comparable
use machine
resolution problem
classification schemes
generalization process
computational mechanism
statistical systems
existing mt
similarity information
important constraint
information access
perhaps one
following derivation
regular expressions
type discussed
bayesian classifiers
common linguistic
treebank data
3 ')
new instance
nlp system
thank dr
individual items
trec corpus
segmentation performance
english texts
interpretation must
\]. thus
semantic functions
research needs
also depends
data contained
specific phenomena
x k
file formats
introduce three
80 %,
adverbial clause
text theory
deterministic automata
provide adequate
decision tree
differences exist
potential usefulness
phrase could
1977 \].
mean ?&
also intended
printed form
contain two
major sources
general nlp
control constructions
l ....
search term
event descriptions
n .,
helps reduce
different inferences
words except
u ,~
individual rules
generative grammar
performs reasonably
system relies
word groups
united states
phrase following
suffix ).
13 ],
word segmenter
word plus
added without
np arguments
either side
could create
~, c
resolution strategy
\[ 1991
multiple inheritance
list used
parse without
etc .&
decision list
almost completely
extraction system
one used
verb eat
necessarily mean
procedure produces
value decomposition
based unification
different classification
would operate
initial input
errors involving
distance features
vector model
second evaluation
two example
xu et
document types
96 ),
efficient parser
view point
values based
identify patterns
morphological features
special words
linguistic formalism
disambiguation procedure
corpus includes
different lines
common technique
;, although
higher average
automatically selected
context ),
corpus given
rule r
highly effective
one year
c ')
typical situation
serious problem
~, q
systems produce
threshold value
theory makes
like hpsg
data consisting
rule associated
statistical method
discourse modeling
9 0
object whose
also associated
improves upon
exact word
joint research
many positive
numbered 1
reliable estimates
., 2003b
qui ont
partial parsing
problem might
eduard hovy
stated explicitly
precisely specified
lexieal entry
semantic rather
value based
known techniques
syntactic errors
software available
referential information
currently extending
corpus provides
slightly greater
definite np
computed directly
information beyond
200 documents
new structure
tile last
asian languages
words involved
reference \[
lexical resource
performance evaluation
separate grammars
must appear
possibly empty
). besides
called lexical
online text
formal linguistic
parser analyzes
following processing
single data
verbal head
phenomena observed
additional information
representation r
often given
four sentences
9 ):
extract sentences
system without
focus stack
ne types
k denotes
system models
acquisition methods
construct two
contiguous strings
left one
correctly analyze
must understand
representation associated
global optimal
perform syntactic
c .~
possible combination
following observation
need another
global maximum
information gain
notions like
account information
grammatical categories
hierarchical discourse
la question
alignment algorithms
prediction model
research centre
better performance
different readings
\[ kasper
carpenter 1992
italian language
hierarchically ordered
., 1986
strong indication
feature structures
automatically acquire
phrases may
probability computation
large syntactic
like c
main question
1999b ).
methods proposed
important points
possible since
corresponding text
noun relations
prolog program
parsing failures
possible ones
may possibly
84 ).
space models
long vowels
using separate
feature constraint
output label
annotated data
configuration file
selection among
84 \]
commonly employed
high accuracy
text corpora
also outperforms
precision would
algorithm shown
comparable corpus
similar issues
figure 2b
top node
second one
appelt et
given natural
grammar theory
p [?]
code ).
work may
scor e
textual context
parsed texts
every object
explicit links
final set
conditions described
probabilities estimated
structure analyses
warren 1980
plan would
labeled using
doubtful whether
expansion techniques
introduced earlier
different corpus
8 million
possible range
3 .)
following linguistic
invaluable help
immediately follows
keyword matching
executive officer
detailed comparison
incorrect one
contains syntactic
different locations
6 concludes
chinese language
complex natural
june 1990
different schemes
every system
individual trees
1989 ).
almost twice
systems rely
basic semantic
much slower
multiple languages
also pointed
system outperformed
search result
consider three
1989 \]
1 9
25 ):
schank et
one condition
classical approaches
formal models
defined later
pragmatic constraints
names appear
basic expressions
given speech
user modelling
difficult due
designed specifically
good result
containing w
bahl et
e w
null ber
distinct senses
es ),
constructed two
words take
generic system
people tend
cluster c
also much
9 ),
automatically annotated
probabilistic version
given semantic
following words
documents relevant
specific meanings
results presented
., verb
;, see
annotations produced
minimal human
parsing information
manually created
hasan 1976
central problem
3 system
). lexical
simple learning
lingual corpus
biomedical information
final punctuation
nlp techniques
small texts
evaluation function
therefore used
cannot contain
text segmentation
user feedback
additional rules
grammar defined
phrase translation
1 may
figure 3a
lower threshold
since new
written chinese
several authors
subcategorization constraints
making changes
darpa community
c .)
occurs exactly
2 compares
explicit linguistic
complex semantic
anaphoric references
better idea
long ).
preferred one
four test
.~ r
verbs allow
synchronous grammars
preceding discussion
learning environment
feature weight
collaborative research
table lists
possibly several
readable form
language phenomena
complex expressions
would hope
may denote
w \[
initial investigation
following models
domain qa
x l
types occurring
good answer
concept definitions
5 compares
significantly greater
flat structure
output probabilities
system receives
also recognized
separate feature
using weighted
2003 )).
one theory
certain part
new position
selection criteria
abductive inference
quot ;&
meaning representations
language engineering
scores based
specific purpose
words .)
state automata
primary function
knowledge representation
six major
training data
;, thus
gpsg \[
per week
model currently
must agree
main contributions
34 ),
full results
25 ),
operation must
show better
precise enough
always better
specific entity
empirically tested
source phrase
column contains
coordinate constructions
let u
depend upon
existential quantifier
also affect
possible argument
wide range
particularly problematic
major factors
less sophisticated
time increases
semantic lexicon
direct translation
test conditions
linguistic devices
face value
syntactic phrases
mean values
process produces
presented three
tile notion
lower perplexity
brief example
model parameters
two dialogue
major topics
). rule
1995 );
quantified nps
~, p
h may
default inheritance
possible matches
writer may
objects like
produce sentences
automatic comparison
model performance
extracting bilingual
different frameworks
adequately capture
common form
actions performed
training classifiers
highest information
similar approach
classi ed
basic one
text structures
argument category
resulting representation
consider example
using exactly
dialogue participants
three tokens
structure consisting
words linked
optical character
grant n00014
primary aim
.) since
much smaller
document contains
preliminary experimental
relative merits
parsing rules
options available
,~ j
different number
together using
rules obtained
system built
thus two
), 3
generally considered
1986 \]).
temporal interval
computation cost
accuracy metric
algorithm found
space model
two paragraphs
less probable
one defined
arbitrary string
possible relations
general heuristics
place ),
grammar defines
driven algorithm
include discourse
f1 ,...,
weighted finite
object belongs
entity ),
ci ),
[?] e
using non
additional level
journal section
broad classes
model serves
resulting analysis
figure also
null let
statistically insignificant
structure developed
knowledge bases
section provides
social research
open question
clir ).
please see
]. since
paper also
practically useful
primitive semantic
rather different
decisions taken
translation among
complex one
assigning weights
view taken
simple tasks
semantic restriction
mental states
native chinese
straightforward task
reliable indicators
work demonstrates
tile second
th position
main objectives
module performs
contains examples
simplest way
automatically acquired
18 months
interested reader
larger corpus
least 0
semantically distinct
often depends
two files
english speech
nl systems
already contains
overall success
learning approach
translation produced
also expect
work also
figure h
stage consists
evaluation sets
automatic data
synthesis system
guiding principle
specific processing
two fundamental
without significantly
language interfaces
automatic methods
probabilistic reasoning
present one
common problems
substitution rules
also studied
english semantic
make sense
fall outside
complex cases
2 l
considerable success
processes must
scientific texts
particular user
annotated according
stores information
irrelevant information
regular patterns
formal model
multiple occurrences
providing feedback
output formats
score computed
analysis provided
7th sigdial
trees containing
easily available
english expressions
complete utterances
specific contexts
notation introduced
000 japanese
good estimate
representations using
questions involving
salient information
system scored
definition 3
somewhat difficult
processing component
put together
reasonable number
5 l
also presents
different predicates
document representations
must possess
., discourse
less detailed
encyclopedic knowledge
paper represents
number indicates
reduced set
since using
400 sentences
design features
;. 4
ron kaplan
give good
traditional rule
simple measure
base ).
algorithm checks
best f
probable tag
always selected
basic assumptions
partial syntactic
1983 ):
poster sessions
previous paper
positive side
whose states
techniques \[
could increase
features computed
preference semantics
constructed automatically
recent linguistic
1 demonstrates
1975 \],
loosely related
line corresponds
group includes
weischedel et
precision scores
appears within
falls short
comparing system
several modules
neural network
novel method
tagged texts
promising approach
one criterion
thus necessary
4 3
romance languages
type n
future extensions
many problems
idiomatic phrases
following model
vocabulary consists
negative feedback
context set
main ideas
interesting issues
basic mechanism
brown university
based framework
one best
specific domain
formed substring
agreement feature
syntactic composition
one segmentation
following form
press et
independent words
initial system
data representation
parsing trees
varies significantly
alignment based
specific attributes
produce correct
construction ).
previous utterance
c ++,
newswire data
following function
conducted using
syntactic constituency
n2 ).
penman system
three potential
could account
basic intuition
algorithms exist
1 h
never occurred
12 ).
prolog predicate
specific words
figure la
level model
high level
character encoding
actual process
position p
dialogue structures
parser presented
feature paths
rapidly changing
sometimes causes
distinct types
particular lexical
constraints within
35 words
original japanese
various categories
12 \]
domain reasoning
affects performance
sole purpose
promising direction
described elsewhere
sense tagged
case must
strong preference
obtain high
descriptive text
art performance
,( x
different rule
fixed window
research grant
best tree
observations concerning
languages differ
equal length
may first
interactive system
accurate analysis
directed graph
multiple instances
', 1
widely agreed
accuracy level
expert systems
parse fails
data analysis
problems involved
open research
logical system
automatic scoring
perform actions
complete description
sample entries
programming languages
english one
node raising
incomplete data
might try
used four
different possibilities
unique terms
3 thus
c knowledge
recognition vocabulary
using terms
labelled training
explicit model
sense distinctions
sequential order
good use
containing syntactic
using combined
utmost importance
recent models
entity tag
fillmore et
said nothing
best list
1983 ),
also measured
sequence x
central concern
ranking process
used ).
explicit form
take one
third set
literary texts
mainly consists
multiple syntactic
good reasons
handle unknown
one right
story understanding
took less
allow many
much success
spoken document
). information
extracting features
artificial neural
string grammar
still get
use syntactic
events based
fernando pereira
manually marked
v ',
particular sentences
widely recognized
following problems
stock exchange
highest level
using dictionaries
independent clauses
applications using
atr interpreting
quite good
adequately describe
names found
hereafter referred
using syntax
using naive
gathering information
association scores
), first
test using
discussion ).
generated sentence
closed track
world semantics
answer passages
n times
training pairs
n candidates
par une
called multi
born ?&
learning module
set contained
experimental procedure
two symbols
sense used
john smith
user already
first solution
expected count
appropriate description
indefinite reference
results section
), 10
lessons learned
understanding requires
tile set
root ),
)- gram
constituent units
internal punctuation
text often
travel expression
4 features
could lead
always provide
get one
rather straightforward
using grammatical
cannot form
another translation
4 %,
correct pos
efficient approach
\[ 26
reasonably large
rules cover
many grammars
given tile
mikheev et
fairly close
\] 9
includes two
frequency range
named entities
dependency links
two specific
generator may
two baseline
feature extraction
annotation tasks
smaller sets
manually developed
syntactic coverage
smoothing techniques
g x
main cause
atis speech
less relevant
distinct language
language processing
linguistic input
view ).
values across
single question
f }.
interesting approach
sufficient condition
related verbs
tj ),
pattern match
patterns whose
research suggests
negative polarity
currently contains
certain feature
among multiple
truth conditions
set consisting
relation appears
grammatical processing
average rate
leftmost child
\] may
would prove
three articles
complex patterns
lexical acquisition
also model
diathesis alternations
hearer knows
deep structure
share common
unknown word
\[ 27
text unit
following sets
vector v
could determine
particular configuration
algorithms require
contains much
minor variations
may choose
follow one
great impact
final note
situation would
improved speech
novel way
root lexicon
groups using
scoring sentence
principled manner
7 ))
linguistic constructions
independently developed
user manual
manual evaluation
geared towards
spanish text
). rules
annotators marked
last n
results achieved
production rule
two drawbacks
one wanted
relations across
h la
data problem
sound rules
analysis produced
whose description
fluent english
new template
forward direction
,~ c
research laboratory
direct application
current work
tremendous amount
intonational phrasing
complete algorithm
also expressed
last clause
particular test
words included
particular aspects
input representation
location within
patterns would
statistical parsers
without waiting
traditional information
support verbs
e b
examine several
determine whether
therefore seems
primarily used
case feature
ranking method
general version
component words
systems typically
et en
score calculated
general grammar
middle column
n 5
source text
identify relations
methods could
may construct
research projects
separate tasks
vocabulary speech
evaluate several
grammars written
currently use
first iteration
abstracts ),
..... 1
work required
performance improves
pos taggers
remaining sentences
normalized version
science department
different lengths
candidate parse
located within
structure representation
initial knowledge
recognition performance
2006 conference
statistical natural
evaluated two
generally applicable
second np
n different
next page
;$& quot
), either
boundaries based
earlier system
e x
1 billion
additional noise
relations provide
learning using
always clear
), namely
noun category
two term
direct comparison
linear precedence
processing technologies
form contains
words indicating
structure onto
., e
multiple examples
tile process
1997 \].
rules also
approach treats
finite subset
would accept
also decreases
errors found
represented within
grammatical words
based upon
requires extensive
r (,
constraints ).
4 ],
could ask
evaluation ).
use examples
lexical chain
two phases
phase ii
parallel translations
computational work
knowledge used
system employing
frequency distribution
based named
decreasing frequency
better job
large positive
two disjoint
alignment accuracy
major changes
n }.
structure analysis
underlying lexical
likes mary
require semantic
(' 1
interactive machine
important difference
elements ).
method might
positive ones
., 2002a
correctly classify
good initial
useful clues
data including
language without
summary contains
text might
currently experimenting
political entities
may shift
two complex
language l
94 %.
contain one
san jose
annotation tools
lexical word
use dynamic
words alone
dependency models
becomes available
passive edges
case marker
text alignment
pruning techniques
pos tagger
four characters
using english
following context
difference metric
description using
temporal dimension
new ones
following hypotheses
1987 ).
different weighting
probabilities obtained
model proposed
core arguments
first vowel
stochastic tree
long enough
semantic domains
combined approach
1987 \]
two consequences
added ).
text extraction
linear interpolation
explore two
change significantly
chapter 3
items ).
training time
first extract
quite short
simply adds
united nations
systems already
parse ).
disambiguation methods
operation takes
time used
expressions involving
first phrase
significant reductions
phrase penalty
lingual information
also incorporated
current performance
certain nodes
corpora based
et des
t2 ,...,
like rule
different speakers
could generate
probability measure
model ),
curly braces
level context
possibly different
word analysis
obtained similar
wordnet synonyms
), together
estimated according
produced output
used simultaneously
part may
morphological alternations
exists one
slot ).
linguistic investigation
adjective ).
p -~
ne tags
local contexts
exhibit different
algorithm iteratively
multiple solutions
bootstrapping algorithm
approach leads
knowledge stored
language engine
main areas
tuple g
tools available
scale experiments
one often
compositional semantic
scores across
first assume
within computational
detailed explanation
handled separately
different items
clarke et
possible non
temporal adverbial
richer semantic
still based
syntactic labels
without including
corresponds directly
12 shows
translation work
lexical statistics
constraints derived
symbol ).
subject would
path may
authors gratefully
sgml tags
positive integers
important component
word candidates
certain characteristics
rather specific
surface realization
e f
words obtained
formally define
answer queries
response would
process continues
also constructed
wider class
across categories
zellig harris
striking difference
sense 4
different entries
general overview
speaker wishes
processing components
also attempted
1 compares
lambda abstraction
sense defined
system ).
possibly including
,..., w
within 2
different rules
fairly natural
axis shows
main limitation
building large
term x
even semantic
advanced learner
... cn
numbers 3
right one
subsequent research
using conditional
algorithm calculates
windows nt
remaining input
involves taking
0 },
several issues
five years
3 9
might perform
achieved without
different usages
two principles
f u
research agenda
would show
local optimum
one session
one unit
tile method
basic steps
similar situation
partial derivation
date expressions
strings whose
r ~)
orthographic forms
grammar might
original model
new collegiate
briefly review
recorded data
context dependent
simple bottom
automatic question
output text
\] h
table summarizes
similar languages
quite strong
modal verbs
test sentence
performance since
already begun
introduction one
language researchers
column labeled
model appears
corresponding sentences
class must
correction mechanism
japanese words
). intuitively
given np
english side
17 \].
), inter
). likewise
answers given
different situations
text sample
html files
alternative set
unstructured text
rule applied
computational morphology
quot ;~&
also applied
sls ).
electronic documents
substantial effort
new vocabulary
positive number
figure l
specific sets
avoid using
3 may
four experiments
1957 ).
phrases without
system 1
language users
describes three
,~ p
systems using
vectors x
appear ),
least frequent
first classifier
still others
complex inferences
differs significantly
relation set
significant words
location ).
classification scheme
complex np
system modules
quantitative information
also within
new argument
order language
constituents ).
flexible framework
language resource
relative prominence
simply put
one clause
shallow text
clear example
logically equivalent
acquiring new
approximately 100
special word
work without
already seen
perfect match
authors report
times smaller
reference corpus
select among
human effort
combine information
spelling variations
\[ 0
labeled x
efficient techniques
low scores
tagged according
probabilities may
inference component
research systems
incorrect answers
actual texts
desired state
10 years
3 details
ongoing project
impossible task
two formalisms
elementary discourse
treebank corpus
better capture
via unification
data files
possible ),
final parse
rule applies
three additional
translation parameters
eight years
one reading
algorithm produced
class definitions
also applies
typing errors
intelligent systems
model offers
terms may
different category
big enough
dependency model
new work
8 \])
every syntactic
1 words
like subject
translation performance
general features
f1 scores
document collections
special class
sequence contains
keep track
may infer
categories rather
c may
figure 15
evidence supporting
dialogue history
syntactic evidence
following four
length less
new relations
general dictionary
parsing procedure
database containing
hundred words
could express
least partly
varying numbers
simplest one
section presents
word occurring
grammatical form
results also
two points
\] la
6 5
structure including
open agent
x ).
bad results
constant across
unclear cases
framenet data
common nouns
various processing
one might
second argument
use k
grammatical function
x \]
dave et
best interpretation
user population
see discussion
predictions made
wsd algorithm
linguistic constraints
also recognize
two noun
making inferences
1 l
-> np
argument order
surrounding words
dependency analyzer
production rules
vp node
directly address
conveys information
simplest cases
6 different
words may
system employed
entire class
various contexts
type 3
), containing
first state
large percentage
full potential
journal data
training would
would exist
), etc
acoustic scores
great advantage
phrase pairs
linguistic system
kleene star
given corpus
help reduce
svm model
lexicalized tags
third way
c ~-
pradhan et
available linguistic
useful source
adj noun
x v
dictionaries used
extracted features
several special
des langues
features --
progress towards
five subjects
segmentation process
relatively frequent
tense ),
equal probability
standard chart
10 \].
explicitly indicates
segmentation could
languages may
john saw
morphological structure
syntactic levels
texts including
constituent order
like information
rules required
easily shown
problem concerning
global search
gap dependencies
including e
productive morphological
two logical
three criteria
correctly interpret
higher score
modi ed
x 1
hierarchical structures
e de
particular level
classes described
l (,
focus exclusively
complete constituents
provide insights
important features
sufficiently large
certain tasks
,+ l
following modules
baldwin et
complete understanding
chinese names
simple representation
sets whose
former uses
link analysis
h ,.
published research
many nouns
possible grammatical
atomic formulae
information value
le 1
corresponding correct
insufficient training
15 sentences
length 6
knowledge organization
semantic effect
normal case
techniques would
simple recursive
probabilistic inference
found several
fill slots
ways ambiguous
stated within
information together
general relation
corp .,
english np
different programming
questions arise
); otherwise
full structure
theory \[
also contribute
case 2
term list
1998 ))
one constituent
clustering process
connected via
structural constraints
dictionary contains
class feature
automatically extracted
various attributes
examples considered
l 0
new parameters
let n
final product
account whether
academic research
simple type
role labeling
various words
150 sentences
dot product
one significant
right manner
lexicalized grammar
six languages
first computed
partially described
windows 95
declarative sentence
tree generated
simple analysis
less data
categories ),
contexts like
word window
input semantics
missing arguments
approach must
issues remain
avoid repetition
accuracy obtained
distribution among
deterministic shift
several conditions
fairly simple
without making
optimal threshold
barton et
scheme would
(~ b
contiguous sequence
learned rules
dependencies may
wi \[
segmentation systems
partially ordered
overall sentence
number 4
test instances
good overview
new developments
l ):
considerably longer
first alternative
among nodes
3 demonstrates
performing system
building blocks
implemented based
user queries
explicitly provided
p ~,
equation 5
sample selection
terminal symbols
important element
window centered
features related
oriented domains
., 4
j 1
evidence comes
possible tag
dictionary access
conversational participants
task specific
2 \]
last component
paris 7
air traffic
free rule
taking place
basic vocabulary
existing grammars
best published
head information
processing requirements
features similar
greater coverage
one sample
basic principle
final semantic
assign part
measure ).
important first
produce translations
rhetorical relations
relative reduction
point within
constraint language
starting points
best performing
). like
null hypothesis
use linguistic
general results
restrictions imposed
sound like
correct word
\[ fillmore
air force
score based
model makes
h (:
combining linguistic
later experiments
occurring discourse
cky parser
like rules
adjoining grammar
ambiguity packing
classes used
much different
2 introduces
process first
important relation
input files
n symbols
hybrid model
speech tagger
accuracy achieved
fairly stable
error occurs
1996 \],
particular sense
automated systems
distinctly different
process involving
tim system
use domain
may interact
acquiring knowledge
another good
.) consider
sentences instead
prenominal modifier
share one
volume 28
less good
final evaluation
would require
quot ;!&
always chooses
takes advantage
first run
summarization process
easily implemented
many tasks
computation time
sentences whose
meaning components
pragmatic phenomena
conversational implicatures
value indicating
;~ j
class names
language generator
unambiguous words
(; n
utterance ).
sentence text
harmonic mean
use words
also examined
given number
coarse grained
hidden structure
segmentation method
term vector
language dialog
1 point
features based
-- based
whose constituents
p ~'
target sentence
path connecting
common source
greater improvement
first text
characters used
21 \],
alternative method
,. e
l ~)
original context
contain additional
interesting phenomenon
horn clause
gundel et
text string
system shows
new idea
application using
two reasons
first ten
person might
speech label
basic relations
describe results
single user
translationally equivalent
experimental comparison
l ),
hindle 1990
functional structures
dependency relationships
relevant passages
inductive logic
quot ;~'
1996b ),
lexical properties
2 word
following paragraph
last 3
fruitful discussions
roughly one
using co
expected information
considered complete
rule 1
better generalization
two representations
kamp 1981
edge represents
best path
c h
requires special
need two
arbitrary decisions
(: ts
first line
clustering method
kernel methods
psycholinguistic studies
b b
feature descriptions
iterative training
important results
perform poorly
core part
lfg f
gives good
always include
many utterances
trees produced
5 word
could select
developing systems
automated method
strategy based
even larger
1975 ),
recall figure
right side
using recall
chunk parser
separate development
phrasal categories
expressions containing
formal expression
may go
canonical order
evaluate different
possible english
choice would
1990 \]).
head rules
less salient
table would
generally occur
1 corresponds
word cannot
generated answer
process results
another possible
crucial component
temporal interpretation
summarization systems
large grammars
extract named
investigated using
interlingual representation
ill l
tipster architecture
models give
previous phase
assign weights
cantly better
without specific
parser also
figure 12
count c
thus propose
least two
35 ).
final one
cannot produce
si l
usually possible
data sparsity
estimated separately
problems encountered
rules match
additional cost
increasing amounts
model selection
possible objects
contained one
menlo park
model attempts
1 )-
often taken
various researchers
shown later
user actually
(~ f
many years
important components
subject areas
prague school
), verb
stochastic model
one gets
two heuristics
provide several
obtains better
also manually
dependency relationship
significant loss
(~ r
consistent way
equivalence relation
simple strategy
modified form
class assigned
specific ontology
labels ),
time constraint
rules state
text available
set including
complex predicates
system performs
4 seconds
significant decrease
value obtained
theoretical analysis
better method
single lexical
following expression
syntactic configuration
actual speech
several proposals
normalized value
generated language
merged together
.) second
achieved higher
actually generated
problems include
naturally expressed
higher probability
desired output
g ~.
support system
event e
remaining columns
information describing
among computational
synonym dictionary
includes one
three letters
without violating
maximize p
actual semantic
). adding
multiple entities
little data
make many
could try
user provides
bnc corpus
one version
column 2
gave mary
induced using
initial probability
feature specifications
corresponds closely
model could
network represents
pattern acquisition
independent word
event described
since f
absolute value
must accept
known technique
less serious
store information
possible set
large context
following shows
parsed corpus
classification could
chronological order
manually tagged
informs us
null computational
errors produced
28 ),
partial function
us suppose
newly generated
specific word
text written
sentences get
segments containing
entirely independent
main conclusions
given rule
input symbols
). wordnet
complex interaction
like pronouns
spoken languages
different definition
old version
summaries produced
). section
based approaches
single sentences
<: e
active research
a2 ).
give evidence
text features
one extreme
,..., x
completely automated
string containing
2004b ),
sentence instead
different meaning
4 explains
human operator
null 2
less powerful
pages ).
1999 )),
greatly increases
several well
approximately two
problem space
sentence whose
goal would
may operate
estimate parameters
original annotation
new functions
frequency ),
selected feature
documents whose
special properties
specific class
evaluation approach
random number
graceful degradation
find several
early stage
procedure works
complete sentence
special kind
single group
many issues
better recall
wsd using
evaluation requires
case marked
best feature
international workshop
contain certain
efficiency gains
particular goal
spelling checker
patterns corresponding
example could
accessible via
relevant sentence
bilingual text
lexical elements
dolan et
), depending
similar considerations
e muc
top part
encouraging result
computational terms
semantic parse
edr japanese
preference heuristics
discuss various
could call
small corpus
maintaining consistency
clause types
complex nominal
\] l
next action
three best
analysis needs
program ),
standard theory
problems may
heavily used
depth 1
usually short
anaphoric relations
phrase whose
sequence must
e ~.
memory required
terrorist domain
nominal arguments
might assume
two initial
base using
four weeks
l (~
recently proposed
also rely
sibling nodes
would rather
tensed clauses
potential problem
revision process
better overall
values may
already done
null ated
two papers
encodes information
using statistical
many high
separate step
many subjects
redundant features
based phrase
data types
individual classifiers
20 words
correct input
prosodic boundaries
always given
), null
would compute
compute semantic
restricted domain
word senses
language acquisition
somewhat misleading
ancestor nodes
reliable source
1000 words
one query
three minutes
alvey natural
human written
still missing
translation ),
2004 ].
component ).
necessary data
expressions found
particular character
frequency distributions
national science
linear combinations
new non
based program
lexical system
4b ).
word classes
current tag
statistical classification
transfer phase
value may
bot h
word specific
'. note
one introduced
since much
appear near
morphological ambiguities
performed slightly
phrase would
document id
using automatically
jelinek et
one training
ad hoe
common approach
must show
sentences --
par la
words separated
also define
one scenario
provide effective
among human
negative instances
unigram features
single test
row ).
components must
training stage
testing corpora
), head
noun co
protein interactions
), discourse
different sections
language ).
output devices
phrases (&
., inc
first mentioned
crafted grammars
free rules
model gives
different ways
arrival times
internal semantic
modifiers like
enables one
comprises three
information found
semantic parsers
present account
speakers tend
would seem
input pattern
experiments also
), 6
subsequently used
purely formal
whose root
sections 5
methods \[
since japanese
cases must
word processor
reference information
bies et
based metrics
set perplexity
corresponding sense
certain role
given segment
order must
stochastic grammars
feature value
pp headed
system builders
possible sources
psycholinguistic data
;~ c
auxiliary tree
rule shows
apply equally
top performing
sets based
requires manual
c k
various sets
lemma 3
three parameters
annotated test
preceding words
given subject
propose methods
godfrey et
conceptual role
also give
exact details
singular noun
programming technique
translation lexicon
training sentences
l ';
3a ).
aligned source
possible applications
boosting algorithm
represent words
major steps
currently supported
3 compares
used techniques
proper interpretation
single correct
processing task
output based
prior context
taylor et
original parser
general inference
pragmatic processing
functions ),
every concept
queries using
sentence p
short overview
dependent clause
target classes
shares many
diagram shows
three documents
uniquely identified
13 \],
string pair
p u
1b ).
seneff et
two intervals
vast majority
two tags
single reference
matrix verb
manual annotations
human computer
french ),
next task
question ),
example gives
learning curves
tile semantic
become popular
-- 7
), regardless
using acoustic
classifiers based
roman alphabet
case role
watanabe et
already known
different modalities
30 years
negative correlation
con (:
based classifier
data points
parent node
surrounding context
used machine
correct candidate
certain key
singular present
principles governing
relatively common
automatic lexical
un texte
backward application
domain structure
6 x
language word
statistical corpus
conventional wisdom
somehow related
suppose also
formal approach
relations extracted
linguistic markers
definition 6
major cities
domain independent
relations given
may force
1 (&
various relationships
every domain
higher proportion
speech errors
features except
arabic data
research aims
discussion see
would consider
long sequences
word term
al language
fully adequate
mapping algorithm
understanding conference
4 ].
later date
experimental setting
given information
stochastic context
high probabilities
another question
test bed
available within
semantic resources
phrase labels
semantic expressions
classi er
appropriate intonation
another time
standard hmm
word lattices
several languages
analysis rather
obtain reliable
lexical disambiguation
less time
develop systems
identify objects
treebank wall
significantly improves
raw frequencies
unambiguously tagged
present participle
still maintaining
4 6
text file
dependency parser
eugene charniak
right fashion
4 indicates
total weight
possible order
french lexicon
10 times
show examples
two highly
manual work
lexicon must
must exist
semantic evaluation
using 20
forces one
2000 ):
\[ mann
biggest problem
five parts
left ),
new terms
within three
categories could
second category
significant drop
query formulation
right answers
tagged corpus
possible method
sentences based
third one
following approach
uses semantic
words contain
sentence belongs
2001 );
partial hypotheses
comparison across
scores may
three quarters
3 words
phrases ).
would enable
sliding window
gao et
least 20
jean carletta
subsequent reference
intermediate nodes
columns correspond
information science
new lexicon
strings corresponding
accurate estimate
q ',
precedence constraints
~. thus
low level
object case
~( l
function --
system due
siemens ag
translation units
-~ n
languages contain
three things
different choice
japanese ).
000 utterances
clause contains
specific document
f n
informal experiments
expressed via
first search
user utterance
venture domain
often produce
adding words
sentence --
next three
often employed
dotted line
et une
human beings
new notion
exchanging information
overall f
next step
state space
three separate
accusative object
3 l
conservative approach
without committing
4 respectively
deictic expressions
information useful
already contained
earlier examples
definitions may
structures generated
two component
similar arguments
recently received
possible co
like character
(: al
five different
new grammatical
permet de
alternative parse
significant ).
free variable
representation issues
el ...
term weights
order feature
lexical knowledge
parsing methods
standard ir
data contains
difference may
occur immediately
even harder
right argument
word cluster
compositional fashion
knowledge extraction
parsers use
generation strategies
also high
2 ')
whole problem
partir de
conditional probabilities
switchboard data
occur within
following conditions
consisting entirely
fluent speech
nsf itr
semantic one
syntactic level
different translation
use within
work related
2000 ),
lexical stress
type shown
system already
significant error
system able
type inference
given token
constraints using
side effect
), representing
separate window
strongly associated
line system
main goal
restricted form
two measures
mutual understanding
constraint must
representational system
c ,.
improving translation
explicitly defined
tree fragment
corresponding translations
compatible semantic
automatic summarization
closely aligned
strong enough
syntactic chunks
word types
evaluation techniques
analysis ),
1992 9
5 ')
easily portable
also varies
terms without
individual phrases
structured set
semantic clusters
1 ).
100 instances
retrieval component
10 %)
use context
particular frame
1 \]
linear discriminant
symbolic knowledge
term must
c l
reasonable choice
would allow
would suggest
another research
data access
possible positions
dialogue strategies
approximately 20
automatic assignment
6 types
conceptual representation
already established
japanese word
towards developing
dumais et
final consonant
practical implications
value represents
horizontal line
questions used
new tag
raw material
ary branching
manually evaluated
lexical inheritance
program makes
perfect word
two collections
multiple roles
timit corpus
without morphological
simple baseline
lingual translation
often lack
along similar
information request
selection process
nodes n1
atomic formulas
10 different
becomes less
..., n
nlp problems
marcus \[
\[& quot
practical level
recognition applications
without invoking
complex situations
treebank trees
certain type
following new
words match
makes heavy
original source
common ancestor
appropriate values
traffic information
using natural
candidate answers
federal ministry
significant word
disjoint subsets
niessen et
), e
order markov
words without
basic category
appropriate senses
wordnet sense
already developed
speaker could
function word
paper uses
found elsewhere
properties may
improved performance
temporal focus
small increase
name recognition
mr .&
conceptual analysis
77 %,
lexical gaps
specific enough
quot ;-~
specific properties
parallel processing
chosen among
next example
rule allows
rule writer
output stream
recognition systems
specific kind
share certain
), described
id =&
also allows
two top
phonological level
may belong
scientific community
three functions
ie ).
independent variables
given category
next problem
languages without
individual speaker
gene names
semantic transfer
1988 \],
statistics derived
first word
third position
yield good
model made
never considered
see examples
svm light
using speech
management system
1 word
template filling
lexical hierarchy
small fixed
confidence interval
english speakers
x z
without access
parsing errors
particular target
provide different
cannot provide
row shows
sentence based
c /.
traditional chinese
unique semantic
indirect evidence
86 %,
using semantic
final classification
case analysis
complex category
smoothing technique
documents based
., 1993
file system
system using
still holds
days later
special techniques
linguistic categories
le module
many variations
probability threshold
produce many
length distribution
declarative language
vector w
existing database
every model
modi cations
without use
word inside
och et
research interest
intermediate states
basic terms
feature space
one file
briefly reviewed
also true
space using
lexical ambiguity
single level
nominal form
current applications
37 %.
whose values
understanding capabilities
objects whose
robust recognition
data leads
null tence
1996 ):
previously existing
provides semantic
special knowledge
whose senses
). dependency
quotation marks
must address
global constraints
15 million
word noun
oriented natural
new item
implicit information
integer k
precisely one
equally likely
difficult part
english entries
best measure
simple pattern
simple term
applied within
6 )).
vector similarity
system finds
special emphasis
sample data
many well
development foundation
computational process
current systems
since muc
makes mistakes
substitution errors
initial words
similar technique
new methodology
extensive manual
narrow scope
three non
similarly defined
execution time
special care
relations r
realistic applications
projection principle
38 ).
used earlier
conditional maximum
language document
frequently co
open tests
different chinese
new objects
corresponding case
modify either
sufficient training
relations associated
th (~
method generates
np occurs
sager 1981
recent approach
type found
component performs
certain parts
input contains
complex terms
model consists
mark steedman
group 1
tree ),
events may
based pos
design criteria
), used
morphological relations
theoretic interpretation
must often
numerical data
2 system
two figures
qualitatively different
reduce computational
interpreted without
selection method
19 ).
basic scheme
several instances
information conveyed
nodes x
cooperative dialogue
;. two
best suited
goals ).
also typically
framenet corpus
v p
top nodes
19 \]
event structures
various modules
also yields
formal basis
may find
much simpler
stochastic parser
text shown
developing natural
question set
computational applications
id rule
key templates
feminine singular
5 system
use clustering
target language
confidence values
operations performed
one way
two descriptions
recognized string
thanks go
need one
highest possible
use four
stumbling block
except possibly
sentences corresponding
chapter 6
represent sets
poor quality
various word
character type
normal processing
forms like
compare well
algorithm might
use ).
1996 ),
speech time
problems still
special handling
represented using
using one
manually collected
computational systems
current method
agreement rates
rules --
new set
oriented evaluation
distributed across
document frequency
first experiment
b ~.
judge agreement
also due
example consists
lexical associations
approach works
single sense
x using
adverbial modifiers
little research
partial parser
query set
define different
la suite
full specification
might introduce
around 1
semantic patterns
least one
translation process
especially important
specific elements
easily find
linguistic relevance
time intervals
volume 26
word contains
); however
ill \]
ranked element
proposed representation
bleu metric
21 ),
provides one
si ).
correct disambiguation
assigned manually
pruning technique
would satisfy
level constituents
1981 ))
less strict
pay special
seems appropriate
format ),
limited success
.) note
sense reasoning
ranked first
level using
input representations
tools provided
2 although
de mori
communication channel
8 %).
tightly integrated
frequency greater
%, compared
cue phrases
parse forest
regular set
v '.
single characters
42 ).
analysis suggests
great deal
original algorithm
far richer
1982 )).
use 1
valuable information
table 7
similar entities
one common
first report
0 ..
syntactic forms
proper names
second definition
changes would
specific object
separate stages
incomplete information
approaches discussed
highly structured
advanced technology
many parts
rejection rate
complete implementation
volume 27
grammar generates
syllable boundaries
haruno et
two questions
fuller discussion
achieved better
recursive noun
tables 7
approach suggested
also say
du langage
commonly agreed
second meaning
discourse --
involves finding
previous works
easiest way
level goals
computational method
ill 1
first assumption
relatively modest
grammatical properties
source code
nonterminal categories
common situation
derived words
patterns like
complete list
one means
must compute
interpreted differently
reduce complexity
discussed shortly
test part
one member
automatically determining
relationships among
approach developed
p2 ).
proved useful
common phenomenon
often enough
sentence test
express two
translation systems
english terms
word use
also displayed
molecular biology
also occurs
kept constant
physical environment
v --
slight decrease
ne task
syntactic framework
use finite
3 sets
figures 5
trec data
sentence splitter
rules similar
every occurrence
quot ;{
keller et
information described
discriminatively trained
representational power
based generator
information returned
preceding context
accurate estimation
html tags
largely independent
two critical
using sgml
lunar system
represented graphically
boundaries may
see chomsky
external source
defined based
full documents
minimal unit
reduce human
expanded version
terminological resources
yi [?]
additional text
may range
word representations
english dictionaries
order 2
average amount
pattern elements
work better
approximately one
contain exactly
important differences
overall quality
many different
-- though
example data
subordinate conjunction
would solve
project 1
altavista search
final argument
used syntactic
sentence accuracy
beam pruning
additional time
among discourse
levin et
final number
formal notation
answer generation
phonological forms
evaluation score
first input
document type
raw data
central element
among three
two choices
clause must
average per
rules based
already quite
consistent manner
two components
thus even
et leurs
appears twice
comlex syntax
2002 ].
actual corpus
much help
require information
speech output
doug paul
methods would
retrieval results
triple consisting
initial model
first translate
cannot see
first place
appropriate answer
usually involves
former case
translation method
associations among
node label
following operations
lower value
quite closely
type hierarchies
string e
new domains
meaning conveyed
(' al
language requires
\] ).
). assume
model \[
naive user
semantic agreement
help much
prevent us
new sources
contexts around
representing words
list containing
sentence corresponding
%. using
statistical information
also annotate
first heuristic
7 c
\] \]
many dimensions
bilingual sentences
backoff model
formal languages
might get
also using
automatically acquiring
1995 \],
linear regression
assignment algorithm
specify one
2005 ))
coreference relation
v ,'
feature structure
preliminary implementation
new applications
also well
processes used
explicitly specified
takes values
logical point
corpora may
every context
variation across
hmm training
parsed trees
metrics based
five classes
system chooses
conventional dictionaries
certain threshold
procedure described
), decision
simple noun
psycholinguistic research
ne type
following notation
written rules
utterance using
reviews related
relatively simple
vocabulary items
feature describes
one shown
). possible
corresponding target
learning perspective
one inference
disjunctive constraints
information used
ambiguous verb
complex objects
one plausible
jing et
price et
recent proposals
k ),
trees rather
without necessarily
hpsg analysis
time needed
program looks
41 ).
labeled nodes
science council
recursively embedded
another user
la langue
alignment probabilities
new systems
), 4
may show
problems without
phrase corresponding
basic meaning
parser output
appropriate language
biomedical text
), defined
... 5
issues must
48 ).
der beek
disambiguation accuracy
query systems
leftmost word
comparable data
following situation
hierarchical relation
collect data
word identification
parsing time
matching procedures
rather large
example \[
joshi 1985
commercial applications
trec collection
single framework
2 thus
features included
', x
many sentence
related phenomena
systems simply
language equivalent
20 ).
performs comparably
usually represented
based formalism
given meaning
decision problem
expanded query
different data
using tile
graphical interface
feature indicates
zhang et
syntactic case
one previous
pair \[
20 \]
occur simultaneously
score indicating
results --
relatively stable
1000 ).
produce accurate
used tbr
quite obvious
transcribed words
entities identified
iv 701
computed recursively
significant enough
using patterns
using various
performance could
add features
ir performance
two together
significance testing
loosely speaking
strong emphasis
r \[
consider first
first candidate
4 \],
('. l
commercial systems
disagreements among
quot ;-&
x w
semantic criteria
work involved
w ('
1985 ))
tested several
search engines
one parse
automatic sense
typically consists
np node
weighted graph
may work
shinyama et
set contains
linguistic entity
number 3
unknown proper
usually based
role slots
information stored
level node
different classifiers
real values
complete morphological
closed form
extraction ).
right edge
,'& quot
phonetic units
13 ):
use linear
analysis process
considered similar
3c ).
functional features
., 3
less directly
1 )'
new method
great extent
jurafsky et
null example
current active
units rather
features indicating
combines two
time available
words taken
independent knowledge
single character
common parent
hit rate
one space
minimal effort
keeps track
whose language
procedure used
model according
language system
nlp modules
usually assumed
might cause
system goes
language families
important clue
idf scores
another lexical
time flies
utterances must
specific knowledge
correct parsing
several recent
joint probabilities
multiple utterances
null tures
techniques presented
call semantic
actual number
word initial
tree set
experimental evidence
id rules
strong indicator
intonational contours
also added
synonymous terms
~. h
current efforts
l ()
derived form
el ),
stated using
element task
comes first
single layer
features alone
binary variables
marginal probability
first selected
normal form
probabilistic distribution
three english
footnote 2
process repeats
part ).
possible semantic
appropriate morphological
), ...
previously generated
us back
test phase
examined three
penn tree
experiment 1
definitions contain
supervised algorithm
particularly appropriate
lexical probability
exactly analogous
tell whether
like many
based search
,~& quot
second order
1 ')
highest rank
makes crucial
algorithm guarantees
possible expressions
gram frequency
often due
;?& quot
results similar
scheme presented
untagged text
analysis systems
one pair
core algorithm
text structure
modi er
sequence consisting
1960 ).
presented first
framework ).
linear model
problem since
', f
going beyond
theories based
structured texts
hybrid approach
data collections
commonly called
also reflects
based acquisition
resource management
linguistic processing
boundary conditions
rule interpreter
score obtained
p n
see nirenburg
13 ),
8 sentences
free translation
reader would
constant k
updated accordingly
different scoring
feature captures
must allow
reliably determine
possible fillers
4 show
sufficient amount
large quantity
vector x
textual coherence
g .,
possessive pronouns
nouns ),
models cannot
also classified
words (&
linguistic contexts
another group
key word
high coverage
kernel based
tile corpus
arbitrarily selected
restricted class
part 1
complex ways
coherent set
simple n
specific events
line 9
parse given
different possible
reduced number
particularly complex
especially given
quantified noun
different sub
generate coherent
number features
\] lave
understanding model
good example
indeed possible
results based
office automation
need additional
), right
30 different
paper .)
features obtained
information theory
represent word
increasing complexity
higher confidence
., features
particularly interested
de l
anecdotal evidence
units ),
appropriate use
fig 1
quot ;*&
brings together
superior results
information implicit
event extraction
semantic ones
provide appropriate
based error
future direction
save space
..... x
average parse
may indicate
92 %,
called text
(, r
various properties
results directly
74 %.
cluster represents
underlying application
\[ \[
used primarily
two requirements
approximately 150
entity classification
brackets denote
analysis method
correctly handle
selected randomly
rayner et
syntactic frame
different options
special syntactic
generalize well
great challenge
seminal work
3 ).
extremely low
one open
marginal relevance
would amount
random set
chinese name
linguistic domain
3 \]
acquisition bottleneck
take full
storage requirements
p ,~
measure similarity
alignment program
one go
strong tendency
presented results
system includes
relevant entities
7 %),
algorithm introduced
mental representations
five types
structure grammar
crucially relies
passed along
b iff
may even
made available
argument labels
parsing would
major category
unrecognized words
techniques like
possible one
clusters produced
tree could
speech --
dans une
components developed
implementation would
simple heuristics
accuracy improvement
computer dialogues
sounds like
uses corpus
doran et
initial context
approaches use
np )/
certain specific
notable exception
interesting part
reduce action
(' x
asr output
square root
one binary
volume ),
priori probabilities
another test
differences among
type e
57 %.
corp .&
one evaluation
sentence .)
multilingual data
grammar structure
senses rather
exact form
possible cases
based word
system provides
states may
e .,
terms ).
du point
widely varying
concept types
might correspond
3 1
recently made
third argument
., 1985
learning phase
still many
lexicon contained
c ).
grammar describes
object phrase
separate language
technical manuals
phrase level
., first
aligned sentence
.) \[
strong similarity
real numbers
possible world
one instance
solution adopted
c \]
language techniques
linguistic models
factual knowledge
]. however
substantial differences
)-( 9
evaluating natural
text genre
coverage increases
stress assignment
pairs ),
existing literature
thus become
parser --
around 40
memory based
metric based
1 !)
structure component
one partial
corresponding probabilities
progress report
every class
individual user
intentions behind
content planning
find appropriate
different pairs
often using
random selection
trigger words
employ two
many theoretical
previous analysis
dialogue structure
); c
). text
research since
february 1989
language knowledge
exhaustive set
already covered
modest improvement
particular individual
preceding paragraph
research field
ed .)
one potential
complete interpretation
words ).
c v
~- l
1988 ):
organized around
first system
already tagged
resolve anaphoric
category x
program consists
previous step
matter ).
text planner
less prominent
system adds
requested information
tile number
syntax tree
generic class
subsequent section
000 different
la plupart
bottom ).
ldoce ),
obvious solution
perform well
precision errors
le traitement
one promising
c 1
important fact
summarization research
lexicon consisting
negative weights
french texts
reduced performance
graph whose
based statistics
recognition \[
svm using
new statistical
best analysis
transfer module
different phrases
1992 l
selected three
compared directly
logical formalism
many values
semantic argument
would automatically
l \[
explicitly indicated
features may
use mutual
candidate terms
previous literature
.,& quot
single frame
gram overlap
strong connection
relevant part
prepositional phrases
generates two
2 9
new active
given translation
larger collection
low cost
trec question
many classes
predictive features
three domains
theoretical reasons
new texts
abductive reasoning
list structure
parsing table
emotional state
x given
one structure
categories according
three sources
words similar
appropriate interpretation
less formal
consists mainly
initial syllable
system operates
morphological lexicon
english wordnet
best candidates
rules involved
combined model
experimental design
tree method
could reasonably
acl ),
generation mechanism
2003 shared
di eugenio
variable assignment
threshold set
decision making
recall score
senses ),
also train
various elements
n [?]
intermediate representation
coreference resolution
used japanese
w ',
could happen
et la
corresponding sequence
example question
relative weight
symbol x
relation detection
must satisfy
parser runs
particular state
appearing within
\[ 29
probability estimate
make similar
),& quot
strategy may
semantic relations
experiment shows
serial dependencies
hard disk
western languages
cognitive processing
word distance
symbols ),
also argued
space considerations
1988 ),
entry corresponding
use special
29 ).
lower degree
aout 1992
could apply
ralph weischedel
dependencies ).
f 2
nearly every
three systems
1 although
graph matching
one simple
standard training
words derived
., two
). c
broader class
rule cannot
operations like
infinitive verb
non trivial
greatly improved
generate features
efficient generation
7a ),
web corpus
also cannot
task completion
tim l
similarity based
initial annotation
main approaches
word feature
correct syntactic
topic description
current speech
reduction rule
space x
whole noun
next subsections
information society
constraints associated
generally based
automatically produce
obviously different
10 )).
could add
comprehension process
current semantic
). accordingly
switchboard corpus
pronoun resolution
using corpus
000 sentence
intermediate step
judging whether
original node
widely different
lb ).
node j
parser based
relative size
strongly preferred
linear context
). future
individual sentences
verb tense
network based
identify semantic
also ignored
sentences included
real challenge
specific lexicons
linear relationship
whose interpretation
time value
statistical modeling
restriction would
difficult tasks
paper assumes
sensitive rules
segment may
currently limited
real language
additional levels
node pairs
linguistic point
range dependencies
generally correspond
least since
patterns also
four main
generally assumed
information structures
recent successes
), often
higher error
related words
modern chinese
lower frequency
statistical tests
quot ;\]
95 %,
large parts
task 2
19 !)
coordination phenomena
virtually impossible
em algorithm
within noun
known algorithm
particular speaker
iordanskaja et
system depends
morphological tag
). instead
remaining words
tile rule
linguistic form
zero weight
systematic errors
us see
japanese machine
must solve
clause 2
experience indicates
focus information
used linguistic
random order
longer term
10 hours
possible connections
following equations
might benefit
possible patterns
know something
system intended
two verb
2 given
graphical models
., 2003a
therefore less
tit (.'
language parsing
'. therefore
coverage problem
x b
element e
fully automatic
preposition ),
particular surface
also greatly
precision increased
information presentation
similar characteristics
84 %.
50 %,
problem even
data created
different question
one provided
multiple layers
daughter categories
human summary
ir ),
differs slightly
every person
different time
main program
muc ),
original sentence
word names
also acknowledge
rules provide
resolution task
best strategy
accurate predictions
relations within
preset threshold
technical sense
still required
one position
singular value
text produced
complete path
paper follows
que la
information exchange
sentence forms
english translation
n 2
errors resulting
standard features
syntactic phenomena
larger phrases
would claim
wise mutual
given chinese
rules .)
form representing
structure descriptions
two event
unknown words
word starting
strong syntactic
space f
occur due
numbers show
place predicates
3 shows
highly likely
1994 ))
pronoun references
papers presented
utterance given
manual process
reflexive pronoun
contains 5
correction process
finite verb
acceptable sentence
thus leaving
., 1981
described ill
undesirable consequences
eric brill
binary vectors
express one
particular text
... x
translated text
could introduce
lexical processing
lexicon development
conjoined noun
performing model
column 4
l .....
task complexity
step 5
corpora contain
system capabilities
based document
communication science
charniak parser
whose truth
inherent problem
theoretic measure
statistically significant
pair whose
contains different
corpus sentence
high levels
first round
\]. although
communicative intention
performed quite
words formed
types found
complex nominals
last year
specified set
contextual features
may become
method improves
following conventions
abstract words
ie techniques
feature combination
)) since
less robust
independent syntactic
took two
nodes must
may seem
three positions
..& quot
predefined threshold
strong correlation
signi cant
weights according
system utterance
lexical concept
user preferences
arg max
errors introduced
~. l
disambiguation experiments
still contain
sample corpus
system outperforms
simply ignore
1 thus
potentially different
little information
z e
also deal
specific syntactic
independent given
dashed arrows
least number
gets larger
generate responses
without supervision
three factors
1984 )).
measures based
valued features
smadja et
classify verbs
form described
usually estimated
null 4
example would
highly developed
generative lexicon
certain points
many types
another level
phonological phenomena
assign different
additional sentences
entire tree
currently used
time requirements
full english
phrase like
lexically based
speakers would
less obvious
discourse constraints
fundamental properties
pair would
translation evaluation
gricean maxims
ongoing dialogue
local ambiguity
though see
l ,...,
single label
certain noun
successful parse
easily become
1995 ):
highest levels
automatically compared
coherence relation
would change
naive bayes
tile subject
two theories
various techniques
ultimate objective
grant number
likelihood ratio
\] provide
approach used
driven learning
assigned using
represent variables
). p
parsing uses
implementation uses
user turns
information present
internal information
frequently occurred
testing results
different research
two independent
programming algorithm
rules acquired
optimal parameter
use explicit
target nodes
provide access
lexical words
important criterion
models make
many thanks
meaning structures
per turn
new natural
cooperative response
learning takes
galley et
second data
x r
nearly half
ii \]
entire graph
various knowledge
test utterances
). table
classify sentences
svms ),
become clearer
closed class
beijing university
multinomial distribution
various machine
may consider
output may
longer valid
empirical analysis
briefly summarized
expressions often
asked two
extraction approach
always 1
consistent performance
class baseline
first 50
new messages
1986 );
simply due
university press
section 5
complete feature
graphical model
month period
short summary
creation process
earlier ).
without considering
.) similarly
sgall et
report precision
tagger described
used differently
two head
would go
two discourse
clearly related
immediately clear
rapid growth
problems ).
also modify
lexical models
rule x
examples whose
provide sufficient
similar tasks
new speech
also share
cognitive point
artificial language
tags given
full set
formatted data
given application
general observations
performance data
systems usually
branching trees
man ).
tile information
dt nn
results comparing
values ).
w represents
including two
first pass
following tree
relational data
form used
information often
parsing sentences
manually written
new semantic
man \]
user groups
without consideration
central task
4 discusses
without human
transition probabilities
1995 ),
pragmatic properties
following utterance
first case
another sense
may enable
failed parses
particular attention
atomic symbols
specially designed
category contains
1991 );
monolingual dictionary
one among
01 ).
full training
evaluation technique
additional test
many language
single application
nodes labeled
must construct
syntactic approaches
atomic values
necessary information
appropriate feature
stanford link
simple top
shall call
ann arbor
architecture allows
multiple goals
rules specify
using rule
make better
coherent texts
syntactically different
list produced
four word
text categorization
labeled bracketing
extract word
dimensional matrix
\[ ti
one produced
poorly understood
news article
ap news
\] z
...?& quot
one bit
examples would
node c
candidate selection
functor category
acl workshop
plausible model
reduction rules
current message
following graph
different databases
improvements could
one additional
\[ 8
parser achieves
determined according
(: ted
obtained without
information unit
rate would
values 1
different ordering
learning tasks
users want
manually inspected
almost universally
additional benefit
always produce
state morphology
might take
shall describe
representing word
recognize whether
either c
structural relationships
already discussed
like text
node q
optimal value
weighted context
special linguistic
60 %)
text snippets
mt evaluation
also sometimes
german sentence
tree consists
b .,
uses information
label indicating
specific examples
current question
natural reading
would behave
tagger used
2 r
word 2
algorithm begins
penn wsj
sentence production
problem rather
term variations
probability estimation
references therein
structures could
driving force
user utterances
right order
see sec
short stories
verbs ),
al .,
surface structures
underlying text
lf ).
work include
missing constituents
scale experiment
increased coverage
second possible
two portions
morphological process
syntactic phrase
basic data
nlp module
second sub
inherent property
5 r
improved recall
system identifies
tile rules
constant factor
following japanese
obligatory arguments
raw corpus
domain ),
parameter selection
\]: 1
matrix clause
7 7
latter interpretation
central topic
million entries
r ('.
language syntactic
general concepts
12 %.
model scores
corresponding surface
two sites
matching features
may allow
may suggest
percentage correct
interesting issue
node whose
similarity metric
language includes
feature specification
say things
computed based
one thing
hierarchical structure
new one
nist evaluation
automatic content
solutions proposed
given data
language given
syntactic variants
simplified representation
main predicate
constraints attached
simple dictionary
time recognition
second requirement
entailment relation
since several
frequent one
generating appropriate
figure 9
automatic tagger
could cause
argument information
top 5
possible readings
interpolation method
conceptual representations
multiple parts
different labels
level language
normalizing constant
est un
evaluation purposes
corresponding predicate
standard way
simply using
latter part
relevant nodes
capture word
recognition research
specified order
one fact
entity information
ideas described
nlg techniques
various parsing
system presents
np pp
reverse process
let e
single iteration
data sparseness
features required
words selected
intelligent information
word wl
rules thus
complex data
new cases
greater likelihood
previously proposed
generalized phrase
punctuation characters
people could
u ),
term goals
\[ np
quot ;=&
study shows
frequent cases
algorithm generates
data developed
sentential constituents
parsing accuracy
third category
previous stages
speech transcripts
14 \].
case n
information might
whenever two
verb pair
applied using
e 5
new world
similarity matrix
domain lexicon
statistical translation
per node
part due
irrelevant sentences
present data
node would
morphological systems
paper may
one says
de ).
languages involved
looks like
lexical model
currently defined
conducted several
one template
decision points
overall measure
uses rules
produce similar
problems inherent
verb tokens
appropriate nodes
empty categories
words versus
lexical form
integer linear
average agreement
information extraction
system simply
audio files
important concepts
actually made
problematic since
observed frequency
scores ).
vogel et
also list
point x
lexical function
provide multiple
chinese nlp
sample rule
problem ),
theorem prover
two iterations
produce word
). based
simplest method
academia sinica
high average
;, using
human dialogues
report generation
webber 1978
que l
role r
88 %,
typical sentence
contextually relevant
every new
specific terminology
unlike conventional
based system
concepts rather
uniquely determines
three previous
lexical analyzer
partial sentence
except one
events mentioned
e }.
subject phrase
tests show
maximum distance
current research
), possibly
identity relation
memory system
would follow
function may
words across
among noun
linguistic annotation
equation 2
succession events
bayesian classifier
another goal
fine grained
semantic category
algorithm converges
occurrences ),
best values
language prolog
recent survey
segment purpose
(; e
corresponding pos
missing parts
single phrase
item may
longer n
person agreement
special purposes
mainly caused
definitions ).
parser requires
16 \]
simple questions
smoothing algorithm
always give
sentence may
finite automaton
highest accuracy
de ses
per sentence
huang et
particularly interesting
document size
multiple dimensions
generally take
possible answers
documents may
large values
idiomatic phrase
would achieve
taxonomic hierarchy
negative impact
whose nodes
). essentially
valuable advice
specific actions
verbal position
constraints expressed
generally understood
using automated
four general
distributional clustering
singular verb
large classes
management domain
phrases extracted
sentences also
spe (:
one combination
employ one
speaker would
overall description
simply added
whenever necessary
test materials
beth sundheim
functional structure
node p
reverse order
six years
final selection
local syntactic
basic requirement
first child
house ),
human annotations
intermediate stages
2004 );
character n
pp attachments
supply information
rules alone
adding new
one since
table also
candidate translation
extraction using
steedman \[
earlier draft
learning tool
show us
words provide
node labels
experiment using
pairwise similarity
must infer
would fit
.) therefore
n -~
would propose
created three
length two
), resulting
). others
problem appears
sentence becomes
logical properties
classification stage
use similarity
components used
similar algorithm
segmentation information
noun group
english data
precision p
potential features
different scores
computational research
tree \[
would force
extraction techniques
language interface
intended interpretation
atomic types
semantic terms
identified two
containing sentence
structure containing
either p
without looking
e sentence
training iteration
hierarchical planner
chart edges
object (&
represented simply
modification relations
6 \].
identification system
). sentences
selected subset
training methods
statistical taggers
v ,,
could attempt
82 ).
largest value
obvious extension
algorithm implemented
representation language
correct words
could correspond
extremely efficient
database query
used within
backoff trigram
1992 ).
old ones
82 \]
logical variables
empirical support
set 2
null ferent
selecting appropriate
approach discussed
helpful advice
inference procedures
possible number
linguistic modules
1992 \]
generates one
tree substitution
intonational contour
new message
provide interesting
-- words
discontinuous constituents
assign values
important functions
already present
unigram distribution
dictionary used
statistics extracted
concepts ),
properties ).
generative process
coordinating conjunctions
c /).
global optimum
