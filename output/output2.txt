alpha	0.75
theta	1.5
category membership	1.5086023602732959
lexical chains	1.5502100456300887
annotation errors	1.5936356009797668
value structures	1.5080727729266272
bos et	1.5906723352433734
prior context	1.5378646275607575
little work	1.8977679713130362
input would	1.591823957496618
unique identifiers	1.5665826729887178
syntactic transformations	1.698739714494442
system along	1.5156323250636556
infinitive marker	1.5707378216701455
program used	1.530072891325321
order markov	1.8209724979701767
features using	1.8018798401562905
parse trees	2.306056400567579
partial structures	1.6383016524124137
something different	1.5645241345053709
third part	1.658060625176417
experiment showed	1.7210742795514369
given word	2.25841447983595
based information	1.9145951757451702
translation example	1.5871040729406385
one situation	1.5037367200448983
diverse information	1.5245348740884508
common information	1.5262546007178928
full parsing	1.8092396760039904
semantic hierarchy	1.7813881427333518
specific phrases	1.5138589183706062
constituent structure	2.077914462383582
basic component	1.5395099438048585
total score	1.7678414025836402
general english	1.7820448258302317
shallow text	1.5195036419368688
english language	1.9989170504489238
word lexicon	1.7815645125282462
carnegie mellon	1.9407729768459274
human annotations	1.5892420313344586
rapid prototyping	1.6346135067283294
speech disambiguation	1.5094411776337044
variable number	1.6545874861827639
structural ambiguities	1.819078190754017
phrases rather	1.5338735539822874
large corpus	2.3064629040537366
resource limitations	1.5275591303469287
identification number	1.5925152475070397
application areas	1.7353784550786506
source side	1.6046055081981683
hmm parameters	1.5225288106905501
generated word	1.566582672988718
generation grammar	1.6557275572945933
v p	1.564219922831998
data compression	1.5444341583162133
new knowledge	1.9072664754991482
lexicon containing	1.7768464080820896
pronoun must	1.554089533378209
lexical forms	1.8327296325649576
strategies could	1.5188051370135929
contract n00014	1.8071727592226263
approaches rely	1.6002993454155385
20 sentences	1.68175686190233
next generation	1.724370082433926
speech rate	1.5085812953494804
minimal units	1.5617071846474357
modified versions	1.5288687562201928
realization component	1.652259185383624
28 aot	1.8651838775060663
derivation trees	1.8836504219963042
data annotated	1.5482870987025659
output language	1.5946320626884896
acoustic model	1.8193222603584318
sufficient evidence	1.7008700806970682
syntactic expressions	1.5361450492096225
general grammatical	1.5037367200448983
par l	1.561518969698109
anecdotal evidence	1.5269930689946518
two words	2.4065636248655493
state technology	1.6253458251748392
initial part	1.648787373441388
optimal results	1.613624660358136
word sample	1.568335694696809
ney smoothing	1.5780839107898867
use context	1.6904151818990054
information necessary	2.0228226092422483
sentence 3	1.690217962657167
resolution system	1.7770407850353154
test set	2.5142414396222383
assumption behind	1.6536910451309068
large class	1.850627631770799
certain state	1.561123157601824
linguistic cues	1.5872398457493246
10 times	1.5179259513333854
contain one	1.8724996818784418
example using	1.6593965399315964
di er	1.5320698902069676
onr muri	1.5547189562170496
time speech	1.6197369406879942
approach considers	1.5445134575869865
similar types	1.5645241345053709
complementary information	1.5361450492096227
subtrees rooted	1.5081682632911075
2 demonstrates	1.5547189562170496
parsing technique	1.7703322968694952
ambiguity may	1.6010101084822783
np complement	1.5848275586653577
automatic speech	2.077828308192062
semantic conditions	1.5611135103157157
right order	1.9077572351674794
present algorithm	1.5496092928695906
mechanism must	1.586137214651611
world model	1.7363799962926112
simple keyword	1.6046743940979167
individual classifiers	1.6221377825744292
detail elsewhere	1.6458797346140277
model takes	1.7663553975095576
appropriate surface	1.5156083168889984
annotation effort	1.7269165536287314
length feature	1.5037367200448983
b r	1.5412463844444777
score function	1.5300398893292526
analysis stage	1.5872675697369218
alternative sets	1.5260638556202752
using equation	1.800502174877852
spoken discourse	1.627793851794891
useful applications	1.5195036419368686
collins parser	1.667100978608556
different layers	1.6342922036019591
data collections	1.5406210028261875
chinese named	1.5600327802566665
phonemic transcription	1.5833598229233639
phonetic models	1.6674050989728513
string w	1.8659645502022106
acoustic models	1.837682791445292
de ces	1.671963875974611
unary rule	1.5101859621866214
source string	1.6515529622443528
rule expansion	1.5177700906840073
different extraction	1.507007268326853
default value	1.8634749786492604
order model	1.657434273800325
binary tree	1.8077120696085556
irregular forms	1.6562676183926621
time intervals	1.727904830136062
line shows	1.7715551346065228
association measure	1.550730085069044
work based	1.5739592165010823
th e	1.9120160396485624
random samples	1.6915852462740624
type classification	1.5176055023927577
list consists	1.609217242875463
phase ii	1.6192190842797096
sag 1994	1.5420261883340527
large sample	1.740013717732364
auxiliary verb	1.9623240164954958
simplest case	1.8985878706414863
programming language	2.0322059284789793
empirical investigation	1.6707749282439646
special kind	1.8518475115639028
vary greatly	1.7178162994368975
relation name	1.5796505472613855
absolute values	1.683041275722836
first kind	1.6977890880751751
system uses	2.2913498590585464
different strategies	1.996766557262975
decoding algorithm	1.746361808299263
chosen based	1.643362624369706
use wordnet	1.8007620833352682
first number	1.5582958710952315
extract information	2.0109551023471157
decreasing order	1.9052065357252
low inter	1.5081682632911075
kleene star	1.7061113776200183
normalization factor	1.850156003139002
precision measure	1.5908167675916909
perceptron algorithm	1.5507055990996066
simple rule	1.8932293296495708
example 3	1.981442940934616
node x	1.7567831347071727
many dimensions	1.507007268326853
morphological description	1.5087993241956938
knowledge plays	1.5328333954091753
system may	2.1735485338397664
defined classes	1.530072891325321
tagged sentences	1.716629541704595
ones whose	1.530072891325321
subject position	2.0221575429376397
multiple paths	1.5216869793881185
system failed	1.6907321516102163
two effects	1.5582958710952315
journal corpus	2.007735699092229
idf value	1.5888396363139936
representation may	1.8235629774326347
patterns involving	1.504934031784749
standard method	1.729492528792866
free rules	1.9910537386895801
using confidence	1.526993068994652
null value	1.5648923336080593
several classifiers	1.608191142133971
whole sentences	1.7019709298887997
output format	1.6432534868546291
temporal reasoning	1.6154555078698143
algorithm determines	1.6112034493728657
selection mechanism	1.6796589902386034
translation problems	1.6629116393660754
x would	1.5338735539822874
row 3	1.519690311731115
following rule	2.032615632499448
information coming	1.5422431186751524
daniel marcu	1.522760613339579
multiple sentences	1.7961489809785582
simard et	1.5448062387808372
phrases containing	1.7619648596121327
order models	1.5470495239047226
evaluation based	1.6631574457448046
several aspects	1.9324323694338479
nominal phrase	1.6971388152211706
constituents may	1.780282513498202
frequency function	1.5010662334333809
new variable	1.5814458637269098
consonant clusters	1.5275974172630429
tree according	1.5739592165010823
medical domain	1.793367590903729
international chinese	1.6163274371635215
object may	1.7271602911957806
semantic aspects	1.7712391178028903
3 lists	1.8359513554634197
retrieval models	1.5216148258836173
represent word	1.6614869878819327
questions could	1.533540614646662
specific nature	1.5971626956604803
case labels	1.533536326366061
two step	1.6371456055941147
use semantic	1.8248917878754602
argument information	1.5461959912075431
main classes	1.7098773566635042
speech assignment	1.5602367449199286
procedures described	1.62412689036662
includes words	1.533540614646662
text fragment	1.6777188329234636
design features	1.5381766791708387
process model	1.630408016871156
first review	1.6315901311540408
lexical databases	1.801939991284127
size 1	1.5087853581980926
weights using	1.5188051370135929
improve word	1.5903064706943568
actual sentence	1.549729132180171
technique uses	1.583051127543801
specific application	1.8952732335438682
plan operators	1.6186243916347927
core knowledge	1.510181823566108
different instances	1.629642236647987
thes e	1.522760613339579
previous dialogue	1.6244843928316635
topic changes	1.564750541543679
describe objects	1.5195036419368686
unbounded dependency	1.614325023689826
nlp component	1.5023878666260249
data like	1.524534874088451
point scale	1.756578709046331
two noun	1.8202516262377098
research tool	1.55955788508706
human generated	1.566582672988718
inference rules	2.0023293805743725
stop word	1.790748150411185
constant factor	1.7088092136999036
universal quantification	1.5772489033805681
two categories	2.0389997869332674
four sets	1.7706466568240415
important task	1.8388415914885579
research described	1.874762836303248
possible solutions	1.8492338547455893
specific parts	1.6056035510036821
computer dialogues	1.5093623489867545
approach towards	1.586137214651611
almost perfect	1.6113346817822476
create training	1.5269930689946518
technique may	1.530072891325321
ney et	1.5539678711467342
words corresponding	1.6246289167870047
individual feature	1.7064486475564697
similarity matrix	1.6499958688046923
theoretic semantics	1.713120768539619
trees using	1.8048819793127076
mitch marcus	1.745609704973605
functional uncertainty	1.5058865062865674
target sentences	1.879438132634173
statistical data	1.8489059402516024
candidates based	1.650762416210899
making use	2.2348979931672863
various processing	1.5445134575869865
since information	1.579873144038688
focus spaces	1.583847530110885
several experiments	1.898680647263061
linguistic objects	1.8371343335438832
data problem	1.9488636259166066
text passages	1.64684205535691
one promising	1.5111306094308554
heuristic rule	1.704394873948126
state q	1.6789089588049317
expert system	1.8930590828413691
two domain	1.510618969052833
assign one	1.640044071538913
lexicon consists	1.7294947296982195
evaluation method	1.7411847348306924
theorem 4	1.612535718318377
surface forms	2.022206257118512
common use	1.731146052909204
confidence score	1.8118266476623053
generate rules	1.522760613339579
larger lexicon	1.530984627359544
several data	1.5354510398033074
aho et	1.587180495432631
experimental study	1.5716157403241997
second corpus	1.6778478586578416
processing techniques	2.031119336683001
orthographic features	1.5922299930337007
maximal set	1.5484545589550258
last words	1.5269930689946518
providing evidence	1.5611231576018243
semantic data	1.6499965586640808
separate file	1.594773126307433
experiments also	1.6635555237714856
research results	1.740045148124913
tools provided	1.5408560126058508
earlier stages	1.5514607572627888
spoken language	2.270992585808351
recognition problems	1.569973072782583
numerical expressions	1.5326611831041732
documents used	1.60913070763407
vectors representing	1.5759930952321457
constraints among	1.6969163101483125
first pair	1.6549809676762743
predicates used	1.6002993454155385
certain nodes	1.5412463844444777
given situation	1.7879877063304817
annotation procedure	1.513130125383453
one constraint	1.7311082078863615
figure 8	2.260759473426293
large grammars	1.7739596325818
anaphoric np	1.5525419192763326
speech group	1.5706734993195162
match words	1.522760613339579
top row	1.5321552374951577
german words	1.6997131295141525
category symbol	1.5151151849639106
anaphoric reference	1.924547613352838
10 hours	1.5656419863875413
depth 1	1.6144455608275865
first approach	2.0432845196820404
interactive dialogue	1.6552980515852136
people use	1.8642445176050289
likelihood estimates	1.8359202172851423
efficient parsers	1.5036575034310684
rule 3	1.7913857591806797
labelled data	1.5157299061091427
examples provided	1.5695222973163587
data type	1.7493137127548164
contain either	1.6084968011212868
main body	1.6768381249411435
simple example	2.2324395976850147
several feature	1.5177700906840073
language corpus	1.7096772409623884
represent two	1.6783930166760777
used machine	1.5973170078864487
question types	1.7168769825565648
consecutive sentences	1.5611265469383597
order n	1.7665789340534601
left implicit	1.669937736193418
bootstrapping method	1.59984978862743
using discourse	1.6073715579162529
suggest using	1.5611231576018243
experimental work	1.6117266025648125
language provides	1.6699377361934178
e v	1.7800372242250344
positive integer	1.5942052067923977
null b	1.741774336800427
next word	2.0602442810226425
source documents	1.6939550246241164
pentium iii	1.6570039092175395
related features	1.6775792181013167
appropriate tag	1.561123157601824
relation holds	1.9067955675925155
equation 8	1.558193976095525
uses three	1.6844174045708415
identify discourse	1.533540614646662
following example	2.343103854353169
high success	1.5482870987025659
punctuation mark	1.74285227009873
classifiers trained	1.6291410447393342
discourse coherence	1.7395802258697328
evaluation framework	1.615410027563157
conducted experiments	1.8629872338525746
entropy tagger	1.5145283495232635
candidate set	1.8039397346593589
three word	1.7127009588092879
words containing	1.6895421649102977
additional sentences	1.507007268326853
ordering relations	1.637622456610155
linguistic processing	2.017560990359495
one dimension	1.795973013592745
language variation	1.511205120616144
provide examples	1.6593965399315964
structure using	1.7856549343874448
sag et	1.6884896531966223
scores assigned	1.7003346395363956
system combines	1.6614869878819327
one utterance	1.8534665629217424
final stage	1.9178902536832734
k times	1.6023158871024248
final configuration	1.558193976095525
language systems	2.1541675107429263
time processing	1.6624358603626326
recognition systems	2.1089797831122183
common ancestor	1.8020946623012175
structures without	1.5887468924062063
default parameters	1.611062367486579
similar word	1.8181620447139815
automatic processing	1.8695416254042185
sense 3	1.51990097190404
model alone	1.616031378434591
source texts	1.7832603750551264
100 randomly	1.507007268326853
another difficulty	1.5111306094308554
systems participating	1.7324183049111395
one location	1.5869693547698929
language parsers	1.6342922036019591
best model	1.9056074333540651
sense tagged	1.7338747790926277
language lexicon	1.5177700906840073
spread across	1.6152953234723224
particle constructions	1.6020065276229738
like np	1.5148387121477087
backoff model	1.524495165730674
low scores	1.664540718454114
small samples	1.5511943810330642
global coherence	1.5320900400985142
branching tree	1.6528177045494785
f c	1.6825290124007908
entity mentioned	1.5226149809040495
systems reported	1.510181823566108
english letters	1.530238248477422
translation lexicons	1.5846278089875143
every state	1.637128868689432
overall task	1.613255515187164
various modules	1.6706426105940735
wang et	1.6667406980373642
new data	2.0693083070558367
one structure	1.704158995192193
generate multiple	1.62291510937581
common patterns	1.6392309045877975
partial analyses	1.7013292681590841
bilingual text	1.6024472450905132
crucial component	1.6002993454155385
possible alignments	1.8320250849043018
correct dependency	1.6300610276118177
xml file	1.5985543375863656
latter case	2.180866000617139
local maxima	1.676486268584806
analysis component	1.898006958092572
selected sentence	1.5648923336080593
em training	1.6364742670786607
lower left	1.6687546349982343
algorithm provides	1.7344099527418115
argument structures	2.0190871992174957
last step	1.9699679886124295
machine readable	1.9987634775518277
reasonable way	1.6164339756999313
syntactic configurations	1.6375625045004873
lexical statistics	1.516326202700883
relation expressed	1.6339957705783554
control mechanism	1.7051519354207447
perform worse	1.6342922036019591
highest values	1.5188051370135929
edit distance	1.929213849504564
lexical acquisition	1.9382357080155732
final structure	1.6401469199210985
empirical analysis	1.6881707359732032
understanding research	1.5300728913253212
several corpora	1.6031224303496519
particular aspects	1.5378646275607575
decision list	1.6709879393351526
four values	1.530072891325321
state machine	1.8698021839160268
best models	1.5611135103157159
various means	1.5445134575869865
similar observations	1.579873144038688
test split	1.5690244007917742
one event	1.7498981888221956
several attempts	1.7635102323276162
segmentation results	1.6420060648426011
semantic contribution	1.7437766657942229
popular method	1.583942701611184
conceptual level	1.7953633316882192
since people	1.5338735539822874
across sentences	1.6846388563743653
recognition procedure	1.6525487091262194
ten thousand	1.6046743940979167
diathesis alternations	1.5535589251664157
john saw	1.7152320425524243
original work	1.667913900271795
speech recognition	2.43260880253852
case roles	1.741546183009218
training utterances	1.5752833408720766
rule describes	1.526993068994652
one particular	1.9112244454396512
using case	1.504072959325348
one sentence	2.1708260899158516
another module	1.5511943810330642
system applies	1.6594660533960885
reason may	1.5792035811631282
john smith	1.6807919392315722
individual components	1.8679414262617988
rules generate	1.5547189562170496
second condition	1.749445753313418
first occurrence	1.8559990487299438
module uses	1.7892849578100742
dans une	1.5430279388400794
among rules	1.5201190767490378
et un	1.5699804777547732
dialogue turns	1.532714913365084
wherever possible	1.8347737830219164
lexical expressions	1.5046880378004521
performing model	1.573671233615979
japanese texts	1.769688263033511
without word	1.6260302594927594
precision value	1.5754161637940383
pp complements	1.5927442747164882
generation research	1.5563893320981306
detailed treatment	1.507007268326853
dalrymple et	1.6134138217185865
ambiguities may	1.579873144038688
goal would	1.5408560126058508
using tree	1.654587486182764
clustering techniques	1.8385471733025716
system interaction	1.5563893320981306
good representation	1.5188051370135929
text planner	1.787339639838315
authors would	2.0809726632368735
applying machine	1.5514607572627885
information even	1.5156323250636554
module may	1.618103745190334
increasing attention	1.5111306094308554
final sentence	1.760655880476912
implementation language	1.5338735539822874
model size	1.6267031387137434
2nd person	1.5040418526142796
features include	1.9029082489093039
different measures	1.7606105285802351
many simple	1.6002993454155385
parser finds	1.5973170078864487
certain rules	1.6429565433589586
lower level	1.607922883426077
knowledge representation	2.24341540031783
different subject	1.533540614646662
state approach	1.646279797293984
first option	1.5482870987025659
two definitions	1.7130162238987903
recent studies	1.844217685692041
basic language	1.5756869131546112
probabilities estimated	1.6049353334624743
perform significantly	1.7010856384233306
linguistic modules	1.5597934331208703
verb clusters	1.5669886194972822
serves two	1.62412689036662
tag b	1.511205120616144
null elements	1.5148138440039105
two facts	1.7387208003373877
carletta et	1.688827223337238
entities within	1.6741803270499043
short list	1.608852218159841
translation may	1.636252690439584
computational system	1.7957807317342946
1 st	1.5612688632382148
paper takes	1.597317007886449
negative words	1.546929508075717
rows correspond	1.5156323250636554
first study	1.5781539692435587
low probabilities	1.6276749302936424
travel expression	1.507007268326853
tile words	1.507007268326853
automatically generates	1.7178162994368975
following problems	1.7693843609764293
rules acquired	1.5798279917567952
also yields	1.507007268326853
contract n00039	1.5645241345053709
like wordnet	1.73827615637037
accurately estimate	1.5195036419368688
utterance interpretation	1.5622818522186857
000 items	1.559620194736931
name types	1.5087853581980926
spatial prepositions	1.5084748684124172
right association	1.581634210504943
second requirement	1.513130125383453
particular data	1.6750643656983746
related formalisms	1.6482958236884362
larger window	1.5412463844444777
control information	1.5554396221407916
structural descriptions	1.8569034465626295
uses two	1.9825724080714098
approach exploits	1.5445134575869865
clear separation	1.6977890880751751
human cognitive	1.538236714861226
considerable attention	1.754682578532674
alignments produced	1.5982673957552485
human translation	1.82629126632483
linguistic means	1.6598399152193164
german translation	1.5504792027311254
simple cases	1.8434371967215164
source languages	1.648468287060438
mother category	1.6254818264989075
short summary	1.6311568551569802
subject domain	1.677371521044301
understanding components	1.5265421490264868
english gloss	1.5656419863875413
elegant solution	1.6213967902634534
correct sense	1.9336612062414897
first place	1.7740554231959695
second uses	1.5111306094308554
lexical meanings	1.6421093653263799
variable names	1.5702560143399067
acquisition algorithm	1.5103302778562397
system output	2.02670127499982
clausal complement	1.5609868491677328
existing knowledge	1.8526411177747388
first work	1.585589015734039
4 lists	1.7997802558487324
following cases	1.8091439430899288
using default	1.504934031784749
automatic means	1.6844919157561307
temporal relations	1.835066546332466
across domains	1.8481816882178252
lauri karttunen	1.6137947517368634
news article	1.7524291617
preceding clause	1.5168185371501322
every individual	1.5412463844444775
clause grammars	1.7819714937622786
results suggest	2.1509275281413
syntactic form	1.9463508553159574
forms like	1.688641176416493
five characters	1.5622818522186859
attachment disambiguation	1.6441017160600806
also covers	1.5611231576018243
expressions including	1.513130125383453
given examples	1.5111306094308554
loves mary	1.5434513823532847
quantitative evaluation	1.814022800014858
order within	1.6481174069026685
empirical distribution	1.649638779877324
dramatic improvement	1.65027618229265
role labels	1.6599506747077988
methods proposed	1.7348520455220569
scoring software	1.613880799650739
example input	1.6103523297384124
generation components	1.740850117300889
inflectional paradigm	1.5877046642631585
language grammars	1.808938682679026
clause may	1.6294439702545596
quality machine	1.5288687562201928
two training	1.66156771763727
current method	1.584483801902334
grammatical structure	1.961956756620139
overall complexity	1.5306061495832655
lexical preference	1.5788712701148455
c e	1.5647409084752288
utterance type	1.5067018411237647
parsed sentences	1.901926392944646
evaluating performance	1.507007268326853
unsupervised word	1.575838245922343
briefly summarize	1.5511943810330642
analysis shows	1.8766250274585867
relative simplicity	1.5378646275607575
functional structure	1.7034395225968653
address two	1.6073715579162529
similar effect	1.636252690439584
word usage	1.7729898800763837
models may	1.77455679016321
state transitions	1.7963940235237252
possible meaning	1.5459182428709468
factors like	1.522760613339579
total ordering	1.5036575034310684
broad range	1.6272746496797978
useful tool	1.8272188571693444
och et	1.7732977817764186
understanding system	2.167963680849052
noun occurs	1.567922330580758
sentence forms	1.543657571903644
nigam et	1.5844716005258568
different degrees	1.9495267733786636
see discussion	1.7088092136999036
one terminal	1.6844919157561309
frequency f	1.5660375160880964
second alternative	1.703338232318599
similarity using	1.616488938894824
frequency within	1.5637776281819353
table containing	1.641155824653703
previous systems	1.8363147451296074
allen et	1.769816211252079
brennan et	1.6228909138559675
log files	1.5165543375933772
3 summarizes	1.8577041997108272
fixed expressions	1.5467646449184995
parser would	1.917902079364856
iterative algorithm	1.8050791145293756
assumption made	1.5973170078864487
following situation	1.5536240126687622
computing research	1.6232870445435097
ongoing work	1.8833720686651778
problems faced	1.5245348740884508
selection task	1.5014944801438541
set b	1.678301429611167
interactions among	1.7991667158516376
main conclusions	1.567922330580758
great care	1.6010101084822788
given utterance	1.8021625880743333
query terms	1.8756028843231616
system also	2.080128385428988
several evaluation	1.5395099438048587
maps onto	1.506173888464562
de meulder	1.549729132180171
grammatical sentence	1.7532357730536854
parameter l	1.60189988307561
one component	1.9482746149898427
key concepts	1.5630379101570404
computer vision	1.5431447863229169
word dictionary	1.7652417509021237
simple measure	1.5482870987025659
classifiers using	1.6594660533960885
acres de	1.9905750683152932
generation system	2.1721869303057764
several dimensions	1.7203266062826659
semantic evaluation	1.5523261282358138
also generate	1.7471845324497086
observed frequencies	1.6297178386410538
various contexts	1.7471845324497086
multiple times	1.886020214422888
useful results	1.6635555237714856
noun must	1.6084968011212868
wordnet senses	1.8617098038708493
selected verbs	1.5035193201428414
different experimental	1.5482870987025659
selects one	1.7364418632547807
actual input	1.5128915725531553
possible translations	1.9416618668585492
first identifies	1.5514607572627888
intonation contours	1.5200765896408694
many terms	1.6864169205234396
share similar	1.6724173909604771
another technique	1.5706734993195162
constraints cannot	1.586137214651611
stack contains	1.5425370803688765
deterministic finite	1.7478566333558185
performed worse	1.5582958710952315
user needs	1.8351175101396389
shriberg et	1.689327051145863
many candidates	1.635876749820076
first names	1.7255896490193388
several sets	1.6996261406379258
wider context	1.7536921686451339
original string	1.6073715579162529
varies according	1.644281423818564
expectation maximization	1.7964175456122922
many names	1.5150717731521737
german text	1.6529368339717727
surface expressions	1.5876887430433677
given structure	1.6031224303496519
text structure	1.9120670354030975
techniques developed	1.8488884403689885
sur le	1.7335078506274648
event e	1.610095976742285
identification process	1.6518622354339902
techniques presented	1.6339957705783554
technical fields	1.530984627359544
similar technique	1.6084968011212868
numeric value	1.5515223067572055
discourse functions	1.5415298239745916
massachusetts institute	1.522760613339579
one relation	1.7672022978977313
computational approach	1.7301217323026563
systems make	1.7178162994368975
grant iis	1.6939639871030243
corpus text	1.5256428192011384
approaches may	1.579873144038688
interactive systems	1.675536105738683
project described	1.5412463844444777
material may	1.5408560126058508
research using	1.5511943810330642
preceding words	1.8389392441410715
sentence patterns	1.7814407637010103
features together	1.5707376783218978
generalize well	1.6515771498200789
small fragment	1.594773126307433
hierarchical clustering	1.7833638371863183
various systems	1.76920023317697
crucial difference	1.7408279821914405
experimental design	1.7364113985326242
two np	1.5941108460767777
level 2	1.7408306969953664
analysis results	1.7752944438476959
potential answer	1.5784904253929217
contain non	1.5973170078864487
coordinate structures	1.6879494393583085
five features	1.6309471736131642
given event	1.5613568731772178
model must	1.8892735320933693
feature system	1.7824143778506933
another case	1.7935968174739088
lexical probabilities	1.6038446520284866
left branching	1.5303826364750015
highest probability	2.101653013232224
sentences contained	1.6768692079990173
extracting relations	1.540109050298748
v b	1.5254407895991404
dependent word	1.6045676149449064
given sentence	2.24017102718172
speakers may	1.7002798641346384
text sentence	1.5060672377841435
sentences often	1.5111306094308554
automatic construction	1.8712105075979637
analysis presented	1.6845577236692315
1 reports	1.5547189562170496
component technologies	1.5422431186751524
svm classifier	1.6491240152342668
hard constraints	1.6798816919718957
bilingual lexical	1.5081395359410799
20 hours	1.583051127543801
rules make	1.6635555237714856
semantic predicates	1.6082354039547413
graph representation	1.6511337109003579
linguistic contexts	1.6650008022438314
method achieved	1.6526280586612014
paper first	1.6084968011212868
discourse markers	1.6938078949921989
initial corpus	1.572024137418117
various kinds	2.178422799405616
prolog clauses	1.5912697234948552
syntactic behaviour	1.571940684334875
syntactic derivation	1.558988459312332
disjoint subsets	1.549729132180171
two context	1.6482958236884362
training methods	1.7500718203361596
form shown	1.7341967871778698
communication among	1.586137214651611
new kind	1.696799107973694
generated sentence	1.7185026757515263
3 compares	1.7663553975095576
statistical approach	2.0895293864140356
retrieval performance	1.800940118079262
annotated documents	1.5663601245015135
traditional approaches	1.8168115203027917
syntactic alternations	1.6227794168657732
mapping process	1.6778478586578416
method achieves	1.8160022016169608
functions like	1.5582958710952315
resolution problem	1.541861252893403
adverbial phrases	1.8709035937000895
basic notions	1.6630522416104851
yu et	1.6103523297384124
cfg rule	1.548592714580157
large space	1.659111358304426
appropriate word	1.7392177248000222
rules written	1.6428413718077
information among	1.5582958710952315
training method	1.7645895309779172
expressions involving	1.6137947517368632
task specific	1.532341016095725
writing rules	1.6073715579162529
system chooses	1.6548911840144864
linguistic variation	1.6562617868331544
phrase pair	1.5938627616065042
words selected	1.6137947517368632
following tasks	1.6802940634668402
null tion	2.0624492659772065
transcribed utterances	1.5440207036648979
prolog code	1.6257068156831858
feature sets	2.0747421942118036
bad ones	1.5459182428709468
chomsky normal	1.725098390511293
significant performance	1.8388415914885579
local trees	1.6771117931943313
train classifiers	1.517770090684007
maximum value	1.604580496280609
semantic approach	1.5845034447270998
acquired knowledge	1.6136283502300106
competitive results	1.5974644755426062
three positions	1.5637776281819353
null tween	1.583051127543801
lisp program	1.5792035811631282
representation makes	1.636252690439584
tensed verb	1.614174846085458
transfer component	1.7529626981257271
new sets	1.5037367200448983
context features	1.8212865628811252
linguistic systems	1.5257176783128208
first process	1.60913070763407
development cycle	1.6177179565678492
annotated texts	1.7006767599459978
standard context	1.6717533031932028
also incorporate	1.6164339756999313
nlu system	1.5112974520259979
geared towards	1.7504812345368606
et el	1.8329210724992722
set v	1.7088092136999036
first problem	1.749516121934537
qa task	1.5888456159933737
automatic systems	1.6690280524444296
le 2	1.5037367200448983
database records	1.580090580759919
words considered	1.533540614646662
sample grammar	1.5949073452801628
increases significantly	1.5536240126687624
different dependency	1.5445134575869865
processing efficiency	1.5670476475609136
promising direction	1.522760613339579
structure similar	1.583051127543801
real system	1.6961394033344752
features listed	1.5011917588019899
method produces	1.7585294036370804
various properties	1.554089533378209
sentence contained	1.5445134575869865
semantic expression	1.52377952472091
context contains	1.6164339756999313
parsers may	1.5338735539822874
discourse representation	1.9899553882522916
programming environment	1.5724391209948299
binomial distribution	1.649113034160676
anaphoric references	1.8593851727348993
summary contains	1.5081682632911075
nominal phrases	1.768045652991724
default case	1.6429565433589586
full forms	1.591122373048817
collins et	1.681232581604787
lingual text	1.5988242883426202
2 defines	1.530072891325321
tree using	1.7722692119182182
conversational speech	1.710534420705442
detailed results	1.7216140009801033
total number	2.3644010924122982
inference mechanisms	1.6444729977397123
general representation	1.5269930689946518
model captures	1.6602505116127793
last line	1.8151837274788793
english syntax	1.771852845785998
original speech	1.530984627359544
extraction performance	1.5157261869621457
approach provides	1.8917387625100843
fernando pereira	1.7656956055343276
resolution algorithm	1.7724787843876921
representation described	1.5177700906840073
analysis tree	1.623281969844577
column 4	1.549729132180171
correct sentences	1.7241618828676302
inheritance mechanism	1.646881311727099
figures reported	1.5637776281819353
closed form	1.6121742428060308
2 nd	1.5194356004765095
adverbial modification	1.5408560126058508
ditransitive verb	1.5365331267030269
lies within	1.548378091852222
simple decision	1.5559715171228279
new representation	1.6232870445435095
several values	1.5112051206161443
spurious ambiguity	1.6491335388747073
investigate whether	1.6843163749032262
results must	1.5645241345053709
major languages	1.5078976439458074
right answer	1.7875532997987094
initial implementation	1.7384345768444642
classification using	1.7518142844030467
discourse annotation	1.5474924554013374
first type	2.0204730603308843
multiple knowledge	1.6715297420241921
particular kind	1.8825749410134003
grammatical relation	1.822043101916574
class 2	1.578677233086251
complete grammar	1.6768692079990173
involving multiple	1.5819174715567357
semantic values	1.6288965613010205
system since	1.6783930166760777
horn clauses	1.7053592227274321
matches one	1.6010101084822783
tile text	1.6197369406879942
original documents	1.7354095497301618
research issue	1.6570039092175395
commercial applications	1.5973170078864487
good candidates	1.829950841087737
following decision	1.5118192695694388
specific representation	1.5887468924062063
lemma 4	1.5268602974177332
spontaneous utterances	1.5414180417122596
incorporate various	1.507007268326853
different manner	1.6112034493728657
overall scores	1.6071185545190714
likely candidates	1.5582958710952313
helpful discussions	1.925120091448104
relative size	1.5061738884645621
presents experiments	1.5111306094308554
another category	1.6481174069026685
every entry	1.6320354670147565
different sub	1.7087882102908172
work needed	1.5111306094308554
sense distinctions	1.8507269039325809
become important	1.6266256462651403
n characters	1.5329785555225088
significant advantages	1.6889191202036424
two templates	1.503660794753864
automatic procedure	1.811378249072753
three examples	1.8880843317666316
simple baseline	1.683041275722836
existing data	1.6911567924922841
different clauses	1.5188051370135929
either side	1.972610588365545
r 2	1.702315909495303
scores using	1.6147495904226123
potential antecedent	1.5720403027116572
v v	1.5133474375692826
falls outside	1.590816767591691
new approach	1.816589680464924
rules 1	1.672539925425231
original texts	1.723831154131648
active learning	1.6134391385842761
separate module	1.6645407184541139
john likes	1.7114532381817928
resulting models	1.5825577359308496
input units	1.5181321393336875
conceptual relations	1.722846329842468
base case	1.6332948211342992
pilot experiment	1.595730494764315
strings may	1.522760613339579
identify possible	1.6388370153723533
different model	1.6828225421438074
linguistic properties	1.9224228517832331
lexicon contains	2.0066615582899003
following sections	2.3288171252197687
formal account	1.62291510937581
property holds	1.5656419863875413
multiple features	1.536061807636894
corpus c	1.6043843288637842
form words	1.5207281603462597
chunk tag	1.6022221421462506
one discourse	1.7307810042852618
undirected graphical	1.5414180417122596
one difference	1.762033687546193
web search	1.882380288190012
thank mr	1.549729132180171
parallel text	1.8517945888535545
simple matter	1.7189745633058606
bayes model	1.6093263690252944
provide input	1.586137214651611
units called	1.6266256462651403
language usage	1.7176932828788232
information related	1.8544450569344653
natural way	2.165376032617579
patterns like	1.660919557194327
speech analysis	1.6816271364089355
generation techniques	1.6525488762732246
model p	1.8853489713509795
isolated sentences	1.6862623517634696
hearer must	1.615102354495647
morphosyntactic information	1.5403034171359224
one developed	1.591823957496618
several simple	1.5559715171228279
sentence 4	1.6661309301322418
different sense	1.8561174263209013
independent model	1.6058152859002197
concrete objects	1.500282895320681
main verbs	1.861002412184003
lexical form	1.8263575686889484
first part	2.01454656149879
input words	1.8658368125581264
basic mechanism	1.6289349127929256
tts system	1.593071314450493
following utterance	1.6993053005225323
2 show	1.9881775728095124
transfer module	1.5846233162854135
given relation	1.62464926904703
joint model	1.5631094901588467
different arguments	1.5037367200448983
important words	1.8025954988572195
class includes	1.5040418526142796
features extracted	1.9321989231783627
time linear	1.6378220404202128
class would	1.5381766791708387
4 summarizes	1.7651107526366039
parsing algorithms	2.0533380101070473
parse structure	1.6392095844459051
human experts	1.8486205188372558
execution time	1.8063313930828755
use syntactic	1.8338322248238494
particular representation	1.5217605875570064
documents according	1.6785607164239145
language output	1.5507127298454184
recognition output	1.7205583685350827
million word	2.0498546735443828
language accepted	1.5410230457446352
translation word	1.5395099438048585
available information	1.9188490160337106
many documents	1.7785421716291088
tree rewriting	1.534627270372999
human judgement	1.6545874861827639
different models	1.9926907820333362
knowledge representations	1.75739580878258
information gained	1.597317007886449
syntactic relations	2.0768985269466085
w using	1.507007268326853
language dictionary	1.5198327810104701
hard problem	1.7458878463811769
demonstrative pronouns	1.6567962828173326
grammars must	1.5226149809040495
tagging process	1.7953431839105247
lexicalized grammars	1.657117788940566
rare word	1.5363485810301154
best performance	2.212526683296757
different sequences	1.594773126307433
existing software	1.5408560126058508
also becomes	1.5111306094308554
probability given	1.786382060222585
word used	1.6303444022729265
particular pair	1.586137214651611
rules provide	1.5412463844444775
every step	1.724849955805923
phrasal category	1.567703845241315
large classes	1.5378646275607575
system proposes	1.6016416686192119
negative training	1.5642694360609983
set described	1.6896181027375323
one cluster	1.8221809795388466
unary predicate	1.5038337808417528
resulting grammar	1.775646543913447
two goals	1.7018465269448129
figures 7	1.5690244007917742
human speakers	1.5804695626215477
following types	2.003657680042129
little effort	1.7414848008364259
q e	1.627182633619367
single model	1.7686918836984074
positions within	1.6816988370728096
additional knowledge	1.9084115488576463
precise formulation	1.5731122769149348
learning paradigm	1.6741313250587324
noun class	1.5456671918534544
rough measure	1.5354510398033074
one run	1.5637776281819353
every noun	1.8108881035691438
implementation details	1.7518142844030464
200 documents	1.5280364400130908
independent knowledge	1.6511280827773667
management succession	1.5772856782679798
empirical studies	1.8963643035106044
second word	1.930134241481484
semantic constraints	2.129125932658911
next sentence	1.9084613643067574
thus require	1.5111306094308554
building blocks	1.7185424457735223
language comprehension	1.7928074474019091
systems still	1.5706734993195162
precision results	1.7621798563207776
answer key	1.7961195170264492
parsing time	1.9087099295402965
standard set	1.7974617205215369
also plays	1.707160349122273
character level	1.6068766341065337
existing corpora	1.592620200341168
basic information	1.795442754002124
using information	2.0089293218599753
training requires	1.5188051370135929
single sense	1.7707026156137526
computation time	1.8459725488479075
set difference	1.552544074218515
rule format	1.6601559374550001
different features	2.0456990575637155
models also	1.591823957496618
often correspond	1.5739592165010823
result might	1.5300728913253212
complete discussion	1.543442208482524
data produced	1.6169986020906966
complete sentences	1.924359416491547
every domain	1.5716157403242002
language also	1.5906852501842725
1 sentence	1.5839427016111838
question posed	1.5743784868483157
preceding text	1.6543652229213646
relative frequencies	2.024957157191019
different learning	1.8324898365032027
undergraduate students	1.506173888464562
every concept	1.5622818522186859
learning approaches	2.041448418096489
incorrect analysis	1.5482870987025659
complex situations	1.5338735539822874
rules allow	1.7369556428122903
japanese corpus	1.6634968178719003
wordnet 2	1.581441915316759
one tree	1.8423999709064263
different grammar	1.61326058900792
incorrect ones	1.6849207890717186
help people	1.510181823566108
open problems	1.592111614460519
yields good	1.543442208482524
immediately dominates	1.6257326646409984
data provides	1.591823957496618
depends upon	1.8958806320810315
100 words	1.9112841377417567
independent features	1.6802525785123257
basic idea	2.2849821629673746
normalization process	1.5381766791708387
sigmoid function	1.6312849180963627
corpus consisting	1.9435490143914085
first tries	1.611203449372866
n n	1.8817085258600468
machine translations	1.5920883271260986
final tree	1.5839972688383932
different entity	1.568863381218478
particular values	1.5111306094308554
table l	1.6066593791852315
one language	2.237708659282638
words contain	1.567922330580758
simplifying assumptions	1.7210275256038292
evaluation showed	1.6246289167870045
long list	1.6342922036019591
probability estimate	1.7896142323035067
reasoning capabilities	1.5276926419628722
improvement obtained	1.5482870987025659
ordered pair	1.787245536929127
4 compares	1.6289349127929256
information transfer	1.511205120616144
different n	1.5111306094308554
unification operation	1.6879782221889976
large body	1.9557514797042224
integrated approach	1.7500406842745784
future evaluations	1.5426832781289812
texts within	1.5328333954091753
problem since	1.7486023401067097
chapter 5	1.6288981647434881
graph theory	1.65545581040525
1 b	1.6959017760440365
state 1	1.522821541079752
grammar given	1.6002993454155385
0 proc	1.7935968174739088
briefly review	1.9000674958771564
np argument	1.589930267058259
best system	1.9148339265000114
final language	1.596528355412953
bayesian networks	1.564873290832325
object positions	1.5041134997716232
syntactic annotations	1.5770253883837002
corresponding sentences	1.6706426105940735
tile next	1.5706734993195162
infinite number	1.9193966626018166
context feature	1.5754199285488597
two members	1.5880900834779994
already contains	1.6289349127929256
position j	1.7774932873413694
les deux	1.5424285049845559
one aspect	1.9962269660095813
evaluation set	1.8443132618005482
different points	1.8361987959851702
human judgements	1.7281627708680811
th element	1.567388877220281
limited space	1.5645241345053709
liu et	1.6821101686598654
pair consisting	1.8072467635462641
false negatives	1.7063482913444674
object descriptions	1.5346833348925666
function f	1.99965398979026
underlying knowledge	1.745293469993684
argument identification	1.5851123918026526
biomedical text	1.6074698187266425
based method	2.1300172727100017
structures corresponding	1.6710766986086554
algorithm also	1.854863870676686
information cannot	1.707160349122273
software used	1.5973170078864487
general concepts	1.6825249985619857
feature constraints	1.683406744863512
exact string	1.570430085810516
5 minutes	1.7573123843210445
common method	1.6084968011212868
existing components	1.582306165059963
multilingual corpus	1.6107492017265248
right child	1.5360026503169557
reasoning process	1.6834680345784634
test sentences	2.1412454254076954
contains enough	1.522760613339579
open class	1.8553315055647805
parsing speed	1.69056980463397
two procedures	1.6445809590877873
canned text	1.5638410643325367
function associated	1.5637776281819353
important concept	1.5886561992520427
paramount importance	1.5338735539822874
english preposition	1.5556301839047322
terms based	1.6169986020906966
infinite sets	1.5839427016111838
surface form	2.1382578449384324
shows results	1.8503103103248704
paragraph breaks	1.505618568123598
generalized quantifiers	1.5904771784117457
parameter estimation	2.0564253804341712
single grammar	1.6546172815199354
computer science	2.2194189116577054
branching structures	1.6200038141661337
end point	1.6670715030583867
data sample	1.5276190326876764
features appearing	1.5395099438048587
creates two	1.5111306094308554
alignment quality	1.5821955152886242
complete understanding	1.5086023602732959
per topic	1.5889588886268688
sections 6	1.594773126307433
column 2	1.7014507751270238
adjacent words	1.9443992980617244
complex examples	1.5887468924062063
compound sentences	1.622400588558207
specific concepts	1.8485239713496517
computational problem	1.5414180417122596
retrieval techniques	1.8156845751175388
general notion	1.6961394033344752
user interface	2.1879709574021486
modular fashion	1.507007268326853
feature description	1.6164828680064454
string x	1.7112400803572294
arrows represent	1.533540614646662
one found	1.5078976439458074
main question	1.510181823566108
pronoun refers	1.5131301253834533
three parameters	1.7036983037241644
schulte im	1.5914624841423683
following parameters	1.6661204761131605
tagged data	1.855287945834699
single entity	1.7678414025836406
expected answer	1.6649449395833593
align words	1.5422431186751526
using term	1.570915482058415
adjunct distinction	1.5227114515967974
recent years	1.6258365572035767
choice among	1.756257461041527
resulting sentences	1.522760613339579
financial domain	1.5258845672739016
given discourse	1.7230579229098404
whole data	1.6751782252712877
last set	1.6002993454155385
discourse theory	1.5827722622100096
cannot capture	1.7527766729997913
alignment models	1.7161805612011085
adjoining languages	1.5180487617644318
common form	1.6037378902512214
performing system	1.6911504919590041
language resource	1.6332903471326536
small hand	1.5338735539822874
better score	1.5887468924062063
variation across	1.5792035811631284
asian languages	1.7125933668020417
capture long	1.5716157403242002
syntactic unit	1.6794854487178401
two theories	1.5593620621629958
special processing	1.591823957496618
15 times	1.509778922004791
general theory	1.8184895797409424
source file	1.5087853581980926
manual checking	1.5159609197822452
one column	1.5037367200448983
based framework	1.8197301470996852
syntactic position	1.679046836400674
text planners	1.5150717731521737
frame based	1.5459806054163279
object control	1.5081682632911075
subsequent sections	1.792162570502364
second phase	1.6049352499608225
certain context	1.7461343450093207
single object	1.6121742428060306
multiple sources	1.8864237690977081
basic level	1.6839032440150774
one view	1.6112034493728657
research question	1.734954551774483
distance metrics	1.6201628892774944
hypotheses generated	1.5498602379321533
speech models	1.5037367200448983
using co	1.7124624094913359
general terms	1.8925999191640983
grice 1975	1.5617071846474357
pairs involving	1.5496092928695908
domain information	1.804226837331545
learned decision	1.5026347083574416
verbal communication	1.5027241808567788
30 minutes	1.8391904414837337
system configuration	1.6112034493728657
final column	1.586137214651611
better model	1.806267122365411
arbitrary number	1.960693325222529
october 2005	1.6388370153723533
role assigned	1.507007268326853
sophisticated methods	1.6876319470917474
major effort	1.5611231576018243
higher weights	1.5748567577278902
document representation	1.6798168875503168
relative importance	2.036389939485941
contains several	1.8463589810990293
structures containing	1.683041275722836
positive correlation	1.6551038157182756
different phrase	1.587161501621602
overall framework	1.579873144038688
new terms	1.5390516392155345
real application	1.7202225397045487
string comparison	1.5360962909498346
phone models	1.6404932003836012
anaphora resolution	2.0610930956743054
given test	1.724281386402852
two disjoint	1.6845577236692315
finite state	2.200964173060934
feature names	1.681881916288912
grant number	1.873943246794527
separate entries	1.5841735639816608
resource grammar	1.5776964804242106
n order	1.6056035510036821
noun may	1.6056035510036821
relate two	1.5111306094308554
bayes classifiers	1.5824752227789518
recognition phase	1.56182880736141
many phenomena	1.5706734993195162
semantic interpretation	2.2660013878241485
simple word	1.9095860543600665
kim sang	1.7852011937887793
phrase like	1.785654934387445
document detection	1.6091442541382524
whic h	1.6427934356039386
document pair	1.5118448840653822
following elements	1.6056035510036821
extraction techniques	1.8486314727745203
theoretical interest	1.7531599391786739
new probability	1.507007268326853
et le	1.6910254651736922
powerful tools	1.5037367200448983
english expressions	1.6793220439418501
tree substitution	1.5183834349199885
given time	1.6329692418686204
sentences might	1.5156323250636556
dependency syntax	1.519144346838184
occur simultaneously	1.583051127543801
third rule	1.611203449372866
sentence 2	1.8023197672713325
natural english	1.5425370803688767
always sufficient	1.507007268326853
lexical item	2.278205682044788
first show	1.6630783357424366
composition rule	1.5359200762243734
central issue	1.6266256462651403
following relation	1.526993068994652
case frames	1.8638428268749958
1 v	1.6006844190904679
complex structure	1.835642526719992
frequent nouns	1.5205092238223816
algorithm chooses	1.5211629864223775
threshold th	1.5500434040293412
evaluation procedures	1.6727284364851402
developed based	1.6037378902512214
method proposed	2.0252174515922343
system whose	1.7793935740675575
current position	1.551972985694923
representation using	1.6255969501847347
system structure	1.524534874088451
produce sentences	1.554089533378209
contains approximately	1.8254810539737294
three ways	2.012287085360442
entire string	1.5226149809040495
transitive verbs	1.9765854611834235
retrieval application	1.5150717731521737
confidence values	1.6011112557545366
approximately half	1.7106802946855557
linguistic point	1.9260659725404434
also perform	1.703359686465308
words outside	1.7136436749098443
manual intervention	1.6990052517287035
similar verbs	1.5181144837827718
associated probabilities	1.6338541077451896
dictionary construction	1.5984574311631046
formal logic	1.596364543675433
following equation	1.9079557347135059
intended meaning	1.9123140848870732
root symbol	1.5434176300418243
constituents whose	1.5739592165010823
model may	1.9317100649560741
certain position	1.5156323250636556
selection based	1.632863169747887
corpora could	1.5445134575869865
complex event	1.5759490043057238
object whose	1.5226149809040495
ce qui	1.6340469363188357
current clause	1.5122944972320866
extract answers	1.5040418526142796
form given	1.6558914147565647
verb complement	1.5929928949821113
avoid generating	1.5381766791708387
logical subject	1.6924949438685246
also satisfy	1.5378646275607575
active research	1.7650139064090151
given source	1.8464801899339573
different constructions	1.5620473238340082
level relations	1.5164911279959081
language used	1.7970473916369019
constraints like	1.5188051370135929
semantic type	2.068561630487066
complex sentence	1.7930549555924815
system outputs	1.8217664991942955
data alone	1.5037367200448983
many mapping	1.7029671289655348
processing work	1.5269930689946518
document classification	1.766326153652956
given tree	1.633708641442126
first sight	1.8578445188092176
syntactic units	1.7717226617316326
second technique	1.5378646275607575
monolingual english	1.5611135103157157
single phrase	1.7409385343750605
later point	1.506173888464562
6 proc	1.7178002527269736
5 percent	1.6152888150903435
additional experiments	1.740544448391902
phrase type	1.8830694684088365
following observations	1.7554047654726699
total probability	1.8024502091298584
section four	1.5111306094308554
agreement rate	1.6100970726324073
several extensions	1.6164339756999313
occurs without	1.5320698902069676
different sources	2.0930478631113405
approaches described	1.5188051370135929
ongoing dialogue	1.5156083168889987
information gathering	1.6010101084822783
model outperforms	1.8295767409416415
president clinton	1.5554246978144852
reasons discussed	1.5706734993195162
previous efforts	1.683041275722836
vertical axis	1.7630703316996241
technology research	1.6946633528721495
useful information	2.1909789046462453
additional data	1.8869995311494552
example 2	2.058323181249296
given language	2.0300373678940744
synthesis systems	1.5881129575489688
every training	1.5378646275607575
validation results	1.5156323250636556
binary relations	1.9240853132854976
lexical relations	1.8230858817388806
former approach	1.6084968011212868
also hope	1.6635555237714856
best guess	1.6535456510181574
tree algorithms	1.5037367200448983
structures representing	1.669489306700537
sentence includes	1.5887468924062063
pragmatic level	1.6202338249864887
final punctuation	1.6392633022257659
different domains	2.1526506351676216
high confidence	1.8150578064808687
word whose	1.8047202938026277
eugenio et	1.511097557028959
sentences could	1.7815007420496276
different slots	1.5328333954091753
landauer et	1.5013748600240961
prosodic cues	1.7018802570510378
word structure	1.577121788867542
translations using	1.6197369406879947
remaining ones	1.591823957496618
es de	1.622988852704934
sentence might	1.643362624369706
first assumption	1.5622818522186857
question answering	2.2187300084797714
initial segment	1.5325679567186172
mechanism described	1.6990666498994726
word types	1.9590235411099095
special emphasis	1.65303397348486
greater improvement	1.5037367200448983
basic task	1.567922330580758
text categorization	1.9460036834307057
grammar formalism	2.161589177940843
priority queue	1.5194192532488042
first determines	1.5547189562170496
extraction application	1.5500434040293412
classification algorithms	1.6714604389400827
another consequence	1.5269930689946518
selection method	1.7708634196145354
l l	1.7789504309762951
statistical part	1.637937700118181
features might	1.6647993279874824
several target	1.5156323250636554
annotated corpus	2.1734346925397494
required information	1.781418055867531
semantic attributes	1.6097920436393216
additional feature	1.9038639608177361
overall accuracy	2.050031137582251
gb theory	1.5907363314740017
method allows	1.8269494440275762
line represents	1.613906107398343
candidate sentences	1.6573450267354182
systems work	1.5445134575869865
1 shows	2.6087415748854044
independent feature	1.5177700906840073
tags may	1.6084968011212868
cannot assume	1.7408279821914407
node must	1.7675276008554752
problems mentioned	1.7044865459645908
represent various	1.5300728913253212
translation equivalents	1.699906586587091
crucial information	1.6741803270499043
relations defined	1.7862837298758947
using patterns	1.513130125383453
appropriate structure	1.5645241345053709
ken church	1.6469586338689495
performance relative	1.5408560126058508
deterministic algorithm	1.5275591303469287
argument relationships	1.5525978349470975
third example	1.667913900271795
algorithm used	2.1018046340952763
million entries	1.5037367200448983
concept hierarchy	1.7792000547343012
common lisp	1.969498000453153
implicit information	1.6993713525853322
case structure	1.5502290102591332
support verbs	1.5569811257517205
standard text	1.6569985858431595
semantic part	1.5763955456335523
verbal noun	1.528356309822257
falls short	1.672219863528485
corpus material	1.513130125383453
information values	1.506240532038721
automatic dictionary	1.530072891325321
feature unification	1.556226883254912
incremental interpretation	1.560368830801559
block diagram	1.7223913317859245
dialogue translation	1.5302191995231849
style parser	1.6275587923441366
independent human	1.5177700906840073
experimental systems	1.5300728913253212
per text	1.5194542015256087
capitalization information	1.532341016095725
current set	1.8415604545312751
current nlp	1.6708590810843598
risk minimization	1.5606132885006492
japanese character	1.5365331267030269
many errors	1.8711084643684233
increasing amount	1.6458797346140277
structures associated	1.7483990608958573
another term	1.5408560126058508
sort hierarchy	1.5443420706380955
set using	1.9462181700689867
domain question	1.6676023079622442
main focus	1.5160756471897905
methods mentioned	1.5582958710952315
new type	1.9549288734328671
rules work	1.5798548254880234
applicable rule	1.5839427016111838
next input	1.7121112208161022
various topics	1.526993068994652
including words	1.583051127543801
example would	1.8080744485703488
nl generation	1.5820153109889084
deep structure	1.7996973284198197
recent experiments	1.583051127543801
certain set	1.704595095221415
reasonable number	1.6904151818990054
official airline	1.5041134997716235
words including	1.659662148122272
words usually	1.6429565433589586
given length	1.611203449372866
results may	1.8633999645732353
example might	1.594773126307433
table contains	1.7199340743580924
text length	1.624702623588453
grammar consisting	1.6169986020906966
linguistic background	1.5514607572627885
warren 1980	1.5217605875570064
constituent structures	1.8518485700566985
use f	1.522760613339579
average word	1.8073720196175944
combination method	1.622988852704934
entropy h	1.6650226666873245
verb arguments	1.7176310426416928
sentence could	1.8007620833352682
word b	1.60157999563475
text format	1.6392309045877975
rule probabilities	1.7426014516957697
different research	1.5637776281819353
underlying data	1.6014274684540788
features employed	1.50119175880199
conceptual categories	1.6779985525124164
reliable data	1.5188051370135929
systemic grammar	1.6864503651405225
000 nouns	1.5360026503169555
parse results	1.6411193148793473
sentence form	1.617280683415936
method without	1.5690244007917742
optimal solution	1.8914373394155348
reference times	1.523508842576295
research focus	1.5547189562170496
also reveals	1.591823957496618
li et	1.7249202455349142
punctuation marks	2.0812658016896273
evaluation experiment	1.5850578177065593
techniques would	1.6392309045877975
features could	1.8335879481729591
unique identifier	1.8475822135625846
sentences randomly	1.6002993454155385
front end	1.8951440090884162
experiments presented	1.782282174332623
short description	1.7178162994368975
features required	1.5188051370135929
human subject	1.6704397883054907
best match	1.8208109534031531
also demonstrate	1.7139483293868785
sets based	1.5328333954091753
e u	1.6126011434656715
rule 2	1.7572156873305984
previous phase	1.5637776281819353
regular grammar	1.652951345232552
particular problem	1.786382060222585
original corpus	1.812003128423471
experimental procedure	1.5214529152145557
base query	1.573651266877873
formalism provides	1.5611231576018243
since e	1.5188051370135929
prepositional object	1.6843404540177929
rather simple	1.9474021420760652
form c	1.6093022139611397
different steps	1.6448472409336694
muc evaluation	1.5582958710952315
mark steedman	1.687233859757661
examples 1	1.6701008999321498
relevant words	1.7108946909138973
automatic word	1.7704168188777132
syntactic pattern	1.8030586632557233
new target	1.512925200463965
optimization problem	1.8883887375498736
joint research	1.5156323250636556
line resources	1.6448472409336696
test suite	1.6863110980287124
different variants	1.6270880267949739
supervised methods	1.7605801987455207
common nouns	1.9970283514281355
incremental algorithm	1.5393982744056087
1 results	1.583942701611184
frequency data	1.7796220683150479
varying lengths	1.524534874088451
unique word	1.6742252425851278
set 3	1.5003968230881544
expressions like	1.9536372976632537
e use	1.522760613339579
every context	1.6330340284286617
parser based	1.9234969195629659
kaplan et	1.5329615000320245
child language	1.506926212840785
roles may	1.5656419863875413
data formats	1.507007268326853
syntactic choices	1.5864336788200233
new part	1.5511943810330642
language learner	1.6255008558833732
operators used	1.504934031784749
words related	1.7131368582787394
scoring algorithm	1.584950008544939
two papers	1.6822413040167319
national library	1.7834754489412699
match one	1.6289349127929256
500 sentences	1.8470312630316317
equal size	1.7261608475873556
logistic regression	1.6240071374512288
contain exactly	1.6210934563561632
operational system	1.6086349493805268
combines several	1.522760613339579
major tasks	1.5749574214167732
000 training	1.6884273843326405
cluster words	1.5731122769149348
added features	1.5078976439458076
deletion errors	1.5500434040293412
hearer may	1.5351323319208936
entropy based	1.5531074607024329
tree bank	1.7400666768690591
incorporate information	1.6481174069026685
specific speech	1.5514607572627885
general purpose	1.8642821944883312
help disambiguate	1.6724173909604771
hybrid approaches	1.5942408057154909
recall levels	1.5951898076646271
new documents	1.6851060647094247
automated methods	1.5421588730604814
parse time	1.7615471502286704
semantic distances	1.50811156264416
system seems	1.6056035510036821
particular type	2.0562026553732586
objective function	1.9260127795057633
sense 2	1.65223995683065
current topic	1.6001663929519294
multiple dimensions	1.5466472792181065
example could	1.6289349127929256
complete specification	1.62412689036662
sentence e	1.694251169313981
template filling	1.7271054817184952
words w	1.9535338477242752
three test	1.5955439842955634
present example	1.613364934425665
xml structure	1.5189601534336892
system always	1.5559715171228279
simple technique	1.65303397348486
deeper level	1.6724173909604771
second constraint	1.6329374072287361
corpus collected	1.558193976095525
answer pair	1.6072712252535497
smaller training	1.7421539136342967
next time	1.7019709298887995
dekang lin	1.5036575034310684
different way	2.01890944945138
tool developed	1.5771944393968738
assess whether	1.643362624369706
describe two	1.972671971380869
shows promise	1.5547189562170496
becomes even	1.6783930166760777
syntactic tags	1.712977674362857
complex ways	1.5645241345053709
grammars may	1.7130771295020022
complement clauses	1.5929943683260914
possible permutations	1.6137947517368632
parsed data	1.6690820382866072
one frame	1.646279797293984
syntactic restrictions	1.6650642982613948
two differences	1.5645241345053709
contains three	1.9227704790696971
phrases cannot	1.567922330580758
mechanical translation	1.5281090902899641
jose mercury	1.5756869131546112
lexical unit	1.8021384904016675
conservative approach	1.5706734993195162
processing technologies	1.6190328102134979
please see	1.6544208897318429
optimization algorithm	1.7118703857255926
document clustering	1.6327548242515841
documents using	1.8204269257572638
many contexts	1.636252690439584
rule would	1.6126881299047024
across corpora	1.5130746305264333
speech ambiguity	1.5170437542680002
pos tagging	2.0715692009358033
underlying semantics	1.6570039092175395
preprocessing step	1.8641512890382863
rule might	1.6580606251764167
palo alto	1.551813542421915
semantics interface	1.660264541736554
al l	1.6445809590877876
translation using	1.752258857023734
reasonable approach	1.5547189562170496
second stage	1.7118138882870615
cue phrase	1.6218660871235095
trees would	1.5177700906840073
related problem	1.731146052909204
hence cannot	1.591823957496618
f n	1.5517323575046895
independent components	1.554089533378209
specific texts	1.5771944393968738
multilingual generation	1.6094785532898026
full search	1.531646256105143
conditional probability	2.242009372686182
new template	1.5266242261615062
develop tools	1.5156323250636554
simple case	1.8817829937615411
entropy method	1.5367968822306057
parse tree	2.3434656034101033
modi cation	1.512405389232597
probability threshold	1.5544232755483263
spoken utterances	1.82748196856942
linguistic resource	1.6083463363279686
particular target	1.6502761822926497
tag information	1.7067275231613273
accurately model	1.507007268326853
quantifier scope	1.6951051137711826
dictionary may	1.6152888150903435
tagging errors	1.8896632645419955
second utterance	1.6906026319861005
also work	1.667913900271795
central question	1.5511943810330642
monolingual dictionaries	1.5919775262523828
recent attempts	1.5328333954091753
lodhi et	1.51910459028178
4 characters	1.636252690439584
negative weight	1.5177700906840073
output units	1.5200363617367738
decision problem	1.5092769060273568
verb object	1.5707376783218978
feature functions	1.897438861970641
following assumptions	1.7635102323276162
certain extent	1.626034093404664
active edges	1.6765317724045656
following case	1.6377134026552522
sensitive language	1.541108204234909
special status	1.711151422700744
smoothing methods	1.6815086337150082
restricted set	1.8397945130419209
lower accuracy	1.760256149909878
different system	1.70689006631715
aided translation	1.7056807149417281
particular part	1.6666455667887732
word v	1.555166582850379
cambridge university	1.6371439398153491
feature weighting	1.5945472703366552
inflected word	1.7415453092733402
output based	1.5511943810330642
represent different	1.8652033957672844
crucial use	1.615262062995654
sense would	1.507007268326853
allowing users	1.5869693547698929
disagreement among	1.5514514275224367
word distance	1.5540629676268218
structural similarity	1.5491738524525975
word matches	1.588557061265679
lists used	1.5582958710952315
de col1ng	1.640910700935025
dependency relations	2.056028434068196
noisy channel	1.8062622421579957
language utterance	1.5388356508720742
speech interface	1.5310694524267352
sufficient detail	1.5739592165010823
method depends	1.6137947517368634
method assumes	1.6276749302936424
novice users	1.5586401652009665
criterion used	1.595491118313467
initial phase	1.6246289167870047
derivation tree	1.9378182533282944
standard syntactic	1.5547189562170496
every document	1.704614565156584
ranked list	1.9826157023217725
sentences per	1.8936495045441681
results vary	1.5338735539822874
independent system	1.6980815656909076
text planning	1.890935816240673
partial semantic	1.7106802946855557
using statistics	1.6010101084822783
three language	1.5929726936913173
unique label	1.50119175880199
gaizauskas et	1.529606820290264
interesting fact	1.579873144038688
model currently	1.5111306094308554
paper uses	1.7051629942513449
fewer parameters	1.5803272240852047
rich set	1.8622059153436292
model parameters	2.1019004589227426
values assigned	1.6980754127121318
many kinds	1.9338875661777017
full range	1.765508768655803
grishman et	1.7417743368004273
simple parsing	1.526993068994652
system recognizes	1.69776499200297
concepts rather	1.5226149809040492
fast access	1.586137214651611
system shows	1.742920534919623
clause structure	1.7782313048242933
last row	1.8814668719596461
level units	1.5354510398033077
specifies whether	1.586137214651611
entropy framework	1.6703833122001477
user feedback	1.6487334099083149
case relation	1.5401944545868433
paper may	1.5177700906840073
final states	1.9196154702938433
making reference	1.5511943810330642
relevant domain	1.5582958710952315
rule could	1.7573141382441586
higher precision	2.001086282160929
recognition using	1.7867811279202186
difficult tasks	1.507007268326853
predefined categories	1.5381766791708387
language whose	1.5991584292605265
language applications	1.9404017705209013
relevance judgments	1.7211413143057808
scoring functions	1.6552907243473016
language structure	1.7906053955108383
elements may	1.7436292604420185
full form	1.6347777481223793
small test	1.7631887903247738
contains many	1.961911856892683
upper level	1.5676517392350746
structure would	1.79528099947873
x c	1.7300568010126556
source word	1.947860423355715
ambiguous words	2.031957202481499
othe r	1.548378091852222
several phenomena	1.5300728913253212
applications require	1.6920919321535788
section briefly	1.7229550745276576
c given	1.5245348740884508
structure might	1.5645241345053709
local structure	1.5485559840271728
second sentence	2.0919350800496623
stuart shieber	1.7044865459645906
varying levels	1.7237747585396452
lexical units	1.9508881649656677
first implementation	1.7723731972113648
support vectors	1.571829529788342
whose purpose	1.661447240829003
whole expression	1.5432756752761154
semantic phenomena	1.7047689458394852
factoid questions	1.6666856759566835
following entries	1.5645241345053709
four kinds	1.7460863667122979
specific objects	1.5181144837827718
tile one	1.583051127543801
two perspectives	1.5731122769149348
nodes along	1.5269930689946518
parsed corpora	1.6266057481361194
preferred interpretation	1.6549541575945776
phrase boundary	1.7294025905588484
problem addressed	1.5992286050546423
specific questions	1.6411992014239676
application task	1.5320698902069676
weighting scheme	1.8921302699774245
single system	1.794505814809103
words cannot	1.756257461041527
possible cases	1.6734445616305185
relation would	1.6320354670147565
ill l	1.6164339756999313
input semantic	1.5671012235059067
accurate description	1.5445134575869865
phrase alignment	1.6105850417435672
identical results	1.583051127543801
interchange format	1.5423485783857451
assign weights	1.5378646275607575
enough knowledge	1.6593965399315964
possible parse	1.7409064743358629
verb phrase	2.219982475681732
important semantic	1.6458797346140277
acyclic graph	1.9076337543262682
times greater	1.5421554529658548
analysis task	1.5648923336080593
modified version	2.1050745841651217
simplified example	1.7280057513570362
input document	1.5895509685998348
language sentence	1.9215063655033302
clustering process	1.6800564655859156
journal treebank	1.6215352580385196
theoretical background	1.7279919232921825
lexical words	1.6002672102353717
special properties	1.522760613339579
every set	1.636127944690715
darpa communicator	1.5775269048312834
systems need	1.7821491510973333
tree transformations	1.6179791736947504
major types	1.80433756561019
discourse processing	1.9931209823781113
observed word	1.6043878265755396
occurring words	1.8634502855720059
quality translations	1.6661309301322418
value set	1.5041134997716232
possible class	1.5288687562201928
learning strategies	1.5479558184161362
specific languages	1.5445134575869865
interesting differences	1.5378646275607575
different machine	1.7727308472331926
4 features	1.5217605875570066
sentence per	1.6012319720994657
nlg system	1.7453990420078753
telephone speech	1.6450215984630603
many researchers	2.1262830168055977
semantic indexing	1.7403926207104194
phrasal patterns	1.535162183583535
chinese translations	1.6031211652881696
brown university	1.5839427016111838
search may	1.5645241345053709
model according	1.579873144038688
reasoning processes	1.6468866812474956
successful analysis	1.524534874088451
side effects	1.7277553851213783
careful analysis	1.6708590810843598
marcus 1980	1.5378646275607575
significant proportion	1.5886561992520427
approaches using	1.627796791624738
evaluation strategy	1.6144398629557442
problems associated	1.8613079003063457
example 12	1.504934031784749
problem seems	1.583051127543801
features per	1.5087853581980926
english noun	1.854286899346197
pronominal reference	1.7551659528040244
strong relationship	1.506173888464562
user queries	1.889959151751953
10 documents	1.6532918980047882
software engineering	1.8157474613226625
programming method	1.5169861570335665
different actions	1.5581939760955252
gives results	1.6802940634668402
information like	1.734954551774483
power set	1.6128707567839276
values indicate	1.5269930689946518
country names	1.63729020511694
discourse marker	1.623015967134346
grammatical information	2.015701570237328
current mt	1.5066881819459041
rules need	1.62291510937581
greater number	1.8146901713459476
4 contains	1.7210742795514369
one level	1.894451097786487
runs using	1.5278165817298155
including text	1.6266256462651403
current text	1.7127967389448444
verb pairs	1.6114238098072504
one annotator	1.7338810001489502
functional information	1.659813706876145
sense reasoning	1.6101257230903023
specific processing	1.5511943810330642
structure could	1.7229556053533144
engineering research	1.5466850417882805
right node	1.5166881074908263
parsing methods	1.8885992288621476
various tasks	1.790450861982897
system developed	2.0695953626667447
errors occurred	1.6411558246537032
first mention	1.6132897424197858
three senses	1.7740412159111871
system presented	1.8469520867451377
successive steps	1.5582958710952315
monolingual corpora	1.625562322108661
machine dialogue	1.5662660232272712
encode information	1.7504812345368606
tagging systems	1.5519900193858673
missing elements	1.5041134997716232
verb constructions	1.6932506644448404
approach used	1.6656740595572472
par des	1.5776105638269788
two learning	1.5760526619236255
data consisted	1.7867811279202186
estimation techniques	1.677668690260293
lexicon lookup	1.6212113212773738
values used	1.6502761822926497
previous examples	1.7266279442877566
meyers et	1.5353827155830526
new alignment	1.5365331267030269
published work	1.583051127543801
input set	1.5719275298188968
sentence segmentation	1.7278531951010072
given frame	1.647799623617861
two evaluations	1.619472687377359
types occurring	1.513130125383453
foreign languages	1.809730535347392
new dialogue	1.5956707385069846
operation called	1.6256278638449597
average user	1.513130125383453
class based	1.6115999023629675
different phrases	1.6514103959727489
phonetic representation	1.57425918739941
semantic meaning	1.728161735649611
parser requires	1.579873144038688
best parameter	1.592620200341168
syntactic disambiguation	1.7064156595426567
improve overall	1.6213967902634532
labels used	1.664540718454114
dans les	1.6459898711368313
one use	1.5547189562170496
different places	1.7650139064090153
human factors	1.5784699981130208
helpful comments	2.240003901225225
ten percent	1.5371982986173842
formal point	1.6266256462651403
user specifies	1.6037378902512214
flexible system	1.5887468924062063
class words	1.9707736016258848
previous stage	1.7293150040551049
following conclusions	1.6593965399315964
adjoining grammars	1.983727003315882
attachment points	1.562251081986695
decisions concerning	1.5111306094308554
general applicability	1.6164339756999313
based document	1.5648923336080593
second component	1.836686821195295
overlapping features	1.6081687115613255
negative ones	1.5773133211810766
support vector	2.177270540869106
lexical pattern	1.545594581538439
simplest form	1.7120828968340218
punyakanok et	1.5414231269575729
given information	1.723238395210732
technique presented	1.5611231576018243
text information	1.6776629517899067
semantic errors	1.5387082898744961
problem encountered	1.507007268326853
iterative scaling	1.7751565017736874
c c	1.8287369326388228
different english	1.6213967902634534
two points	1.5227319694183312
previous utterance	1.8570373280194865
unix operating	1.6214578999580553
tile fact	1.6190328102134979
differ considerably	1.6845577236692315
null figure	1.8535783236360657
model makes	1.8777755025166785
following principles	1.705774165806181
probabilities p	2.031479362157712
examples demonstrate	1.5459182428709468
subsection 3	1.6196264913696634
simple present	1.5816276775846598
noun classes	1.6561453103595858
many authors	1.7088092136999036
lexical choices	1.7433130346450518
unique name	1.60189988307561
next section	2.569398564173312
utterance must	1.567922330580758
parsers using	1.5354510398033074
text chunking	1.656999600661141
test queries	1.6167162888953146
relevant facts	1.5969765857928704
markov chain	1.6748358557641705
appropriate discourse	1.6147192301219748
matsumoto et	1.8039765153881078
time parsing	1.6075783640375185
results according	1.5887468924062063
levenshtein distance	1.6093296101979033
use heuristics	1.6213967902634532
answer candidates	1.507707512324008
various examples	1.5188051370135929
language component	1.772422894770946
constraint c	1.6322265411135053
column gives	1.7634700042186353
basic operations	1.6562742970295767
morphological processes	1.7174038492354042
input without	1.5749574214167732
different techniques	1.8513473125722297
best precision	1.6533416103458913
theoretical problems	1.511205120616144
matching pairs	1.506173888464562
variables representing	1.5245348740884508
three stages	1.9325503504576045
model theoretic	1.619043850236166
different grammatical	1.8242753544773058
recall errors	1.5321810481204974
substitution nodes	1.6372794381959626
model predicts	1.7734073935738353
50 sentences	1.7102842123854296
words approach	1.5383664160010768
choices made	1.8578445188092176
level translation	1.5089007023912593
logical object	1.5679332442954022
positional information	1.7234949533162558
specific modules	1.5037367200448983
complex language	1.6802940634668402
capture different	1.6056035510036821
using probabilities	1.5707376783218978
data collection	2.0307343525829387
g u	1.516993433520448
word cluster	1.5036589189128708
disambiguation performance	1.5111236346881762
test conditions	1.6169635240879603
document summarization	1.899307366974617
verbs must	1.5338735539822874
prior knowledge	1.9643320041316923
surface words	1.6847314174748618
model described	2.082256031711772
technical text	1.5884654558997142
first character	1.8291680437478839
right side	1.9912275304722848
structures contain	1.522760613339579
two fragments	1.592111614460519
smaller set	1.8505426117197377
one task	1.7786800160328815
une autre	1.5150717731521737
based recognition	1.587161501621602
possible uses	1.5217605875570064
lexical translation	1.673077661670734
software tool	1.6581363223870687
diverse set	1.5806234267011035
detail later	1.5739592165010823
better coverage	1.7178162994368975
based qa	1.5140838598892352
also need	1.8726509046126916
develop techniques	1.5547189562170496
intermediate steps	1.5188051370135929
weight given	1.646226844257487
thematic role	1.8482500602810938
syntax alone	1.5241968977937237
language engineering	1.9160996521197873
reasoning systems	1.5806234267011035
object language	1.6130953582849266
empirical data	1.8850644801451177
communicative act	1.6622858957480378
foot nodes	1.6024203815530134
previous sentence	2.0110773304746177
segments within	1.5211544130435135
different weights	1.8116529108134674
single english	1.510181823566108
lexical element	1.5756869131546112
large test	1.5974644755426062
training text	1.7696509383801244
node may	1.7991879462554783
grammatical structures	1.8387589152547814
tests performed	1.526993068994652
case time	1.5868782222878055
role played	1.8170465211631583
hpsg framework	1.5288687562201928
score values	1.524534874088451
parsing task	1.711501189079204
separate test	1.655018484805693
great majority	1.6339957705783554
model results	1.5256428192011386
functional structures	1.6502734975434228
independent variables	1.5315936431761732
specific example	1.6388987829322057
strings whose	1.5706734993195162
new errors	1.5637776281819353
basic rules	1.6324314291325863
answers may	1.5459182428709468
using rules	1.8719637829932534
enormous number	1.6661204761131607
two paths	1.7393261496659205
text would	1.842157298093909
july 2006	2.325936388178086
pos tag	2.10135574766632
linguistic examples	1.5036589189128708
du texte	1.54220043537997
significant advantage	1.6477541750656948
structural configurations	1.5266242261615062
senses based	1.615262062995654
less training	1.7230579229098404
kind described	1.5547189562170496
1000 sentences	1.6441796864302127
3 system	1.558672288689075
lower perplexity	1.6733445309961685
possible tag	1.7458914003754722
different queries	1.5378646275607575
two sentences	2.2896708662817007
parse failure	1.5054226276451934
various applications	1.8119659926140157
use knowledge	1.6862623517634696
concepts associated	1.5117685557139826
dependency based	1.5037367200448983
acceptable sentence	1.5156323250636554
3 illustrates	2.0207400997575142
substitution operations	1.5408560126058508
various entities	1.507007268326853
simple bottom	1.5611231576018243
open classes	1.549729132180171
class categories	1.5164911279959081
implementation based	1.507007268326853
since wordnet	1.6010101084822785
syntactic formalism	1.548378091852222
initial stage	1.7368520569668005
preference rules	1.5734924027984545
arabic word	1.5078474939041207
related phenomena	1.6012319720994657
carbonell et	1.5117685557139824
initial configuration	1.5621227545901766
algorithm seems	1.5408560126058508
work differs	1.789586914958145
fundamental difference	1.6084968011212868
semantic contexts	1.5156323250636554
v u	1.5316566430519973
different application	1.7529570749550978
conversational interaction	1.510181823566108
two studies	1.5959348121784669
sense per	1.7362431857107619
system found	1.6023597487734618
different sizes	1.9454664702763995
draw attention	1.5288687562201928
major difficulty	1.6783930166760777
flexible approach	1.5037367200448983
paper extends	1.644763252134947
dialogue strategies	1.6557375907441394
english nouns	1.7667510450525168
data used	2.199078170693163
spelling variants	1.516491127995908
combines two	1.773475133795545
k x	1.5576326944000112
mary likes	1.5870950865396134
similar meanings	1.777003269519583
nlg techniques	1.5177148170228878
initial value	1.7226557503819233
german grammar	1.5959939505178844
different parts	1.9086994313369332
recognition hypotheses	1.5144479606261376
given level	1.7435972121129724
different derivations	1.6520042621483706
probabilistic finite	1.5275591303469285
represent information	1.707531013552988
text level	1.62144031588269
aotrr 1992	1.5511943810330642
disambiguation system	1.756238300621895
control mechanisms	1.587700203918434
particular slot	1.5806234267011035
dictionary word	1.5260185053825464
method needs	1.6169986020906966
representation format	1.5729380896393128
particular meaning	1.6773210626499964
computational implementation	1.688641176416493
frequency values	1.5378646275607575
primary interest	1.62087177324138
additional constraints	1.945041349551252
appropriate language	1.5612815851895034
categorial grammars	1.83239646165371
knowledge may	1.8189037239643289
state transducer	1.8888713697186796
nsf itr	1.5806234267011035
proper word	1.513130125383453
subtree rooted	1.796714930692821
errors may	1.803601051155516
collection contains	1.5612815851895034
anonymous reviewers	2.3603839129009767
text structures	1.6685844731202923
roles assigned	1.506399225957958
improve information	1.5559715171228277
trained models	1.7124286445136205
cases without	1.6164339756999313
covariance matrix	1.530320126517967
later stage	1.6133806382672107
many thousands	1.5839427016111838
necessary condition	1.7844248250215464
structural level	1.594773126307433
subjects used	1.5061557323373171
let p	1.9284462639985067
summaries produced	1.5888688730856373
using methods	1.635876749820076
results described	1.6845577236692315
onr grant	1.62412689036662
corresponding data	1.5656419863875413
l w	1.507007268326853
phoneme conversion	1.6045090919775342
generic information	1.5180580646513062
causal relation	1.5536031447032237
detailed analyses	1.6056035510036821
model trained	2.0061544609359174
sentence shown	1.5473635513992758
larger discourse	1.5334916422267963
well understood	1.9809448682002577
steps described	1.530072891325321
whole story	1.6031224303496519
local optimum	1.6159284666509541
empty categories	1.6375947606009629
sentences generated	1.7436292604420185
likely tag	1.6619527821529805
occurring text	1.6029414918504359
entire corpus	2.0214466211367217
language change	1.525378784009309
assumption underlying	1.5906852501842725
apply rules	1.5906852501842725
sentence position	1.6329323301604215
linguistic applications	1.739718085106179
semantic lexicon	1.7787994102305256
cluster c	1.5063836293071025
segmentation rules	1.5532249734208623
linguistic structure	2.074327565052652
processing modules	1.859417122283655
french text	1.6420588978478803
often contain	1.887223474459146
english training	1.6096503035106329
pair grammar	1.7492349936440599
time using	1.802909129659832
features shown	1.567922330580758
possible value	1.7722692119182182
basic method	1.6023708245704025
word found	1.583051127543801
one chunk	1.529656722248166
longer sequences	1.509778922004791
common pattern	1.5781539692435587
test data	2.4317327921182836
two domains	1.8841842417481083
disambiguating information	1.591444238245721
form one	1.7229556053533144
entire parse	1.6570039092175395
nominal compounds	1.7262074686681994
suitable representation	1.5582958710952315
mail address	1.50119175880199
performance gain	1.7551594794645844
single variable	1.554089533378209
word length	1.843239366139457
verbs occurring	1.5525419192763326
web server	1.5916025190190952
core meaning	1.5523757278333838
first point	1.7873685540637707
algorithm checks	1.5188051370135929
average sentence	1.9256023736642955
present evidence	1.5806234267011035
motivated features	1.6093233396780433
feature logic	1.5446494084571656
alignment techniques	1.656074792582219
whole process	1.952757367316477
noun entries	1.5676517392350746
increasing amounts	1.55955788508706
found evidence	1.5111306094308554
entity detection	1.6763807481892299
single constituent	1.678973586324036
two persons	1.5640746467440696
given rise	1.6056035510036821
head h	1.5640746467440696
travel agent	1.5949369433787157
involves identifying	1.6822401779866478
rules must	1.6151206813618408
general corpus	1.554251777517706
relevant context	1.6666455667887732
document processing	1.5927926823891998
4 discusses	1.890104821014712
new contexts	1.50119175880199
righthand side	1.5551665828503791
segmentation based	1.616031378434591
specific problems	1.767564348904915
grammatical patterns	1.5225911753473969
primary purpose	1.6785607164239145
papineni et	2.010454145040082
quality speech	1.5709154820584152
speech signal	1.9681496853118021
understanding process	1.7820988140100962
nonterminal labels	1.5790788048883706
first rule	2.0086440721959504
lexical selection	1.7290591393731456
professional translators	1.5877002039184338
5 shows	2.390631179224772
candidate list	1.7213071175405772
resulting data	1.7348520455220569
utterances within	1.5971626956604805
different agents	1.5361450492096227
trees built	1.556672071876148
algorithm shows	1.507007268326853
possible interpretations	2.0191374718642883
less context	1.513130125383453
accuracy drops	1.5288687562201928
different dictionaries	1.565058776904122
based theories	1.6569504373202975
larger numbers	1.5459182428709468
probability model	1.9907606579627888
translation time	1.523907736875841
relative difficulty	1.5254706284993405
word senses	2.2424859940015436
number n	1.7470962887320456
automatic sentence	1.6349068916737024
accusative case	1.78268452665747
raising verbs	1.5991096543594099
many discussions	1.5338735539822874
perplexity reduction	1.559929596891282
polynomial kernel	1.741538481267515
example shown	1.834210957254088
specific corpus	1.626959163831613
verb morphology	1.5403034171359224
morphological parser	1.5122639093520438
wsd task	1.6541717910258056
20 documents	1.5375958957980183
distinguished symbol	1.5731122769149346
new scheme	1.5188051370135929
called feature	1.5390877051636749
bigram language	1.8176391281796913
generation process	2.163776986262344
message understanding	2.077717973121344
system correctly	1.7282654604347374
specific cases	1.698978605212456
word belongs	1.8589504521394966
search space	2.3200115253842393
obvious difference	1.5188051370135929
various parameters	1.6315901311540408
two base	1.55159780071999
probability function	1.613278290891074
recognition algorithms	1.7514017754422089
bilingual corpora	1.9928569387501736
general lexicon	1.5761065049083909
category names	1.6128408409726123
w 2	1.7105120570718246
following query	1.6479791814036409
noise introduced	1.6845577236692315
broad semantic	1.561596045888415
sentence generated	1.5676517392350746
cue phrases	1.716090974071992
edges represent	1.6229151093758096
local discourse	1.5878351563719835
entity classification	1.6088481177921536
second test	1.744966138573997
first segment	1.6152888150903437
like verb	1.5408560126058508
vector machines	2.12563107185956
models developed	1.5992286050546423
standard measures	1.6431586438232257
several researchers	1.945088462582941
theoretical perspective	1.591823957496618
original word	1.9293762568958954
classification methods	1.7355198204387265
frequent ones	1.6235405672398668
given task	1.8072861590049856
major classes	1.6637019978300875
union operation	1.514469500969361
language developed	1.5188051370135929
models presented	1.6442814238185641
approaches include	1.5887468924062063
version 3	1.7400743655251079
new lexicon	1.559137125070758
algorithm creates	1.5408560126058508
scores would	1.5582958710952313
traditional sense	1.5156323250636556
analysis performed	1.513130125383453
appropriate senses	1.561113510315716
final state	2.0274347438708533
features generated	1.5487453399078306
wordnet noun	1.5985543375863656
2 indicate	1.5445134575869865
basic categories	1.7243471170821425
specific class	1.6751782252712877
segmentation method	1.60618260118681
great variety	1.7008700806970682
analyses produced	1.6514103959727489
der beek	1.5177700906840073
set contained	1.795383007234003
feature representations	1.6274503739058592
longer n	1.5851219252927344
lexical disambiguation	1.7800480934865652
phrases like	1.9967352204286026
probability pr	1.740753149195013
probability models	1.7837310700631677
template structure	1.6407170829765727
three states	1.532714913365084
response times	1.6446141204626565
chinese texts	1.753509929591799
second time	1.806013161304568
identify noun	1.5338735539822874
40 sentences	1.5556301839047322
german sentences	1.6371439398153491
partial analysis	1.7105667835720224
better fit	1.5645241345053709
head words	1.9794236487906536
np node	1.787025396551435
type definition	1.714316349266601
groups according	1.6266256462651403
probabilistic approach	1.769780906312884
1 states	1.5676517392350746
edit operations	1.6345649394754806
future experiments	1.776409225489638
new kinds	1.5771944393968735
best classification	1.5731122769149346
values 0	1.579873144038688
tile grammar	1.608299184330961
subsequent sentence	1.5418612528934028
types may	1.692183044618731
situation described	1.7102278623622071
english bilingual	1.609228225846834
uses information	1.8370748193370194
russell et	1.5139194401612355
operational semantics	1.5153789779603475
l r	1.5811086650859956
straightforward task	1.5395099438048587
corpus provided	1.643362624369706
subject phrase	1.5275591303469285
categories within	1.529606820290264
particular contexts	1.6950058532679169
phoneme sequences	1.5117222072898806
probabilities may	1.5300728913253212
general procedure	1.6332313690513016
scoring scheme	1.6242059667933528
inductive learning	1.6929332199529774
third component	1.6342922036019591
attachment site	1.635592977309385
objective evaluation	1.7019709298887997
sparse network	1.501575214823395
test corpora	1.967438775210582
disambiguation decisions	1.5053790653551418
particular corpus	1.7551855586895913
ill order	1.7447050080321944
paper makes	1.507007268326853
development tools	1.5006743580950452
structure built	1.7181848619222264
thompson et	1.6573935993462616
recognition time	1.5244302129704104
soft constraints	1.5846169458491282
isa hierarchy	1.5044682565519583
reported performance	1.597317007886449
paper suggests	1.6112034493728657
user cannot	1.567922330580758
various elements	1.5037367200448983
topic shift	1.6858300043387329
new implementation	1.5459182428709468
syntax rule	1.5011525773696108
order terms	1.6205729699594007
grammar model	1.6388822924413124
fall outside	1.7449291525196893
subcategorization frames	1.8875400527947535
one solution	1.993331369027447
relevant document	1.776220044533375
tense forms	1.6129530110701733
experiment involving	1.5771944393968738
second tree	1.5459756252045591
data derived	1.6548911840144862
figure 3	2.6970945124092998
discrimination net	1.530320126517967
algorithm developed	1.643362624369706
surface text	1.6915343752306193
unigram probability	1.6394604024414847
transcribed text	1.524534874088451
special word	1.6137947517368634
hidden units	1.58819907211269
general properties	1.7425248074331754
additional problem	1.6527294781610564
various ways	2.179549459241147
context independent	1.5823051515128412
preliminary results	1.826371821229519
reference system	1.620458561878074
inflectional paradigms	1.590816767591691
empirical evaluation	1.8281270363679556
simple term	1.5156323250636554
highest average	1.5395099438048585
dependency accuracy	1.577173767927178
relevant portions	1.5188051370135929
second language	1.9223824785899055
rule shown	1.5893265024752146
frequency terms	1.6442193465395285
roman alphabet	1.59555403645897
patterns automatically	1.592620200341168
words surrounding	1.7683441808994629
untagged text	1.5647847313011076
approaches require	1.7088092136999036
indicate whether	1.6196294771681137
xml format	1.889259393059419
important fact	1.567922330580758
finer grained	1.6429565433589586
based mt	1.8900147206486972
one disadvantage	1.5547189562170496
traditional grammar	1.5515397035329022
performance increases	1.6477541750656948
issues regarding	1.5547189562170496
tagged texts	1.6101516352251333
also deal	1.5445134575869865
development system	1.5290368564371193
use general	1.5156323250636556
candidate pairs	1.5848946808057316
grammar models	1.5424609776958955
horizontal line	1.6710495829972014
english wordnet	1.5023190551629528
local tree	1.6538126073318646
expression patterns	1.5281860741587594
verb phrases	2.158764427165934
application rules	1.519839541601499
network formalism	1.5647505415436789
particular kinds	1.6502761822926497
example words	1.5222638249634683
use data	1.7178162994368975
speech sequence	1.5309731874553978
several authors	1.7179570254828351
hierarchical organization	1.791885469677209
original definition	1.6265370361225344
next level	1.871440472926594
human understanding	1.5935818176824688
reference number	1.5131301253834533
performance level	1.724467168687773
hav e	1.5637776281819353
model directly	1.586137214651611
possible attachments	1.5511943810330642
limited time	1.6553530875798104
second aspect	1.6056035510036821
disambiguation techniques	1.5706734993195162
parse structures	1.6480356029652419
linguistics community	1.6977890880751751
dependency links	1.641601528811539
second mode	1.5037367200448983
forms used	1.5432756752761154
test instance	1.7580329490309095
unification algorithm	1.7434147427016309
50 times	1.62291510937581
default rules	1.7036654496237458
2 reports	1.5408560126058508
techniques based	1.8645168845635849
similar methods	1.6699400022790052
positive weight	1.5037367200448983
correct classification	1.706359894416354
get lost	1.5008529684573726
every syntactic	1.5767760138793054
context sensitivity	1.6073715579162529
linguistic elements	1.6540508032466854
strategy used	1.8265428287052055
different parameter	1.7754459833774268
autre part	1.5371654689121383
answer questions	1.9933939980839626
stative verbs	1.5055580911855762
des mots	1.537505091881899
string must	1.5037367200448983
log scale	1.533540614646662
formal characterization	1.5612815851895034
identity matrix	1.5306061495832655
principle behind	1.5111306094308554
possible connections	1.530984627359544
source sentence	2.010844494066263
level formalism	1.5365331267030269
semantic well	1.529606820290264
training iterations	1.622981102910659
boundaries using	1.5482870987025659
4 evaluation	1.5309846273595442
words like	2.1533935945678113
key word	1.5093553489751728
relevant set	1.5371571001451403
model using	2.134826047703201
partial syntactic	1.6828679251614567
core lexicon	1.5538680014808501
particular implementation	1.6462268442574868
looking center	1.7118405937574694
simple question	1.6107689014994975
word token	1.7925795351046072
correct values	1.549729132180171
run using	1.764552527683778
use feature	1.6614869878819327
unique path	1.5078976439458076
narrow scope	1.682296784032074
higher recall	1.9418258996498405
head noun	2.202665377844573
processing systems	1.9912790555906694
use tile	1.6320354670147565
sentence pattern	1.5499327837566104
multiple solutions	1.522760613339579
best translations	1.553296096968758
driven techniques	1.5156323250636554
corresponds directly	1.5111306094308554
process takes	1.7158821441137806
discourse may	1.6838006798046168
classification task	2.1357131613377325
vast majority	1.8315368811148676
linguistic relevance	1.6344743064287952
vocabulary speech	1.7945448060133413
ones based	1.5511943810330642
following question	1.7690105662542992
analysis based	1.8798479714347573
predefined set	1.6849971640223216
candidate feature	1.525884567273902
use human	1.5156323250636556
exponential complexity	1.509778922004791
linguistics computational	2.0564366684283035
directly compare	1.6911567924922841
disambiguate word	1.6876319470917474
a1 a2	1.5064508452069112
dimensional space	1.9622716509981963
data structure	2.2441116348419676
another form	1.6570039092175395
speaker wants	1.703522559041963
actual words	1.8502168703765998
important differences	1.8152838735013597
data types	1.8193650528089336
dialog context	1.5214166155777633
error rate	2.359099703077575
additional attributes	1.5037367200448983
textual content	1.5844553965702959
particular properties	1.5973170078864487
corresponding number	1.5611231576018243
keeps track	1.9890355468924057
retrieval process	1.7410948254282994
matching approach	1.695688610480095
based analysis	1.8440100278154063
possible word	2.07489270366516
previous discourse	1.8930734444733663
smaller ones	1.7002798641346384
second column	2.072963315455107
simple test	1.5445134575869865
previous results	1.5655715603286047
structure shown	1.8098163248304047
inversion transduction	1.5215688769149451
n x	1.8032710823209948
selection restrictions	1.6954480041943136
value obtained	1.510181823566108
based ones	1.6125328925113007
examples suggest	1.5536240126687624
bold font	1.522760613339579
number feature	1.5679440886127702
individual languages	1.5617399533493153
similar form	1.507007268326853
common ones	1.5611231576018243
conventional methods	1.593962518173376
stochastic language	1.6900887919720113
first definition	1.586137214651611
two times	1.8108881035691438
segmentation accuracy	1.6644142604430008
lexicon development	1.619457641926315
current goal	1.5658064437585617
already exists	1.8751819476934337
model 3	1.680584242273933
weights based	1.55955788508706
given state	1.789031222003582
style parsing	1.5459182428709468
right processing	1.594773126307433
empirical approach	1.5553381419921846
high agreement	1.6794247694221331
larger constituents	1.627796791624738
complete representation	1.5707376783218978
single parameter	1.65545581040525
formalism used	1.7941293597161414
type coercion	1.5404842331944684
proper name	2.0455158154800337
cannot model	1.5547189562170496
system finds	1.7908149971194876
observations made	1.6246289167870045
corresponding sets	1.5217605875570064
summarization research	1.583051127543801
following forms	1.731146052909204
differences may	1.6031224303496519
resulting classifier	1.5702560143399065
encouraging results	1.6051823454832994
modelling techniques	1.5354510398033074
complex information	1.6646686690727936
every sense	1.606339064235895
speech data	1.9651935299507077
r c	1.7597965469833574
tags using	1.5269930689946518
strong indication	1.6084968011212868
last iteration	1.5378646275607575
criteria used	1.8063973711864887
gives information	1.6339957705783554
tables 4	1.8185963874271713
mental states	1.625668521126475
journal section	1.5081682632911078
analysis method	1.6882956424431295
sentence translation	1.536518709476098
concept names	1.610151635225133
learning model	1.735879611099064
higher accuracies	1.5217605875570064
certain property	1.5408560126058508
w e	1.9114500638577643
e e	1.8769384883331355
human performance	1.9764615512960013
utterance may	1.7573123843210445
association scores	1.5294822362894396
plural forms	1.7889135033114565
results confirm	1.7471845324497086
test sets	2.2913167338273643
first phase	1.5898946875856725
general overview	1.6246289167870047
arc labeled	1.563687738145949
contract number	1.818771077201047
parser without	1.6565475758379593
aligned corpus	1.785063462847377
length three	1.5354510398033074
alternative parses	1.5792035811631284
inui et	1.5107844011873681
interesting way	1.6213967902634534
aro grant	1.7651107526366039
structures involving	1.5408560126058508
different conceptual	1.5177700906840073
represent linguistic	1.5886561992520425
next task	1.6289349127929258
recent progress	1.5959319094939208
extraction process	1.955078640963684
acoustic cues	1.539107015933578
method consists	1.8005498094342076
precision scores	1.8620663974791491
real speech	1.526993068994652
desired result	1.7443551339933419
various discourse	1.5645241345053709
likely word	1.7432301526183056
philip resnik	1.5156323250636554
slots filled	1.59555403645897
search technique	1.5365331267030269
results using	2.1962462358631982
words observed	1.50119175880199
quirk et	1.8464015894946761
syntactic subject	1.6641479111775095
extraction problem	1.6259474792597857
appropriateness conditions	1.5149639400339465
software architecture	1.612840840972612
best candidates	1.6641214008902543
analysis could	1.6548911840144864
several thousands	1.5547189562170496
model works	1.5706734993195162
supervised approaches	1.5705525079500664
achieve comparable	1.5408560126058508
verbs may	1.7538341131741775
assign values	1.5354510398033074
propositional attitude	1.5891348789899808
unsolved problem	1.6246289167870047
planning component	1.6580661526531622
simply means	1.6570039092175395
learning models	1.7642168557577964
structural relations	1.7973292739611306
task data	1.6283373457607024
wise mutual	1.6111337654434246
training instances	1.9862432651825501
morphological generation	1.5994067202124636
bayes classifier	1.8184049829757072
variation among	1.593094242776905
error rates	2.159598318984386
cannot appear	1.6690280524444296
section illustrates	1.5706734993195162
sentences given	1.6112034493728657
end time	1.5926105017992542
entre les	1.6213710343594767
three patterns	1.5500434040293412
two heuristics	1.5559715171228277
functions used	1.6679521469523344
hyphenated words	1.541505072604315
following patterns	1.651410395972749
semantic relatedness	1.680980389905053
question might	1.5445134575869865
word information	1.706941210604198
system response	1.8057860029799127
case frame	1.9085492770183077
languages like	2.0447974979300354
9 sentences	1.5324452505798665
different kind	1.8851996381651792
research literature	1.5187014267751215
fill slots	1.5952494708925822
direct approach	1.5763955456335526
finite set	2.176237314147912
uniform framework	1.5497725439919083
h l	1.7832211299202094
describes experiments	1.6679521469523344
dans la	1.6966987805093807
basic issues	1.5354510398033077
per verb	1.6094040109095726
broader coverage	1.636252690439584
names like	1.7314807712355123
system configurations	1.5387082898744961
memory limitations	1.6068987126002128
labeled precision	1.7253462693781008
subjective evaluation	1.642049178581458
features capture	1.5580764217656649
attachment decisions	1.7018425065699014
word j	1.6182203723348287
real texts	1.8603360689191424
intensional logic	1.6741561296266723
default reasoning	1.5932969222698028
semantic point	1.6500924538007047
node label	1.7470926280887056
modifier relations	1.6154015786163354
specific roles	1.510060731399269
one edge	1.6388987829322057
idea behind	2.1049619444156846
dif culty	1.5258845672739016
numeric expressions	1.6576058689750806
approach makes	1.8540774145670442
system would	1.7723382693382495
representing knowledge	1.5771944393968738
selected words	1.7436466687560772
initial results	1.555236518441112
appropriate information	1.7301217323026563
resulting representation	1.5656419863875413
6 words	1.734415693746389
name identification	1.575236643075911
representation would	1.7943022735270753
nominative case	1.8111311157192815
case information	1.7175251127254123
human annotators	1.9696837270253595
major constituents	1.5929928949821113
wordnet glosses	1.5775160173416134
remaining features	1.6998973603836447
steps 3	1.5371982986173842
atis task	1.6805335112349802
abstract data	1.5823893485824279
modeling techniques	1.8224438253268183
based strategy	1.5459182428709468
open issue	1.5771944393968738
sentence text	1.507007268326853
speech corpora	1.7210792541770605
joshi et	1.7233301991693268
describe various	1.5306061495832655
semantic rule	1.7500346558272122
two places	1.6342922036019591
participle form	1.5876463561123457
reasonable assumption	1.5511943810330642
best recall	1.5514514275224365
single classifier	1.6121574112594272
object would	1.6084968011212868
research programme	1.5612815851895034
allan et	1.619593373514356
feature indicating	1.616590602753631
document retrieval	2.054415506198742
summarization system	1.8732662112175602
annotated examples	1.6626800097200731
possible classes	1.694330956352163
standard model	1.5740535294173463
grant iri	1.7776011133071408
person name	1.85215385555527
multimodal dialogue	1.6251006329561908
following figure	1.74632671903536
speech synthesizer	1.8232313197850079
2 types	1.5440207036648976
different metrics	1.7060017085050698
complementary distribution	1.6637019978300875
actual translation	1.6535456510181574
1 figure	1.5656419863875413
extreme case	1.806267122365411
empirical study	1.891696205510565
grammars using	1.6606791936831158
select information	1.5547189562170496
maximal number	1.7152215891075944
taskar et	1.5779112201493601
grammar g	1.9702552513381875
clustering procedure	1.6333172744768008
corresponding values	1.7014225922673947
two subsections	1.6960474084795663
approach described	2.0354466785998024
also cases	1.6458797346140277
question could	1.586137214651611
sentence given	1.7526660750199676
two knowledge	1.6487873734413878
taxonomic relations	1.6258574628751319
conceptual analysis	1.596327503538796
average time	1.791374403364642
prevents us	1.6822401779866478
set 2	1.5976071334487614
one entry	1.8721213894753688
user study	1.6089256123156477
explicit use	1.6724173909604771
different representation	1.699626140637926
complex structures	1.9005677603092026
collocation information	1.5637776281819353
concrete examples	1.6411558246537032
potential problems	1.6902599836287888
de smedt	1.6159251485737178
pick one	1.579873144038688
technique would	1.6056035510036821
used measure	1.522760613339579
briefly mention	1.5338735539822874
words linked	1.5453687309821331
expert systems	1.8964684672342802
mor e	1.5914442382457208
third case	1.7617026276762804
absolute value	1.773656493186697
collier et	1.5703715244044352
objects mentioned	1.583942701611184
features given	1.586137214651611
exact word	1.526993068994652
input grammar	1.552425499333972
column lists	1.5808501342373025
data elements	1.5545094471985448
form n	1.5371982986173842
sentence selection	1.685240783873301
weight associated	1.6505104160065103
communication process	1.510618969052833
research workshop	1.522760613339579
object level	1.5048418722609265
various language	1.6977890880751751
shown ill	1.5926202003411678
semantic dictionary	1.5856077056777689
general point	1.591823957496618
translated words	1.5414180417122596
c l	1.7864911824707443
unlabeled data	1.7826716607258442
entity type	1.8075519860539895
evaluation score	1.5548556827926392
important practical	1.5739592165010823
one method	1.713727310253324
see footnote	1.6978216692284374
approach results	1.5188051370135929
three items	1.5226149809040495
high complexity	1.5378646275607575
conll shared	1.6066593791852313
training material	2.039999889475542
natural speech	1.7565523504196627
example text	1.6844633832435152
common prefix	1.5001879870213375
model generates	1.6576700820653434
optimal set	1.6586910333010416
fig 3	1.506896343095999
full translation	1.584963793531656
whose right	1.659111358304426
parallel corpora	2.047076496694735
types according	1.6565475758379593
representation could	1.5111306094308554
interactive mode	1.6828225421438074
complexity analysis	1.588669192462068
constraints must	1.7370644233219208
provide users	1.6210934563561632
specific communicative	1.5111306094308554
dimensional matrix	1.5156323250636556
semantic nets	1.666033713372261
integrate information	1.526993068994652
additional annotation	1.5338162779728006
features describing	1.65264503846304
patterns used	1.7714711599478519
application requires	1.5482870987025659
cannot solve	1.5188051370135929
transfer rules	1.8248609621838707
2 b	1.5703715244044352
linguistic assumptions	1.5656419863875413
incremental processing	1.6393659677667805
rules may	1.8438862586689806
many situations	1.703359686465308
subsection 4	1.52584415318421
new rule	1.627366286956121
lexicalized tree	1.8732805435290665
issues must	1.530072891325321
let b	1.6679521469523344
unambiguous words	1.5465717448129954
preferred center	1.5052072450696206
tree built	1.5226149809040495
problems related	1.7649639737737153
system implements	1.5706734993195162
feature must	1.5645241345053709
initial tree	1.7814684489380692
longer version	1.530072891325321
perform experiments	1.6802940634668402
example given	1.9501310679837818
generation modules	1.6265573004837734
analysis requires	1.594773126307433
8 show	1.5445134575869865
also represent	1.6377134026552522
candidate sets	1.5516784685288352
kappa coefficient	1.644076248707175
user inputs	1.665233954077787
brown et	2.245295334801253
grammatical rules	1.9084450721792185
task involving	1.5445134575869865
brief introduction	1.8155090847728768
feature fi	1.6014047581545088
ongoing project	1.5645241345053709
notational convenience	1.613624660358136
seminal work	1.6112034493728657
set includes	1.815467690324536
gram statistics	1.7393947914911896
next state	1.6503091092498052
also measure	1.5739592165010823
probabilities computed	1.5525978349470975
scientific abstracts	1.6082991843309609
main clause	2.019154031508489
single representation	1.6976636234158158
state parser	1.5367186077855648
different classification	1.6699400022790054
leads us	2.053064741221574
lexical property	1.5296068202902637
process proceeds	1.6031224303496519
character recognition	1.8203428965103798
general methodology	1.5408560126058508
character set	1.7475943568840722
automated text	1.596364543675433
bayesian network	1.5769719246078697
subtle distinctions	1.5652297135534465
information supplied	1.6210934563561632
one step	1.8628780090996273
solve problems	1.7294947296982195
four tasks	1.610636317494032
size k	1.6051749816048617
larger amounts	1.719365286993631
dialogue model	1.6828216870357484
actual text	1.76680755117803
references therein	1.66195509821619
term matching	1.5211629864223772
oov words	1.6245630660726424
narrative text	1.5408577714205225
richer information	1.5756869131546112
news sources	1.809104281134224
onaizan et	1.6476839484107846
general dictionary	1.5283106970018872
wordnet synonyms	1.5205092238223816
information regarding	1.9180528509368375
new segment	1.5690993570204987
order languages	1.8693000007008456
since c	1.507007268326853
annotated sentence	1.6169986020906966
using wordnet	1.978267983588257
single pair	1.5354510398033074
relative likelihood	1.5836260411907608
input symbol	1.622146292616318
computational effort	1.6573127369315375
correcting errors	1.5230483704868893
worst results	1.5645241345053709
translation probabilities	1.847420131924223
neighboring words	1.8203985223586607
automatic language	1.6565442934658274
textual input	1.5971626956604803
expected frequencies	1.553503522351105
word use	1.554089533378209
final step	2.028919057300995
based learning	2.1097360756088785
unrestricted texts	1.7445793388054498
2000 sentences	1.5908167675916909
new dictionary	1.6109068462032587
noun groups	1.8089511050652078
one noun	1.7364418632547807
original utterance	1.5266242261615062
fine distinctions	1.6031224303496519
boundary detection	1.7035531112471585
document may	1.681252707662353
entire space	1.5361450492096225
tile system	1.815498644111005
null table	1.8535783236360657
complex task	1.5540642639565079
automatic creation	1.6768692079990173
system capable	1.854863870676686
extraction program	1.5652297135534465
particular point	1.757229496751171
incomplete sentences	1.6474849805767613
lexical material	1.7824692961452513
tree traversal	1.5642743049359682
empirical probability	1.5482899355062387
standard n	1.6088522181598408
feature extractor	1.501403253108717
chinese corpus	1.8363321570654736
stochastic tagger	1.586510666920259
possible pos	1.623483754928356
two conditions	1.7371198324152417
phonemic representation	1.5453023587589123
standard practice	1.7804821942340125
large structures	1.5445134575869865
formal treatment	1.6256278638449597
la structure	1.5270107701555016
inflectional morphology	1.881933302960822
empty set	1.9787319907875913
computational problems	1.6246289167870047
complete theory	1.5306061495832655
algorithm works	2.0181500835111255
better result	1.7726713279365947
entire word	1.5656419863875413
child nodes	1.792791573266195
parsing approaches	1.6118697210297996
linear kernel	1.6892382050648767
large range	1.6849207890717186
description may	1.6732100893771809
model similar	1.6002993454155385
various senses	1.650092453800705
zero frequency	1.50119175880199
features across	1.5482870987025659
complex patterns	1.6613891836858694
items used	1.613364934425665
deep structures	1.6020493608550883
six features	1.569522297316359
great interest	1.6862623517634696
semantic tagging	1.8128443658963442
14 shows	1.5445134575869865
third category	1.6010101084822783
roles played	1.5408560126058508
anonymous referees	1.8293720283840755
particular rule	1.7606738632426728
full text	1.8988388574075437
wordnet semantic	1.6327406427880258
search using	1.6844174045708415
appelt et	1.6166948003172705
related languages	1.769879395564825
average length	2.1170903270800707
extended domain	1.7263939727436735
distinct words	1.8298968163477558
extra work	1.5767760138793054
syntactic dependency	1.9023162559549303
information alone	1.7813351618712552
approach avoids	1.5378646275607575
pragmatic context	1.509778922004791
edinburgh eh8	1.522760613339579
previous stages	1.5350845321041717
input sentences	2.105804181956197
figures 6	1.754992328975788
set could	1.6002993454155385
also mention	1.5408560126058508
possible positions	1.50119175880199
ann arbor	1.952275443092547
selection strategy	1.5587255214251448
important sentences	1.6669179281143562
phrase p	1.5246980774116032
implemented system	1.867624321066968
supervised training	1.900664600442847
facilitate comparison	1.6320354670147565
reasonable choice	1.5408560126058508
general word	1.6589224276852048
semantic classifications	1.505379065355142
annotation would	1.5582958710952315
700 sentences	1.567922330580758
english mt	1.6673070207950622
avoid unnecessary	1.664540718454114
english text	2.1485419584738583
value associated	1.550620126773261
atomic categories	1.64255650506619
rule given	1.6961394033344752
data described	1.6458797346140277
head driven	1.543442208482524
directly use	1.6481174069026685
linguistic motivation	1.6996261406379258
text based	1.592282084973678
angle brackets	1.7803826599075734
two formalisms	1.5050951275294642
clear advantage	1.6939639871030243
additional meaning	1.507007268326853
consider example	1.720436549981399
language information	1.9756369491585537
recall curves	1.5288473482789515
tasks using	1.6358767498200761
module would	1.5445134575869865
prolog terms	1.5798567244455874
replacing words	1.5300728913253212
original sentence	1.905366891995888
personal names	1.7466422200094396
proposed approach	1.9272412948690567
improves performance	1.9145727728297701
structural units	1.5058322686772407
independent meaning	1.5676517392350746
ge et	1.5263917952197188
first utterance	1.8472389569505614
system represents	1.567922330580758
early versions	1.597317007886449
computer model	1.5759490043057238
2 v	1.6150883315865432
local information	1.8726525410970813
syntactic factors	1.5151405891926544
sentences consisting	1.5111306094308554
four sections	1.5731122769149346
let v	1.769069834369823
construction process	1.7893251197382343
text words	1.5328913976227052
specific word	1.9613716774517453
definition would	1.5582958710952315
commercial products	1.6805174920443955
one author	1.5329785555225088
th feature	1.504934031784749
correct target	1.5440207036648976
two aspects	2.079448358560719
equivalence class	1.9552973122702948
translation would	1.672219863528485
several problems	1.9316974804754188
worst case	2.080824684857537
rambow et	1.5339405281266383
tag derivation	1.5107760324572368
low degree	1.57425918739941
computational power	1.6562676183926621
different translations	1.7515364373215787
used sections	1.507007268326853
system searches	1.6802940634668402
relations within	1.799040872336043
semantic problems	1.5511943810330642
tagged corpus	2.1046200710875267
first train	1.5511943810330642
summarization techniques	1.5760526619236255
word patterns	1.7494841107150836
sentences rather	1.5690244007917744
selectional preference	1.541600124832538
practical experience	1.6152953234723224
complex linguistic	1.8445325823008774
words tend	1.9106240221946806
intentional structure	1.6380991753461354
state may	1.5275591303469285
inflected language	1.563687738145949
bell laboratories	1.780667789439209
increasing availability	1.5037367200448983
whole corpus	1.9670017659590366
highest value	1.702091221900512
copyright 1988	1.5445134575869865
certain combinations	1.6190328102134979
decision point	1.5195036419368686
representative corpus	1.5245348740884508
28 aotrr	1.5511943810330642
much discussion	1.5645241345053709
match exactly	1.659662148122272
discourse connectives	1.5210787152928913
sense given	1.5328333954091753
accurate enough	1.6802940634668402
selection restriction	1.5278580912138442
grammatical framework	1.6516070556637539
shorter ones	1.667913900271795
many applications	2.1375759922080064
whose output	1.743960294459502
various stages	1.8022217079329124
translation methods	1.6237870549237545
syntactic model	1.631804375368732
type ii	1.5026752386502622
basic set	1.6513823106343972
data presented	1.757636103678803
occurrence information	1.7857601659462947
set e	1.7030773732699256
using nlp	1.6392309045877975
model called	1.596528355412953
top part	1.5690244007917744
test collections	1.6837828370705044
standard phrase	1.5296095598069797
relation links	1.5371982986173842
possible category	1.5211629864223772
many types	1.963572192370669
longest matching	1.591495523950492
000 sentences	2.1293202463262855
available online	1.7044865459645908
role information	1.637297148429706
specific topics	1.511205120616144
presents several	1.6699400022790052
search spaces	1.5309846273595442
different concepts	1.7961446965684997
following utterances	1.5583098370928328
multiple choice	1.5365180471940842
assumptions underlying	1.6037378902512214
string may	1.5818515289928636
np must	1.6256278638449597
functional relationships	1.5419819962962804
paper describes	2.509941280087118
hybrid model	1.5624125853001019
decision rule	1.6120533789764768
time consuming	2.1147782695455106
input consists	1.6783930166760777
heuristics based	1.786911455498122
testing material	1.5657848105751835
earlier ones	1.5611231576018243
syntactic function	1.9268487771589144
performed experiments	1.9006070897125151
three nodes	1.6487334099083149
methods cannot	1.6320354670147565
certain verb	1.5771944393968735
grammatical categories	1.9335583465127912
english analysis	1.5908285090945737
require human	1.513130125383453
objects representing	1.513130125383453
f1 measure	1.6447956955587935
longer phrases	1.5577353413090067
funded project	1.5378646275607575
process must	1.8045229618861958
previous works	1.9645991992912688
two experiments	2.0228952591767073
contains entries	1.6002993454155385
systemic functional	1.5782217914607295
underlying assumption	1.775588620446508
syntactic arguments	1.7762282869904218
high scores	1.770963117017772
matching algorithm	1.8888171549398516
english ones	1.6107689014994975
using parallel	1.761727623513095
larger structures	1.7084259213367536
derived fl	1.5111306094308554
generalized quantifier	1.5931932229532988
clause would	1.5645241345053709
four types	2.103957368059966
source language	2.28181267449748
understanding systems	2.0717837635986767
two data	1.9413697016390414
foreign word	1.6176812795475524
use simple	1.7501491085716443
algorithm generates	1.6783158675754
structure given	1.7229556053533144
request information	1.5190584552425839
grammar system	1.6068327896362855
syntactic models	1.5112051206161443
full generality	1.5749574214167732
9 times	1.561123157601824
multimodal systems	1.5083806475760921
quantitative measure	1.620425994955554
little time	1.6939639871030243
models would	1.6190328102134979
underlying meaning	1.5321552374951575
linguistic principles	1.7407556363369296
common property	1.5482870987025659
general definition	1.6934058718890235
open test	1.6721361595474673
scale experiments	1.6816988370728096
hierarchical structure	2.17073311512724
different criteria	1.7606007710377158
di eugenio	1.7023270933087664
average similarity	1.5762561890798228
usually involves	1.6002993454155385
based studies	1.5582958710952315
one extra	1.5706734993195162
focus primarily	1.5459182428709468
single node	1.9081222842596948
current model	1.894607340585705
called upon	1.6701008999321498
pour l	1.6034171237281671
theoretical assumptions	1.5716157403242
overall goal	1.863956857337683
statistical parser	1.9430028106862327
natural dialogue	1.595154937476357
chang et	1.5523088703632102
scale data	1.5111306094308554
generate n	1.549729132180171
second level	1.9617775617812798
salient properties	1.5188051370135929
extract relevant	1.6853874840250063
il est	1.685158028805632
becomes part	1.5886561992520425
sense disambiguation	2.3128751270101695
generation systems	2.0814288937140653
extracting sentences	1.5893265024752146
candidate answer	1.6120682112130496
every iteration	1.5571495092265906
native speakers	2.0901862215558307
past participle	2.02194681081905
concluding remarks	1.9405434836994389
gram probabilities	1.7510021857581965
translated text	1.6191204510284858
statistical measure	1.6840711963273551
v e	1.754838435530643
good examples	1.6388370153723533
generating text	1.6411558246537032
main approaches	1.7204874537317836
whole procedure	1.5771944393968738
clause level	1.8561928495477398
subsection 2	1.5771054949602565
provide support	1.6629116393660754
data source	1.8012772983285348
main categories	1.7572294967511712
states correspond	1.5656419863875413
semantic parser	1.7193531718398671
given noun	1.7165902145706742
terminology used	1.6785607164239145
application context	1.5156323250636554
computer programming	1.5288687562201928
user need	1.5977445137418091
node whose	1.6785607164239145
rhetorical structure	1.9573970807540857
neural networks	1.963224013519868
constraint grammar	1.7273005722381307
clause structures	1.543442208482524
c n	1.768055354765038
free phrase	1.8685649076721627
multiple occurrences	1.733243017665905
later stages	1.864788617674602
b b	1.617301124779499
two extremes	1.8091439430899288
syntactic features	2.1867428788602403
sentence basis	1.660919557194327
dialog manager	1.576204564249021
contain important	1.586137214651611
adjacent sentences	1.686882332990053
analysis gives	1.530072891325321
japanese language	1.9195858460888433
standard feature	1.6261307844552735
another clause	1.5749574214167732
independent phone	1.5214049691612763
better prediction	1.5188051370135929
estimation process	1.6588338525751822
language structures	1.5423687272762843
approach using	1.964818893654593
low score	1.7261608475873556
annotation task	1.8207310823864453
five levels	1.5583098370928328
high probabilities	1.600330740560592
text domain	1.5269930689946518
free order	1.5784219547503153
est une	1.5924332937904309
syntactic grammar	1.7617159680941112
interesting aspects	1.586137214651611
unary rules	1.6305003348802698
possible orders	1.5302382484774217
deep linguistic	1.6937107413621812
evaluating systems	1.506173888464562
parser needs	1.609217242875463
possible outputs	1.5111306094308554
using pattern	1.567922330580758
context window	1.814236682225955
overall f	1.7562910228824573
ap news	1.5586109308625686
generation procedure	1.6051749816048617
scores based	1.6635555237714856
specific classes	1.5886561992520427
gender information	1.508366082952001
surface subject	1.7018505634933196
relation defined	1.6682934633704967
possible sentence	1.675201431538174
table show	1.5338735539822874
evaluation experiments	1.7068274097866478
optional arguments	1.5558827079957558
wiebe et	1.687889368689552
quantified variables	1.539859689770269
case complexity	1.71532114467507
deterministic parser	1.6976236018965554
figure 18	1.5551545888889253
certain knowledge	1.5300728913253212
computational perspective	1.6500007742396732
subsequent section	1.522760613339579
step 5	1.879829902083137
noun followed	1.530072891325321
grammar specification	1.6297844132986592
alarm rate	1.556412581038796
learning tools	1.558193976095525
spoken text	1.6005843102359503
like machine	1.5111306094308554
dagan et	1.8933033026091308
automatic content	1.6966751627217396
algorithm similar	1.6977890880751751
english syntactic	1.599900419936812
contain fewer	1.5445134575869865
language use	2.067525222875042
database system	1.7748495324021618
distribution p	2.002552391457371
evaluation data	1.9096409360482738
many parameters	1.660919557194327
pos tagged	1.816914067868859
systems also	1.7061395803133497
correspond directly	1.7425221016807453
one training	1.5996348445739617
free skeleton	1.5230482200667932
translation task	1.9247085725890154
figure 4	2.6143313093955394
modi ers	1.5947817955793098
complex expressions	1.700487206593315
single terms	1.5501936280492035
language syntax	1.810849286262513
2 results	1.6031224303496519
intermediate representation	1.8379016667182415
different representations	1.8080744485703493
two corpora	2.014882711684435
object relations	1.638001886668907
formalisms like	1.5061738884645621
depends crucially	1.6699400022790054
dictionary look	1.8108210177229402
sample texts	1.6649546071876982
syntactic object	1.5421805716912829
prototype system	1.9675579086078583
see example	1.7861808790923344
based interface	1.7328490912725107
whose presence	1.5547189562170496
human agents	1.5066881819459041
three rules	1.8410064101606782
errors using	1.530072891325321
much research	1.847324782572126
rule formalism	1.7381791287480448
column labeled	1.5668117204815992
following rules	2.0216244229081894
among features	1.5571495092265906
optimization problems	1.5459756252045591
search query	1.6411938887350184
tense form	1.6471606074671064
paper shows	1.9782276474104898
experiments based	1.5991584292605263
language called	1.6661204761131605
knowledge structure	1.6941947399455104
alternative ways	1.8292009407010608
accuracy would	1.646226844257487
first component	1.8563094472776231
techniques proposed	1.5482870987025659
empirical comparison	1.5649774855459642
one bit	1.5226149809040495
generator uses	1.6785607164239145
information overload	1.558193976095525
third person	1.990723764922278
equation 4	1.7870440065583704
words produced	1.5156323250636554
four parameters	1.5425370803688765
infinitive form	1.7423205947131029
charniak parser	1.6460387425355099
scale corpora	1.5461602256098455
general technique	1.6920919321535788
entity recognizer	1.8231304185746346
regular expression	2.0434834236982398
certain features	1.9037014967685864
rules also	1.6939639871030243
mitre corporation	1.6169986020906966
corresponding target	1.7922922794096197
first describe	2.064510071620984
short example	1.5156323250636556
important way	1.6056035510036821
context size	1.5742297370414604
information must	1.995796711713107
weight function	1.5921579080732082
limited range	1.649843436022881
also applies	1.8419161265361546
high correlation	1.78250403801545
one line	1.73698243005138
generator may	1.513130125383453
context dependent	1.8860911997383865
performance based	1.5118192695694388
several tasks	1.6482958236884362
efficiency reasons	1.7229550745276576
second task	1.7426543199393394
significant effort	1.6002993454155385
english lexical	1.8767340284628558
features relevant	1.6482958236884362
learning system	1.9802644114468388
corresponding part	1.626990247505205
phrase headed	1.6969170152803252
measure used	1.8213054685486696
accuracy increases	1.6996225985138183
national science	2.2233402989605535
items whose	1.524534874088451
next words	1.5180580646513062
square brackets	2.0556898615325885
system designed	1.8652033957672842
also report	1.9381853483218836
trigram language	1.9881397658588782
sense assignment	1.5088214873535928
many elements	1.5445134575869865
technical sense	1.5188051370135929
uses rules	1.5338735539822874
ones proposed	1.5111306094308554
figures 8	1.5440207036648976
task oriented	1.6369558071118375
existing rule	1.5156323250636554
reasonable size	1.711151422700744
web page	1.900879130513967
english speakers	1.9024644790842091
five languages	1.5267215368556717
verbal complex	1.519012494635691
immediate left	1.628939712903112
structure sharing	1.6894651434061956
modal operator	1.5247086052856087
system already	1.5440207036648976
identification module	1.5310785803838152
known problem	1.7378109296453574
case 4	1.530320126517967
accuracy level	1.5620473238340082
various information	1.7141428077458374
simple search	1.5582958710952315
different behavior	1.5706734993195162
important clues	1.613624660358136
incremental parser	1.5276926419628718
perform actions	1.50119175880199
text requires	1.5482870987025659
different lengths	1.835377345784773
absolute difference	1.5201190767490378
word expressions	1.764417995145988
important advantages	1.5771944393968735
evaluation software	1.511972965276975
recognition task	1.9882666236389717
simple methods	1.664540718454114
better translations	1.5484077345134049
aligned word	1.5818515289928636
necessarily correspond	1.613624660358136
key component	1.7895869149581451
great deal	1.9900192992474688
show two	1.6903000289233914
possible referent	1.5679440886127702
work attempts	1.5037367200448983
experimental results	2.439706282373815
left hand	2.051519995845371
daughter nodes	1.7924796589212197
many ways	1.803768594083131
approach works	1.7884354035243888
contains 3	1.5445134575869865
algorithm called	1.7266279442877566
similar words	1.9904277944844193
algorithms discussed	1.5611231576018243
structures like	1.821766499194296
four elements	1.5582958710952315
following methods	1.62412689036662
john told	1.5028598174994467
descriptions may	1.6699400022790052
wider coverage	1.5408560126058508
single string	1.5219618966045698
one np	1.6741797752731205
available text	1.5156323250636554
language patterns	1.5425370803688765
concept node	1.5226615774440826
one reference	1.7371478545596704
particular situation	1.7130771295020022
error reduction	1.9756949555052061
syntactic phrase	1.74612979097734
network model	1.54340917276031
verb classification	1.598400348570702
exhibit different	1.5798548254880234
processing phase	1.7678633487601778
verb stem	1.7397674889097305
space complexity	1.6848986106005297
output quality	1.5768891186463634
estimating p	1.6392309045877975
lexicon size	1.6098077738014362
word ending	1.6669753881293854
event sequence	1.509778922004791
qualia structure	1.5937265399374092
direct comparison	1.5277133244180252
first construct	1.6229151093758099
directed graphs	1.7692216073184515
formal run	1.549015470552123
single process	1.5011426563129797
error analysis	2.033204833195894
leaves us	1.7016656224425795
used word	1.7635102323276162
grammar covers	1.5559715171228279
user questions	1.6338810723661863
four ways	1.6320354670147565
precedence rules	1.581010805798165
coverage grammar	1.854248640659588
analysis showed	1.6919937194855956
logical formulas	1.6553024834172088
full document	1.5832301298908869
alignment probability	1.5769403688637265
ontological knowledge	1.6184033873746564
resolution module	1.7301302201419237
significantly outperforms	1.8338949789668868
labeled recall	1.6917806876406174
oov rate	1.6166997296975953
inflectional affixes	1.5399542488609383
gale et	1.8718801069047692
computer assisted	1.664969344571237
phrasal units	1.6099915955873183
multiple levels	1.7925831027363652
level model	1.770486698366878
one context	1.7573123843210448
annotated test	1.523907736875841
model 4	1.7862340916385704
two annotators	1.8965831036520677
future extensions	1.5582958710952315
first checks	1.6903000289233914
two finite	1.648787373441388
valuable feedback	1.6527294781610564
expressions used	1.7273763285181958
words along	1.5536240126687622
constraints may	1.8694526821479807
formalism described	1.594773126307433
presents three	1.524534874088451
darpa grant	1.8059073329144144
initial query	1.6493226448562486
control strategies	1.715772378758725
nodes labelled	1.510618969052833
documents per	1.6683792217105364
new tasks	1.6778478586578416
linguistic information	2.278063716115662
one action	1.6769442985710543
heuristic search	1.7135902122759092
simple form	1.7815167712603146
task description	1.5777946478302516
language modelling	1.8337061320053487
space requirements	1.6746175961122665
occurs frequently	1.7509416063625278
occurrence patterns	1.700880100828495
ground truth	1.6355715012605199
rule extraction	1.5234275767421737
10 percent	1.5729509548614216
b respectively	1.5269930689946518
recursive call	1.6535456510181574
heuristic methods	1.6235405672398668
sets containing	1.6227790810587184
machine learner	1.5452607690192934
dimensions along	1.5211629864223775
using text	1.7461896493793025
first module	1.5808501342373025
types using	1.5412463844444777
examples may	1.6593965399315964
lexical model	1.5359454487672428
briscoe et	1.5658904152201698
different speech	1.6388987829322055
word distributions	1.6078703431602825
section 1	2.24537108828407
interactive environment	1.5288687562201928
automatic information	1.524534874088451
feature may	1.7210742795514369
following form	2.0939374359844978
street journal	2.2634517416081845
next step	2.125194584395385
initial states	1.6273885876859233
one article	1.6049986840269537
two evaluation	1.6998973603836447
control flow	1.6361279446907149
tasks include	1.5188051370135929
problems discussed	1.6056035510036821
current phrase	1.540209180046852
probabilities using	1.7663553975095576
two dimensions	1.8823264981424994
intended effect	1.5406635112607758
par un	1.6029693328799988
structure consisting	1.7141428077458374
syntactic constructions	2.102308073457826
level models	1.5461317120818736
different documents	1.7327434379184519
rows show	1.6409107009350248
paper outlines	1.757636103678803
extraction systems	2.0094170941929597
time interval	1.7876561849657107
prosodic structure	1.6541998433672642
plural form	1.845037837566576
independent speech	1.5247856454681745
null set	1.5511943810330642
issues relating	1.5188051370135929
semantic value	1.692566779047171
length principle	1.5201190767490378
search algorithms	1.7994779379783643
see tables	1.592111614460519
state space	1.7148034769655807
derivation step	1.5506315539715478
verbs belonging	1.5426832781289812
formalism presented	1.507007268326853
adequate account	1.5408560126058508
scale evaluation	1.6976654847000265
one field	1.5582958710952315
agents may	1.5622818522186854
tipster program	1.6688620058170072
two sets	2.3805663843534144
conditions must	1.6445809590877876
version 2	1.7636670304236146
domain specific	2.0907571488818992
running text	1.9096573629159743
word appearing	1.6819762996803398
three noun	1.511205120616144
variable binding	1.687052541763227
methods discussed	1.6246289167870045
three categories	1.9166226722230522
rule allows	1.716869108797698
combine two	1.8095394855795697
abstract notion	1.558193976095525
contain errors	1.64082299778787
gender agreement	1.6322732044424475
improve accuracy	1.8798945896310395
makes sense	1.6396869696047767
structure proposed	1.526993068994652
certain order	1.5338735539822874
standard evaluation	1.7744985774486137
overall frequency	1.6285930450163058
different texts	1.754992328975788
one position	1.7909134423055653
000 times	1.5748567577278902
achieves state	1.5602367449199286
given training	1.7898679922822172
similar approaches	1.641155824653703
dependency analysis	1.7871794326122028
graph whose	1.586137214651611
alignment process	1.7502886304541354
better choice	1.5739592165010823
general constraint	1.5829912568215723
graph structure	1.8129237829616787
context must	1.6103523297384124
linear relationship	1.5080160371465428
alphabet e	1.5423485783857451
another domain	1.7497703463414664
several types	2.10415140282922
likely path	1.5459806054163279
sample dialogue	1.6285421123283488
produce better	1.5157179068026116
correct errors	1.720650346539263
semantic entities	1.6569343007997546
another concept	1.551541541473524
sequential order	1.640965319692465
relevant sentences	1.7131869860184712
document collection	1.9937117482705546
phrase whose	1.667913900271795
semantic types	1.8826671393238694
annotators agreed	1.6417187195986458
mechanisms used	1.6431586438232255
multiple languages	1.9548893723687324
every english	1.5690244007917742
current experiments	1.6560747925822192
phrase structure	2.341532822176027
pp complement	1.5704284771841746
sentence depends	1.5156323250636556
algorithm searches	1.6190328102134979
parameters may	1.5707376783218978
entities may	1.703359686465308
corpus 1	1.585226770463617
language resources	1.8578837050453367
class membership	1.734993979787421
different levels	2.3163719946736006
work aims	1.613624660358136
structural disambiguation	1.6835150811587019
level features	1.810348016684915
bbn systems	1.5194321743502117
phrase types	1.8081261526433139
extraction procedure	1.667694784601927
produce texts	1.5269930689946518
certain pairs	1.5156323250636556
1992 l	1.5414180417122596
specific category	1.538719277039856
linguistic environment	1.64675133452636
two predicates	1.6978241750753513
query results	1.637822040420213
basic ones	1.5078976439458078
induction algorithms	1.5118192695694388
complex rules	1.6379377001181812
speci es	1.5149639400339463
specific examples	1.7606738632426728
lavie et	1.534771069588773
knowledge engineering	1.8601493034406358
similar concept	1.5188051370135929
cognitive science	1.9765767191027617
another paper	1.643362624369706
one document	1.8473840516281195
partial knowledge	1.5156323250636554
level grammar	1.5436113955345827
words alone	1.5977895501010146
performance achieved	1.7328446802270456
traditional dictionaries	1.5559715171228279
simple combination	1.5716157403242
proposed algorithm	1.8187406562859856
tasks require	1.5037367200448983
many discourse	1.5771944393968735
include word	1.6741803270499043
word identification	1.6528604037325945
figure 2	2.7599680645812623
lexical features	2.0637217661589564
another element	1.5441583487573467
4 show	1.9243941914967064
third question	1.513130125383453
also belong	1.5156323250636556
log probability	1.7822108111815924
relation extraction	1.7321900689689007
incomplete knowledge	1.5784904253929217
compute precision	1.643362624369706
tree given	1.6768692079990173
v np	1.7951815328580951
feature value	2.0217484845368405
two argument	1.594773126307433
grammar specifies	1.5445134575869865
semantic predicate	1.6824189422804567
sentence one	1.5111306094308554
current feature	1.554089533378209
trees without	1.597317007886449
analysis procedures	1.5269930689946518
make inferences	1.741584719252339
new value	1.7180538360077764
new utterance	1.6972402763727832
stage 3	1.5397488977406404
newspaper texts	1.8137379317727012
conventional approaches	1.5965283554129528
tentative conclusions	1.672219863528485
single transducer	1.5182069877105362
zhou et	1.6092699862296986
standard machine	1.6251573559222987
grammar writing	1.6422084453197123
original source	1.677761776945173
model scores	1.7269196371721147
structures provide	1.5111306094308554
logical representation	1.7615698832270394
readers may	1.524534874088451
wide range	2.1688140224368686
forms found	1.5338735539822874
original tree	1.5620992070146569
discourse structure	2.1613938301988767
newspaper articles	2.1365002965649413
system based	2.0050967060161726
learning techniques	2.2239829524047567
approach offers	1.6761656524213182
dialogue system	2.072419044731528
lexicon must	1.6666455667887732
concatenation operator	1.506173888464562
best systems	1.6830657171447783
certain feature	1.6637019978300875
structure allows	1.6289349127929256
high accuracy	2.1990202106750276
number 1	1.9256132321864536
linear discriminant	1.6077123291794404
processing tasks	2.10779541273091
model performed	1.6070370629408204
confidence measure	1.6409485841752938
generate appropriate	1.5926202003411678
design decision	1.5466472792181065
general structure	1.824863944960065
unsupervised method	1.802600487305827
linguistic structures	2.0307292284311225
different objects	1.7518142844030467
qui se	1.5175790191927958
following derivation	1.5320698902069676
hoc retrieval	1.50249733128034
mixed initiative	1.6938728685317885
entire sentence	2.0852271978596453
initial question	1.5217605875570066
mark johnson	1.6331563410016883
intuitive idea	1.6084968011212868
original language	1.6214578999580556
000 characters	1.5565549888082182
important knowledge	1.5637776281819353
sentence analyzer	1.5351419134892126
feature types	1.6559134499537174
theorem 5	1.5537891126172103
pruning threshold	1.516870559202521
related tasks	1.801832580875303
current technology	1.632350904386585
central concept	1.5771944393968738
another interesting	2.009646501142428
possible input	1.6229151093758099
inference system	1.6239124135583538
new types	1.8097772100813396
parsing strategies	1.8714354255103698
one set	1.8916694972633528
even simpler	1.5188051370135929
standard approach	1.862637313885043
approach include	1.507007268326853
based heuristics	1.5818304274438093
syntactic variants	1.6985913799854724
document matrix	1.588860174200573
global structure	1.676267592810602
possible co	1.55955788508706
dependencies among	1.8363602568144641
problems connected	1.507007268326853
approach similar	1.6802580409155776
different languages	2.2698511795592498
chinese word	1.9740439400558936
earlier studies	1.5412463844444777
task becomes	1.5482870987025659
using c	1.5611231576018243
derivational morphology	1.8424299324190367
surrounding words	1.9074078018450433
synchronous tree	1.5620190733053647
small domain	1.6031224303496519
new event	1.528297849460409
evidence provided	1.5905396773117442
process uses	1.6213967902634536
following table	1.8503255807792498
np may	1.594773126307433
example one	1.5188051370135929
two values	1.9594267265145642
parser consists	1.6481174069026685
language expressions	1.908421980937265
statistical parameters	1.5093464527050404
technique based	1.721573893842553
1 table	1.5466472792181067
every variable	1.5081682632911075
trivial problem	1.6388370153723533
patterns described	1.579873144038688
overall score	1.8350993220511236
pragmatic analysis	1.664969344571237
extracted words	1.6005843102359503
participating systems	1.7388158562493758
obvious solution	1.6593965399315964
word based	1.9089639761566437
output sentences	1.7179520738135299
significant advances	1.5637776281819353
central element	1.5482870987025659
meaningful units	1.5631650419471608
semantic net	1.831373137587794
process may	1.8974003767752694
corpus linguistics	1.6442436606719513
current research	2.0920704147670888
system participated	1.524534874088451
c q	1.5602758437600655
first case	2.175975754492835
line lexical	1.5378646275607575
right word	1.625547116012144
expected value	1.8107180217740892
verb takes	1.7080393517333912
one basic	1.698978605212456
structure without	1.6844174045708415
parser builds	1.6673214954745952
examples taken	1.6939639871030243
real life	1.7594458100941692
method 1	1.5779705102543615
performing systems	1.7099827286906155
improves upon	1.6361279446907149
particular value	1.7178162994368975
later use	1.6822401779866478
several parameters	1.5354510398033077
particular utterance	1.6961394033344752
various patterns	1.5408560126058508
incremental generation	1.50642733284372
tagger used	1.586137214651611
dependent words	1.51759123686799
first tree	1.6096503035106333
semitic languages	1.650135523624738
individual elements	1.5645241345053709
different processing	1.6756673695466664
model probabilities	1.7654455865072995
word might	1.6614869878819327
translation purposes	1.517770090684007
added benefit	1.5547189562170496
input contains	1.5547189562170496
skut et	1.6044489211325175
information retrieval	2.4751810053425674
thus allows	1.5511943810330642
n denotes	1.5706734993195162
basic strategy	1.6661204761131607
present algorithms	1.50119175880199
slight modification	1.721573893842553
np vp	1.9080595779750211
word entry	1.553296096968758
nonterminal symbols	1.8694873897881805
system answers	1.618103745190334
model 2	1.840420972750984
possible realizations	1.6342922036019591
verbs using	1.615262062995654
computer system	2.007032404670774
templat e	1.630872175667311
disambiguation process	1.8634516837429567
characteristic properties	1.5611231576018243
verbs whose	1.659927606232618
preprocessing steps	1.6601760242005998
information measure	1.626990247505205
following definitions	1.7684939314789965
psycholinguistic literature	1.5906723352433734
effective use	1.8166493319197705
original motivation	1.5706734993195162
particular applications	1.6666455667887732
dependencies within	1.567922330580758
phrases must	1.6213967902634536
first draft	1.6358767498200761
particular structure	1.6002993454155385
little information	1.8378862010677828
semantic component	1.9560406165045388
lower frequency	1.7301991863273134
lexical correspondences	1.537748012382048
syntactic clues	1.608218627574815
particular types	1.8321214596289352
possible forms	1.6681052948574107
every sequence	1.5150717731521737
post processing	1.5616214674546969
automatic determination	1.5300728913253212
based knowledge	1.8647202399468492
wordnet lexical	1.5806234267011035
news wire	1.61404069957545
weischedel et	1.7360925925719612
constituent analysis	1.5588986769700433
complex relationships	1.5440207036648976
two simple	1.9117124572781978
linguistic task	1.558193976095525
daughter categories	1.6184835566989624
existing parser	1.50119175880199
three nouns	1.533540614646662
processing stage	1.7959452040498372
two nps	1.7824824373483883
simple extension	1.7051629942513449
show examples	1.6741803270499043
dimensional vectors	1.58255773593085
different dialogue	1.6569504373202975
one topic	1.7733087537053678
answering task	1.582281806492251
good results	1.8297136104314888
complex noun	1.8212830114156637
translation unit	1.5748506428024152
approach called	1.5547189562170496
token frequency	1.5139888811956013
surface features	1.5745185360865697
language utterances	1.8050791145293759
separate words	1.7727308472331929
two error	1.567922330580758
discourse referent	1.7713548921921467
particular event	1.6977890880751751
five cases	1.613364934425665
modular approach	1.572165926051522
alternative view	1.5634115707207188
many entries	1.6213967902634532
semantic elements	1.6530482990363917
strong constraints	1.5908167675916909
declarative sentences	1.8600231625040313
last argument	1.5620473238340082
bootstrapping approach	1.680145181544324
latter group	1.5547189562170496
humphreys et	1.5736712336159786
translation candidate	1.604861393422239
prima facie	1.6031224303496519
linguistic patterns	1.8016507449342778
whose structure	1.6481174069026685
feature principle	1.6746217556670895
probability 1	1.7618329294818371
wordnet sense	1.7524881620390578
output consists	1.6388370153723533
fold cross	2.2020808972243175
semantic relation	2.0541549806304347
inside probabilities	1.5485157836114951
hybrid system	1.713169333183919
communicative acts	1.6816626665742975
semantic distance	1.8495295504266565
relative scope	1.5492069339612722
character encoding	1.5781539692435584
identifying word	1.5739592165010823
word could	1.7633213752432315
embedded sentence	1.7571379192713195
alignment program	1.5614085980065187
nouns like	1.7017362832860266
object noun	1.7416077717655303
pronoun reference	1.5630673933379575
lowercase letters	1.5690244007917742
term weight	1.5069182537838017
common features	1.7709631701752158
minnen et	1.5817586626294706
general inference	1.5217605875570066
given document	1.885550943882897
rules derived	1.5956707385069844
three phases	1.9004436789989527
text document	1.738269202458912
semantic relevance	1.5226149809040495
category must	1.5408560126058508
model includes	1.712733419282903
partial order	1.890074850203969
using cross	1.7740279971365007
processing applications	1.8551351786972352
previous state	1.7194199014145106
exhaustive search	1.8765912674529726
passive sentences	1.6839702984774076
model might	1.7714711599478519
current version	2.138329417285457
cfg rules	1.7138294097664337
line learning	1.5266242261615064
rich feature	1.5466472792181065
two constraints	1.8543596556330737
sont pas	1.503143551713097
length k	1.6279930974577694
texts produced	1.675653782291889
input sequence	1.8660733971624057
significant contributions	1.5973170078864487
short overview	1.6084968011212868
query expansion	1.8773614388288706
treebank annotation	1.7054013636239174
propagation algorithm	1.5430422762165759
thank aravind	1.522760613339579
different modules	1.8171701892119112
20 percent	1.5036575034310684
three issues	1.6429565433589586
following issues	1.6502761822926497
like algorithm	1.5156323250636554
sample corpus	1.5884511056131143
text extraction	1.6142007621322634
analogous way	1.583051127543801
value pairs	2.119080472239401
segment boundaries	1.8498097625096137
current system	1.6683756846638056
describe linguistic	1.5412463844444777
several observations	1.5111306094308554
phrase refers	1.5189601534336892
new component	1.5211629864223772
sophisticated models	1.6429565433589586
unification succeeds	1.55955788508706
next line	1.5177700906840073
various resources	1.522760613339579
qui est	1.6313889982234313
two inputs	1.5452167257960319
user provides	1.6194726873773588
likes mary	1.5627373622154526
system achieved	1.868008129697415
common practice	1.5805972685494774
end state	1.574257790371604
barzilay et	1.6208760575332914
noun form	1.595491118313467
building process	1.6919937194855956
feature space	2.042836216251504
deeper analysis	1.7151558643929241
work discussed	1.572834038198836
training corpora	2.0541101435131273
150 words	1.511205120616144
steps 2	1.7841928294529117
template task	1.5430897927342064
important aspects	1.9279858344943843
3 displays	1.583051127543801
list used	1.5482870987025659
formed utterances	1.5217605875570064
information may	2.102060861940492
words automatically	1.5011917588019899
dependency grammar	1.995217232160184
sufficient training	1.7638268621637399
distributional hypothesis	1.642135095336982
dependency relationship	1.6256918985851745
become possible	1.6339957705783554
current evaluation	1.6227790810587186
systems whose	1.626990247505205
complete corpus	1.5361450492096227
either case	1.815053752997954
word matching	1.610911509801581
word e	1.7206860596002738
function takes	1.5973170078864487
emnlp 2006	1.8191665297540134
bnc corpus	1.5150717731521737
new elements	1.671162162283824
vocabulary word	1.5678009097741024
mixture weights	1.515829619540709
data increases	1.672884134739408
good source	1.592111614460519
w c	1.582930136174351
translation units	1.5474141395461252
p r	1.570591357287721
e c	1.8500858912654368
language identification	1.6460026563915091
semantic module	1.51003864472744
confidence value	1.641671934872376
strategy works	1.5111306094308554
corner parsing	1.6841992452988204
new relation	1.6135998898978339
system accuracy	1.6603622500309283
appointment scheduling	1.7241352844794675
given set	2.074336103171617
graph shows	1.7615962632264215
successful approach	1.5511943810330642
algorithm performed	1.5482870987025659
domain experts	1.7456232860242016
certain number	1.6021234357143066
knowledge source	2.0519986334418503
parses using	1.5037367200448983
pattern recognition	1.9303561378580594
like part	1.5300728913253212
incomplete data	1.5465763373152988
one assumes	1.7355686473586731
equivalence classes	2.0514602754780875
scope ambiguities	1.7514316345876728
oriented dialogue	1.7213231803248377
free variables	1.7617846266109072
lower score	1.7121209095509229
resolution using	1.506173888464562
intelligence research	1.530072891325321
billion words	1.5180342944572933
model improves	1.6246289167870047
using standard	2.045212420514061
different processes	1.6214578999580553
model could	1.9572454518736035
rules using	1.852772243857825
lessons learned	1.7139655438888184
data structures	2.2353478661737944
ocean systems	1.5078797213408794
classi er	1.6768181301449938
rules whose	1.8549715094367745
poorer performance	1.5459182428709468
like x	1.522760613339579
specific verbs	1.5137880841716953
million words	2.2642117487216638
briefly present	1.6614869878819327
system requires	1.9338599631431972
extract features	1.7021555513601787
two collections	1.5941108460767777
two names	1.589604938005269
store information	1.6137947517368634
copy otherwise	1.925120091448104
optical character	1.8256548434963114
semantic label	1.6149967920884671
two probability	1.6794247694221331
linguistic resources	2.024594158321359
system produces	1.9489693731901097
large domain	1.6169986020906966
independent processes	1.5408560126058508
consistency among	1.6010101084822783
bayesian model	1.519176257738459
grammar writers	1.6158614642133013
rare words	1.8313242969301657
terminal node	1.9579957293305341
time per	1.7475761546445443
whose referent	1.5611231576018243
input data	2.0925339091648665
governing category	1.577424884862612
make parsing	1.522760613339579
random field	1.73353286606418
string positions	1.554799420236599
address issues	1.583051127543801
small training	1.9172273655541017
research laboratory	1.5188570493532692
similarity threshold	1.6515437341153352
two synsets	1.582117970332684
cpu time	1.821230640066632
fig 2	1.6354102761745795
logical terms	1.5441583487573467
factors involved	1.6152953234723224
defined threshold	1.5771944393968735
represent semantic	1.6708590810843598
last letter	1.554855682792639
low coverage	1.7181848619222264
algorithms perform	1.5378646275607575
alphabetic characters	1.71590619997792
comparable accuracy	1.5037367200448986
organization names	1.8120852472838271
conceptual knowledge	1.7796168008555664
qa track	1.713099293522467
based formalisms	1.82041975041133
word accuracy	1.8687576835243662
parser operates	1.6939639871030243
next action	1.6555687198366011
different scenarios	1.5582958710952315
mother tongue	1.6181255420440066
lexical research	1.558193976095525
several classes	1.8367548769074316
frame contains	1.57425918739941
great flexibility	1.5111306094308554
linguistic analyses	1.8534269727386345
lin et	1.7187586637688868
artificial intelligence	2.1387823132876678
constant time	1.7447392408229068
separate paper	1.533540614646662
syntactic phrases	1.7216323859596026
text without	1.8619464975072226
closed vocabulary	1.5342151532884414
partial ordering	1.9119788868005014
based classifiers	1.6510252375593581
statistical results	1.5077592941591993
various expressions	1.5201190767490376
formal query	1.5506943376184632
one experiment	1.7329292822928468
based wsd	1.593664149772333
language communication	1.6835891090237516
web interface	1.6170222343612837
document structure	1.771344690795847
gold standard	2.102909350153797
interesting work	1.526993068994652
users could	1.6977890880751751
real word	1.5150717731521737
relational databases	1.5051660769519701
models based	1.983684648223385
speech dialogue	1.6159761124195384
development process	1.7418452132679398
previous utterances	1.7654737810995536
certain characteristics	1.5408560126058508
particular relation	1.7301217323026563
new module	1.5360026503169555
language based	1.7682373774207518
interpretation process	1.8997051184040277
recognition based	1.531904457852976
assumptions concerning	1.507007268326853
generation phase	1.6923374218886946
worth pursuing	1.6002993454155385
subject nps	1.547854629402815
cubic time	1.6280519432771974
main challenge	1.5706734993195162
human translations	1.646927482213033
final experiment	1.613624660358136
illocutionary acts	1.516003483529197
rule compiler	1.528766713172021
core component	1.5880900834779992
information value	1.6267088423401717
typical example	1.9987678491137169
representation given	1.507007268326853
000 pairs	1.5678009097741024
aligned sentence	1.5724972781046953
resulting corpus	1.641155824653703
efficient method	1.8083721100016197
semantic difference	1.5328333954091753
grammatical theories	1.5991253496404458
given topic	1.7474968022467974
term selection	1.5360051791233795
ie system	1.7585129810713862
entire input	1.7341967871778698
current approach	1.5469761463744027
blind test	1.7280014091041935
feature makes	1.522760613339579
corresponding surface	1.605265068015642
functional tags	1.566841256789921
processing component	1.6983405775614073
minimum description	1.7756818246793045
type information	1.8696391662187213
tag given	1.5041134997716235
pairs may	1.6164339756999313
second method	1.9476309331598876
global optimum	1.6162929335924088
unsupervised training	1.7764341835277706
labeled arcs	1.5195036419368686
language engine	1.8040791692792646
recognition engine	1.6174489017711058
individual speaker	1.542683278128981
chomsky 1981	1.6270880267949739
constraints described	1.586137214651611
indexing scheme	1.5547186061409763
hand sides	1.9359877507738334
translation step	1.506399225957958
languages must	1.543442208482524
null c	1.6112034493728657
official test	1.6752116193177922
logical representations	1.5414180417122596
information types	1.5207475096071397
longman dictionary	1.8960350553331806
conditions may	1.6023597487734618
used hand	1.5612815851895034
another group	1.65027618229265
shallow processing	1.6752751024558283
smaller data	1.6266256462651403
standard ir	1.5525978349470977
negative values	1.6413904216258086
additional problems	1.6229919786001088
resulting lexicon	1.5188051370135929
control verb	1.5190584552425839
left column	1.6658904115324105
diagonal matrix	1.63601460385856
linguistic expressions	1.949738842575328
based data	1.6084204247323792
different corpus	1.765014364489331
question remains	1.6658904115324105
grammars would	1.5445134575869865
simple clause	1.5226149809040495
modification relations	1.5371982986173842
translation approaches	1.519176257738459
restricted number	1.5288687562201928
theory must	1.5578214127463763
declarative way	1.539187799152955
action performed	1.507007268326853
john wanted	1.5072125870144037
top right	1.6276749302936424
algorithm based	2.0918150371778976
without context	1.7158238755088633
improve recognition	1.706208768185209
internal representation	1.9970350584215957
word definitions	1.546428357536028
final selection	1.5378646275607575
grammar development	1.9698906123032227
human annotated	1.550251972604018
individual user	1.536127431994032
diverse sources	1.5378646275607575
better understand	1.5530245705687293
context may	1.8694261984744618
false starts	1.7708367984175761
every instance	1.7523602873394073
following clause	1.5582958710952315
sentence grammar	1.5491899194507934
patterns using	1.6716367265613719
estimation procedure	1.687504458290486
system training	1.5738708991826256
important contribution	1.6570039092175395
methods perform	1.630290930375684
estimating parameters	1.5111306094308554
web site	1.8568672327717242
polysemous words	1.888850908262893
one analysis	1.8051135122330944
development center	1.5378646275607575
texts may	1.65303397348486
briefly describes	1.7882649160849862
morphological forms	1.6930755887020918
general discourse	1.5445134575869865
level system	1.640965319692465
random set	1.5194321743502117
zero values	1.5917536315722915
second step	2.227962416009941
corpus annotated	1.855517284269721
based feature	1.6415587725424399
two type	1.5354510398033074
dictionary would	1.5887468924062063
different granularities	1.50119175880199
another document	1.5408560126058508
entire test	1.7244401980357718
different systems	2.03740508880006
extracting data	1.5395099438048585
english dependency	1.5198893458484917
common feature	1.7721760934600654
language could	1.586137214651611
phrases used	1.7172397008154916
church 1988	1.709819921843014
new discourse	1.8113837582471704
final analysis	1.5787598537974745
katz 1987	1.5041134997716235
relations involving	1.65545581040525
new results	1.6169986020906966
alignment errors	1.6119299447517952
last names	1.7275163677376462
section 00	1.5827101976183544
french version	1.559620194736931
different parsers	1.6769442985710543
arguments must	1.583051127543801
algorithm returns	1.6630783357424366
semantic case	1.677673850814764
testing corpora	1.5657848105751835
structures produced	1.7602680220517135
associated features	1.5973170078864487
second noun	1.5117884522435159
english verb	1.9763770113668142
measure described	1.522760613339579
system utilizes	1.507007268326853
rich morphology	1.6371456055941147
grammar representation	1.5482870987025659
phenomena within	1.5078976439458078
distinctive features	1.664910359715373
individual entries	1.5531855081374653
writing system	1.7774849858507558
see also	2.2326142398115363
standard unification	1.5821179703326838
null lexical	1.5445134575869865
linear combinations	1.5781539692435587
mt research	1.5399963142284547
monolingual dictionary	1.5579097887614934
probability values	1.6961645709294055
type would	1.5408560126058508
sentences drawn	1.55955788508706
speech errors	1.5617071846474357
different kinds	2.333986934089756
chart edges	1.597472287161362
cannot provide	1.7486023401067097
city university	1.604628374697278
graphical form	1.5037367200448983
relevant knowledge	1.6677446394327706
lexical association	1.5696254931843967
becomes clear	1.8614575311131887
characters long	1.5997057677865547
different surface	1.7795871039517752
question mark	1.8786300891566785
newspaper corpus	1.813233001079292
feature matrix	1.5220866851405268
indicator function	1.7106430744882983
level morphology	1.777815280706894
develop systems	1.5188051370135929
travel planning	1.7765909317632813
classifier used	1.613364934425665
clauses within	1.5111306094308554
generalized markup	1.5306061495832655
agglutinative languages	1.6762872709347327
statistical parsing	1.9653280153741293
full model	1.6015454161821774
electronic form	1.7573123843210445
column 5	1.50119175880199
discriminative model	1.5851445575908438
two target	1.5810686572743542
two attributes	1.7588758108740261
provides support	1.5515415414735243
best output	1.7155194164397944
example grammar	1.6809961099438144
lexicon may	1.6606791936831158
functional dependencies	1.5368165087569075
utterance u	1.744418871989359
paper would	1.5511943810330642
relations r	1.501575214823395
standard algorithm	1.5466897292582984
paper concentrates	1.6388370153723533
direct speech	1.5303177793333842
30 words	1.7595874308748352
configuration file	1.5017280215789064
categories like	1.7489291135018947
performance comparable	1.640044071538913
file containing	1.7353960437628704
possible meanings	1.7762112562243155
relations cannot	1.5706734993195162
two part	1.5156323250636556
semantic parsing	1.7521973016675532
mixed results	1.6169986020906966
finite verb	2.005755171396162
4 displays	1.5706734993195162
supervised learning	2.1337645180567497
relevant portion	1.5201190767490376
algorithm could	1.8680893185344112
600 sentences	1.591303481945301
coverage english	1.5511943810330642
complex np	1.7002541582984276
simple words	1.627431404656222
verb occurs	1.6421487562548307
boguraev et	1.527692641962872
cornell university	1.500089141564707
smaller corpora	1.5611231576018243
system constructs	1.5749574214167732
data contained	1.7733669865556945
typical cases	1.5739592165010823
f denotes	1.5188051370135929
bleu metric	1.6820043299633254
appropriate training	1.5582958710952315
italian language	1.5083048851311085
parsers based	1.6309471736131642
active area	1.6002993454155385
ungrammatical input	1.6217009679023282
data items	1.6621002057103538
reduction rules	1.5615879413053202
action rules	1.672903142240206
module produces	1.5679223305807577
particular discourse	1.6829162710790206
argument pairs	1.5249942996423669
compares two	1.522760613339579
discourse tree	1.6300406042696427
new speaker	1.5579375095755599
different properties	1.7856549343874448
two ideas	1.5188051370135929
large training	1.991717914537206
list structure	1.6209764831444602
interesting problem	1.7389537457907243
similar goals	1.579873144038688
selection among	1.6776926583814098
terms related	1.549729132180171
current application	1.620425994955554
many information	1.5893265024752146
second author	1.862480872660861
nlg systems	1.77528977442187
constraint violations	1.5445755674396429
system incorporating	1.5338735539822874
token sequence	1.5525101323731425
first query	1.526993068994652
different labels	1.7178162994368975
tool allows	1.5627352601669195
name lists	1.5815739789581462
since g	1.5514607572627888
event description	1.526611051483608
lexical specification	1.570316782164213
process makes	1.6002993454155385
noun relations	1.511205120616144
new tree	1.7508746200681462
sentences within	1.830977504859071
deterministic parsing	1.6622497604723905
predicate arguments	1.5306095239523139
first generates	1.5887468924062063
two articles	1.5580525647564991
john took	1.5332519261072983
right end	1.646744608758814
general language	1.8295667756301486
martin et	1.6583627310569802
words used	2.0947452402038325
search would	1.5156323250636554
sentence 5	1.5357202826431067
length 3	1.5987853107912973
opposite order	1.5111306094308554
grammatical analysis	1.861219650067904
specific source	1.522760613339579
performance without	1.636252690439584
tagged examples	1.6109485453156451
viterbi alignment	1.6520331986401513
phrases headed	1.5177700906840073
best word	1.7835709027754512
certain constraints	1.8289520070923908
high percentage	1.7724452244005127
consistent results	1.5482870987025659
representation formalism	1.7573580678673808
cannot perform	1.507007268326853
feature name	1.547363551399276
phrase length	1.5846971394574594
one strategy	1.6933483923290389
solution could	1.5706734993195162
mark liberman	1.6756673695466664
certain parts	1.7440244734618833
summarization process	1.5576258094240625
limited domain	1.8960156547888023
representative examples	1.60913070763407
three readings	1.5595834466874934
important consequences	1.6770235151848842
equal length	1.5836260411907606
causal relationships	1.530984627359544
summary generation	1.582214991160709
unsolved problems	1.5395099438048585
features provided	1.5156323250636554
also describes	1.7360338540431246
two solutions	1.668667523959836
initial probabilities	1.5363485810301154
recognition accuracy	1.9811965276304644
search methods	1.6619550982161897
tags per	1.6238980180175153
two functions	1.895723952724911
correct output	1.667624855366924
human behavior	1.6429839558380457
english newswire	1.5395099438048585
specific language	1.9168037002125442
composition rules	1.636646823017417
rule learning	1.728313524866515
achieves high	1.5818515289928636
key information	1.5894574345512695
rewrite rule	1.7778783188746965
un texte	1.5008147468321362
one needs	2.078829580996261
wer e	1.5955127300501624
last type	1.5300728913253212
refer back	1.78367501742586
new parsing	1.6125713976431841
mt system	2.103580793038742
based transfer	1.5152507881845452
processing results	1.56819296354213
achieves higher	1.5839427016111838
extraction phase	1.5524289354793175
automatic identification	1.8536078899835366
full grammar	1.630872175667311
different grammars	1.737796564580819
unification grammars	1.818355993970704
chinese sentence	1.8664587742150578
lr parser	1.6898178602471314
every pair	1.9786669971499953
bootstrapping algorithm	1.568699490311856
different uses	1.6791278880803084
discuss two	1.8169609508852202
sentence pairs	2.0486392531867335
n may	1.5771944393968738
feature descriptions	1.5521385124841287
problems arise	1.90909006170953
new entries	1.7752998143791774
language requires	1.6458797346140277
information according	1.530072891325321
phonological constraints	1.527798843283345
becomes easier	1.5645241345053709
preceding paragraph	1.5300728913253212
cannot hope	1.5445134575869865
word context	1.8031172240991835
problem solver	1.5737878897847057
pronouns may	1.5378646275607575
worse results	1.7310382465063303
simple pattern	1.8258638900984314
g c	1.6451812514041233
various fields	1.618103745190334
european union	1.7976770886249251
whole utterance	1.636127944690715
set c	1.8551184618967624
frame semantics	1.6084755856610966
separate tokens	1.5482870987025659
multiword expressions	1.7114133464348789
small set	2.326924120501176
key idea	1.903086119735952
rule n	1.5656419863875413
cannot go	1.6056035510036821
modular design	1.6243785718679462
form representation	1.6263504263989454
clusters based	1.6349068916737026
particular case	2.0623730333830017
discourse focus	1.642322701735992
report experiments	1.507007268326853
right hand	2.1207470092968106
therefore conclude	1.5156323250636554
texts taken	1.5011917588019899
space defined	1.6029051454724903
inference algorithm	1.6020328751843078
score reports	1.5153476455739863
particular source	1.5188051370135929
following pattern	1.5771944393968738
alternative structures	1.5188051370135929
simple top	1.509778922004791
incremental parsing	1.5505066366781595
sentence within	1.6388370153723533
fundamental issues	1.522760613339579
text analysis	2.0802831666304122
query term	1.7695561678556486
linguistic rules	1.8625501947232996
makes decisions	1.5445134575869865
presented results	1.62412689036662
traditional notion	1.5821179703326838
slot fills	1.5637591848239811
complex semantic	1.775412687599116
som e	1.5288687562201928
6 months	1.598785310791297
features within	1.6805174920443955
appropriate context	1.7390716057151705
japanese expressions	1.5338162779728006
model built	1.6853236803857272
2006 association	2.3578327704833804
van noord	1.8856487905848842
whole structure	1.5992286050546423
unseen word	1.6004455166204201
first term	1.6322708143422067
longer strings	1.5611231576018243
probability theory	1.6123019943253594
selectional restrictions	2.0964770375005783
zero probability	1.8982606347172657
system proposed	1.698978605212456
precision rates	1.729055186483526
travel information	1.9248097662199768
structural analysis	1.7307082666906441
foreign language	2.0241354012930253
intended use	1.5904771784117462
case assignment	1.579024394252398
state verbs	1.5602367449199286
general view	1.5188051370135929
different pieces	1.5656419863875413
distance metric	1.825958954485273
systems developed	1.881355605540516
last part	1.7804821942340125
formalism called	1.6288981647434881
smaller number	1.970792475501502
multiple types	1.6244843928316635
systems presented	1.5037367200448983
parameter value	1.64005253972614
cannot simply	1.7824175027740568
alignment procedure	1.635608154457774
validation experiment	1.6128408409726123
optimal model	1.5214529152145557
plural noun	1.917622588029473
system supports	1.6169986020906966
hand tagged	1.5461959912075431
human speech	1.791502542625577
relevant aspects	1.7570070675034386
written rules	1.707248020051786
declarative knowledge	1.6739611532350311
6 concludes	1.8085266261493145
based evaluation	1.9189894695652845
parsing models	1.8600238934534383
inchoative alternation	1.5131301253834533
promising approach	1.8421833956739693
algorithm requires	1.8948606889825381
common evaluation	1.6390828885872206
resolution algorithms	1.642161933660403
data sparseness	2.1633431401495473
psychological research	1.5164911279959081
identi es	1.5981937492554081
problematic cases	1.7793935740675575
seven types	1.5156323250636554
simple translation	1.5432756752761156
linguistic insight	1.5078976439458076
string z	1.5530428448904363
evaluation result	1.6486519159995732
entity referred	1.6010101084822783
sections 4	1.9817988720673485
word position	1.7843594972978276
poor precision	1.50119175880199
structure grammar	2.153981660998231
good indicators	1.760655880476912
going work	1.5893265024752146
different character	1.646527080057797
actual sentences	1.5707376783218978
v c	1.6556519153644185
start symbol	1.9701296948729654
turing machines	1.513588614190905
intransitive verbs	1.835397226553859
unsupervised fashion	1.6103523297384124
overall evaluation	1.5306061495832655
rules developed	1.5582958710952315
attributes may	1.6229151093758099
important advantage	1.8335879481729591
crucial property	1.522760613339579
spelling errors	1.7846066411847679
cannot find	1.5443036363499523
12 words	1.5478241562993658
commercial mt	1.532341016095725
languages used	1.6445809590877873
function must	1.6190328102134979
specific senses	1.6431586438232257
3 corpus	1.6110259200690864
immediately precedes	1.7074092050419734
research papers	1.6490634212159685
first question	1.9055997199599979
sample output	1.6349068916737024
technique proposed	1.5611135103157159
tile process	1.5511943810330642
possible configurations	1.5633179841307894
two alignments	1.547363551399276
distinctive feature	1.625247141868945
word processors	1.5585101349998933
system showed	1.513130125383453
one corresponding	1.854863870676686
matter whether	1.6822401779866478
unique words	1.7209238802191167
wrong answer	1.587700203918434
technical writing	1.5982293755605608
names used	1.6010101084822783
increased accuracy	1.512925200463965
retrieval tasks	1.7290897920609734
single tag	1.6459404763673076
finite list	1.5230483704868893
temporal expression	1.6165408623564324
universal set	1.5036575034310684
processes involved	1.7244811761730507
processing language	1.5177700906840073
one relationship	1.5278165817298155
better retrieval	1.524534874088451
applicable rules	1.62291510937581
german texts	1.585836497883611
first use	1.8214042020426562
human memory	1.6289397129031118
constraints using	1.5408560126058508
complex set	1.6388370153723533
also play	1.7761936996438532
adjectives like	1.5046880378004521
paper represents	1.5511943810330642
size 2	1.570915482058415
different roles	1.8151628315113486
every occurrence	1.7461343450093207
various relationships	1.5111306094308554
node would	1.5466472792181065
c 1	1.7576251547849264
information theoretic	1.8285126987707607
systems trained	1.648531911795155
relevant data	1.7711984434479504
contains 2	1.5156323250636556
positive instance	1.511808504950452
language defined	1.6227794168657732
3 indicates	1.5445134575869865
speech labels	1.5978089694258568
use large	1.6002993454155385
clustering results	1.6229258234877961
sophisticated system	1.5111306094308554
better measure	1.5156323250636556
two phenomena	1.60010422258619
system allows	1.960463739034261
grammar theory	1.6235405672398666
semantic content	2.1707244807162835
semantic head	1.6569656508125055
shallow parsers	1.5534033615805192
full discussion	1.6862623517634696
ne sont	1.526587396242133
values based	1.5547189562170496
representation allows	1.7002798641346384
des r	1.502119026774131
language training	1.5258093094263683
fixed phrases	1.6622075368931803
automatic method	1.8854140860881703
structural differences	1.6960474084795663
event type	1.719386761076026
parsing grammar	1.5533105783372858
set consisting	1.8760339018620866
corresponding rule	1.7113320920759185
less data	1.7164164810084093
passive voice	1.9713430289732994
different trees	1.6001726480423764
maximal projection	1.733673481428304
constraints expressed	1.6253145416137307
information described	1.572834038198836
manner similar	1.869426198474462
method involves	1.6663716312166414
target predicate	1.5166365174551064
acceptable translation	1.5609868491677328
message processing	1.600330740560592
second line	1.528361137676114
tagger described	1.7130162238987903
subsequent words	1.549729132180171
many forms	1.7693843609764293
approach might	1.757636103678803
another one	1.5449695027782204
english phrase	1.7349657493438793
c x	1.5558827079957558
contains rules	1.6276749302936424
parallel implementation	1.5288687562201928
lets us	1.6943309563521631
recognition rate	1.7485952162355485
effective search	1.5371982986173842
different formats	1.6635555237714856
different interpretations	1.9051770647732917
single character	1.8169126795335666
sample space	1.5830532630763527
verb construction	1.5993476136747842
also permits	1.636252690439584
de nition	1.6642381999162033
whose length	1.7059629702105157
search engines	1.9847508983784776
trees representing	1.611203449372866
thorough discussion	1.5547189562170496
density function	1.6901441215325457
program committee	1.6835891090237518
present methods	1.5354510398033074
linear sequence	1.7915238101209197
agreement constraints	1.6046743940979167
several works	1.586137214651611
best parses	1.5518490056013698
large document	1.6681052948574107
deep semantic	1.795416273705927
context word	1.667525290769006
model shown	1.581010805798165
disambiguation systems	1.632350904386585
main clauses	1.821362726615168
similarity function	1.7736511010860982
parser assigns	1.579873144038688
extraction tasks	1.8808969094272883
additional types	1.567922330580758
darpa contract	1.6822401779866478
finite sequence	1.60290514547249
using dependency	1.651167532075364
linguistic framework	1.6774002949116091
based approach	2.3213012911995436
briefly introduce	1.7527766729997913
interesting questions	1.6761656524213184
internal representations	1.7956993955459795
simple text	1.7106489753511558
following experiment	1.7051629942513449
one containing	1.8526411177747388
bottom half	1.5408560126058508
strong assumption	1.5582958710952315
discourse planning	1.5359030103656437
effective method	1.8081612299749892
document vectors	1.5926954580670591
necessary information	1.7391214881560129
compact form	1.5887468924062063
technique allows	1.522760613339579
current algorithm	1.6864415156037387
word set	1.7348373970382251
making decisions	1.6977890880751751
author would	1.8693342036195517
linear classifiers	1.5111765881035293
two syllables	1.5887541074190639
journal article	1.5445134575869865
strong emphasis	1.5511943810330642
one complete	1.5739592165010823
loss function	1.6195242311988007
knowledge would	1.6785607164239145
litman et	1.570307039750304
time step	1.62314706631481
correct responses	1.5899498895443456
three reasons	1.8400549160686026
errors would	1.530984627359544
person singular	1.955390528915072
following reasons	1.9346264136603446
system without	1.9468296031689871
following text	1.8247418743150783
longer documents	1.5862046971870256
earlier experiments	1.6661309301322418
consider sentence	1.5821179703326838
written form	1.7570753244955584
overall approach	1.6961394033344752
limited training	1.7098259615519555
second np	1.5324203178689446
extraneous words	1.50119175880199
rapid development	1.8067649940329007
linear order	1.9868063256825843
explanatory power	1.6806432720347573
main function	1.6507624162108991
correctly identifies	1.6581363223870689
null consider	1.5408560126058508
probabilistic parser	1.624733715539981
different communicative	1.554089533378209
principles underlying	1.6213967902634534
computational cost	1.9435204264534947
parse selection	1.5536877604242343
many patterns	1.5559715171228279
1 depicts	1.7706038030264342
approach improves	1.5188051370135929
appropriate set	1.8160399182853657
different point	1.5739592165010823
permet de	1.5266927341903875
represent partial	1.530072891325321
verb class	1.8005220360038268
symbols x	1.522760613339579
new content	1.5226149809040495
selected sentences	1.8200833780386292
conversational participants	1.519311937779724
approach could	1.9295999867003262
size increases	1.7748214684206798
feature spaces	1.653055951158421
examples illustrate	1.862480872660861
real applications	1.7837977667040845
longer texts	1.5771944393968738
smaller values	1.5037367200448983
full understanding	1.6548911840144862
textual units	1.5118337798379557
pairwise comparisons	1.5509681765605983
disambiguation procedure	1.5134608250161965
human languages	1.759445281451644
constraints specified	1.5908167675916909
containing words	1.7127967389448444
large set	2.206478689703216
partial description	1.6118962467644753
metaphorical uses	1.514963940033946
resulting output	1.5188051370135929
perform quite	1.5195036419368688
transformation process	1.5176055023927577
l p	1.585589015734039
bracketed corpus	1.5238520435615595
structure representing	1.8269893905061079
resources required	1.6849207890717186
binary classifiers	1.7388221068890681
share certain	1.5707376783218978
recognizer uses	1.5378646275607575
using model	1.5320698902069676
knowledge structures	1.806153624744625
prosodic information	1.8033843972028378
also test	1.579873144038688
look like	1.789549144834797
labeled trees	1.544725016152176
source words	1.8685646355136083
available knowledge	1.643362624369706
recognition rates	1.6709639962923304
radial basis	1.5238894580118156
intelligent tutoring	1.7055661321474647
larger unit	1.513130125383453
whole relation	1.5484773744288014
semantic frame	1.734614865007277
frame representation	1.7064401341673925
certain part	1.6392309045877975
always use	1.5926202003411678
contains 100	1.507007268326853
one goal	1.7804821942340125
findings suggest	1.586137214651611
recognizer may	1.5037367200448983
atomic symbol	1.5245348740884508
theory may	1.5378646275607575
tagger trained	1.6120909756050135
five classes	1.6336649929396208
correct parses	1.7213538476676387
substantial differences	1.5111306094308554
xml document	1.6532251944337588
concept may	1.6602505116127795
category b	1.6355513724121953
human processing	1.6184033873746562
statistical information	2.0498641489165697
two errors	1.5201190767490378
function p	1.7014120471593681
pipeline architecture	1.688905913995982
query vector	1.528470231222848
result reported	1.5188051370135929
text rather	1.6570039092175395
either use	1.5645241345053709
local context	2.1083087779229324
accuracy rate	1.8182019713055593
retrieval applications	1.744024473461883
lower number	1.583051127543801
one drawback	1.5611231576018243
verbal predicates	1.5190124946356913
good use	1.6669753881293854
errors could	1.699626140637926
morphosyntactic features	1.7107454413293794
test example	1.638132008192031
parser attempts	1.5378646275607575
final goal	1.6202558521618853
description language	1.8505615112699982
represent syntactic	1.6169986020906966
see examples	1.7664483542916203
rule base	1.747010743134966
average accuracy	1.9146575774102481
parsing sentences	1.7392306202979373
lines suggested	1.5188051370135929
results shown	1.9727498880413366
corpus processing	1.5783139971010591
linear interpolation	1.9237976887489092
sheds light	1.6056035510036821
earlier drafts	1.842361963116756
learning theory	1.6534809088276403
actual performance	1.6761656524213182
different output	1.5749574214167732
parameter space	1.7255437861187248
whose similarity	1.5111765881035293
generating sentences	1.6548911840144862
results seem	1.7692801772988378
structure within	1.7266279442877566
semantic information	2.4278025676600676
hmm model	1.7746917047787159
available features	1.5963618381704534
inheritance hierarchy	1.8481833871141922
general mechanism	1.7125697263999058
art results	1.732418304911139
problem faced	1.5482870987025659
arguments may	1.6289349127929258
good agreement	1.702319805493666
basic knowledge	1.5774621158089812
text corresponding	1.5156323250636556
stage 2	1.5964798514858418
present task	1.5408560126058508
additional parameters	1.6289349127929256
difficult issues	1.5111306094308554
alternative analyses	1.7638519134473611
much work	1.7841818105179341
based error	1.6997945042000155
technical documentation	1.680316228658861
human dialogue	1.7480890713257706
unannotated text	1.638266100002176
temporal precedence	1.5025792150970103
processing stages	1.7600309622262276
np whose	1.5195036419368686
structures must	1.6741803270499043
produce output	1.7722692119182182
conceptual representation	1.8682142152421233
phrase chunking	1.7348780748705352
systems employ	1.636127944690715
length greater	1.640910700935025
role assignment	1.66867928477435
dumais et	1.6063718989139955
low precision	1.9464728493925139
p x	1.5482549302212707
describes several	1.5338735539822874
data might	1.558193976095525
process using	1.7903084084950078
anaphoric pronouns	1.6874396814175017
tree model	1.7011850419129293
current hypothesis	1.5271898876957897
western languages	1.535992443649265
also introduce	1.8347737830219166
limited context	1.6867138399296573
broad coverage	2.0563574229240293
represent lexical	1.6699400022790052
document length	1.777076908323775
algorithm gives	1.6479791814036409
example illustrates	2.0628150761439272
segmentation model	1.589868156857337
stochastic context	1.7533020269767863
training algorithm	1.9475436787926206
character sequence	1.7802557409925852
hmm models	1.6916055323576757
system assumes	1.6870613014843863
coreference relation	1.553006516391327
english corpora	1.7926317794512274
program consists	1.5306061495832655
also serves	1.7663553975095576
important terms	1.5001499530265456
extraction applications	1.5836260411907606
user enters	1.6440429413357316
overall error	1.7832211299202094
data entry	1.6339908167801867
nist scores	1.5332433564048185
tagging task	1.871709375796212
simple transitive	1.5156323250636556
exist two	1.6289349127929256
feature path	1.5768549909399514
uniform way	1.873688261732734
define one	1.5111306094308554
possible derivation	1.5459182428709468
handle cases	1.7125697263999058
null tem	1.5645241345053709
large margin	1.5879016565709436
reasonable amount	1.771158110485396
typing errors	1.56809128866196
xml files	1.5156159685357806
constituent boundaries	1.7473451508274778
pioneering work	1.6658904115324105
likelihood score	1.6242274386814821
phrases according	1.522760613339579
occur less	1.7447050080321944
binary feature	1.9092153685793047
performance differences	1.6595315975547575
using features	1.8873120634175615
performance increase	1.5894574345512695
inflectional languages	1.5721984255842532
state language	1.5145911489630133
correct pos	1.6371456055941147
n word	1.5044920645792554
node represents	1.882040647376088
description length	1.6986300774148018
development corpus	1.858489000549072
sophisticated techniques	1.5973170078864487
word tags	1.5737809180564613
richer set	1.7362528178634165
thousand words	1.78367501742586
language processors	1.5555254798167089
objective measure	1.569522297316359
like speech	1.5887468924062063
many helpful	1.8143530076667913
models proposed	1.5706734993195162
single output	1.5969765857928704
derived trees	1.7490082332363142
syntactic representation	1.9764769681715497
different word	2.1182940641177233
collected data	1.7770765038189547
general information	1.7571585132549057
spatial relations	1.645710613844713
4 cases	1.5887468924062063
text encoding	1.665966286195054
2 reviews	1.6002993454155385
selection approach	1.506350970756098
common parts	1.5523637516609006
existing entries	1.5110975570289593
tree models	1.6149873578651759
measures used	1.764552527683778
local semantic	1.6023597487734618
definition allows	1.579873144038688
que l	1.6528169813755222
parser cannot	1.7642806527794817
reasonable results	1.6920919321535788
web data	1.5841582983757339
noun noun	1.558946154842551
accuracy significantly	1.5081682632911075
wordnet hierarchy	1.82432239491436
automated system	1.6851582698729612
wordnet contains	1.613364934425665
empty sequence	1.603690257582228
known example	1.6761656524213184
simplified form	1.7805487594034692
new information	2.0618199760576594
partial interpretation	1.6108402953637166
rules extracted	1.5777946478302516
published results	1.7892119437137033
text containing	1.7344099527418115
new query	1.6432761587737845
documents relevant	1.646255086919721
salient discourse	1.5440207036648979
thus consists	1.5739592165010823
compare three	1.6525488762732246
two contexts	1.60290514547249
technical writers	1.530984627359544
grammatical sentences	1.8222829059612011
independent word	1.735212277203662
basic operation	1.6920919321535788
classification models	1.5548907159877219
using n	1.8462870448898998
quickly becomes	1.5582958710952315
approach requires	1.922770479069697
domain model	1.9371670550608986
specification language	1.730725343741615
online dictionary	1.5324452505798667
global information	1.6589220683040198
processing architecture	1.535504115117507
linear regression	1.733211756998617
operational definition	1.5177700906840073
2 explains	1.6164339756999313
majority class	1.8010289324079962
alternative solution	1.5511943810330642
general idea	1.9583482121506481
verb may	1.8419375686344317
linguistic facts	1.724249543634281
location names	1.7044184397639865
category v	1.5525978349470975
test collection	1.8491164788198113
category symbols	1.5732948773806894
different topics	1.8404573653732725
null tures	1.597317007886449
constructions involving	1.6190328102134979
learning process	2.0975229152529646
data required	1.7301217323026563
function used	1.7329292822928468
fragments may	1.5111306094308554
primary source	1.7098773566635042
slot values	1.639079401450054
therefore consider	1.5111306094308554
relations like	1.8242641397573343
sentence appears	1.6562676183926621
null l	1.5547189562170496
use three	1.8633999645732353
ralph grishman	1.5922778633779613
text fragments	1.7395780358989157
components used	1.5887468924062063
lexical semantics	2.0860907267336932
language generated	1.8478740865601109
second set	2.083910817334663
full version	1.5440207036648979
pragmatic knowledge	1.829574004708821
also extract	1.6339957705783554
complete parses	1.5978485181861375
priori knowledge	1.6728570782491565
semantic input	1.685906792264415
input strings	1.8325906521088684
number generator	1.5037367200448983
wsj section	1.5707004824113016
random sampling	1.5681789869710123
multiple classes	1.590816767591691
declarative semantics	1.5555118553223353
short sentence	1.6083264157155346
every type	1.7242495436342813
transition matrix	1.5991584292605263
conditional likelihood	1.5339863333367143
line 7	1.5148722773105285
limited semantic	1.5037367200448983
system deals	1.6339957705783554
phonetic symbols	1.6109477731841269
rule system	1.7740789352835293
subjects could	1.5482870987025659
full system	1.6756888541613426
nominal groups	1.51003864472744
similar manner	1.5674466438917918
whole discourse	1.50119175880199
single word	2.2968797443065214
translation without	1.5706734993195162
graph structures	1.5763955456335523
substantial improvements	1.7440226570048845
japanese noun	1.5973753659740138
language recognition	1.5492747978941586
another model	1.5771944393968738
definite clause	1.9087458082472966
tree grammar	1.5074674485067843
lower values	1.6570039092175395
writer may	1.507007268326853
finding word	1.5148387121477085
retrieval component	1.5559046111700163
interface allows	1.7294947296982195
anderson et	1.5194321743502117
avoid making	1.507007268326853
similarity metrics	1.7208322121853676
previous context	1.72910662135661
using maximum	1.8666128851603478
phrase structures	1.7865079901822158
several algorithms	1.6431586438232257
based probability	1.571979880564157
morphological level	1.5906723352433734
important problem	1.9153542114272786
precision measures	1.7586738799323494
first solution	1.5795488348231213
nearest neighbor	1.8553359938352953
one iteration	1.7186826026103983
document similarity	1.5711959820034005
given sentences	1.5611231576018243
two kinds	2.3695681801466844
processing mechanism	1.6669380998074221
identifying discourse	1.5213306860670808
english system	1.710654079339478
maximizes p	1.6902353788351323
similar model	1.583051127543801
past research	1.6761656524213184
particular information	1.664540718454114
words according	1.8763024203315541
disjoint sets	1.8263207686248442
html document	1.5216788181120133
another sense	1.694287390306969
clearly outperforms	1.55955788508706
particular query	1.5706734993195162
formal rules	1.561123157601824
insufficient training	1.6564793622764267
content word	2.0094822008889404
small differences	1.6481174069026685
line dictionaries	1.720650346539263
chinese language	1.9436961892439344
function ph	1.5035193201428414
overall probability	1.6637019978300875
linguistic issues	1.673181149443875
like agent	1.5216869793881187
automatic training	1.678973586324036
lexical heads	1.854784846344389
hierarchical relations	1.7615050511946673
q x	1.6276711628602274
possible state	1.580090580759919
word dependencies	1.5830069681883314
generated sentences	1.7348354891197109
relative position	1.983644452473635
order relation	1.568503829514135
score computed	1.5781539692435587
information using	1.8595568985950037
information carried	1.7140252682775756
similar function	1.5445134575869865
test used	1.5037367200448986
det n	1.5969178045584518
engineering point	1.524534874088451
parsing performance	1.8222837726891417
generation algorithms	1.7501975015649065
occur without	1.567922330580758
best sentence	1.7137586147564197
task easier	1.5582958710952315
certain phenomena	1.5547189562170496
model achieved	1.6308389319954621
possible segmentation	1.5731122769149348
encoding scheme	1.6388335241987575
every part	1.5482870987025659
research focuses	1.5338735539822874
new term	1.6627770739647336
discourse level	1.8675265134102181
entire range	1.6056035510036821
complex ones	1.7280666768298492
added value	1.506173888464562
seldom used	1.5111306094308554
extensive use	1.9554243730568317
valid word	1.5875418145433098
morphological form	1.549777418278953
open domain	1.6604711826815066
acquisition methods	1.6184508370660797
several forms	1.6446419523031575
objects like	1.5445134575869865
known problems	1.6694443901436333
parameter k	1.548378091852222
present state	1.8072659033415688
sur l	1.6125279820720833
relative clause	2.1861771813554376
semantic distinctions	1.8406755333101232
basic aspects	1.50119175880199
semantic forms	1.5677436430884708
bilingual lexicons	1.7024628004900304
nominal forms	1.5602367449199286
information expressed	1.7025517926879408
discourse structures	1.891038649875648
primary focus	1.6933217836194359
model achieves	1.8333359711367634
time line	1.5723199797161809
models without	1.6031224303496519
limited set	2.0000676175133485
knowledge resources	1.7669953285369213
word given	1.8739053835872146
intensive task	1.636252690439584
extraction component	1.530588693084265
gram counts	1.5221690386086855
feature co	1.511916738139604
section 6	2.470294971165524
arabic words	1.5748339774940283
temporal adverbial	1.57028692827999
domain contains	1.522760613339579
human effort	1.8958713955635838
section provides	1.789586914958145
linguistic generalizations	1.8411444574570064
map onto	1.7109326698964096
primary motivation	1.6570039092175395
possible relations	1.846614951791573
algorithm employs	1.5425370803688765
utterance would	1.5078976439458078
great promise	1.583051127543801
rhetorical relations	1.9069140431507476
000 lines	1.5525978349470975
nsf grants	1.815669969260329
le 1	1.611203449372866
category label	1.6832801341126764
simplest way	1.826320768624844
dictionary based	1.5688866561109633
significant ways	1.5338735539822874
use p	1.5926202003411678
patterns extracted	1.6103523297384121
lower error	1.7160749903747887
reliable estimates	1.6229919786001088
columns correspond	1.507007268326853
native english	1.799603378177739
evaluation scheme	1.750582945643058
tools developed	1.6699400022790054
attitude toward	1.5521877619062816
word selection	1.7770523073890436
collection effort	1.5847359287102778
global level	1.6379314810255206
many utterances	1.6190328102134979
one morpheme	1.5612815851895034
new structures	1.5371982986173842
6 illustrates	1.710004784518707
link clustering	1.5101280856670714
phonological phenomena	1.5421554529658548
rules involved	1.5111306094308554
finite number	2.0151859414536526
based interpretation	1.5760874077004159
dictionary words	1.5303826364750015
also section	1.6548911840144864
lexical conceptual	1.7015677796164128
applicability conditions	1.5034216128201032
different modes	1.7356874193708185
mechanism allows	1.6169986020906966
file contains	1.6354102761745795
also compute	1.7344099527418115
text consists	1.5511943810330642
intervening material	1.5556026693790028
whole tree	1.6566290674162105
correct structure	1.6360308115630153
paths leading	1.50119175880199
html files	1.5111306094308554
entropy approach	1.6960957932826415
n k	1.5787377981024027
use two	2.1376648434847905
broader class	1.5611231576018243
likely state	1.548378091852222
auxiliary verbs	2.0220187057059604
general use	1.6500007742396736
sliding window	1.603375614541128
higher confidence	1.5731122769149348
figure 2b	1.642017961592922
examples presented	1.7051629942513449
verbs would	1.5547189562170496
structural ambiguity	1.9308062898844882
dictionary entry	1.9809447556698785
model produces	1.6169986020906966
cases like	2.019915102286255
frequency estimates	1.5574209599123607
rule schemata	1.6622626776667984
tile following	1.9420492179227302
possible world	1.6416216139130013
another grammar	1.5156323250636556
language interfaces	1.9508007600379882
review related	1.5887468924062063
unified treatment	1.5338735539822874
randomly split	1.6943309563521631
english machine	1.7947162658408184
important part	1.514407274817541
questions regarding	1.5156323250636556
finite sets	1.798737353211453
pieces together	1.522760613339579
accuracy comparable	1.5559715171228279
basic data	1.7885756102303294
syntactic node	1.532341016095725
several patterns	1.5378646275607575
german corpus	1.613393312591771
retrieval module	1.5075023054539842
provide clues	1.6213967902634534
following standard	1.6388370153723533
input documents	1.6551170345707917
becomes difficult	1.583051127543801
w n	1.677921968683669
semantic patterns	1.682949557599061
e n	1.7320914908472842
particular term	1.5195036419368688
partial result	1.5177700906840073
three distinct	1.9210964090780225
much interest	1.6630062643878185
algorithm runs	1.7629040348615774
rough estimate	1.6190328102134979
initial context	1.510982586079437
based unification	1.6036172388145076
sentence constituent	1.5037367200448983
highest scores	1.637143939815349
clarification dialogue	1.5002231491234963
lexicon entries	1.9497079459951687
crucial role	1.7551708233835797
actual english	1.5511943810330642
phrase patterns	1.600330740560592
derived word	1.6327406427880258
partial functions	1.5916991976366937
stolcke et	1.644206051302086
whose left	1.7264874414341258
system begins	1.5547189562170496
linguistic context	1.9383339528227037
shows one	1.8095394855795697
experimental settings	1.6487334099083149
problems must	1.5111306094308554
right column	1.6481174069026685
biased towards	1.7723423583015616
output representation	1.6329266367902655
appropriate analysis	1.5156323250636554
useful features	1.8240492113508866
maximum entropy	2.2203010454325147
corpus provides	1.6862623517634696
lexical functional	1.911613097087989
case analysis	1.5937866607272961
share common	1.667913900271795
knowledge intensive	1.6556402997689075
implicit knowledge	1.5015752148233952
feature vectors	2.064434395111772
direct comparisons	1.641194066082254
basic feature	1.6525487091262194
using feature	1.7315454616552706
observation sequence	1.6735982667927942
another object	1.7266192375553124
terms like	1.7654468093970939
information specified	1.5771944393968738
word lists	1.954951094644153
clear definition	1.533540614646662
predicate argument	1.816250090801573
easy task	1.82817797915447
complex syntactic	1.82489178787546
original rules	1.548378091852222
assuming independence	1.5637776281819353
two partitions	1.50119175880199
methods rely	1.6002993454155385
better word	1.530072891325321
discourse modeling	1.5105194311609613
tag combinations	1.506173888464562
model given	1.5582958710952315
multiple entries	1.5838204221214762
context free	2.0961952346128605
full morphological	1.548694506454534
fo r	1.7596314455962725
pairs using	1.7229409862835219
based algorithm	1.9271881542239413
immediate dominance	1.8345107278653698
network representation	1.6534585518633962
automatic assignment	1.5432756752761154
uses standard	1.5445134575869865
graph represents	1.5611231576018243
treebank parse	1.586322666941451
good solution	1.5894574345512698
rich variety	1.5156323250636556
summarization systems	1.838350694411104
main research	1.5869693547698929
likelihood estimation	1.9912366268494885
major approaches	1.5739592165010823
bill clinton	1.6636491299482319
5 seconds	1.8390089975983497
deep analysis	1.7522737025012411
tree pairs	1.5456894733342945
major categories	1.7998238312625219
performance drop	1.5195036419368686
model selection	1.6128323792582497
current node	1.8608176934634084
modus ponens	1.599825332167974
available corpora	1.5496940864561997
structures used	1.8772858029117085
general task	1.6246289167870047
random sentences	1.5226149809040495
trees contain	1.522760613339579
line corpora	1.5081682632911075
symbolics lisp	1.5977445137418091
using latent	1.533540614646662
parser fails	1.6431586438232257
300 sentences	1.6573573789325868
1 summarizes	1.9991298619823588
detailed model	1.513130125383453
new directions	1.62412689036662
candidate parse	1.6026021709237592
third clause	1.5635202226326745
syntactic criteria	1.6881159108913937
language specific	1.9722802742583074
hidden variable	1.6636875569150291
level representation	1.7447191026627054
modi er	1.5222926525275147
extraction module	1.7469082772669065
two authors	1.6073715579162529
user utterance	1.695126215017562
classifier trained	1.7285754857283318
strategy employed	1.5338735539822874
acquisition task	1.513130125383453
main steps	1.8891393526069922
optimal number	1.7077239302724851
r r	1.6046375370034922
cue word	1.6458825585023167
syntactic theory	1.7671236477159042
particular use	1.613624660358136
best path	1.8453860641794237
one verb	1.9155776323059375
et a1	1.6189908188575388
without loss	2.034969625529664
class classifier	1.6620346946445324
recent results	1.7637790091991556
weakly equivalent	1.740683076749423
general patterns	1.6039224906565077
close relation	1.558193976095525
strategy could	1.5156323250636556
japanese nouns	1.51910459028178
initial position	1.9098700105470894
approach gives	1.6919937194855956
among others	2.0738009143447953
3 minutes	1.5482870987025659
complex lexical	1.6888951120289843
context using	1.507007268326853
among concepts	1.6556402997689075
mapping rules	1.7686899110008232
provide significant	1.586137214651611
specific task	1.9537552505870721
reliable results	1.572834038198836
utterances may	1.616031378434591
syntactic terms	1.526993068994652
structural similarities	1.507007268326853
computing time	1.5953077643986358
label set	1.6187405900535827
lexical representation	1.9146833928778826
based part	1.6746889186410345
system needs	1.5802330116725902
overall design	1.7130162238987903
lower probability	1.646171744841988
larger text	1.6031224303496519
unification categorial	1.5201190767490376
different pos	1.6908658143327813
new patterns	1.647606153198635
word distribution	1.698533065182804
interesting properties	1.7151558643929241
complete utterance	1.516491127995908
mean length	1.5790358641614346
vilain et	1.5344215722993897
proper noun	2.0371961680115733
also enables	1.7178162994368975
best matches	1.6774002949116094
natural consequence	1.5781539692435587
retrieval results	1.7397217385392463
occurrence restrictions	1.6916392061912078
present purposes	1.8512188678561705
document information	1.5652297135534465
100 documents	1.7225082191984329
smoothing techniques	1.7328496088132712
closely matches	1.530072891325321
systems would	1.8146944333552
small grammar	1.6329374072287361
pruning techniques	1.5282833842964396
system prompts	1.586137214651611
special rules	1.701871126262405
parse sentences	1.8171701892119112
tags given	1.5306061495832655
distance information	1.5760526619236255
real time	2.066448668899419
using corpora	1.648787373441388
average document	1.5177700906840073
many rules	1.77530945408058
information represented	1.7395722263917062
000 articles	1.5784904253929217
constituent order	1.7813226570029788
provide valuable	1.6881707359732032
candidate parses	1.541879256945566
30 seconds	1.6160313784345912
spurious ambiguities	1.5864461078551109
eugene charniak	1.5445134575869865
programming approach	1.6408261147098293
results support	1.7125697263999058
another phrase	1.530072891325321
whose syntax	1.507007268326853
introduce noise	1.5547189562170496
xinhua news	1.7101157332562664
method produced	1.5547189562170496
final score	1.8019934740712262
original method	1.5378646275607575
choose among	1.9266047724500268
extraction tool	1.5389844662749663
large differences	1.6164339756999313
using bleu	1.5320698902069678
lexical associations	1.575791861867291
complete structure	1.561596045888415
100 texts	1.6424832065884565
less work	1.6822401779866478
various nlp	1.7990408723360427
actual application	1.5706734993195162
american chapter	1.5328333954091753
linear algebra	1.5131301253834533
three sentences	1.9505406047324352
data comes	1.6566640459477509
automatic semantic	1.778315678572417
sample sentence	1.8590820793621392
coordinated structures	1.536533126703027
rules would	1.50984903814168
judged correct	1.6058630881077414
little training	1.6190328102134979
possible candidate	1.7014225922673947
v l	1.5061557323373174
dorr et	1.5066734506285937
independence assumption	1.9253038834791274
two propositions	1.6969648365535273
vector model	1.5226035116345313
related question	1.554089533378209
certain criteria	1.5511943810330642
category name	1.5784904253929217
alternative method	1.816243338498942
type e	1.509240505370061
method provides	1.7483010339378418
ascii characters	1.6400525397261398
four rules	1.6741797752731205
following property	1.702091221900512
corresponding value	1.5408560126058508
tile corpus	1.591823957496618
uniquely determine	1.6365087328513843
history h	1.5635202226326745
results given	1.7785412351430137
orthographic forms	1.5188051370135929
algorithm assumes	1.6708590810843598
base contains	1.7430424321630982
similar sentences	1.7613650996483032
resolution methods	1.5360785028369
general linguistic	1.8663414731337835
character sets	1.6653939603544723
lexicon using	1.6429565433589586
sentence including	1.5137580895709593
feature vector	2.1361336617685396
word clusters	1.6193143568724875
inductive logic	1.5981652528739312
previous topic	1.5823061650599628
restrictive relative	1.6615567209776185
main advantages	1.7842948765920488
discourse participants	1.647823647453129
human readers	1.721391785438688
best hypotheses	1.6973344415033287
earlier study	1.543442208482524
straightforward way	2.037220780165326
semantic analyses	1.8511842927044548
highest correlation	1.5635436331872727
extraction results	1.5006743580950452
syntactic representations	1.8115349187237877
log p	1.704851906782138
1 compares	1.6881707359732032
major bottleneck	1.5111306094308554
sense inventory	1.737664283721998
deep syntactic	1.8274114577231202
argument type	1.683806785865954
specialized domain	1.5836260411907606
typical case	1.7680599214474204
parsing decisions	1.6564979233619477
single pos	1.507007268326853
features including	1.6978216692284374
question marks	1.6467286292933436
obvious reasons	1.7006344357165344
cognitive model	1.6183073390782323
single user	1.5874817977813944
selection problem	1.6039224906565077
introduce errors	1.5111306094308554
first search	2.02805739888767
local dependencies	1.6902912181296048
ambiguity inherent	1.507007268326853
significant time	1.5445134575869865
l b	1.5645241345053709
different frames	1.5484072632580106
supervised wsd	1.6553715426359463
real language	1.6998973603836445
given l	1.522760613339579
2 table	1.5559715171228277
features make	1.583051127543801
tag sequences	1.873052408912465
est la	1.547136738623032
component could	1.5408560126058508
rule application	1.9942488421845612
rayner et	1.532155032332514
one role	1.5559715171228279
based frameworks	1.5894574345512695
binary values	1.636252690439584
overall result	1.6741803270499043
many methods	1.716869108797698
relations holding	1.7650393044753439
basic components	1.5746629122870885
manual annotations	1.5851219252927344
parameter optimization	1.6513198720665856
grammar productions	1.567922330580758
initial set	2.00484034174229
document would	1.5482870987025659
whose task	1.636252690439584
relevant terms	1.6174346226419531
typical user	1.6117266025648125
empty sets	1.579873144038688
sentence initial	1.6734445616305185
associations among	1.5156323250636554
novel features	1.571949426413436
different areas	1.7229550745276576
functional dependency	1.5988376904296486
first method	1.9201474411779325
200 words	1.7968489223306638
dependent models	1.7093238242316486
generative probability	1.5375741924898052
electronic text	1.549729132180171
new items	1.7408763045039124
28 aol	1.5111306094308554
require extensive	1.6315901311540408
time recognition	1.5896049380052686
syntax rules	1.6134572115167787
leaf nodes	2.0365322909097077
cannot deal	1.708737834943295
key insight	1.5408560126058508
fundamental differences	1.5195036419368686
university press	1.6448472409336694
active sentence	1.5440207036648979
morphological segmentation	1.5081419489853416
also exhibit	1.643362624369706
automatic recognition	1.8272891384792251
precise way	1.594773126307433
cannot understand	1.522760613339579
second analysis	1.561113510315716
description would	1.511205120616144
one correct	1.8137066482575526
four methods	1.6428721706698721
order language	1.9006803576142963
sentences 1	1.5829912568215723
significant source	1.5156323250636556
nlp technology	1.7609869990584186
system evaluations	1.5482870987025659
parser identifies	1.5111306094308554
rule specifies	1.7458878463811769
tag features	1.5844299030199056
several possibilities	1.6861938694548688
column 1	1.554089533378209
syntactic behavior	1.7597065451790501
sentence becomes	1.585589015734039
second row	1.884022422253084
linguistic considerations	1.659111358304426
chapter 3	1.6734445616305185
implemented systems	1.6058952092043035
word rather	1.530072891325321
authors wish	1.8672507822430129
selection algorithm	1.7961243554191844
previous tags	1.5669629240805358
learn patterns	1.5118192695694388
document level	1.7320145016031534
attachment point	1.5507684665328343
parsing techniques	1.9978651004167058
resulting structure	1.8210968407178914
classification performance	1.7612134087595024
lr parsing	1.6733017867376878
following reason	1.530072891325321
semantic expressions	1.5204616214312412
uses four	1.5338735539822874
content planner	1.5683820376497573
preliminary experiment	1.8717303964747591
provide feedback	1.7056872478061593
language analysis	2.0993473589819898
adequate treatment	1.5547189562170496
boosting algorithm	1.5386754045589035
male speakers	1.5360265912120767
include information	1.8543755849033745
surface representation	1.559310343330874
information relative	1.5156323250636556
constituent ordering	1.558193976095525
training database	1.5368535240041454
human intervention	1.9842877342096832
following measures	1.522760613339579
domain corpora	1.5963645436754332
condition part	1.568045616265294
formed text	1.5716157403241997
possible words	1.8058012426546128
systems today	1.6339957705783554
phrases using	1.6419409233029296
one contains	1.5188051370135929
subject area	1.7068560584550412
individual utterances	1.7277524208272168
stressed syllables	1.563745002290025
nlp research	1.8449063217480755
separate models	1.585589015734039
class classification	1.7081102697705386
noun group	1.791106843127193
weighting methods	1.5245918492409605
words found	1.9284723538295767
one entity	1.8610108167031887
formal definitions	1.734954551774483
word basis	1.6858754220956373
last character	1.672270001196345
technical terms	1.9663966491104754
level node	1.5324203178689446
useful feature	1.731146052909204
one operation	1.5378646275607575
occur either	1.6056035510036821
simple one	1.861457531113189
based account	1.552544074218515
section 9	1.720121596462908
two rule	1.5420261883340531
two operators	1.5245348740884508
16 khz	1.561123157601824
best results	2.2293559647157313
computer speech	1.5482899355062387
rewriting process	1.543676045678863
time without	1.636252690439584
generate several	1.5887468924062063
hmm system	1.5447363611042522
nonterminal nodes	1.6726737667535245
relationships within	1.530072891325321
discourse relations	1.872469346338013
different sorts	1.6921138523491286
university corpus	1.5624089877084573
given verb	1.9178106151340926
matching word	1.579873144038688
single constraint	1.5210740331567267
appear within	1.7875643064398807
available machine	1.5269930689946518
individual scores	1.5521452212545803
useful suggestions	1.583051127543801
length 2	1.6725804717867598
different applications	1.9401615262046534
models assume	1.507007268326853
since x	1.530072891325321
something like	1.9289866128040307
different structure	1.635876749820076
final syllable	1.5997071859401508
one list	1.5942408057154906
incorrect parses	1.6212858034871387
several things	1.7051629942513449
several meanings	1.6164339756999313
classifier using	1.7653643433083566
level concepts	1.6066593791852313
french translations	1.561596045888415
data needed	1.6481174069026685
date expressions	1.6517595614954423
sentence 1	1.8593892187680063
lower part	1.7242495436342813
another research	1.5338735539822874
source document	1.7158343870058146
text available	1.582117970332684
based annotation	1.5514084498024894
questions must	1.5482870987025659
linear precedence	1.9526131727632223
positive result	1.554089533378209
final list	1.5874817977813946
easier task	1.5739592165010823
low recall	2.0299476889167227
basic intuition	1.583051127543801
multiple possible	1.776197982006045
perl scripts	1.5188051370135929
type definitions	1.6768878485136742
model consisting	1.5269930689946518
l v	1.5180580646513062
features 1	1.5117685557139824
crucial feature	1.5511943810330642
estimate p	1.8266430915528944
system differs	1.5445134575869865
different event	1.507007268326853
different orders	1.753141620628011
several utterances	1.5037367200448983
background information	1.9193279391000972
knowledge based	1.8181156389846953
state methods	1.6130355690310525
labeled examples	1.6537135667911953
next steps	1.7723731972113645
syntactic cues	1.6595053160608113
grammatical role	1.7977110419234872
parser performs	1.691150491959004
city name	1.5083664593472759
many variations	1.65303397348486
natural language	2.7661901577571224
specific relations	1.752957074955098
segmentation system	1.7409406303564812
corpus evidence	1.6768381249411435
thematic roles	1.952690013502039
functional role	1.7187721456738094
detailed discussion	2.05276417529398
grammatical knowledge	1.8475796985629658
johnson et	1.797164775880391
problem mentioned	1.6002993454155385
different sentence	1.8012230473134534
earley algorithm	1.582140954606702
single utterance	1.7994132093067445
singular present	1.635478837248017
free languages	1.8425152187363072
sentence x	1.6694693816064237
graph nodes	1.5087853581980926
data shown	1.5941108460767777
based alignment	1.6571272988274606
pure bottom	1.5364292984639674
b e	1.8911837928448592
interaction among	1.7507002914769163
language differences	1.5087853581980926
bigram models	1.6132189658684781
using machine	1.9038194339738157
latent semantic	1.9721861090718962
linguistic choices	1.5133843094067987
name entity	1.6522417818837116
automatic procedures	1.5690244007917742
previous experiment	1.791532813992259
contains words	1.6977890880751751
complex interactions	1.645814372624544
nlp components	1.6749070851382584
function symbols	1.5650135440475994
lexical database	2.0350749810146986
virtual machine	1.5770376923626213
verb classes	1.8519577615073872
subject matter	2.028760670724547
unrelated words	1.590816767591691
existing algorithms	1.692345113033763
advanced research	2.1645105308374455
epsrc grant	1.5739592165010823
work must	1.579873144038688
tagging method	1.5620277646246894
words directly	1.5559715171228279
vp rule	1.6520801854479243
default inheritance	1.5325990619796177
third element	1.509778922004791
srilm toolkit	1.5881174695673654
one sequence	1.6593965399315964
empirical observations	1.511205120616144
similar performance	1.8679773486290112
exponential model	1.522539932907843
speech community	1.5604609558898792
second strategy	1.6904151818990054
salient information	1.5183170024876456
essential step	1.5412463844444777
single structure	1.682216411264344
patterns could	1.5781539692435587
initial weight	1.5040418526142796
g n	1.5427206568569156
computing similarity	1.5300728913253212
name list	1.6116815153004185
set n	1.774296903622264
summer workshop	1.5211629864223772
example 1	2.092604769378806
translation function	1.5186578431725237
timit database	1.5212787788597375
individual probabilities	1.506173888464562
bilingual word	1.619007917060849
ai systems	1.5963136299601688
much noise	1.588669192462068
current speech	1.7382653107240496
local syntactic	1.8395231738197289
first author	2.046192095774349
translation quality	2.0063819807499508
correct results	1.75176197502999
extension would	1.586137214651611
practical advantages	1.6802940634668402
possible approaches	1.6802940634668402
basic approach	1.8002132763314427
another way	1.8908964625215765
character error	1.5046015111554794
internet search	1.5556026693790028
dialogue processing	1.6557741103156594
possible categories	1.6885416404927547
annotated training	1.9537205679334093
syntactic levels	1.6935976067520353
corresponding elements	1.5408560126058508
gaussian prior	1.748151579009167
crossing dependencies	1.6071744165950947
two example	1.6462268442574868
formal devices	1.5187014267751215
bayesian classifier	1.5481276547450395
flexible control	1.5245348740884508
nlp system	2.0200593432079463
translation models	1.9812133500668416
computer program	1.7895346266212195
figure lb	1.5637776281819353
speech acts	2.016130745371435
interesting point	1.6783930166760777
columbia university	1.757999262960783
processing requirements	1.5037367200448983
appendix c	1.5944802544253833
task may	1.707160349122273
given knowledge	1.5378646275607575
probability density	1.6403479883154333
similar way	2.1562685597284403
given moment	1.6481174069026685
mapping table	1.569108177582045
wrong choice	1.5582958710952315
two arcs	1.5281892460232738
serial dependencies	1.5522235552884596
unseen test	1.8846529260221856
single sentence	2.1278687364794404
good approximation	1.7918347255296279
sentence hypothesis	1.5102757155001294
output strings	1.630794959456562
domain coverage	1.5216148258836173
literal interpretation	1.575120517602147
training process	2.0057260680792224
next version	1.5803694295911774
algorithm begins	1.5973170078864487
good predictor	1.7238311541316482
algorithms may	1.703359686465308
nouns used	1.5036589189128708
e ect	1.6166327574984063
last point	1.7531599391786739
constructions like	1.8083839297316797
words long	1.8862237705222862
lexical sign	1.505682674067244
simple fact	1.591823957496618
candidate generation	1.5294363739387484
data generated	1.5406210028261875
conditions hold	1.7623638616523687
higher likelihood	1.5156323250636554
classes may	1.7210742795514369
relevant part	1.7105799695961408
null another	1.7408279821914407
apply machine	1.6192219158965702
tag pairs	1.662410346934857
e system	1.7573141382441586
various reasons	1.7178162994368975
various text	1.579873144038688
higher similarity	1.6056035510036821
answer would	1.5749574214167732
phrase identification	1.6247938537692155
al 1992	1.558193976095525
possible translation	1.8174512830777065
third experiment	1.7592150718640294
table entries	1.587161501621602
1 l	1.789920514697096
different parameters	1.7419066941020038
another system	1.7796783399493172
human interaction	1.8001383502657808
last rule	1.6724173909604771
previous analysis	1.6266256462651403
relevant documents	1.981012906526584
database systems	1.6862673227706972
comparative analysis	1.6147495904226123
schabes et	1.6284316044082066
v n	1.6856016919852128
words tagged	1.7776321613977448
lazy learning	1.519886284952312
parses per	1.568564911196704
previous ones	1.8784650087360475
technical terminology	1.6614954022607242
using xml	1.6724173909604771
data set	2.3534333392825135
system consists	2.1230199979442452
called word	1.7002798641346384
one exception	1.9077671215243044
prototype systems	1.542348578385745
syntactic objects	1.5171904119543882
representation consists	1.561123157601824
parse times	1.5564903760588482
types based	1.561123157601824
one data	1.596528355412953
present paper	2.188095778112306
long vowels	1.59361666843504
tile right	1.5656419863875413
overall meaning	1.5466472792181065
cannot parse	1.5706734993195162
reduce computation	1.50119175880199
june 2005	1.9583178457069306
involves several	1.6164339756999313
queries used	1.5226149809040495
system builds	1.6677521240969928
probabilistic parsing	1.7819411328446269
time one	1.5511943810330642
lexical representations	1.7977667739292988
text contains	1.7050242287283273
remaining cases	1.7164164810084093
operating systems	1.729364655157064
directed graph	1.98472511013973
several rules	1.8025991388226652
original training	1.7334330566270602
computational methods	1.703745723125991
long history	1.757636103678803
details see	1.8063973711864887
corpus automatically	1.5300728913253212
initial experiment	1.6169986020906966
research reported	2.039573676891782
semantic rules	1.9501536098870003
lexical strings	1.5042845945627992
hearer knows	1.6178177795915514
analysis components	1.6661309301322418
database retrieval	1.5478356033360376
rule 1	1.7028468612773113
three times	1.5275754705818603
speech applications	1.6411992014239676
measure scores	1.6400027567351274
algorithm needs	1.7125697263999058
case studies	1.613693223189432
many definitions	1.513130125383453
problems may	1.7244811761730507
correct tree	1.5002828953206808
computer processing	1.6502855605841442
phrase rules	1.530072891325321
potential benefits	1.613624660358136
one feature	2.062117866297538
rules consist	1.5037367200448983
algorithm selects	1.734954551774483
set provided	1.5037367200448983
baseline models	1.6054547574156264
pattern match	1.5838923815193273
empirical results	2.0072152557430156
learning systems	1.8742306398154291
generator would	1.594773126307433
external knowledge	1.7454576179250547
c denotes	1.6031224303496519
function described	1.5111306094308554
coreference information	1.562547160752719
speech generation	1.5575133409974997
class names	1.6097015195559559
partir de	1.535812007829782
approach achieves	1.7331845413058429
programming problem	1.5378646275607575
linguistic reasons	1.6169986020906966
first item	1.7615110690109856
using giza	1.6637019978300875
magnitude faster	1.6317923193893897
logical connectives	1.7130162238987903
short vowel	1.524519541445589
proposition 1	1.5472886652691467
many nouns	1.7127334192829031
sentence representation	1.5303756126671857
grammar uses	1.5634115707207186
class probability	1.5668117204815994
different relation	1.5091072366943206
likelihood approach	1.510181823566108
need access	1.5821179703326838
much effort	1.8866230207806127
entities involved	1.608498466467613
one human	1.6358566425614818
variables used	1.5656419863875413
grammatical formalisms	1.812438348688847
analysis cannot	1.6190328102134979
operations may	1.50119175880199
recent works	1.7125697263999058
different verbs	1.7905003219474533
wider class	1.533540614646662
terminal symbols	2.088046082565981
methods may	1.756257461041527
technical documents	1.7047037099610378
two problems	1.9649160250435522
grammar may	1.8382894255788533
among objects	1.570430085810516
ditransitive verbs	1.567922330580758
see section	2.4186731054395914
syntactic processing	2.0060726850989683
human mind	1.6479791814036409
tile best	1.611203449372866
different ways	2.226458599444898
part contains	1.5584368837489078
procedure uses	1.5656419863875413
language competence	1.507007268326853
matching technique	1.5908167675916909
languages using	1.6785607164239145
sense 1	1.8022155849286472
sentence pair	1.9834041329414582
statistical methods	2.2098382964348002
term research	1.5649365246686138
c must	1.6112034493728657
conduct experiments	1.6614869878819327
semantic fields	1.6297208476524856
known word	1.7242496292119955
analysis system	1.95320949566427
functions defined	1.583051127543801
cannot represent	1.657773782950173
textual material	1.5786032299799224
informatio n	1.6094040109095726
tagging accuracy	1.8851766098313978
written text	2.0349017196608035
algorithm introduced	1.5582958710952315
names using	1.5525978349470975
computer dialogue	1.6278935154044052
segment boundary	1.718663140808102
evaluation efforts	1.5195036419368686
represent knowledge	1.7130162238987903
accuracy obtained	1.7715893053696299
recognizer using	1.5037367200448983
word depends	1.7301217323026563
algorithm terminates	1.6344743064287952
combinatory categorial	1.894837927511114
gram model	2.0375231405963126
recognition grammar	1.5513886204203602
also note	2.0573933666526365
predicate calculus	1.906100299947273
additional cost	1.60913070763407
several methods	1.70808907701393
technical texts	1.7841095940035385
primary task	1.5553381419921846
mutual translations	1.501288165335286
good accuracy	1.7314807712355127
whose nodes	1.8741568758370561
among languages	1.618103745190334
discriminatory power	1.5575616444532088
context vectors	1.6623642478609608
5 iterations	1.512536810936869
artificial language	1.704775376727731
experiment described	1.7738788136980264
last example	1.8505426117197377
cormen et	1.5702560143399067
learning procedure	1.715456961422259
two cases	2.215420562108503
present participle	1.7661892115894728
general rules	1.973745890928323
horizontal lines	1.5482870987025659
following factors	1.6266256462651403
simple clauses	1.5381766791708387
two parse	1.5602367449199288
programming algorithm	1.9836976125740247
human evaluators	1.6546713756026201
free productions	1.524534874088451
related meanings	1.5808501342373025
high score	1.799823831262522
following manner	1.9353279199956488
j p	1.7001871137416193
franz lisp	1.5037367200448983
paper explores	1.8475625472830248
discourse units	1.7953328590835493
gazdar et	1.832121268920017
following sub	1.5378646275607575
hansard corpus	1.7090691254162078
single path	1.6340150320111853
given sequence	1.7051541227358542
classification techniques	1.655558763497436
machine communication	1.657573539378286
theoretical approaches	1.5194321743502117
contain different	1.597317007886449
possible worlds	1.7038487444124804
leave aside	1.5188051370135929
ideas presented	1.7793935740675575
training texts	1.7816709791176688
simple model	1.8395887547046341
corpus also	1.6960474084795663
three characters	1.7235632730239798
human expert	1.781245525783019
different hypotheses	1.5514607572627885
truth value	1.892840493774585
verb frames	1.6501189339703934
linguistic analysis	2.1601925562414492
theorem proving	1.750937974505343
project aims	1.586137214651611
one attempts	1.5408560126058508
new edge	1.7595129886556051
normalized form	1.658543119888802
single relation	1.6030647054745097
argument slot	1.6056742238543773
systems designed	1.6289349127929256
w appears	1.5473635513992758
experiment results	1.8717488800996613
corresponding results	1.5706734993195162
maximal projections	1.5973763813135606
gram models	1.9724960186494997
problems arising	1.6741803270499043
answers given	1.5188051370135929
inference processes	1.5382275852283713
constraints would	1.6802580409155776
main aspects	1.5645241345053709
base system	1.740172951832948
tables 5	1.771031308881795
output texts	1.5580042495673951
acoustic information	1.7202111083648168
question asked	1.6354102761745795
automatic analysis	1.8467169083158186
search starts	1.50119175880199
simple function	1.5378646275607575
bleu scores	1.7632684542462473
standard parse	1.538817697792084
clause containing	1.683041275722836
cannot distinguish	1.7522485220301847
multiple passes	1.5482870987025659
original context	1.5078797213408792
categories assigned	1.5704284771841746
section two	1.5992286050546423
question q	1.6093993367849806
class problem	1.5233468905393979
software package	1.874423063750951
julia hirschberg	1.5509191069304942
test results	1.794157975043103
results without	1.583051127543801
agreement figures	1.5117685557139826
usual approach	1.5037367200448983
given order	1.6487334099083149
incident type	1.57101686482638
feature would	1.5758014413678827
following function	1.592620200341168
ideas behind	1.6056035510036821
two parses	1.6155439029340544
million sentences	1.664540718454114
combined effect	1.6190328102134979
performance even	1.6388370153723533
semantic form	1.7113455924377576
darpa spoken	1.559620194736931
database may	1.5037367200448983
corpus consisted	1.829826350131039
nominal form	1.5671012235059067
phrase could	1.6031224303496519
simple models	1.6229151093758099
lexical cues	1.5729459237482084
take arguments	1.5226149809040495
characters per	1.6504453495314007
correct senses	1.6166761201373094
1 presents	2.007582354459875
ar e	1.8119413960353288
finite automata	1.681963345344938
atomic formula	1.6114092222740677
surface representations	1.5309846273595442
important properties	1.810888103569144
ambiguity resolution	1.9205672083171583
significant gain	1.5559715171228279
information extracted	2.037663753429344
contains much	1.5906852501842725
scores reported	1.5969765857928704
new search	1.5260185053825466
kernel functions	1.7150492538499305
following models	1.5547189562170496
specific words	1.89672373226575
problem consists	1.5992286050546423
terminal label	1.540254183076581
model depends	1.5739592165010823
surface strings	1.8316046838502424
new methods	1.883481969462783
complete tree	1.6746175961122665
major goals	1.5188051370135929
surface string	1.9229959479565268
paper proposed	1.7178002527269736
semantic relationships	2.0230259411993243
based lexicon	1.668430889966019
order information	1.7156169013559484
rule schema	1.586924628279514
time periods	1.5207329706375616
gram features	1.5805642276608287
uniform representation	1.5611231576018243
one instance	1.9529610951183163
first np	1.8056144459104624
female speakers	1.624519165400001
tile output	1.5156323250636554
general method	1.8608041928618357
best overall	1.847438170787841
described briefly	1.591823957496618
language teaching	1.614908190007114
parsing efficiency	1.7916609384765174
cannot explain	1.6190328102134979
text span	1.6076896091431925
theoretical foundations	1.5445134575869865
detailed evaluation	1.653691045130907
first examine	1.6190328102134979
relation r	1.8952408207873381
research council	1.808733013921795
new method	1.659198615002321
second goal	1.659662148122272
strategies may	1.5906852501842725
character type	1.5639661210674762
given data	1.7602194347776197
intermediate structure	1.5258353474821802
aha et	1.5321552374951575
text data	1.9207868232543421
lie within	1.5511943810330642
stop list	1.6818921613576834
occurring texts	1.5876477894309007
low frequencies	1.7234546058425648
representation system	1.8434359062650403
significant information	1.594773126307433
frequency information	1.9522456632031624
french texts	1.5519690017139713
experiments reported	2.108233306725995
entity tagger	1.6990421829566462
constraints could	1.6002993454155385
euclidean distance	1.8218164009111453
additional input	1.5771944393968738
documents within	1.6514103959727489
systems become	1.5973170078864487
standard measure	1.5637776281819353
supervised classification	1.5061421912726483
concepts may	1.6844174045708415
5 summarizes	1.7141428077458374
two exceptions	1.6315901311540408
making process	1.6440429413357318
hyponymy relation	1.5429320869722685
test run	1.6578324088115175
one cannot	1.9378923182762464
several variations	1.567922330580758
wordnet synsets	1.8477307552410476
random variable	1.9290690684843732
pragmatic constraints	1.7288604163065846
module consists	1.5637776281819353
specific events	1.6031224303496519
morphological categories	1.6565197046607572
verb whose	1.6169986020906966
generated data	1.530072891325321
synonymy relation	1.5698921596906512
context vector	1.6316506058360805
projects agency	2.1262114563034813
verbs based	1.571949426413436
conditional distributions	1.6588577629299015
corresponding source	1.6113346817822476
similar techniques	1.7888752560939947
phonological rule	1.597658809324252
confidence scores	1.762624015235132
error type	1.6120646542146346
verb tense	1.6943820040002884
longer word	1.600575098443783
different discourse	1.8300989340108782
meaningful way	1.648787373441388
web documents	1.7623236562596294
following features	2.0929906302815424
dialogue corpus	1.6161919043931583
corresponding tree	1.6064310100927286
personal name	1.5941687936598614
simple grammar	1.8092049906041372
performance reported	1.561123157601824
social sciences	1.594773126307433
scoring methods	1.6109026579090986
syntactic errors	1.6288864676228823
experimental set	1.6037378902512214
less coherent	1.5037367200448986
set performance	1.5155485471539711
given feature	1.79079318381344
understanding task	1.5951549374763572
system operates	1.7591353459076389
ambiguity arises	1.649843436022881
method also	1.8081612299749894
p must	1.5681388860688288
general framework	2.0377411093875333
grammars without	1.5771944393968735
errors occur	1.7788415192237228
assign different	1.6164339756999313
following grammar	1.649843436022881
corpus collection	1.5698733514826038
learn rules	1.6727259709579814
automatic syntactic	1.6036848953283132
every character	1.63729020511694
original document	1.8726711953681552
negative example	1.6705056157515683
processing rules	1.5028598174994463
spontaneous spoken	1.6553167927357262
word l	1.507007268326853
structures described	1.5706734993195162
produce results	1.6289349127929256
1 contains	1.8723534450394568
detailed specification	1.5511943810330642
initial tests	1.5445134575869865
lines represent	1.5415886924949058
different realizations	1.521614825883617
tile word	1.7461796145793567
dialogue systems	2.068176541613105
actual values	1.6870613014843863
sentential complements	1.7750450270341593
generation task	1.8244416284071152
thank mark	1.591823957496618
two situations	1.7582864366126905
different test	1.792045038782874
words include	1.5706734993195162
two judges	1.6994160048571505
two tasks	2.0740028918501077
traditional dictionary	1.5275591303469285
every man	1.7484725234463494
uses features	1.6213967902634536
general categories	1.7053455296190099
consistency checking	1.649780473793619
order would	1.5547189562170496
28 aout	1.530072891325321
current example	1.725393852123075
words similar	1.510181823566108
system learns	1.749310794708903
response generation	1.552394247983951
walker et	1.8091804242439729
without knowledge	1.6614869878819327
state devices	1.5329785555225088
involves three	1.7329564081810807
rich languages	1.632688676106534
comparative evaluations	1.530072891325321
whose words	1.5269930689946518
evaluation measure	1.817489578511643
final number	1.513130125383453
discourse domain	1.57101686482638
therefore cannot	1.6699400022790052
la recherche	1.59218768856392
japanese text	1.8673478351625645
final evaluation	1.824437745050875
1 corresponds	1.582117970332684
particular method	1.579873144038688
subtle differences	1.7086358066249532
est le	1.5874449708806095
annotations produced	1.50119175880199
different time	1.743960294459502
selection criteria	1.7470753890381703
resulting tree	1.8199374983622565
3 describes	2.1935350178237676
5 sentences	1.5509191069304937
first position	1.8824669690273192
complex verb	1.5357555524904467
free parser	1.6264622790085423
processing based	1.567922330580758
independent test	1.702248916669248
estimating probabilities	1.513130125383453
copyright notice	1.9273825503280846
words co	1.6684478356965393
pairs extracted	1.7075055874179887
specific noun	1.5354510398033074
optimal solutions	1.600330740560592
superior performance	1.7805731024715366
first candidate	1.6434932464528418
another application	1.815669969260329
coreference task	1.6757353758714073
specific categories	1.5657848105751835
many candidate	1.567922330580758
brief description	2.0521520024765625
search strategies	1.794957561196114
automatic lexical	1.5818515289928636
ap newswire	1.5580042495673954
single discourse	1.5216869793881187
equations 1	1.5558827079957558
local ambiguity	1.642733761133985
notation used	1.8059073329144144
three conditions	1.8224904727112157
kl divergence	1.5255464664613592
analysis fails	1.6548911840144862
structure representation	1.809893248167888
questions may	1.6631574457448046
priori probability	1.6598709184814338
explicit model	1.5408560126058508
first clause	1.8741094239378502
transcribed speech	1.6826929736610348
build systems	1.643362624369706
5 compares	1.6388370153723533
text corpora	2.1298313397076116
information scores	1.5061557323373174
specific domain	2.0094486298914416
algorithm compares	1.5408560126058508
input character	1.6254892635717173
valuable discussions	1.6658904115324105
random order	1.730088528439628
al 1998	1.5762350649766068
readable dictionaries	1.9970235470745854
handcrafted rules	1.5500434040293412
know exactly	1.6031224303496519
original set	1.7601330463163738
related fields	1.6002993454155385
specific concept	1.715052118874864
np would	1.554089533378209
classes according	1.7344099527418115
functional components	1.5111306094308554
corpus used	2.137665502595728
techniques discussed	1.5511943810330642
word combinations	1.7906273598535103
syntactic construction	1.8282330220169773
different source	1.6964136633312572
conditional probabilities	2.1000460339314615
chinese segmentation	1.5398596897702692
level processing	1.7488947289910683
trigger word	1.5863515618805972
probabilities given	1.5459182428709468
word count	1.7773134597676898
confidence interval	1.6875463407854192
process would	1.8443534686976397
entity information	1.5524289354793173
basic type	1.5657848105751837
understanding conferences	1.8941479609209808
base de	1.5259435746553578
successful parse	1.7034683095535441
word boundaries	2.043648551362743
value indicating	1.5188051370135929
work uses	1.6392309045877975
systems based	2.08145205255874
specific translation	1.5459806054163279
feature combination	1.5746527805676704
inference procedure	1.5425450718322482
keep track	1.9562566287689878
genitive case	1.5943087517128787
gradient descent	1.7176833787341788
vector machine	1.945410990991728
processing method	1.5880900834779994
sentences spoken	1.5260185053825464
tile two	1.7518142844030467
fourth column	1.7856549343874448
different language	2.0558186228854707
transfer system	1.5176981284339437
translation performance	1.7409888304479628
marginal probability	1.599125349640446
theoretical foundation	1.6229151093758099
basic algorithm	1.9070735588027132
values obtained	1.6946633528721495
work may	1.656664045947751
lexical processing	1.5556026693790028
sufficient conditions	1.6943309563521631
30 sentences	1.561123157601824
structure building	1.6456311770584695
algorithm applied	1.5676517392350746
parameter vector	1.6809309469455145
single cluster	1.5935883792113952
sense distinction	1.6361168766430527
parser errors	1.533724025759546
systems produce	1.613624660358136
one argument	1.965317483006671
increasing order	1.6813700712556885
two baseline	1.6778270390199774
competing hypotheses	1.5778973208841207
orthographic form	1.6162383534717668
two sections	2.0656846008698606
single elementary	1.5061305693099565
simple sentences	1.9357942128266545
evaluation tasks	1.575838245922343
rule looks	1.5188051370135929
three classifiers	1.5531410824667216
key templates	1.576452133530595
function using	1.5645241345053709
limited domains	1.7792698904465134
following phrase	1.5536240126687624
one approach	2.1008419445455786
idf weights	1.521798762287671
figures 4	1.845368968654202
semantically coherent	1.6326415737683198
rules based	1.9254549006787527
number k	1.5991253496404458
different aspect	1.567922330580758
semantic definitions	1.5156323250636556
system presents	1.6339957705783552
three aspects	1.8012230473134534
basic design	1.6413904216258086
full nps	1.5825693178357125
types defined	1.5078976439458074
fundamental frequency	1.5740112642679762
spontaneous speech	2.028933789784043
linguistic competence	1.7894252012309337
given instance	1.6073715579162529
results concerning	1.6883585682921605
following discussion	1.8766464816052948
robust processing	1.599885190860193
every point	1.595491118313467
also thank	1.9548847474577176
considerable improvement	1.7455319583030278
event occurred	1.5430674042673345
computational mechanisms	1.5194321743502117
two subjects	1.6901520212992622
refer either	1.5188051370135929
algorithm builds	1.6152953234723224
grammatical coverage	1.7288894645189885
appropriate verb	1.5190584552425839
following order	1.7518332963081182
evaluation may	1.5408560126058508
acquisition bottleneck	1.6705056157515683
linguistic basis	1.5526611030689654
greater detail	1.6623908747673088
speaker could	1.506173888464562
alignment results	1.5124114984816661
second iteration	1.630821253027857
perfect match	1.556022552786445
grammar also	1.7229550745276576
many nlp	2.020422160315749
complex knowledge	1.5408560126058508
probability assigned	1.7505914352364276
syntactic parsers	1.8423750361494258
possible antecedents	1.8466955346775626
syntactic lexicon	1.5276339896116844
preceding word	1.8611536971419533
alternative readings	1.510181823566108
text genre	1.5635698549179589
like word	1.771158110485396
single tree	1.790905463758051
hybrid systems	1.6005251959718443
coherence relations	1.5986342817341068
evaluation methodologies	1.6636153140564507
involves two	1.9673465602789326
system knowledge	1.5539999452055475
smaller size	1.6570039092175395
often results	1.6822401779866478
human user	1.8210818768919212
audio files	1.5847316634463147
following paragraphs	1.6655102285632384
literal meaning	1.6944688263809176
word model	1.7599235077272546
every rule	1.8101104533812002
certain problems	1.597317007886449
structural transfer	1.6426185316259385
using precision	1.5637776281819353
language analyzer	1.6364867589279721
extracted phrases	1.5408560126058508
use case	1.5378646275607575
training example	1.8577211896842378
procedure also	1.522760613339579
efficient way	1.5785521055328393
data preparation	1.6259121276584785
whose domain	1.689290298275095
three pairs	1.6056035510036821
becomes possible	1.832335205414049
proposed technique	1.5363485810301154
following pairs	1.5511943810330642
lexicon acquisition	1.5749525076754054
role fillers	1.6331167140614231
single language	1.7975714026726275
experiment using	1.9668043204042212
value must	1.563687738145949
contextual factors	1.6849273316898554
estimate parameters	1.5612815851895034
syntactic surface	1.522760613339579
two applications	1.7370644233219208
different length	1.7021555513601787
encoding initiative	1.5886561992520425
previous studies	2.085661357126972
three experiments	1.7875440830649525
performance data	1.6050942289310326
assisted translation	1.6348823045665801
3 shows	2.5527744024118233
two entities	1.929955390815985
important characteristic	1.6960474084795663
two types	2.498190586457758
contain information	2.0352370903152877
segmentation algorithm	1.7851668984141855
initial seed	1.605265068015642
task described	1.583051127543801
bilingual sentence	1.6416936803920255
footnote 1	1.5188051370135929
complex features	1.6543189996660619
three data	1.7015663939288705
vocabulary items	1.8181108527073286
english translations	2.0099179792281103
psycholinguistic research	1.5991253496404458
lexical mapping	1.5051557504291142
one correspondence	2.005907715159233
certain cases	1.5679353525326787
within words	1.5087853581980926
baseline performance	1.8912651795566087
step may	1.5739592165010823
method used	1.8504698400296882
standard deviation	2.1169377221728976
good candidate	1.8248139332291624
may share	1.6889191202036424
among systems	1.533540614646662
potential candidate	1.530072891325321
correct information	1.6666664790751589
time points	1.6762046106232034
perform word	1.6613891836858694
important point	1.7596632094151865
used words	1.7100476490041971
fourth section	1.5408560126058508
similar data	1.6289349127929258
speaker intended	1.5041134997716235
maximum performance	1.50119175880199
given number	1.7486023401067097
data distribution	1.5107844011873681
powerful mechanism	1.579873144038688
values corresponding	1.5195036419368686
different senses	2.077288607170036
oriented language	1.5701536868589447
certain class	1.8652805146255802
dependent knowledge	1.7419718795871564
benchmark tests	1.5230482200667932
logical structure	1.8432574979333738
multiple possibilities	1.5378646275607575
k 2	1.622031701107661
morphological disambiguation	1.5915239873181912
man loves	1.5747137552199288
appropriate morphological	1.5078976439458076
one extreme	1.6903000289233914
rewriting systems	1.6605409178323147
three feature	1.6221843551054547
studies show	1.594773126307433
utterances using	1.5111306094308554
black et	1.8141885705403735
sentence lengths	1.76043574445708
following feature	1.7482442165567584
system contains	1.8281779791544694
carlson et	1.539865273002305
utterances whose	1.5742591873994098
topic segmentation	1.5419990701114041
latter kind	1.5739592165010823
markov model	2.1958942487916957
languages use	1.5156323250636556
basic functions	1.5395099438048585
noun sense	1.6208226404436687
syntactic properties	2.0450755946400987
new item	1.676287065354511
experiment 3	1.5669340961987233
one occurrence	1.892454307344343
clustering method	1.8399236152052119
information includes	1.6525488762732246
entire training	1.8523420732326754
input must	1.611203449372866
resolve ambiguities	1.7841811153993108
interesting examples	1.5582958710952315
specific facts	1.524534874088451
results returned	1.5091450357832548
information would	1.931195363608616
binary branching	1.8255771417347517
long vowel	1.5735132742057183
atomic values	1.743435986269163
carreras et	1.636263917962817
sufficient data	1.6708590810843598
annotation guidelines	1.7083745487429098
uniform probability	1.6856195651524377
different branches	1.5300728913253212
parser takes	1.7051629942513449
information might	1.754682578532674
7 concludes	1.6315901311540408
current parser	1.506491873290213
syntactic labels	1.6557085930163806
recall rate	1.7882582705386834
case role	1.6073507964196216
generative lexicon	1.6720816189293701
introduce two	1.8873003499044052
van benthem	1.6036902575822278
contains less	1.507007268326853
irrelevant documents	1.6462151957591447
appropriate knowledge	1.5634115707207188
graduate fellowship	1.6031224303496519
chooses one	1.613624660358136
algorithm reduces	1.50119175880199
acl reviewers	1.5645241345053709
constituent type	1.5459806054163279
atis data	1.5761147671561666
thanks go	1.9173237920562396
montague grammar	1.7358745454173323
phrases extracted	1.704895197139951
one answer	1.7309723544238431
one error	1.7446302858591407
transition networks	1.843377081589064
word features	1.9175485887626094
incorrect results	1.5645241345053709
language l	1.9214756196963747
since speech	1.5459182428709468
one letter	1.6838629292350045
relies upon	1.6904151818990054
one study	1.510181823566108
noun sequences	1.5031697501154293
first pattern	1.5156323250636554
knowledge extraction	1.6261211053099405
view point	1.6246289167870047
large knowledge	1.6998973603836447
detailed description	2.2526166621713406
use dynamic	1.572834038198836
c may	1.5645241345053709
techniques like	1.5707376783218978
words listed	1.6602505116127795
coordinate conjunction	1.525286547181802
every example	1.5078797213408794
data obtained	1.7924041787542124
upon work	1.8821367424794175
applying rules	1.6734445616305185
le premier	1.5215015830842873
problems described	1.6802940634668402
asking questions	1.706208768185209
space available	1.5111306094308554
state based	1.511205120616144
computing resources	1.5445134575869865
active chart	1.7009656827244934
limited way	1.5887468924062063
determine possible	1.5408560126058508
expert users	1.5891900701960695
hard copy	1.5487714020511911
extraction patterns	1.7089553390113594
compare favorably	1.6614869878819327
word contains	1.5973170078864487
multiple meanings	1.745267260538239
cohen et	1.658951070478885
words contained	1.8353773457847733
parsing error	1.5771944393968738
general issues	1.586137214651611
revised version	1.6639614321696403
initial goal	1.5600508151170625
subject areas	1.6697888669088234
controlled experiments	1.582117970332684
words often	1.703359686465308
existing methods	1.8728047622175115
4 describes	2.0807524947846012
portability across	1.511205120616144
features need	1.5408560126058508
one place	1.8889147715961723
coverage parsers	1.533540614646662
th word	1.7740853914926524
specific training	1.647958933553254
syntactic grammars	1.5354510398033074
last utterance	1.6626205634618159
work properly	1.5378646275607575
derived tree	1.7249560775118975
word n	1.856952763045408
army research	1.524534874088451
another parameter	1.5338735539822874
different stages	1.872953425484142
remaining ambiguities	1.547363551399276
good set	1.5226149809040495
predicate logic	1.8018256473898662
methods described	2.0015070797748447
process stops	1.586137214651611
different similarity	1.7165172031371114
small data	1.7646782915318144
three corpora	1.7790561303755028
ones like	1.672884134739408
one constituent	1.855251353134279
grammar fragment	1.640648390867379
contextual features	1.8058458642864177
different notions	1.5525978349470975
word corresponds	1.6117266025648127
considerable research	1.5378646275607575
tile user	1.6368457365018974
remaining word	1.5188051370135929
tree classifier	1.7169096519388494
approach differs	1.9279858344943839
training corpus	2.4356655421262685
learning word	1.5656419863875413
interpretation component	1.5024338381011866
actual process	1.5111306094308554
drawing inferences	1.506173888464562
scope relations	1.5714568848346864
disambiguation results	1.5693340908967248
2 lists	1.8777148766292115
linguistic studies	1.7633639934418421
textual features	1.5785581999916318
particular item	1.5188051370135929
word list	2.0124962202144303
problem even	1.5408560126058508
experiments involving	1.721573893842553
information could	1.6951421914966174
two parameters	1.924809766219977
system incorporates	1.7408279821914407
semantic annotations	1.6637073632840675
information required	1.7163822704010196
dependent variable	1.5305488330675046
source grammar	1.580029442260006
main advantage	1.944988931468079
8 proc	1.7016656224425795
section 0	1.5088214873535928
w 1	1.7092894454569412
following steps	2.190666086977478
analysis problem	1.5037367200448983
crucial factor	1.530072891325321
based rule	1.5500434040293412
select features	1.50119175880199
processed text	1.6741803270499043
clause boundaries	1.7541671499700795
linear logic	1.5346558803478492
signi cant	1.9052880103378718
structure theory	1.9741714214621204
potential users	1.7474933348665354
sun sparc	1.530072891325321
verb groups	1.724286085948977
similarity values	1.7867460373463797
ie systems	1.7755452956804967
past work	1.696799107973694
structures using	1.6213967902634536
constituent x	1.516491127995908
segmented words	1.6528011766599353
include words	1.756257461041527
average f	1.6584442559724004
fundamental role	1.5188051370135929
section iv	1.5216788181120133
intermediate results	1.8720242628291088
missing arguments	1.515548547153971
event may	1.627796791624738
chart parsers	1.5951876284987991
selected terms	1.5163508245214703
value p	1.513130125383453
average ambiguity	1.566059426073936
inverted index	1.501786892577917
systems requires	1.5111306094308554
two component	1.6034321301038696
taking advantage	1.7351504899595989
syntactic parallelism	1.5666946500713785
given expression	1.5559715171228279
trivial case	1.5547189562170496
generative capacity	1.799762377863877
transition probabilities	1.9953447381777396
correct structures	1.5226149809040495
model features	1.5955813367695213
straightforward approach	1.65027618229265
error propagation	1.532234208718959
complete information	1.672884134739408
restricted class	1.5354510398033077
element x	1.540170457492069
previous iteration	1.7098259615519553
simple way	2.0362190166510716
subsequent sentences	1.6495561370023335
improve coverage	1.5078976439458076
cases must	1.5511943810330642
based generator	1.5330285946932867
algorithm operates	1.6288981647434884
vary considerably	1.6411558246537032
boolean queries	1.5617071846474357
large list	1.5706734993195162
2 c	1.6816988370728096
morphological root	1.5230483704868893
certain tasks	1.6388370153723533
figures 2	1.9237888227477677
longest match	1.634674005696298
let c	1.9805253813894428
feature specifications	1.6488069886647005
learning curves	1.7633207068506456
logical inference	1.5316566430519973
copestake et	1.7131227426529407
show high	1.5037367200448983
tiffs paper	1.5906852501842725
figure 13	1.8103361712349584
relative reduction	1.6530426716551943
research done	1.6213967902634532
5 times	1.7605175444520813
semantic field	1.6355710698373536
find words	1.6794854487178401
procedure used	1.8094147398306992
two elements	2.0024263954363044
three components	2.0083116684622997
morpheme sequences	1.521074033156727
random number	1.6093022139611397
following components	1.7678099140508592
seem like	1.6458797346140277
word sequence	2.1994254804631845
art methods	1.5771944393968735
relation names	1.5207380465582334
many chinese	1.5706734993195162
says nothing	1.6481174069026685
analyze sentences	1.5656419863875413
human sentence	1.6466011542427883
figure 5	2.4988902930165713
discourse context	2.04557702865875
include multiple	1.5559715171228277
analysis algorithm	1.5590609402475502
high scoring	1.5881174695673657
equation 7	1.5407205022720676
semantic unit	1.7194470207498316
training sample	1.7971469601812242
information helps	1.6002993454155385
two symbols	1.7394384994026204
scientific articles	1.5283321177951092
people understand	1.5645241345053709
several candidate	1.5156323250636554
word correspondences	1.526121974677685
general context	1.6994553138130084
detailed presentation	1.579873144038688
domain independent	2.0167839937817575
simple version	1.579873144038688
processing capability	1.524534874088451
symbolic features	1.5219618966045698
unification failure	1.5879362713996326
current tag	1.549703491457814
translation knowledge	1.5928125863481761
corpus sentence	1.530984627359544
training procedure	1.9588369613190475
acquired rules	1.515378375605084
simple frequency	1.5676517392350744
node marked	1.506173888464562
noun compound	1.704252785432713
one antecedent	1.5258093094263683
reliable statistics	1.5111306094308554
word phrase	1.5986374288963399
dialogue interaction	1.5621439758713946
context includes	1.5408560126058508
french corpus	1.6493226448562486
external argument	1.5646902113523573
form two	1.5408560126058508
structured data	1.8256985147742582
dataset used	1.5188051370135929
equation 5	1.7591892522855794
chinese words	1.9596681345138907
following advantages	1.7504812345368606
syntactic evidence	1.5245348740884508
procedural semantics	1.56819296354213
tree representation	1.8386834920745965
oepen et	1.5465519314989193
future works	1.7591353459076389
small corpora	1.7105788625254252
yields better	1.7497703463414669
system including	1.643362624369706
system usually	1.526993068994652
translation techniques	1.5748567577278902
chapter 2	1.6641647390002994
first experiment	2.1066015790298214
use hand	1.669489306700537
structure representations	1.7034858427159205
eac h	1.579873144038688
error correction	1.773908737767658
global context	1.7175648468263636
word order	2.3058257353243077
three annotators	1.6414207689176052
specific context	1.881712716476741
syntactic roles	1.8869254141431795
processing phases	1.5190584552425839
standard word	1.7887453455412843
wsj text	1.6123811030203221
input two	1.507007268326853
results appear	1.6635555237714856
first text	1.5773133211810766
interpretation module	1.532714913365084
system assigns	1.7098773566635042
lexical preferences	1.6341798597718706
per word	2.112180047507885
many reasons	1.6903000289233914
many text	1.674180327049904
possible reason	1.7573141382441586
complex feature	1.8469108038909992
adverbial modifier	1.5188051370135929
functional application	1.7616843796575812
system goes	1.522760613339579
grammatical constraints	1.9006557948092804
k e	1.654190056995362
frequent class	1.6469586338689493
different viewpoints	1.561093275236317
graphical model	1.6687334306132224
two observations	1.626990247505205
traduction automatique	1.5568082587133127
significant decrease	1.5511943810330642
semantic grounds	1.653691045130907
current computational	1.5188051370135929
original ones	1.6160313784345908
100 word	1.5245348740884508
extra information	1.8995196727763595
labeling system	1.5678009097741024
values associated	1.6164339756999313
coverage grammars	1.7569128363307462
performance evaluation	1.796989835410843
content planning	1.65095831636709
high coverage	1.7307062983515042
partial parse	1.7383892583730358
syntactic trees	1.8999589998470288
important resource	1.5338735539822874
processing mechanisms	1.5893265024752146
another kind	1.842000872799971
words ending	1.7124501163651882
newswire data	1.5378646275607575
models contain	1.5188051370135929
word pairs	2.1057152038991918
learning phase	1.6820384960855264
three evaluation	1.533540614646662
section ii	1.5290812457000524
individual words	2.220469589440855
robust statistical	1.580090580759919
matching algorithms	1.6288981647434881
negative value	1.7242813864028517
three dimensions	1.7635231595638654
ad hoc	1.787264272656303
source sentences	1.7097435506131071
robust systems	1.5188051370135929
mother node	1.8241903321340436
grammatical description	1.7186254409342192
approach depends	1.5821179703326842
problems posed	1.6112034493728657
without fee	1.9296247178237733
label l	1.6025176164483694
entire document	1.7948511655384471
language knowledge	1.7688828901267684
basic assumption	1.803503820493083
speech waveform	1.5128153640906288
actual texts	1.5825577359308496
number 3	1.883337472452606
robust parsers	1.5571495092265906
sentences must	1.6056035510036821
lexicon includes	1.611203449372866
functional description	1.604769086979554
lexical sample	1.7769839192295278
narrative texts	1.5671012235059067
indexed grammars	1.563635993859621
statistical nlp	1.783976099793588
one interpretation	1.908196728900629
argument must	1.6056035510036821
contains four	1.780282513498202
features indicating	1.5781539692435587
larger n	1.5194321743502117
another possibility	1.9487050203573433
three sets	1.7275694026388604
questions remain	1.62412689036662
term unification	1.7081238963886003
definite clauses	1.509345822714676
new piece	1.586137214651611
rules describing	1.6482958236884362
feature bundles	1.5835490608857694
empirical question	1.7392306202979375
graphical models	1.6693753526908273
select among	1.7669373105292956
new candidate	1.6210934563561632
program written	1.526993068994652
event types	1.8022235325963034
one problem	1.8027596083099398
based tagging	1.6634249740948763
final interpretation	1.5880900834779994
general semantic	1.8939402200159177
tile structure	1.533540614646662
full documents	1.5403034171359224
acoustic scores	1.5591100851420445
limiting case	1.5300728913253212
several nlp	1.6002993454155385
user question	1.501050602204299
pronominal anaphora	1.7254924978796058
lexieal entry	1.5156323250636556
performance metrics	1.6112657873977674
partial parses	1.827032487382686
sufficient number	1.8055148355029196
new clause	1.5792035811631284
ce que	1.6178702020030291
individual events	1.5354510398033074
main concepts	1.5645241345053709
sentence processing	1.8229954164471087
time performance	1.731727971636949
en la	1.5035193201428414
english speech	1.5908167675916909
analysis result	1.5778458894111118
word feature	1.7161251093414782
5 presents	1.9858639744841378
given constituent	1.6375625045004876
rules defined	1.6276749302936424
appropriate data	1.6578324088115175
parsing process	2.189998096510185
conversion process	1.6121742428060308
big problem	1.591823957496618
partial parser	1.765672425456948
3 senses	1.5378646275607575
variables ranging	1.587700203918434
different information	1.8954875806407991
several words	1.9384397736036696
cpu seconds	1.5190584552425839
logical consequence	1.532341016095725
strong effect	1.5445134575869865
summaries generated	1.5553829313699454
computer aided	1.5519900193858671
language features	1.5416041062057482
normal distribution	1.7505420609835107
corresponding term	1.5036575034310684
parsing problem	1.8515695129363756
free word	2.0213345055439182
research work	1.7329292822928468
testing corpus	1.7594451062549312
english spelling	1.5103105106075492
first row	1.924326985239013
extensive set	1.597317007886449
parsing technologies	1.613624660358136
sentential form	1.6006041210047415
form used	1.6031224303496519
local features	1.7634556257419287
task using	1.8365016880187703
syntactic point	1.6266256462651403
lexical properties	1.822430115916605
full texts	1.58749753694102
current generation	1.6046743940979167
parser proceeds	1.5324452505798665
large data	1.9391497766314663
clause contains	1.6288981647434881
relations used	1.81988945056023
upper part	1.6861938694548688
relation type	1.7067512041184585
verb plus	1.5869693547698929
generation based	1.6058952092043035
participating sites	1.5926083359526775
speech tagged	1.8401489783127332
end times	1.5525419192763326
structural constraints	1.792225382255426
higher nodes	1.5716157403242002
words appearing	1.993889755390221
tile sentence	1.7864061763415415
following characteristics	1.8059073329144144
null second	1.6002993454155385
algorithms using	1.6977890880751751
system attempts	1.7908149971194873
sigdial workshop	1.7017362832860266
building natural	1.507007268326853
parent nodes	1.656074792582219
bikel et	1.826671975674784
limited knowledge	1.5194321743502117
certain types	2.167565944561937
identity function	1.5615960458884148
system gets	1.5547189562170496
newspaper stories	1.54083189062529
set including	1.5245348740884508
appropriate feature	1.6580606251764172
relative clauses	2.2284552683088457
word processor	1.7262904919457576
language contains	1.5037367200448986
magazine articles	1.6169986020906966
one considers	1.8987731755156656
case feature	1.5373225215361594
model adaptation	1.5376849437879063
different states	1.6392309045877975
production process	1.5737663064492282
learning technique	1.8869843231350885
candidates according	1.620425994955554
higher scores	1.8936494664275274
two classes	1.8864252948282476
aligned training	1.5352230312817907
frequency distributions	1.6907167455631833
two subtrees	1.5118192695694388
classes using	1.6844174045708415
natural solution	1.5645241345053709
whose root	1.8299362772957284
also support	1.6977890880751751
following actions	1.507007268326853
one algorithm	1.5511943810330642
choice would	1.5111306094308554
subsequent steps	1.5611231576018243
higher dimensional	1.5306061495832655
computational burden	1.5511943810330642
similarities among	1.5459182428709468
bilingual lexicography	1.5602367449199288
provide access	1.6429565433589586
word ordering	1.6213960123831654
objects referred	1.5041134997716232
average frequency	1.539087705163675
null hypothesis	1.8138346892428778
computational approaches	1.8304251051328828
nl understanding	1.5938628727053523
network may	1.5482870987025659
methodological point	1.5111306094308554
minimal amount	1.6708590810843598
common problems	1.5645241345053709
particular position	1.6125713976431841
work suggests	1.65027618229265
filled pauses	1.7166659349163247
particular piece	1.5188051370135929
small number	2.39521478846175
berger et	1.989298584228422
definition 7	1.583051127543801
development test	1.8225680226758352
connected component	1.5107113719541359
accuracy even	1.5547189562170496
two results	1.765906995481154
mt systems	2.018004129342134
contains one	2.005457095331167
uses machine	1.5395099438048587
reported results	1.6401575622781492
common phenomenon	1.5547189562170496
solid line	1.7108097190701925
action verb	1.511176588103529
additional rules	1.817947532591733
indirect object	2.069286391539533
another advantage	1.9648168260433125
general set	1.5395099438048585
slot filling	1.5790803872679346
boundaries within	1.586137214651611
untagged corpus	1.5964628892050525
term x	1.5937587436184821
retrieval effectiveness	1.606762744328779
model within	1.5771944393968738
word wi	1.9130274529174482
chinese translation	1.7075683239562247
formal basis	1.5008529684573726
positive example	1.73820155785154
world wide	1.6798699300896116
selected documents	1.6037378902512214
parallel sentence	1.57415435044505
various combinations	1.829200940701061
sample task	1.7341206858681453
one target	1.8027307653945472
np nodes	1.5999796228251049
phrases might	1.5445134575869865
development time	1.8201732987557644
generation method	1.6548286117831166
comparable corpora	1.6893659358032578
relation exists	1.7017362832860266
subject relation	1.5211629864223772
result indicates	1.7329292822928468
nl interface	1.602223579107508
large quantity	1.6963149953958458
simple heuristic	1.9217278025283773
cfg g	1.6304017903028405
text collections	1.8021481536265247
definition 5	1.6650360806498918
alternative strategy	1.6429565433589586
generate word	1.5328333954091753
good reasons	1.8036245265731268
section shows	1.7414848008364259
lexical cohesion	1.6576860619968898
probability 0	1.5506700242016946
following subsection	1.5645241345053709
vocabulary words	1.9380595262604676
small sets	1.6315901311540408
another expression	1.5338735539822874
boundary information	1.6702584537481897
log likelihood	1.8255218440564698
writing grammars	1.5188051370135929
detailed comparison	1.7392306202979375
line 6	1.5669629240805358
common cases	1.558193976095525
present experimental	1.7763027180350157
set fills	1.5544464379972678
work described	2.125221977830866
one part	1.7890401046552018
verb senses	1.8250624768229464
400 sentences	1.6003307405605918
preceding utterance	1.6713977298683025
internal structure	2.092836687008358
5 contains	1.6658904115324105
possible analysis	1.5266067982298943
specific set	1.8463589810990293
classification error	1.6695372076433517
individual concept	1.5767850251961737
problems using	1.6117266025648127
testing procedure	1.526993068994652
possibly non	1.5338735539822874
resulting structures	1.6342922036019591
must share	1.567922330580758
syntactic transformation	1.5230482200667934
tree node	1.800347408564407
enough detail	1.5338735539822874
knowledge concerning	1.5973170078864487
3 proc	1.6002993454155385
subordinating conjunction	1.7232317075458827
specific goals	1.5771944393968738
plural pronoun	1.5201190767490376
describe briefly	1.5739592165010823
unigram probabilities	1.6413904216258088
information comes	1.6084968011212868
example dialogue	1.6378377786030573
based smt	1.5050476306737601
many tokens	1.507007268326853
whose antecedents	1.5040418526142796
present proposal	1.5087853581980926
semantic pattern	1.5729854960511371
thus need	1.6084968011212868
words 2	1.5887468924062063
takes care	1.925101079543033
true positives	1.5885128957268442
three values	1.7656956055343276
2 summarizes	1.900348707524374
journal data	1.5843044140731486
jones et	1.5756406293698042
two numbers	1.7088092136999036
n increases	1.5156323250636556
results provided	1.561123157601824
figure 11	2.0203858812493847
several respects	1.7735861405555255
vector x	1.6115218515463947
idiomatic expression	1.5891348789899808
value may	1.6770235151848842
several approaches	1.9783891405784482
rewriting rule	1.6217022890121124
class labels	1.8019242786865737
system cannot	2.023253760062601
intended reading	1.5131301253834533
evaluation tool	1.5360785028369002
helpful discussion	1.5887468924062063
user requests	1.742787941468091
simple techniques	1.6645330200623705
methods using	1.8346610811041353
large corpora	2.2411242379996774
parse fails	1.5011917588019899
enhanced version	1.549729132180171
frequent tag	1.5282170470313718
knowledge stored	1.522760613339579
sidner 1986	1.722081198505967
complex system	1.6822401779866478
correct analyses	1.639712650226961
probable sequence	1.5758494263954068
grammar whose	1.6246289167870047
paper concludes	1.6635555237714856
different level	1.6785607164239145
simple modification	1.6458797346140277
process continues	1.863701977038993
second order	1.8024292734520322
tags within	1.530984627359544
answering track	1.5995504718848754
defines two	1.522760613339579
brings us	1.6862623517634696
term weighting	1.8389625744050033
parameters based	1.522760613339579
produce similar	1.5582958710952315
user modelling	1.5784849055618664
connections among	1.5482870987025659
previous knowledge	1.578490425392922
lines indicate	1.7143000499855667
based retrieval	1.7171377398007575
shows tile	1.5269930689946518
following diagram	1.6010101084822783
first conjunct	1.6636186912791326
syntactic ambiguities	1.8535336135275255
functional unification	1.7843438499376167
phrases could	1.561123157601824
two assumptions	1.7796583190082031
standard technique	1.6862623517634696
language phrase	1.5012567846283982
different structures	1.778317426599752
two case	1.5230155441883526
based parser	1.97401434163728
one template	1.7152826942828467
corpus shows	1.6082268313113994
full account	1.5037367200448983
word wj	1.5795488348231213
one second	1.5908167675916909
spelling mistakes	1.5050618374545186
intermediate stages	1.6469586338689493
syntactic word	1.5418395969957306
score based	1.8239405806296551
semantic tags	1.8348619440613054
particular set	1.9774522967640409
one result	1.6600417838454895
propositional logic	1.5876555530936265
word lattice	1.8307567647847118
dictionary containing	1.7733570356336974
wsd problem	1.5226149809040495
based measure	1.7115143803683572
preceding sentence	1.8533547999227233
translated texts	1.5468665831408417
authors also	1.507007268326853
event space	1.5156083168889984
semantic restrictions	1.859911067934058
g 1	1.5236238824328985
system evaluation	1.8708863952690884
alignment information	1.5820070727743132
measuring similarity	1.5395099438048585
machine interfaces	1.5511943810330642
first selects	1.507007268326853
automatic conversion	1.5695222973163587
text interpretation	1.6207776420394997
dialogue fragment	1.5381766791708387
method seems	1.613624660358136
new topic	1.7674002593281979
three variables	1.5582958710952315
experiments discussed	1.558193976095525
equal weight	1.857063689073693
final result	2.060818086807506
system obtains	1.5582958710952315
new paradigm	1.6303444022729265
parser must	1.963483582228064
one hypothesis	1.648787373441388
three simple	1.5300728913253212
augmented phrase	1.6943920975417521
text written	1.5582958710952313
given problem	1.5379921841739324
data collected	1.9346320970586706
real users	1.7385191149814831
known algorithms	1.5297905781299383
subordinate conjunction	1.5000725031149704
higher correlation	1.5794243998957866
computational costs	1.5943471281025636
words might	1.7229550745276576
first reading	1.6650008022438314
speech information	2.03538957147206
scheme based	1.7017362832860266
second data	1.5840278237858718
domain concepts	1.7416559987167264
7 illustrates	1.5645241345053709
error prone	1.6419409233029298
consistent set	1.6214578999580556
links among	1.7293646551570643
90 percent	1.5195036419368688
manner analogous	1.5511943810330642
new pattern	1.6121742428060306
interesting results	1.8296566393987894
set x	1.7037243019190464
language pairs	2.11495175019511
time available	1.526993068994652
model performance	1.620486177768032
evaluate performance	1.649843436022881
future use	1.7341469152558133
romance languages	1.7196011397397877
two parsing	1.6308389319954624
using string	1.5245348740884508
using spoken	1.586137214651611
ambiguous interpretations	1.5483780918522223
therefore provide	1.5378646275607575
single level	1.6010101084822783
technical report	1.6867219229812012
use similar	1.5887468924062063
0 means	1.7471663351033109
propbank corpus	1.5903064706943568
cannot handle	1.8674900629395847
standard pos	1.5411597423384369
correct solution	1.6690280524444294
various knowledge	1.742320594713103
best scores	1.613624660358136
get information	1.6920919321535788
2 compares	1.6939639871030243
new top	1.5283573744567056
current word	2.0604551434807115
views expressed	1.5645241345053709
consists mainly	1.5378646275607575
evaluation technique	1.5203522197267523
initial model	1.7670224159013779
method outlined	1.522760613339579
two entries	1.6987279659751713
discourse theories	1.50119175880199
among word	1.619472687377359
generalized lr	1.5505333368729834
simple application	1.5300728913253212
every utterance	1.7202571136966487
also implies	1.6960474084795663
logical language	1.697578827483523
sampling rate	1.5371982986173842
frequency based	1.5170563067942409
second feature	1.6618077693135969
wsd algorithm	1.5611250512528885
select appropriate	1.6853236803857272
candidate term	1.509738047284853
parsing program	1.5658822295516854
given type	1.8652621865101078
contextual clues	1.7080578647287594
small values	1.6469586338689495
text type	1.8036625653501979
queries using	1.6320354670147563
another observation	1.591823957496618
database using	1.6339957705783554
generator produces	1.6469586338689495
contains either	1.507007268326853
previous n	1.592111614460519
generally use	1.611203449372866
depth analysis	1.5992286050546423
extraction rules	1.6822276939468386
rules proposed	1.513130125383453
expository texts	1.5757843973352497
les r	1.5004502201896939
retrieval engine	1.7686714800088208
unsupervised approach	1.733142512247401
certain word	1.859549146903458
occur frequently	2.0155949641848485
information beyond	1.678973586324036
idf values	1.5528815055502099
ordered set	2.007749668733147
extrinsic evaluation	1.574514888210389
confusion matrices	1.6025185031498406
structured representation	1.732906989876918
three constraints	1.6200671009956482
possible sub	1.6289349127929256
latter type	1.7888752560939947
standard parsing	1.6806432720347573
alternative strategies	1.5771944393968735
following parts	1.6056035510036821
realization process	1.5002075792603644
darpa community	1.511808504950452
new tags	1.513130125383453
positive class	1.5060571129436973
implicit assumption	1.6614869878819327
column indicates	1.726584657641364
user requirements	1.5678009097741024
similar work	1.6960474084795663
different directions	1.5459182428709468
string c	1.5459182428709468
memory capacity	1.5290208843393218
effort required	1.9273048529524266
employ two	1.5739592165010823
word sets	1.5443159329020681
particular topic	1.7393270952128785
various features	1.8924543073443427
text chunks	1.5231242788593953
retrieval method	1.5451191586357087
predicate must	1.509778922004791
larger corpora	1.899882056655873
related words	2.015072597011839
different task	1.7329292822928468
sentence construction	1.524534874088451
appropriate number	1.725393852123075
local feature	1.5310499836071938
say nothing	1.5864336788200233
entity identification	1.5581968658672878
less importance	1.5111306094308554
compare various	1.5445134575869865
software components	1.5971866572397082
using web	1.5898019097650744
similar concepts	1.5813570899282783
words could	1.8292369066971375
extraction based	1.6073715579162529
al language	1.5739592165010823
structures called	1.6548911840144862
telephone conversations	1.7317499628379962
automatic discovery	1.6045962492509205
words form	1.583051127543801
annotated sentences	1.7917294757153648
time stamp	1.524404575690582
v x	1.677493731473086
following description	1.7017362832860266
practical terms	1.6031224303496519
solving process	1.5006743580950452
several reasons	1.8901053914895614
resolving anaphoric	1.510181823566108
method shows	1.570430085810516
accuracy improvement	1.615760819779826
given target	1.7492265655841717
observed frequency	1.6501622378854464
edge e	1.5831851382373663
large number	2.348551878320584
system relies	1.7902853972306825
semantic dependencies	1.73505478197822
compositional semantic	1.6115694852294895
every event	1.579873144038688
p v	1.5105194311609613
particular pattern	1.6388370153723533
posteriori probability	1.5648583093794035
training samples	1.7404480516037029
different patterns	1.8679501480121337
linguistic knowledge	2.317975586812631
experimental studies	1.509778922004791
arcs represent	1.6349068916737024
complete analyses	1.511205120616144
concepts using	1.5739592165010823
crossing brackets	1.740176634828948
incorrect information	1.5288687562201928
vector representation	1.8329461520359107
limited data	1.5988242883426202
language processor	1.7037456219423053
underlying form	1.5595526312829278
input utterance	1.8398554221674865
possible sense	1.6908035353903144
language technology	2.040678209050976
sentence corpus	1.541959479744658
interesting case	1.672219863528485
inflected verb	1.5074121615021703
given part	1.619472687377359
speech recogniser	1.5714568848346864
open vocabulary	1.5254475780350207
evaluate system	1.5445134575869865
relation holding	1.69956960136264
standard hmm	1.6075783640375185
automatic knowledge	1.5440207036648979
relative improvement	1.7519095400491063
grammatical functions	1.9567469418760965
retrieve documents	1.742292913915703
c k	1.5610207580512117
information science	1.8823643376348143
tree adjoining	2.0594634453570935
related events	1.63819988416383
large sets	1.804725963492348
algorithm assigns	1.5973170078864487
main purpose	1.6035443768923665
last case	1.757636103678803
probability estimates	1.9988980434892836
possible models	1.5806234267011035
test items	1.5309984882390484
th sentence	1.5430456533255328
desirable property	1.6699400022790054
van rijsbergen	1.8171265892705295
given x	1.60913070763407
software system	1.7022690686007484
constraint satisfaction	1.8034131290816149
always selects	1.5188051370135929
figure 14	1.7838751756313243
plain text	1.8747617969419592
ov coling	1.6136932231894319
coreference links	1.5937194223097002
adequately capture	1.5338735539822874
significant effects	1.5612815851895034
pos sequences	1.5951720357917587
grammatical frameworks	1.5491899194507934
prosodic analysis	1.5032485859252782
composition operation	1.6304903734340297
classification process	1.7539013662726877
dictionary consists	1.6761656524213184
x axis	1.586137214651611
extraction method	1.8274011827330137
possible sequences	1.799395952312458
important applications	1.5188051370135929
linguistic features	2.020136490466549
results compared	1.583051127543801
short words	1.5272972964829599
tjong kim	1.7833936681405156
temporal structure	1.617589592922188
particular instantiation	1.5131301253834533
one alignment	1.6213967902634534
classification problem	2.082164204316345
feedback loop	1.576707112483232
10 runs	1.5148387121477085
main memory	1.5376939429146934
binary rule	1.5160682091319875
larger vocabulary	1.6050288834324327
event described	1.6873446991526628
languages might	1.5739592165010823
words e	1.5230483704868893
method described	2.18428212135367
brants et	1.6648996270374434
language corpora	1.7411345878540514
free parsing	1.7922937116458295
somewhat ad	1.522760613339579
earlier example	1.6565475758379593
simpler ones	1.5977445137418091
user responds	1.5275591303469287
using speech	1.777003269519583
label sequence	1.6309398847394811
two constituents	1.9342763953818725
share many	1.75176197502999
particular attribute	1.5212396502385475
last name	1.7528780905032713
actes de	1.6934793309340153
systems without	1.65027618229265
final alignment	1.5354510398033074
best performing	2.00022265640808
clauses may	1.5908167675916909
allow non	1.5511943810330642
human language	2.154279083926573
sources including	1.5111306094308554
original sentences	1.6920901417156688
german language	1.706438546413843
note however	1.803624526573127
based baseline	1.5081682632911075
line 8	1.5470117266713654
original goal	1.5150717731521737
alternative paths	1.513130125383453
example 6	1.7616104684983709
sequences like	1.5211629864223772
many relevant	1.5929928949821113
algorithms exist	1.530072891325321
semantic hierarchies	1.5354510398033074
systematic differences	1.592111614460519
central notion	1.6002993454155385
many instances	1.9386991664566202
complex grammatical	1.5037367200448986
two principles	1.7497703463414667
system demonstrates	1.5611231576018243
4 proc	1.7563379226837867
order variations	1.5258093094263683
document set	1.8656657003326165
often use	1.8633999645732353
method results	1.5037367200448983
classification accuracies	1.5525419192763321
major research	1.6164339756999313
complete solution	1.5778973208841207
simple data	1.6194726873773588
mccarthy et	1.5549538239663718
sgall et	1.65387744462589
different sites	1.6822939189998194
new text	2.0299050690906757
majority voting	1.5710320713364703
preprocessing phase	1.6479791814036409
5 discusses	1.8359513554634197
constraints given	1.63179231938939
representations may	1.572834038198836
methodology described	1.667913900271795
initial list	1.5582958710952315
possible referents	1.6466154613965813
specialized domains	1.6035104581026864
adding information	1.6741797752731205
second instance	1.5541060999723646
paper consists	1.5973170078864487
local minima	1.5984615586550819
related approaches	1.583051127543801
node corresponding	1.7594013028872317
system fails	1.6706426105940735
corpora may	1.5611231576018243
rules without	1.7057741658061807
questions using	1.6094143286760043
larger system	1.7032661601026524
method presented	1.865183877506066
morphological analysis	2.288716249698428
correct identification	1.6392309045877975
john wants	1.604490822538553
features represent	1.6500007742396736
two daughters	1.5731122769149348
ordering relation	1.6840711963273556
practical problem	1.6580606251764172
certain classes	1.8619464975072229
multiple relations	1.5275591303469287
major steps	1.711151422700744
different items	1.507007268326853
posterior probability	1.915145312319167
ensemble de	1.545707024163681
japanese dependency	1.5503947994079637
previous experiments	1.9478796313005078
abney et	1.532181048120497
science laboratories	1.5338735539822874
weight assigned	1.7400137177323638
system looks	1.6428721706698721
information units	1.561394983560478
resulting state	1.6016814303414122
individual tokens	1.65303397348486
semantic network	2.0746647389961295
potential antecedents	1.622334866094814
f j	1.5488506011214074
rules define	1.6593965399315964
algorithm attempts	1.6213967902634532
using explicit	1.5338735539822874
languages could	1.5037367200448983
related concept	1.510711371954136
different resources	1.5248509123974712
language components	1.6360308115630153
japanese verb	1.6486551358721306
usage patterns	1.5134483070876725
user utterances	1.8227016005533039
allows us	2.49837011815947
data bases	1.676564456793552
ambiguous word	1.9213309985582028
actual corpus	1.5037367200448986
supervised systems	1.5184479708929823
system error	1.5558982289862366
noun phrases	2.4381329351132
sentences may	1.965179025976846
nlp researchers	1.7253938521230747
discourse analysis	2.015619515053645
word penalty	1.596978513380296
assign scores	1.5511943810330642
three lines	1.6036848953283132
running texts	1.5037367200448983
nodes labeled	1.6975841751304364
argument may	1.6920919321535788
boolean expression	1.5803272240852049
work using	1.8314975434690972
right context	2.0000219150169647
one slot	1.6879860523018786
object nps	1.5354510398033077
single linguistic	1.5111306094308554
higher accuracy	2.010713700515892
alignment task	1.666781559298848
one meaning	1.7618356310082155
evidence supporting	1.6388987829322057
appropriate values	1.660919557194327
different requirements	1.530072891325321
best method	1.7414848008364259
include semantic	1.5973170078864487
become necessary	1.5156323250636554
first goal	1.7522485220301844
based generation	1.8015788749734403
one paragraph	1.5195036419368688
similar process	1.6388370153723533
good basis	1.6920919321535788
algorithms developed	1.5354510398033077
models cannot	1.5887468924062063
last category	1.5739592165010823
object oriented	1.681252707662353
possible non	1.5482870987025659
general ones	1.6770235151848842
language cannot	1.579873144038688
statistical distribution	1.5381766791708387
different analyses	1.6743043666663548
generative power	1.7770615272114594
passive transformation	1.5211629864223772
small probability	1.5188051370135929
lexical ambiguities	1.8169150781276737
recursive calls	1.5620473238340082
automated systems	1.532181048120497
percent correct	1.673200237149869
existing nlp	1.5702560143399067
system detects	1.5794243998957866
compare different	1.8119659926140157
template contains	1.507007268326853
contains features	1.5706734993195162
large values	1.619472687377359
probabilistic methods	1.6901874423804126
research questions	1.6013202535181605
different state	1.5078976439458074
per sentence	2.1582447792702335
lexical word	1.5724446428905166
new ones	1.714064521305018
particular text	1.8305753698237082
second kind	1.7737289647431678
many studies	1.8432267974189886
theoretical model	1.63601460385856
japanese characters	1.561707184647436
node containing	1.5230483704868893
occurring word	1.6111096505227929
grammatical phenomena	1.6853874840250063
varying degrees	1.6998319539943458
complex constraints	1.5594698199757022
second evaluation	1.7120828968340218
language system	2.162702580704823
french word	1.80392447465274
brill tagger	1.7019537943281668
frequent word	1.8387182513328892
length less	1.6477541750656948
interpolation method	1.5283078246733592
project started	1.5338735539822874
brackets indicate	1.6596621481222722
linguistic formalisms	1.5979661313746822
existing words	1.518288803007152
trees produced	1.7613903638475996
various experiments	1.5378646275607575
conference poster	1.944780873277881
use automatic	1.5771944393968735
every relation	1.5531855081374653
evaluation using	1.823287569379658
training materials	1.6272194714174604
full corpus	1.6534017505549203
rule 6	1.633034028428662
another type	2.040122790653622
cannot express	1.6112034493728657
data model	1.6590035934138139
model yields	1.658060625176417
categorial grammar	2.036011411663325
200 sentences	1.8001633864676887
methods include	1.5781539692435587
speech patterns	1.5543915029845932
whose aim	1.5328333954091753
electronic version	1.5541060999723646
individual speakers	1.5269930689946518
one node	2.0941195728193724
unseen words	1.7867349741911163
major advantages	1.591823957496618
tile source	1.5156083168889984
garden path	1.6950408042482552
closest match	1.532714913365084
verb would	1.5634115707207188
main processing	1.5111306094308554
several versions	1.6339957705783554
tile tree	1.5354510398033074
parsing errors	1.839446740315701
unified framework	1.6951212542623044
k words	1.632124845132815
functions f	1.6816988370728096
statistical tests	1.734415693746389
various sources	1.8834556554926263
concepts represented	1.5771944393968738
approximate solution	1.5645241345053709
language evaluation	1.526993068994652
similar type	1.5156323250636554
segment may	1.5430674042673345
syntactic positions	1.69293654056235
contains n	1.5869693547698929
existing ontology	1.5222638249634683
planner uses	1.558193976095525
theoretical models	1.5731122769149348
short paper	1.5749574214167732
actions may	1.6286406976823904
words per	2.1077431989005695
descriptive power	1.6575744095824656
important component	1.8673889301270783
rule systems	1.629428271499692
sentence may	2.0527553663872546
gram precision	1.5772474556743727
tile language	1.550611280307232
corresponding nodes	1.7335235400599143
inconsistent information	1.5087853581980926
various feature	1.6760269815500826
algorithm cannot	1.7562014595765985
training phase	2.009677271700706
relative performance	1.852394857267724
term clustering	1.549494937688089
translation pair	1.7874536304895565
describe experiments	1.6845577236692315
additional mechanisms	1.5887468924062063
candidates using	1.5020678562141567
appropriate answer	1.6963149953958458
test utterances	1.5611263554415697
garside et	1.5036575034310684
mt output	1.5722900601047212
features may	2.00723959326357
expressive power	2.0854163015345195
contain multiple	1.7279919232921825
allows different	1.591823957496618
need two	1.5887468924062063
dialogue structure	1.6977679819498712
current user	1.5076767984207575
case letter	1.559137125070758
processing text	1.5269930689946518
intermediate structures	1.6709704522077142
preceding discourse	1.8376465248174907
case study	1.8915408070819149
fit together	1.7192050380109327
maximum number	1.9864454074311997
figure 3a	1.5355252124278633
several clusters	1.5354510398033077
ad hoe	1.6087623527820814
positive evidence	1.5763110087210772
initial experiments	1.9952180496621932
unseen events	1.59559322318777
single state	1.6656225051066411
possible sentences	1.618103745190334
null string	1.60189988307561
representation theory	1.9095420562434353
process produces	1.6164339756999313
encyclopedic knowledge	1.53985657766184
tables show	1.5839427016111838
little knowledge	1.507007268326853
preliminary version	1.7626132976152016
positive value	1.6913244324825505
physical world	1.6317923193893897
x b	1.5558982289862366
text alignment	1.5623017773735164
related text	1.5395099438048585
first reason	1.522760613339579
input symbols	1.6483499264575152
construction rules	1.5203140759630642
l 2	1.6241567971066324
2 briefly	1.62412689036662
test scores	1.5618288073614095
without access	1.60913070763407
syntactic frames	1.623426692238798
system identifies	1.7836550413109447
temporal relation	1.6941763903610423
also notice	1.6861938694548688
five words	1.8037966347043266
structure produced	1.7077239302724851
grammar developed	1.6961394033344752
current parsing	1.5582958710952315
high precision	2.198668745420239
also define	1.919283286772654
paper introduces	1.9332238980652194
methods use	1.7720369317764424
press et	1.5622818522186857
grammar presented	1.5217987622876712
appropriate place	1.594773126307433
one proposition	1.5784904253929217
present discussion	1.7329292822928468
general syntactic	1.8152836877710878
limited number	1.8746199617954122
node representing	1.7749326120716244
lexical resource	1.8535435773255287
efficient approach	1.5306061495832655
larger phrases	1.5081682632911075
regression model	1.5769990681619763
important ways	1.731146052909204
insufficient data	1.5306061495832655
3 task	1.5102577783164786
pour la	1.710431666059581
second argument	2.031406722428808
methods require	1.75176197502999
domain dependent	1.9178917160951565
oxford advanced	1.6297468697667572
human listeners	1.6073715579162529
memory usage	1.6327247404420862
correct answers	1.9646140732754358
fixed word	1.639712650226961
syntactic constraint	1.6765176924951533
following formulas	1.5482870987025659
following notation	1.7305508816393718
similar documents	1.637575348261155
second approach	2.028898280550964
text indexing	1.5211629864223775
resulting scores	1.5111306094308554
argument types	1.6217171084045376
general strategy	1.7139483293868785
linguistic feature	1.6444364637541418
english parser	1.665159100934992
information seeking	1.6410838314105116
find relevant	1.6761656524213182
ill section	1.507007268326853
use statistics	1.597317007886449
approach suffers	1.5188051370135929
extracted terms	1.5574102931451854
multiple candidate	1.5037367200448983
basic rule	1.6235405672398666
existing approaches	1.7905488812931116
four measures	1.5177700906840073
forward way	1.5378646275607575
line dictionary	1.6997413372630379
step 3	2.13781046776891
various research	1.5037367200448983
corresponds exactly	1.613624660358136
several iterations	1.6221843551054547
probabilistic modeling	1.5548975154096096
nominal group	1.5374179184569106
various corpora	1.6190328102134979
reasons given	1.5612815851895032
measures based	1.726584657641364
program takes	1.5739592165010823
main modules	1.7178162994368975
system accepts	1.6525488762732246
automatic feature	1.6329374072287361
speech recognizers	1.9290134501714176
data contains	1.865764701860884
word bigram	1.6402989477550278
peking university	1.6718201054745863
result also	1.6481174069026685
node raising	1.5083332766176836
c using	1.530072891325321
ambiguity problem	1.7268506560571117
n nodes	1.5829912568215723
texts based	1.5378646275607575
syntactic configuration	1.5111765881035293
conceptual structure	1.9027550152256447
disambiguation task	1.8977889770082976
general lexical	1.6802580409155776
matching methods	1.616031378434591
modular architecture	1.731922435932006
significance test	1.6590180783841593
simple finite	1.5887468924062063
information specific	1.65303397348486
context words	1.8112534826001996
one choice	1.6761656524213182
conceptual entities	1.5487461920851402
sections discuss	1.5195036419368688
existing domain	1.5037367200448983
different nodes	1.7116468676241006
hobbs et	1.8669999430942732
verbal predicate	1.5824798263750006
surface word	1.8312284666371454
best accuracy	1.8102125770358926
regular patterns	1.539087705163675
three functions	1.640965319692465
several improvements	1.5037367200448983
experiment 2	1.715191156827036
different purposes	1.8207496307000293
following functions	1.6961394033344752
right fashion	1.6699400022790052
target position	1.5902537176167117
recent literature	1.6785607164239145
rules concerning	1.5188051370135929
user might	1.8882048617768683
final choice	1.5111306094308554
empirical methods	1.9753820174469625
ambiguous parses	1.5559715171228279
new speakers	1.516491127995908
construction method	1.5321552374951573
following hypothesis	1.594773126307433
exclamation mark	1.5786067012560172
given slot	1.5559715171228279
several layers	1.579873144038688
system like	1.8673889301270783
system performs	2.051165208090371
given meaning	1.5906723352433731
unit may	1.5188051370135929
text meaning	1.6400618406060914
independence assumptions	1.9253415520041344
semantic basis	1.5189601534336892
graph corresponding	1.5188051370135929
overall organization	1.6411558246537032
parser may	1.8224278217877459
naive approach	1.667949814118166
certain category	1.5177700906840073
null first	1.719365286993631
nist score	1.5887280907032952
parser failed	1.507007268326853
parsing scheme	1.5875276711315967
common word	1.8664093121752798
corresponding rules	1.6010101084822783
remaining input	1.506350970756098
complex problem	1.7074092050419734
particular syntactic	1.906966923846392
currently contains	1.7425248074331754
another program	1.530072891325321
continuous density	1.5165338306996712
active edge	1.6514594291629803
shallow linguistic	1.5330982449913544
set theory	1.6155714032618729
penman system	1.5238894580118154
syntactic structure	2.4363543302728887
parsing systems	1.8722231162300405
distribution across	1.584483801902334
development environment	1.5489609474007966
de recherche	1.5078797213408792
therefore propose	1.6056035510036821
vector representations	1.6262025659080146
detect whether	1.507007268326853
current stage	1.6593965399315964
specific sense	1.6804951004384119
large collections	1.8091042811342237
several strategies	1.708635806624953
cannot guarantee	1.6802940634668402
function returns	1.6332596225990514
lisp code	1.533540614646662
partial results	1.7543529274810592
algorithm relies	1.6845577236692315
human activity	1.5408560126058508
various components	1.890440451702713
relations described	1.5195036419368688
unsupervised algorithm	1.596978513380296
making inferences	1.541748679610567
distance dependencies	2.0093040338421213
phonetic context	1.5102436084604778
x v	1.6205336782271196
five categories	1.6525938603103605
formal model	1.7860060113537617
preprocessing stage	1.720843381086416
preposition phrase	1.5195036419368686
decision lists	1.648405834161855
gather information	1.6037378902512214
results reported	1.9547034914756494
previous evaluation	1.5011917588019899
whole set	1.965993059951167
computational tractability	1.6286406976823904
deeper levels	1.5156323250636556
knowledge associated	1.5941108460767777
flight information	1.5182069877105362
possible fillers	1.6327406427880258
space constraints	1.7294947296982195
prep np	1.504934031784749
specific models	1.545015347712111
reduced form	1.5611666480079123
representative set	1.5706734993195162
sense inventories	1.5264702417655651
second occurrence	1.6447221446450688
knowledge consists	1.582117970332684
single nodes	1.5582958710952315
production systems	1.546024543678514
important tool	1.5511943810330642
different operations	1.5078976439458076
unification operations	1.5262286512835543
following schema	1.6190328102134979
value matrix	1.6169635240879603
entire grammar	1.5459806054163279
accurate translation	1.6303444022729265
example 9	1.5991253496404458
technology based	1.5445134575869865
randomly select	1.7630115553268326
application dependent	1.5459806054163279
competing analyses	1.6213967902634532
uchimoto et	1.5590375748389647
user could	1.8489330052841644
ambiguity exists	1.5110975570289593
multiple words	1.828464340679315
distance calculation	1.568211939672261
corpus made	1.5111306094308554
phrase attachment	1.974585433659911
system considers	1.6246289167870045
parsing procedure	1.717587431146898
marker passing	1.501208863272471
root nodes	1.8552048060029989
english may	1.579873144038688
discourse referents	1.7867830100017004
includes information	1.7961446965684997
larger constituent	1.522760613339579
word initial	1.5321810481204972
corresponding entry	1.6342922036019591
simple representation	1.5378646275607575
analysis proceeds	1.6581363223870689
formal models	1.6213967902634534
cognitive process	1.5573537001177145
several instances	1.6770235151848842
effort needed	1.6476109821002263
object type	1.550855793293556
source data	1.6672488650431438
vary significantly	1.5977445137418091
testing phase	1.7204954089259474
system thus	1.6031224303496519
dynamic nature	1.5116176889192867
minimum distance	1.6130635411009078
halteren et	1.5578095587294318
time efficiency	1.6078424239387177
relation labels	1.5901580552621326
version 1	1.851190911344189
discusses related	1.6084968011212868
important difference	1.9855105157211872
constituent coordination	1.5385275495385051
following representation	1.600330740560592
two input	1.7261178590655901
possible variants	1.5354510398033074
incorrect analyses	1.5078797213408792
case like	1.5037367200448983
little improvement	1.5328333954091753
feature extraction	1.9589831033138028
data provided	1.9024611366110071
first page	1.9602999598058422
precision recall	1.574419975438257
domain must	1.5749574214167732
running example	1.6536994969175356
linguistic components	1.6066906129856133
appropriate interpretation	1.5647505415436789
arbitrary text	1.5585101349998935
clear preference	1.5011917588019899
document analysis	1.5709753846661263
2 features	1.530072891325321
two co	1.5971626956604803
orthographic transcription	1.6232932581152908
correct links	1.506173888464562
processing strategy	1.6457839295193488
physical sciences	1.5706734993195162
information given	1.532054292466229
interesting information	1.5156323250636556
recognition error	1.86983795269227
existing text	1.6614869878819327
w occurs	1.5040418526142796
extract word	1.5771944393968738
system design	1.9422990409200436
frame elements	1.6083593142949513
every case	1.8549364534591946
knowledge base	2.3415906381458766
prior probabilities	1.7618409908827224
involves using	1.6411558246537032
current sentence	1.9789120612745221
distributional similarity	1.6859099708032939
montague semantics	1.5130250143142798
multiple events	1.5011159506850649
various dimensions	1.5706734993195162
e denotes	1.5425370803688765
different rules	1.886792850565565
bigram probability	1.6258574628751319
highest degree	1.541423896831627
contain additional	1.5887468924062063
reader might	1.6002993454155385
experiments show	2.1689128492661833
structure grammars	1.9585048449647708
models discussed	1.6235405672398668
maxent model	1.5625680157815818
specific grammar	1.7341395890835507
new strategies	1.507007268326853
longest string	1.530072891325321
observed data	1.7605218564467084
assigning probabilities	1.586137214651611
relevant entities	1.609217242875463
l e	1.6590568496057747
features automatically	1.5582958710952315
canonical forms	1.6752482671501479
separate process	1.5408560126058508
automatic scoring	1.5062881789045195
research effort	1.8385104919728776
constituent must	1.5338735539822874
verbal elements	1.5036575034310684
example taken	1.5582958710952315
two algorithms	1.954547575324185
hpsg parser	1.5341693283890263
complete noun	1.5288687562201928
algorithm fails	1.5886561992520427
two subsets	1.7906600182749972
reiter et	1.5724591485153083
smt system	1.6589371655891147
human information	1.510711371954136
paper presents	2.4604202467883454
estimation method	1.7603186820990187
process starts	1.7884354035243888
possible way	1.8802566231707112
much harder	1.5994578640159953
adverbial modifiers	1.6221843551054547
training examples	2.1554993944226912
rules correspond	1.5269930689946518
initial system	1.618801192835399
work include	1.6002993454155385
correct predictions	1.6950814181449951
level descriptions	1.5175269497604567
2 n	1.7589027972617595
mathematical model	1.5485561843397788
properties associated	1.6242840557944538
first briefly	1.6844174045708415
let n	1.9064823551166652
text produced	1.6619816694718796
based tool	1.507007268326853
feature allows	1.5354510398033074
linguistic system	1.6735850693612517
computational theories	1.5156323250636556
unification based	1.8418600744551106
length one	1.6314078645246426
wide coverage	1.9253459420044112
knowledge necessary	1.7106339901670682
human reader	1.7458727309731377
formal grammar	1.7002867521325629
figure 1	2.817828195251619
carroll et	1.7754519027372242
language constraints	1.519012494635691
previous approach	1.5748567577278902
speech utterances	1.56946415445543
1 show	1.7844902445128514
appropriate type	1.7998749809710393
n words	2.005961994009442
method works	1.8329489432622579
translation candidates	1.7889459820961104
average number	2.104490176479521
little data	1.6169986020906966
analysis revealed	1.6392309045877975
local constraints	1.6974768093786063
transfer process	1.5929671961208305
many combinations	1.5188051370135929
data within	1.524534874088451
capture important	1.5973170078864487
data sources	1.8702038104203103
lexical descriptions	1.6236260536395308
semantic property	1.5432756752761159
translation probability	1.8807556779968875
standard algorithms	1.50119175880199
8 words	1.7294947296982195
specified feature	1.5652297135534465
initial evaluation	1.6265370361225344
sections 5	1.8272891384792251
training stage	1.5754199285488595
find one	1.7805487594034692
linguistic coverage	1.8006197636907706
per discourse	1.5676517392350742
fourth step	1.5177700906840073
new instances	1.7022690686007484
classification errors	1.6300478957277753
position p	1.513153217011753
graph representing	1.5611231576018243
figure 16	1.6937086927194753
historical reasons	1.522760613339579
dictionary contains	1.8751336706429145
two forms	2.010979839637431
previous subsection	1.7356468461869352
expected counts	1.5592638654802111
partial parsing	1.9057840126402315
et une	1.5258093094263683
different users	1.7560373930395001
conceptual relation	1.5974933392540063
various machine	1.6458797346140277
scheme proposed	1.511205120616144
since word	1.7329564081810807
basic problems	1.5645241345053709
language description	1.697125898687969
additional words	1.613364934425665
verbs take	1.6235405672398668
becomes smaller	1.5188051370135929
algorithm discussed	1.5111306094308554
syntactic component	1.8591396899325647
mathematical models	1.5854098583082799
different settings	1.734400305455702
independent words	1.5513680167829076
conditional distribution	1.8134173661176083
first verb	1.6421322605132707
input tree	1.6196176797600823
multiple categories	1.5698085472091134
starting points	1.7880620248977583
occur infrequently	1.567922330580758
realistic applications	1.5726639908262192
automatic evaluation	1.8672099319928566
subordinating conjunctions	1.6843898550477743
incremental fashion	1.579873144038688
present version	1.7293646551570643
consistent improvements	1.5111306094308554
entire sentences	1.6602505116127793
vertical lines	1.546195991207543
combining information	1.6500007742396736
figure 15	1.7194528402606855
model becomes	1.5511943810330642
segmentation bakeoff	1.6147171332363546
type hierarchy	1.909127949450037
standard test	1.8157366854698929
known methods	1.507007268326853
appropriate target	1.6115236136845899
different method	1.7244811761730507
grows exponentially	1.7230579229098404
using corpus	1.7127967389448444
successive utterances	1.5107844011873683
word x	1.7973075951455235
distributed processing	1.510618969052833
give rise	1.849318460813143
use rules	1.5482870987025659
research systems	1.6097370524441605
words generated	1.5414180417122596
sentence also	1.5547189562170496
regular expressions	2.1052730768465118
many factors	1.7932322850313147
example output	1.543442208482524
training using	1.6535456510181574
whose corresponding	1.5483780918522223
binary decision	1.6933430063299286
others like	1.5645241345053709
first manner	1.5226071645445156
compares favourably	1.5037367200448983
100 test	1.587700203918434
first sub	1.5188051370135929
grained distinctions	1.7912633783955423
newswire articles	1.774041215911187
takes place	1.9467072194188124
linguistic intuition	1.7071550364333241
hajivc et	1.5148387121477085
appropriate part	1.5111306094308554
features also	1.6320354670147565
human would	1.510181823566108
english generation	1.6256278638449597
syntactic relation	1.8718126304962048
editing operations	1.5761571994770096
causes problems	1.6289349127929256
special characters	1.7631642455155774
number 2	1.8881035648482092
problem becomes	1.792162570502364
ascertain whether	1.5547189562170496
systems mentioned	1.5354510398033074
frequent category	1.5087853581980926
paper contains	1.591823957496618
10 iterations	1.66195509821619
improved version	1.7210742795514369
many characters	1.526993068994652
per cluster	1.5177700906840073
e following	1.5037367200448986
multiple translation	1.5189601534336892
general goal	1.5361450492096225
output structure	1.669777280074386
produce multiple	1.6411558246537032
using class	1.549158634580789
every category	1.6343543529378615
corpus would	1.7980956075726102
mt project	1.6015443630283652
different effects	1.567922330580758
common technique	1.5445134575869865
model parameter	1.7094318443496084
analysis must	1.6104229222909037
first model	1.7824175027740568
different answers	1.5378646275607575
two patterns	1.7506850593398975
improved system	1.6084968011212868
new goal	1.588669192462068
statistical measures	1.8485964650682378
transformational rules	1.6851528937425124
morphological decomposition	1.5014109736935004
briefly mentioned	1.507007268326853
search techniques	1.6647470839663425
following paragraph	1.5887468924062063
four sub	1.5217605875570064
tree construction	1.5818585409216537
rule applies	1.9125596417826487
translation process	2.1422018520354826
inflectional information	1.5563893320981301
linguistic characteristics	1.709076712145899
single entry	1.6544208897318429
researchers working	1.636252690439584
subject would	1.5511943810330642
strong influence	1.591823957496618
implementation used	1.522760613339579
structured objects	1.5413044632696593
particular instances	1.5482870987025659
network whose	1.5245348740884508
training sentence	1.7026590988050134
theorem 3	1.7171934990687312
relations found	1.635876749820076
investigate two	1.5869693547698926
network structure	1.6369537838372867
temporal adverbs	1.5562287730998787
four characters	1.6990961045663537
translation problem	1.732287446762944
definite description	1.6748882142863337
location name	1.5925754035510153
structure trees	1.9332733902893138
scheme allows	1.5645241345053709
several measures	1.6124187588443077
deleted interpolation	1.762761182330992
interesting question	1.8826557466236107
associated word	1.5815949664651068
one agent	1.680145181544324
content words	2.2309259824837895
tree produced	1.6285930450163055
evaluation test	1.8221330996410852
binary value	1.549729132180171
semantic selectional	1.5106521830350297
explicit representation	1.8899400202299197
subordinate conjunctions	1.5257218622243724
main properties	1.5408560126058508
system receives	1.5378646275607575
model 1	1.866623667493405
returns 1	1.5719494264134362
example also	1.6822401779866478
maximum probability	1.7328355575418408
selection module	1.525989407425406
hypothesis space	1.6490502841459782
local coherence	1.6254103831426994
computer screen	1.640044071538913
probability mass	1.9291794521907921
word hypothesis	1.5788813623144793
target translation	1.6083616809654038
word formation	1.8359804942273674
negative information	1.562122754590177
role names	1.5621227545901766
tile information	1.664540718454114
occurrence probability	1.5606608542765912
intonation patterns	1.549729132180171
level structure	1.7372651683228564
electronic dictionaries	1.7052073878215785
entropy principle	1.5619819959392902
gain insight	1.703359686465308
different noun	1.5354510398033077
spearman rank	1.5360591638400813
method generates	1.5992286050546423
linguistic approach	1.674183266673865
example word	1.5081682632911075
different categories	1.9735071954109538
algorithm identifies	1.6840631149502634
special role	1.5338735539822874
weighting factors	1.5508853729212349
question processing	1.5316984903375421
standard language	1.5865550052724773
word subset	1.5110975570289593
semantics based	1.597317007886449
broadcast information	1.507007268326853
new material	1.7088092136999036
among verbs	1.5148722773105288
speaker utters	1.5211629864223772
previous study	1.6137245240376052
second interpretation	1.595491118313467
help explain	1.5037367200448983
comprises three	1.613624660358136
comparative study	1.7761890186457476
per language	1.701168686493555
following observation	1.573112276914935
provide clear	1.5395099438048585
k c	1.5156323250636556
depend upon	1.8416641527439797
inference algorithms	1.5256428192011384
systems take	1.5408560126058508
manageable size	1.6315901311540408
structures could	1.5306061495832655
automatic approach	1.6819762996803396
certain verbs	1.7669373105292956
second problem	2.0604285473298445
lexical structure	1.732857115769352
underspecified representations	1.504567398389933
considered correct	1.8542210190154604
important consideration	1.6783930166760777
ambiguous sentence	1.8098318775526256
planning system	1.638025570532872
focus space	1.547744747866347
considerable work	1.591823957496618
columns show	1.764066791297902
singular noun	1.825745017865925
efficient access	1.552032937886318
systematic way	1.9111254763680066
models like	1.66195509821619
tag ti	1.5791672342079381
scores produced	1.5955573878777638
offers several	1.60913070763407
sentence aligned	1.590058644131361
level 0	1.575427116304498
000 entries	2.0165306717351856
attachment sites	1.6467737480586901
alignment based	1.6217781681471894
command line	1.5335951076004797
four models	1.650513919720341
rule says	1.7412776883377425
search queries	1.5839523796116213
language interpretation	1.718294535071881
merging process	1.6290475254752605
time expression	1.6773786988298016
proposed methods	1.8390799381838392
detection system	1.5755449042867204
average values	1.6409107009350252
tagged text	1.9484975880655773
term frequencies	1.6524840678993469
e cient	1.514469500969361
classification scheme	1.7820404627283082
rules corresponding	1.5690244007917742
model estimated	1.5613568731772178
focus word	1.5519514319853962
particular goal	1.5736712336159786
based parsing	1.9342588082323973
whole text	1.8702546453007938
optimal value	1.7417931745396025
process allows	1.5445134575869865
put forth	1.6785607164239145
nlp problems	1.8366246584261536
two segments	1.7652044302783785
classification systems	1.6018077686652454
significance testing	1.582384654874436
obvious problem	1.5547189562170496
recognition problem	1.8484182852596844
embedded sentences	1.6700643039320042
single step	1.658594716820426
question must	1.613624660358136
expensive operation	1.5559715171228279
intermediate nodes	1.6669753881293854
weighting function	1.6259501876487639
la traduction	1.5009013018779656
case markers	1.6924224932507022
simplified version	2.0172031353776623
specific domains	1.8981851947603252
random choice	1.7105788625254257
additional restrictions	1.506173888464562
machine interface	1.5387082898744961
many features	1.936623816982368
manual evaluation	1.697851408814584
improve readability	1.5320698902069676
semantic analyzer	1.669519245302935
annotation schemes	1.7465603845453617
word bigrams	1.6506026263637312
testing examples	1.5649774855459646
second type	2.0732970849354606
efficient algorithm	1.8789462403200168
one characteristic	1.522760613339579
approach relies	1.8190027986131616
xml documents	1.54718408009792
complex phenomena	1.587700203918434
languages generated	1.5983071019664772
french sentence	1.7332869171772325
standard vector	1.530072891325321
e par	1.5522962411909402
binary predicate	1.5571495092265906
knowledge bases	2.162697980689171
possible phrases	1.513130125383453
short summaries	1.5756510084230786
traditional information	1.689290298275095
outside algorithm	1.7790202029035052
small fraction	1.8526886660128121
worth pointing	1.707160349122273
following criteria	1.855526973126632
single noun	1.742522101680745
text alone	1.5441583487573467
clause type	1.6162383534717666
special care	1.5547189562170496
phonological information	1.6143076825990914
include syntactic	1.6593965399315964
second algorithm	1.6766459301700114
dictionary data	1.5857665730820467
simple terms	1.5511943810330642
negative integer	1.5201190767490376
whose goal	1.7854718751434602
tile left	1.5864336788200233
sentence fragment	1.7783119910798986
validation process	1.5306061495832655
k clusters	1.52377952472091
log probabilities	1.7325198671385464
knowledge available	1.6946633528721495
alignment method	1.6112479038805891
cross entropy	1.515321081304624
four features	1.7566598193485885
young children	1.5767760138793054
standard format	1.5511943810330642
treebank tagset	1.544253993341759
morphological variants	1.8387283568644535
text window	1.5701536868589447
system simply	1.7125369004275144
variable bound	1.5106189690528333
existential quantifier	1.7193819685963985
first n	1.805626615021896
following terms	1.5338735539822874
derived words	1.6426329177034502
approach yields	1.6724173909604771
rule also	1.6266256462651403
algorithm performs	1.9094584404762147
target text	1.856379702843615
approaches like	1.6300610276118177
recent approaches	1.757636103678803
japanese system	1.502837018931634
method similar	1.6593965399315964
system exploits	1.5111306094308554
possible english	1.6666664790751589
yields two	1.5547189562170496
inactive edge	1.6405549011847265
modified noun	1.6312311959186723
word alignments	1.9247386671895248
steps 1	1.7631952217682356
basic tasks	1.5111306094308554
approach adopted	1.7650139064090151
journal articles	1.9416833561334939
classification procedure	1.5269930689946518
nodes n	1.6169986020906966
text passage	1.5374275426179649
cognitive processing	1.5194321743502117
recognition algorithm	1.8790614334493734
complexity results	1.534771069588773
rank 1	1.5916835531851148
full word	1.6774002949116091
one rule	2.048074536210314
first list	1.510181823566108
another corpus	1.6002993454155385
annotated data	2.1252179404921425
good model	1.7360338540431246
rules containing	1.6332313690513016
research area	1.7752151020183247
model still	1.530072891325321
use e	1.5338735539822874
requires information	1.5706734993195162
task includes	1.513130125383453
nouns appearing	1.5525978349470975
average error	1.6502368184476786
summarization task	1.686662732021649
task would	1.7425729783880297
behave differently	1.77530945408058
similar contexts	1.9272182377235958
relies solely	1.5511943810330642
back end	1.5992653994743735
test item	1.5081888163363
johns hopkins	1.7084259213367536
report results	1.7912255910069304
bies et	1.5396249822688837
lowest error	1.596743419722522
state models	1.7089283222739302
second part	2.126513141838516
kappa values	1.573413461213816
context c	1.7761494097610684
initial letter	1.5354510398033077
every token	1.7182011479768873
translation memory	1.5955302039721797
interface design	1.6320121285521267
verbs taking	1.6084990473994227
character types	1.6077692960169132
english word	2.1675585093419296
type feature	1.6300610276118175
calculate p	1.5226149809040495
make sense	2.136110555142654
current input	1.8137895666977164
computer time	1.5078976439458076
textual representation	1.5188051370135929
linguistic study	1.5887468924062063
original order	1.591823957496618
comprises two	1.592620200341168
used tbr	1.5716157403242
classes would	1.530072891325321
compound nouns	1.842237352837104
higher weight	1.7251961279191768
knowledge discovery	1.6188737135280857
de ce	1.664296370857411
information extraction	2.385875541493154
meaning components	1.6135168292006372
recent paper	1.7006344357165344
syntactic realizations	1.740622383475347
length model	1.5179205899831658
contain semantic	1.522760613339579
detailed example	1.594773126307433
entire vocabulary	1.5839427016111838
additional experiment	1.506173888464562
expanded query	1.524534874088451
data sets	2.267440053713101
failure occurs	1.5338735539822874
issues concerning	1.7471845324497086
noun pair	1.5892280464518518
best set	1.761425548635389
particular subject	1.669937736193418
easy way	1.891738762510084
error types	1.736495582457465
monte carlo	1.6579695815994424
similar systems	1.6213967902634532
disambiguation rules	1.6265895081506163
algorithm considers	1.731474526704502
practical implementation	1.5771944393968738
research labs	1.5459182428709468
varies depending	1.6802940634668402
1 iff	1.5172519159413136
simple string	1.733462634283148
document containing	1.729676127238116
high recall	2.012102983166763
system determines	1.745199123377234
3 words	1.8576148439355098
production rule	1.790165770211509
word prediction	1.5170814534116788
using decision	1.7272547836096264
parser makes	1.6661204761131603
questions correctly	1.565313292052156
closure properties	1.5187014267751215
surface structure	2.0267750261143624
fig 1	1.6477541750656948
ranked candidate	1.5365331267030269
node contains	1.6864169205234396
correspond closely	1.5739592165010823
among discourse	1.5132264108652627
important content	1.517952851680927
two possibilities	1.9172525188683882
linguistic constraints	1.9865958553354723
simple process	1.5771944393968738
structure also	1.6002993454155385
different definitions	1.6822511556851647
ordering information	1.6123019943253594
distinctions made	1.7203266062826659
tree grammars	1.5423419575395276
method called	1.7474933348665354
two expressions	1.7877229048435859
lexical component	1.6344743064287952
produce high	1.6816988370728096
sets described	1.5547189562170496
information flow	1.699076011048176
square test	1.5944919890835965
time constraints	1.8959810797408299
unclear cases	1.5111306094308554
original data	1.8385113282834213
either correct	1.5514607572627885
models outperform	1.5111765881035293
lower case	2.0241673417529915
c language	1.6227790810587186
input token	1.6611512767006444
object slot	1.6127169244574246
disambiguation based	1.6747135833174478
state grammar	1.7327272138315346
first phrase	1.6926965670534369
example used	1.5445134575869865
following process	1.591823957496618
japanese machine	1.5379084734603226
15 words	1.7285747316777371
information allows	1.636252690439584
task could	1.6377134026552524
behave like	1.7742737125117056
logical relations	1.538329297213694
different nouns	1.5466472792181065
daughter node	1.6394714231876284
common structure	1.6010101084822783
system errors	1.5697186546136246
models provide	1.6756673695466662
selecting among	1.6845577236692315
good features	1.5605435464625712
pairs consisting	1.6550184848056928
specific way	1.757636103678803
r b	1.5956707385069846
linguistic description	1.9921972627014248
surface sentence	1.6393816210180354
parser described	1.8543755849033747
statistical dependency	1.5342151532884416
first order	2.0315217878354095
possible application	1.6741803270499043
de ned	1.9067435466966722
component within	1.5188051370135929
information relevant	1.929136759328457
unique id	1.5188051370135929
aligned sentences	1.8238314389612482
work correctly	1.507007268326853
constraint language	1.6021792560010155
main contributions	1.6447632521349471
one example	2.1920959623404945
potential referents	1.5813577555243494
timing information	1.5381766791708387
context around	1.6256278638449597
syntactic tree	1.9979827592496833
annotated corpora	2.126683611743519
original feature	1.5409683792456326
national corpus	2.0815965813693253
word immediately	1.577462115808981
techniques used	2.0911286824294306
stochastic part	1.5107113719541359
also points	1.5111306094308554
correspond exactly	1.665045212820309
national institute	1.8993798405765432
learning mechanism	1.5988488234402043
lexical relation	1.5705731643189509
possible derivations	1.76283209231902
following conventions	1.5656419863875413
estimate probabilities	1.705774165806181
contains exactly	1.7908276745408773
relations involved	1.5275591303469285
fruitful discussions	1.8121238105123403
important issue	1.8656432090548636
computational model	2.030679004102772
scaling factor	1.5201114931278805
grammar would	1.8569144212719968
strong preference	1.7423526532414262
final example	1.6844174045708415
5 x	1.5862046971870256
absolute performance	1.5547189562170496
semantic dependency	1.514015875914581
tasks including	1.586137214651611
word meaning	1.642883768934592
simple probabilistic	1.5008529684573726
french verbs	1.5217987622876712
certain forms	1.5408560126058508
p u	1.512536810936869
grammatical rule	1.5365331267030267
frequent verbs	1.632798583735609
subsequent work	1.7017362832860266
pages 1	1.5739592165010823
adjacent constituents	1.513130125383453
typical text	1.5338735539822874
probability greater	1.5188051370135929
within sentences	1.7255694781821613
evaluation criterion	1.706476282651602
phrase boundaries	1.8458669908043992
two things	1.9704076557595158
two meanings	1.708946955828761
different phases	1.524534874088451
unsupervised clustering	1.6135687113977537
development purposes	1.5338735539822874
many pairs	1.6481174069026685
certain time	1.6717963628375563
one branch	1.6392309045877975
correct set	1.5432756752761156
generate different	1.6904151818990054
many thanks	1.9165118400639256
different training	1.8842757240132084
paper attempts	1.6010101084822783
internal structures	1.7678563979201016
average probability	1.5657848105751837
annotation style	1.50119175880199
disambiguation method	1.7228663046303365
similar properties	1.6115694852294893
semantic verb	1.5538473409678044
sense definition	1.5728396601789383
underlying text	1.5037367200448986
first experiments	1.7893817762876487
key aspects	1.5893265024752146
separate modules	1.6977890880751751
ones used	1.8107886510873719
translated word	1.511205120616144
language independent	2.0561622652794274
many experiments	1.5459182428709468
section 4	2.683440573417278
information without	1.7229556053533144
alignment accuracy	1.5665773212504739
score given	1.5412463844444777
degree 2	1.5633982884258661
rules operate	1.653691045130907
actual situation	1.5297905781299383
strings like	1.608457009348709
development team	1.5177700906840073
domain may	1.7051629942513449
parameter c	1.5245348740884508
two utterances	1.7292541811438569
terms using	1.710310974172882
small lexicon	1.5724326830802915
two distributions	1.8512654701935796
briefly consider	1.5739592165010823
sentence planner	1.6869854709837124
basic types	1.837170523247984
appropriate meaning	1.5637776281819353
case slot	1.551187866671829
words given	1.7232829055338486
clarification question	1.5267700168847178
audio data	1.5029971219239424
porter stemmer	1.5665017841781155
structure assigned	1.6031224303496519
2 illustrates	2.089219820885587
independent training	1.6300710094014716
computational models	1.9597939176691306
cognitive processes	1.7792116640367985
verb entry	1.5859118906985712
several sub	1.6961394033344752
subjective judgments	1.5378646275607575
word translations	1.796275187507151
considerable interest	1.7377714803473596
degrade performance	1.5887468924062063
reyle 1993	1.5079211428921844
mani et	1.7496381508146799
better translation	1.7808269606315892
assign semantic	1.588669192462068
relation p	1.5622818522186859
ng et	1.5227375180290683
statistical model	2.135516452530825
given point	1.8960066179059116
representations using	1.522760613339579
nodes within	1.5526611030689654
four experiments	1.5286035293545495
previous algorithm	1.515071773152174
interactive problem	1.5179418449925395
standard chart	1.547363551399276
large fraction	1.5111306094308554
optimal word	1.5482870987025659
disambiguate among	1.5156323250636556
two scores	1.6631574457448046
acoustic analysis	1.5211629864223775
good summary	1.640044071538913
lee et	1.782939124279258
sufficient condition	1.707187641270796
yang et	1.683048849580149
individual models	1.5424047667353025
incomplete information	1.6246289167870047
corpora using	1.855112953140215
constituent may	1.640044071538913
foreign broadcast	1.5111306094308554
special feature	1.5839427016111838
based architecture	1.6742252425851278
theorem prover	1.7902910667114582
nlp community	1.8924543073443434
formal system	1.7187543108683756
current language	1.5453687309821331
first feature	1.7633213752432315
entity tags	1.584963793531656
pos taggers	1.8137931689135884
earlier results	1.5611231576018243
viterbi search	1.767271255100248
performance figures	1.7557648698659123
matrix clause	1.7668533246525484
generation strategy	1.5656419863875413
multiple speakers	1.509778922004791
together constitute	1.5582958710952315
rule used	1.7806583530562388
raw frequency	1.5315012344638657
young et	1.5260185053825464
models perform	1.7052511356295248
gave mary	1.6538648742477402
turing machine	1.703178368710506
semantic model	1.8098744744675437
given rule	1.6934141525857016
directed arcs	1.5977445137418091
element may	1.6392309045877975
confidence measures	1.6044824010599326
trees used	1.6213967902634534
full parses	1.5309694935737626
linear chain	1.5296567222481663
precision errors	1.5719513192504324
overall architecture	1.907963868431275
recall score	1.6903509924344629
nist evaluation	1.5195036419368686
system produced	1.7585294036370804
graphic representation	1.5466472792181065
training algorithms	1.6701781587809954
text must	1.7474933348665354
complete semantic	1.7203266062826659
two views	1.6433885100347294
weighted sum	1.9425715150497036
representation produced	1.591444238245721
bipartite graph	1.5883972574949223
single training	1.594773126307433
time adverbials	1.5094704992760475
new dependency	1.5363485810301154
probabilistic approaches	1.5533866622158037
daily life	1.602359748773462
statistical models	2.1141298951217173
words occur	1.9112345213151223
semantic function	1.6914234310205054
parser trained	1.5764036452468027
input information	1.6846942276709567
problem solving	1.9398738330085197
june 2006	1.7897207708399192
four feature	1.530984627359544
weight vectors	1.5502636015172229
important information	2.087942143571778
several arguments	1.50119175880199
sentences using	2.0405158715894993
sorted list	1.7569128363307462
thematic relations	1.567294684024799
input query	1.6304299965665978
isolated words	1.6608211300959936
temporal information	1.8356550160818477
likely sequence	1.6468721846794314
sentences whose	1.8658160569090456
side effect	1.8223459758596463
experimental system	1.8375936876190864
semantic arguments	1.770794616848423
features derived	1.8843626629809145
system provides	1.9749469246719684
stress assignment	1.5150381598236655
parsed text	1.719094657067616
volume 23	1.5106670366917783
new verb	1.576573624892295
users find	1.5269930689946518
representation provides	1.583051127543801
corresponding position	1.5078976439458074
tile linguistic	1.5338735539822874
rule interpreter	1.5347884507438205
several models	1.7114634743108255
text generator	1.7694331795825178
computational grammar	1.5840529461688893
individual features	1.7810079540997485
sense used	1.5547189562170496
likelihood value	1.5024435053733107
four human	1.5736712336159786
main dictionary	1.5026881473115052
another alternative	1.6785607164239145
text segment	1.788205803336532
fundamental assumption	1.594773126307433
linguistic interpretation	1.5707237269685685
word clustering	1.718054560341717
vp nodes	1.5364044762030862
whose frequency	1.7127967389448444
matching words	1.6412680267547148
verb stems	1.5599117799228996
labeled training	1.8319131982463563
semantic role	2.0236528942866565
category c	1.8946834902271554
1 displays	1.63179231938939
various parameter	1.5188051370135929
graphical representation	1.9160423023430637
answering process	1.5041134997716232
lexical look	1.5313099505010341
distance measures	1.6578000048818593
algorithm development	1.5217987622876712
linguistic problem	1.5771944393968738
experimental evaluation	1.8435738950087979
procedure called	1.5706734993195162
reliable estimate	1.5188051370135929
prosodic features	1.8074423645602455
sample set	1.6617517248895468
representation systems	1.6533654826776694
either order	1.5354510398033077
empty elements	1.5586483683416592
two probabilities	1.659662148122272
every term	1.64005253972614
exact match	1.9684430865017781
current focus	1.7917955608127578
information exchange	1.5847316634463144
training time	1.9517593136213847
ways similar	1.5973170078864487
lisp machine	1.6894956608102378
different context	1.8569144212719966
system lexicon	1.5306061495832655
structures built	1.7047689458394855
next character	1.5191210201529293
many research	1.7229556053533144
constraint equations	1.5162617479024965
interpretation rules	1.7511860998333146
la r	1.5414175776987407
one parse	1.9110964397588361
level discourse	1.5881561233003554
bounded number	1.5563893320981306
shieber et	1.8178088013885894
analysis tasks	1.5459182428709468
pairs without	1.5188051370135929
distinct word	1.7020903750698642
selection using	1.594773126307433
translations produced	1.6156176540003735
language phenomena	1.8237780785799478
unification formalism	1.6388112047987287
formal specification	1.7008484397977492
every input	1.7660159833858404
learning environment	1.6328915102616997
much information	1.8768483357420147
basic principle	1.6920919321535788
specific heuristics	1.5716157403242
propose using	1.6849207890717186
dependency treebank	1.6893645036459624
logical operations	1.569522297316359
application may	1.6342922036019591
improvement comes	1.507007268326853
analysis uses	1.5887468924062063
structure used	1.820426925757264
patterns within	1.5771944393968738
parsing phase	1.6993812786179885
paper aims	1.636252690439584
module determines	1.5395099438048585
new model	1.6355237582648297
words across	1.5749574214167732
additional layer	1.561123157601824
one class	1.6676239419466783
form would	1.6977890880751751
final parse	1.681882386819574
expression must	1.586137214651611
strong correlation	1.736273168700745
reuters corpus	1.6559395212412262
tagging experiments	1.535727155868841
complete set	2.0691336192471996
several features	1.8892543018367538
text comprehension	1.5963645436754335
results demonstrate	1.906688539181782
null mation	1.5445134575869865
word preceding	1.5320698902069678
f e	1.7142780926943435
analysis may	1.8055148355029196
hirschman et	1.7027513867840578
great help	1.6411558246537032
cl reference	1.822614860287098
semantic features	2.229138214814962
unbounded dependencies	1.7565871583016242
verbal head	1.6713558174354377
feature function	1.8138501900704485
word choice	1.7334444219574383
student may	1.5626637299088917
flat structure	1.671753303193203
measure score	1.5620473238340082
sentence whose	1.6974749152736237
new function	1.5806234267011037
two rules	2.1026611660286685
appropriate contexts	1.5188051370135929
condition 3	1.5297208513723244
single np	1.6320354670147565
relation used	1.5371982986173842
identification system	1.5449549971131245
sentence structure	2.092209623573332
correct tags	1.6190975549190845
common verbs	1.6221843551054551
major advantage	1.7663553975095576
given example	1.57262558874565
place relation	1.5607083421258507
word delimiters	1.5329785555225088
lemma 3	1.583087993348005
single reference	1.5842793761550902
system implementation	1.5887468924062063
particular verb	1.839758795430603
languages differ	1.6023597487734618
every question	1.6265370361225346
sense discrimination	1.5981953612228064
4 words	1.796884243490919
returned documents	1.533540614646662
features whose	1.7680599214474206
disambiguation accuracy	1.651465290095232
prior experience	1.591823957496618
order differences	1.5678009097741024
string fill	1.5010213412661277
discourse knowledge	1.5169627539909816
english pos	1.5506654726508569
ongoing research	1.895273233543868
largest number	1.867552695688794
earlier draft	1.815669969260329
four steps	1.8536230068384094
based query	1.544378505133437
encouraging result	1.5547189562170496
words sharing	1.5620473238340082
highest performance	1.5354510398033077
simple questions	1.524519541445589
efficient means	1.5482870987025659
constituent types	1.5947063573320868
processing times	1.592111614460519
certain restrictions	1.636252690439584
applications may	1.5906852501842725
english name	1.5251285650209419
general knowledge	1.9851242328386756
cannot refer	1.5925561662226717
texts without	1.6844174045708415
phrase contains	1.6413904216258086
small window	1.6655102285632384
syntactic similarity	1.5773133211810768
appropriate choice	1.705774165806181
spanish word	1.5816140258181144
output tree	1.5261301164726653
end result	1.8081612299749896
features present	1.579873144038688
second assumption	1.6103523297384124
channel approach	1.5611265469383597
translation errors	1.681251932651806
conceptual content	1.6806432720347573
single context	1.5378646275607575
equal weights	1.5041794290271748
corresponding set	1.7305508816393718
following questions	1.8763287557227915
particular sub	1.5078976439458076
selecting appropriate	1.6717533031932033
dependency relation	1.943073519945208
significant differences	2.0200585178940624
good feature	1.5706734993195162
bilingual parallel	1.5177048280388055
model theory	1.561942089751915
chinese sentences	1.834746020084196
error metric	1.5052613788761844
single action	1.5230483704868893
5 show	1.8191665297540134
top element	1.625521830238159
extracting information	1.9035018112934432
building block	1.671029888778162
word string	1.847872776952402
query q	1.7317779320969553
possible explanations	1.6413904216258088
extremely time	1.5338735539822874
already knows	1.7882713782531896
weight vector	1.7008994108404494
two fields	1.6637019978300875
times slower	1.5738708991826256
english equivalents	1.6104483116739061
scoring function	1.8740129293176264
specific knowledge	2.123177644214716
also attempt	1.5582958710952315
br example	1.5445134575869865
score obtained	1.5887468924062063
single root	1.5926616085953929
wider range	1.69186448893072
frequency distribution	1.8869026534020392
result shows	1.9540572804939997
several non	1.543442208482524
presents one	1.5111306094308554
whereas others	1.6976636234158158
ranked element	1.5495809227395583
using word	2.0414441090379203
compositional way	1.5690244007917742
obtain information	1.7309723544238431
operate within	1.6117266025648125
total function	1.5069839707301198
x j	1.5602367449199286
particular model	1.6570039092175395
input structures	1.585496716830146
notions like	1.5306061495832655
intuitive interpretation	1.5582958710952313
words obtained	1.5195036419368688
multiple instances	1.6438766862662457
research topics	1.5902029213022895
section explains	1.6562676183926621
first version	1.9700504952250297
text genres	1.7025455033165855
query system	1.778837113835672
allows efficient	1.5645241345053709
identify word	1.643362624369706
different document	1.6785607164239145
large lexicon	1.819304566274304
including punctuation	1.5729286010575074
two methods	2.130891914078431
practical point	1.7678099140508592
four components	1.8076574414915252
many advantages	1.7722692119182182
cannot decide	1.6570039092175395
across speakers	1.5612815851895034
rich language	1.5226149809040495
user interfaces	1.8355716800767965
ne task	1.571810674127621
highest scoring	1.892694943350386
input question	1.5140548915918028
better estimate	1.5037367200448983
xml markup	1.530848631876323
new features	1.9831574494473476
word occurrences	1.8290556183032056
noun compounds	1.777809269058638
information concerning	1.7855992804578515
connected nodes	1.5354510398033074
important question	1.8957944452536417
particular function	1.5408560126058508
canonical form	1.9358281001684985
background material	1.5975120076421967
dramatic increase	1.6002993454155385
small size	1.7212415138177213
cannot detect	1.5156323250636556
entities mentioned	1.8076993936199448
entities introduced	1.628974524367037
tree transformation	1.5531112361910642
first system	1.7020903750698642
question may	1.7536775400520195
structures shown	1.5929928949821113
nouns denoting	1.5424211838345776
modeling framework	1.5880288594247598
null computational	1.7435972121129721
level language	1.7745354067978096
resulting set	1.8681313484516056
sur les	1.6143698340108508
times faster	1.8865300127663798
trees derived	1.600091704657666
unknown word	2.0645415514334875
seem promising	1.5188051370135929
high frequencies	1.5241968977937237
noun like	1.5767760138793054
readable dictionary	1.9046164745144474
best analysis	1.6587197099831164
recognition errors	1.992961850805295
mean something	1.543442208482524
example sentence	2.104223348540817
document frequency	2.018821293057404
information stored	1.9257647476555149
incorrect segmentation	1.5087853581980926
sets using	1.6679521469523344
focus structure	1.5058628011615491
standard methods	1.659662148122272
solved problem	1.5156323250636556
another point	1.8143840697291487
bleu score	1.8711125244971076
spreading activation	1.7634089480520492
work includes	1.9654493656423506
best ones	1.6169635240879603
statistical estimation	1.6159284666509544
particular role	1.5156323250636556
centering theory	1.7967304234254498
first evaluation	1.7780946637328394
computational linguists	2.0760891018973022
term may	1.6573127369315377
ranking method	1.504640671081387
algorithm iterates	1.5338735539822874
research projects	2.188940183229228
communicative intention	1.5314169148210515
different circumstances	1.5269930689946518
next rule	1.6892902982750948
words consisting	1.549729132180171
test runs	1.727569967534496
one file	1.5781539692435587
incremental learning	1.5567920625746656
par exemple	1.6173869342349851
frame information	1.5322636280062105
processing model	1.7475559374940008
normalizing factor	1.5882137880604763
integrated model	1.5826595449826808
3 training	1.5719513192504326
speakers tend	1.5300728913253212
text collection	1.8424151432342535
subcategorisation information	1.5025681847093177
parser using	1.8765681116647295
single semantic	1.7337029866062104
work focused	1.522760613339579
step 2	2.233933063000433
classify verbs	1.5673852315252703
segmentation ambiguities	1.5738819901231071
grained level	1.5695222973163587
learning task	1.883586734515989
core arguments	1.533608991020158
initial hypothesis	1.5526668739562086
different set	2.041341292658551
le cas	1.5566101883629924
inside probability	1.5351923291327556
words rather	1.862310359635364
cannot possibly	1.619472687377359
anaphoric relations	1.667140797027808
systems available	1.5511943810330642
single document	1.8761931005356216
third possibility	1.5269930689946518
robust language	1.5408560126058508
parse forest	1.7429675040483894
whose role	1.6031224303496519
control structure	1.680316075297653
linguistic frameworks	1.530072891325321
balanced corpus	1.8008959869070444
evaluation would	1.6169986020906966
annotation scheme	1.987547153661136
function tags	1.5161730548228263
different goals	1.5571495092265906
similar rule	1.506173888464562
node r	1.5163178607272851
categories based	1.6266256462651403
definition 6	1.681252707662353
also introduces	1.594773126307433
concept c	1.6719481740871922
reference translation	1.7623757159727056
parsing tasks	1.5072125870144037
require special	1.7016656224425795
independent recognition	1.726645580943106
low similarity	1.5408560126058508
particular features	1.762033687546193
unmarked case	1.5461586260253584
human operator	1.5746557973382116
given parse	1.5818515289928636
alternative approach	2.038136440021207
parser outputs	1.6213967902634536
several paragraphs	1.632937407228736
improved precision	1.5926202003411678
extensive discussion	1.5887468924062063
definite nps	1.7448091287750012
common denominator	1.6481174069026685
adequate translation	1.5395099438048585
contains 1	1.6961394033344752
simple mapping	1.5328333954091753
language database	1.6532252061900867
structures generated	1.699911021301493
paper demonstrates	1.7417743368004273
rule whose	1.7337029866062104
structure derived	1.55955788508706
rule applications	1.789489275116363
specific conditions	1.5511943810330642
evaluation methods	1.935858023593969
known words	1.8966764721063851
coding scheme	1.7664623458683943
based algorithms	1.8088136105979151
decision function	1.5217207042697556
framework using	1.522760613339579
trees whose	1.7458914003754722
enable users	1.65303397348486
surface realizations	1.7085045157455634
one tag	1.8459159963932052
given class	1.8371123248766341
selected examples	1.5991253496404458
logical framework	1.513130125383453
detection algorithm	1.6394842367002591
trigram probability	1.5506112803072318
conceptual information	1.7623516955672114
classifiers used	1.594773126307433
special characteristics	1.5217605875570064
polynomial time	1.920847373490834
many machine	1.703359686465308
grant 01	1.5445134575869865
standard deviations	1.8202189557794055
9 proc	1.6593965399315964
translation systems	2.2096251041410793
provide detailed	1.6073715579162529
research since	1.5111306094308554
human communication	1.8609299355620523
particular entity	1.7177247972056076
implementation described	1.6031224303496519
method performed	1.5612815851895034
syllable structure	1.6596285401684026
phrase pairs	1.671610785167407
phonetic alphabet	1.636969735352857
work required	1.6010101084822783
word window	1.8205491792820006
precise definition	1.8624808726608613
correct analysis	1.9274121464024014
syntactic frame	1.635608154457774
content extraction	1.6892160123120346
practical importance	1.6190328102134979
two variations	1.579873144038688
translation lexicon	1.6796561047336378
following modules	1.6213967902634532
answering questions	1.8551148010051306
distance dependency	1.6997148616892797
training criterion	1.516998064959031
correct ones	1.765906995481154
different readings	1.7533693582446195
possible number	1.579873144038688
group contains	1.5519690017139713
text used	1.6388987829322057
procedures used	1.6570039092175395
communicative function	1.5897049668336813
direct translation	1.7196335975326669
levin et	1.5163736545608288
several sources	1.8535808586068256
model involves	1.5645241345053709
first argument	2.0309437610886043
full sentence	1.883164151803492
formal proof	1.592620200341168
type discussed	1.5111306094308554
le plus	1.575737929585438
extensive experiments	1.6661204761131605
data becomes	1.6289349127929256
worth investigating	1.6031224303496519
pattern used	1.5511943810330642
judges agreed	1.5738708991826256
hoc manner	1.507007268326853
something else	1.5920195558655967
open problem	1.7756465439134472
corpus based	1.883252482115445
preliminary work	1.8248139332291624
new phrase	1.6393847571554783
name may	1.651620883115958
contextual cues	1.6147884150363512
text parser	1.5400345475795927
column shows	1.9734435090606892
local contexts	1.6789697679874656
human users	1.8195221170229412
formal description	1.884035477266622
sag 1987	1.6559636051434925
syntactic information	2.3752897421932095
interface provides	1.5156323250636554
different topic	1.5764616871385764
empirical support	1.597317007886449
semantic feature	1.892835246521821
frequent senses	1.6061461120632587
computational research	1.533540614646662
et ah	1.5425370803688767
semantic understanding	1.5707376783218978
either system	1.6285930450163055
hierarchical representation	1.6009525095393649
approach still	1.5111306094308554
english sentence	2.1482713825563504
best strategy	1.5838843180483249
emission probabilities	1.5199969715060413
pustejovsky et	1.7310221364489933
consider sentences	1.583051127543801
conditional model	1.5737390942052962
performs much	1.7542610148908173
type constraints	1.65827228862315
entire data	1.671211480799467
parser works	1.6682934633704967
specific aspects	1.756257461041527
translation procedure	1.5784904253929217
useful comments	2.0030791066189586
textual documents	1.5731122769149346
using k	1.601264852276993
substantial effort	1.522760613339579
use words	1.7932820854853881
another word	2.008392596666408
rules contain	1.611203449372866
interpretation requires	1.579873144038688
uniform treatment	1.6959017760440367
termination condition	1.5781244489475452
many phrases	1.524534874088451
briefly discuss	1.8981035021437518
sentences include	1.5445134575869865
one value	1.8155538344978128
classi cation	1.7661380896787287
structure whose	1.667913900271795
compact representation	1.8470260664207458
input vector	1.650839823162756
free form	1.5707237269685685
previous paragraph	1.79791571192699
good coverage	1.860790286818639
ranked lists	1.6123699962407936
candidate words	1.7718543507993854
specific dictionaries	1.5514607572627885
linguistic annotation	1.7473847654980723
parsing stage	1.6256278638449597
heuristics used	1.6681052948574107
13 shows	1.6112034493728657
constraints within	1.533540614646662
efficient implementation	1.8564968549891148
frame structures	1.5926083359526775
functional grammars	1.6529072584635718
common source	1.594773126307433
tag trees	1.516058707239509
sentence hypotheses	1.6278016210663109
word endings	1.6169635240879603
particular sequence	1.6266256462651403
tree fragment	1.6270455537438928
automatic annotation	1.7376731721993184
baseline systems	1.694662378138207
terminal categories	1.603528159002232
internal nodes	1.8950481376939938
biomedical literature	1.6701477204753754
linguistic phenomena	2.230203473938329
j e	1.682783519391248
including information	1.8380078102489092
e et	1.5523865943980586
ordered list	2.0508688171347105
relation types	1.7219998017716982
distance function	1.6774182969642468
choice depends	1.507007268326853
per hour	1.6629293541107222
chosen randomly	1.7193652869936307
facto standard	1.5926202003411678
david yarowsky	1.5371982986173842
np object	1.6826098927578554
common language	1.6859250382369861
additional material	1.530072891325321
algorithm takes	1.9203414966555599
constraint would	1.5445134575869865
different theories	1.62412689036662
language spoken	1.507007268326853
list l	1.55778907382561
possible pair	1.6500007742396736
proper names	2.233741277451591
large size	1.804522961886196
right way	1.6056035510036821
zero probabilities	1.664540718454114
fixed length	1.7818811442229627
allows users	1.8992795899848927
several research	1.6939639871030243
single topic	1.6580606251764172
different weight	1.5767760138793054
similar topics	1.5216148258836175
computational tools	1.6747135833174478
specific data	1.8106233196871493
graphical representations	1.6066593791852315
significant drop	1.5645241345053709
research project	2.0290187407186644
proper probability	1.524534874088451
automatic alignment	1.6265573004837734
general feature	1.5622818522186857
many techniques	1.572834038198836
roles like	1.511205120616144
two drawbacks	1.5111306094308554
tile rules	1.6285930450163058
lexical similarity	1.670011522600087
atomic value	1.6811422670306908
1 p	1.7194199014145106
subsequent processing	1.8921653486699301
particular order	1.74724601164107
much linguistic	1.6289349127929256
new feature	1.9576567262364248
dependency tree	2.034951304214692
corresponding text	1.6037378902512214
following recursive	1.6246289167870047
syntactic environment	1.60290514547249
combining multiple	1.7609869990584184
smoothing parameters	1.5578214127463763
basic assumptions	1.6761656524213184
beam search	2.022378079153117
thank david	1.5111306094308554
longer words	1.6913188774720105
relative ease	1.6481174069026685
one moves	1.50119175880199
extensive research	1.5973170078864487
several steps	1.8474671221898884
ntt communication	1.522760613339579
european languages	2.045783011958556
past tense	2.1228266509094844
relevant information	2.041000486814397
tree used	1.507007268326853
verify whether	1.6514103959727489
conceptual structures	1.8857581370441447
intended application	1.6663716312166414
allows multiple	1.6570039092175395
departure time	1.6103507681913636
several variants	1.6819762996803398
method based	2.116700139194999
german noun	1.538236714861226
whose primary	1.6570039092175395
one predicate	1.6577870338263803
existing model	1.5002075792603644
test text	1.7027313952900003
shallow semantic	1.7501204892332
automatic indexing	1.7210951595448016
computational advantages	1.669444390143633
mainichi newspaper	1.6094960057644818
web browser	1.7251502255379478
stage 1	1.5532407085816247
complete analysis	1.8677702128798037
considerable effort	1.7501491085716443
consistent improvement	1.530072891325321
scores indicate	1.5798548254880234
understanding conference	2.042099790835505
reasonable degree	1.5111306094308554
less information	1.8380078102489092
component parts	1.7564482002335247
using pos	1.6589667385758484
documents may	1.6751782252712877
ordered sequence	1.788864182464497
verbal forms	1.6778333038110285
predicates may	1.5511943810330642
statistical tools	1.5111306094308554
first answer	1.5395099438048585
raw data	1.8887477564591781
underlined words	1.548378091852222
validation procedure	1.5425370803688767
first letter	1.8041830478334395
language researchers	1.548378091852222
translated sentence	1.5363377053406655
macleod et	1.5036575034310684
control verbs	1.5925561662226717
system automatically	1.7852702117890247
dependency parsing	1.8076029978041928
discourse semantics	1.5445402430642434
selection algorithms	1.533540614646662
three problems	1.8005498094342076
system must	2.239912995318617
syntactic level	2.000335724697159
example shows	2.110994085352699
new software	1.5361450492096225
subject noun	1.7634019862638344
development foundation	1.5188051370135929
lexical content	1.7075808180581706
jurafsky et	1.5420261883340527
vander linden	1.5093145186723191
que le	1.6988322117126147
500 words	1.822117728337554
distance features	1.507986450408946
improve retrieval	1.6488447391751828
also presents	1.7105799695961408
null word	1.6911009502066159
possible values	2.131526166787084
two nouns	1.877864934494215
parser generates	1.630445049391759
semantic closeness	1.5288687562201928
generation conference	1.522760613339579
resulting text	1.7164164810084093
individual system	1.544919098524184
next phase	1.8131865847259343
row shows	1.757441231647708
basic parsing	1.5936481309260868
syntactic structures	2.3451945802336525
convenient way	1.7788903461436698
constituents must	1.5378646275607575
provides evidence	1.7878606605370726
word frequencies	1.8890603663049703
existing models	1.5326885418449672
object must	1.7572294967511712
correct reading	1.5897894246890258
several components	1.7377714803473596
theoretical linguistics	1.8829148702920262
also contain	2.076874780099942
experiments indicate	1.8258474532000917
document contains	1.6708590810843595
rule matches	1.6853236803857272
detailed comments	1.5338735539822874
speech events	1.556547353018995
word occurs	1.9816195373166603
selectional preferences	1.6937547749796225
sophisticated language	1.5869693547698929
semantic components	1.8439316075586138
score associated	1.5275591303469285
use domain	1.5894574345512698
preliminary investigations	1.507007268326853
linguistic meaning	1.5360322697562396
expert knowledge	1.6710468355004502
phonological rules	1.79848005504723
bilingual training	1.5413199213576938
main areas	1.5973170078864487
accurate model	1.6338810723661865
paper addresses	1.9894017601910812
three cases	2.0214226114652307
first application	1.7489291135018945
u r	1.5577064531206402
koehn et	1.7604900251983355
approach suggested	1.5037367200448983
words associated	1.7661269875382664
many application	1.5338735539822874
new corpora	1.5275591303469287
current methods	1.5839427016111838
also discuss	1.6513322605155005
translation equivalent	1.5835213395509224
first constituent	1.6769680651604606
computational morphology	1.570817932703902
chunking task	1.6126472723058276
good indicator	1.8107065668599707
search terms	1.6451953981856817
following tables	1.5547189562170496
probability based	1.5378646275607575
null ments	1.62412689036662
verb group	1.6958393733241028
higher f	1.6221843551054547
l c	1.6519763444349675
close examination	1.5445134575869865
de l	1.8338676010541868
statistical system	1.6288981647434881
experiments used	1.721573893842553
tight integration	1.5838923815193273
6 describes	1.672219863528485
wordnet synset	1.7279647904796858
several advantages	1.992635376785865
english verbs	1.8877869030406014
expressions within	1.582117970332684
two phases	2.0848962535326
definition 8	1.5547189562170496
analysis module	1.7840278314280769
fourth row	1.5511943810330642
method employed	1.6647993279874824
distribution within	1.5414180417122596
possible explanation	1.8870844806609082
synonymous words	1.6227144805807012
nl processing	1.594259983008413
word must	1.8535808586068256
subsequent modules	1.522760613339579
different reasons	1.6229151093758099
research areas	1.6316742720073956
method improves	1.6445809590877876
window around	1.6665893162717775
temporal expressions	1.8124887690103892
sentence test	1.570503449504594
sentential complement	1.7241215686989388
language speech	1.5006743580950452
score measures	1.506173888464562
con guration	1.5159613971018289
people often	1.6615677176372698
immediately precede	1.524534874088451
e k	1.5985543375863653
information conveyed	1.9002650688398832
indirect speech	1.7381297396320363
statistical association	1.509778922004791
method could	1.8341727285633498
initial node	1.5171904119543882
larger size	1.5188051370135929
manual annotation	2.01544832575026
expression may	1.7072113359402379
candidate translation	1.6241204433821832
authors use	1.5535653072454743
trigger words	1.6572188885589738
english query	1.5712989768621033
arbitrary features	1.6128408409726123
question word	1.5950522442954933
messag e	1.5061738884645621
state techniques	1.695765933332211
various criteria	1.5354510398033077
theorem 2	1.79350597369351
semantic system	1.581527593088579
two issues	1.5853350832812225
slight decrease	1.583051127543801
e used	1.7680599214474204
accurate models	1.5690244007917742
recall r	1.5706734993195162
function c	1.6050288834324327
two mechanisms	1.6482958236884362
steps towards	1.6496419421411992
case marker	1.7333404594695994
verb semantic	1.5038340301825783
simple surface	1.526993068994652
avoid overfitting	1.6976636234158158
four parts	1.7309723544238431
form suitable	1.591823957496618
elements like	1.5971626956604803
something similar	1.6570039092175395
processin g	1.5226149809040495
antecedent must	1.5257027020067777
adjacent segments	1.5150717731521737
analysis suggests	1.6990666498994726
appropriate rule	1.5378646275607575
techniques described	1.8886410074868087
noise level	1.5441376180289481
two graphs	1.648600219321605
axis shows	1.5730893618601307
beeferman et	1.5794690951806278
transformation rules	1.8958247093026093
jerry hobbs	1.6661204761131605
produce text	1.6136932231894319
theoretic approach	1.5552581331940458
uses statistical	1.65303397348486
basic category	1.5683356946968088
existing grammar	1.5988552578459148
much room	1.6315901311540408
complex example	1.6213967902634534
given value	1.5767760138793054
empirical tests	1.522760613339579
open questions	1.72914919045146
semantic coverage	1.54684623265169
complete syntactic	1.7950374333941341
first select	1.522760613339579
using examples	1.7141428077458374
regular set	1.601577470782716
short sentences	1.9519711378900788
simple procedure	1.6741803270499043
riezler et	1.671039011654611
using constraint	1.5611231576018243
also facilitates	1.5887468924062063
current form	1.6073715579162529
agent role	1.639144915773583
performance measure	1.630524001333311
dialogue participants	1.7458070335661515
yields results	1.5111306094308554
new tools	1.6169986020906966
signal processing	1.5412759104137737
also contains	1.8944105620095166
elle est	1.5158072072367401
similar situation	1.6881707359732032
boolean operators	1.6304481342440433
richardson et	1.5473635513992758
new instance	1.7376746974523911
sql database	1.504934031784749
classification tasks	1.946320914210428
algorithm first	1.7915623825727776
remaining nodes	1.5645241345053709
2 x	1.807752167728899
process within	1.5111306094308554
following procedure	1.9250110367667246
like units	1.52382007077128
finite automaton	1.6485762416802787
small collection	1.5771944393968738
traditional method	1.5378646275607575
present study	1.9620997633869046
analysis tool	1.6371456055941147
possible orderings	1.5381766791708387
tool used	1.5611231576018243
context model	1.5705849146099382
electronic documents	1.5338735539822874
ones described	1.572834038198836
words chosen	1.5324203178689448
analysis described	1.591823957496618
tile noun	1.5037367200448983
also match	1.5771944393968738
template slot	1.502752670601207
extensive evaluation	1.5869693547698929
pronoun may	1.5771944393968738
transformation based	1.6341373388195768
parsing component	1.5974644755426064
tree learner	1.5385056319339772
use contextual	1.5269930689946518
specific instance	1.5690244007917742
patterns match	1.5066965250642075
logic programs	1.5437068144629347
systems usually	1.699626140637926
user selects	1.7595953217826543
systematic approach	1.5338735539822874
newspaper text	1.9610281309710598
speech tags	2.2321435913862993
process consists	1.8804981310802222
vector space	2.0320146811416837
palmer et	1.7793129486261603
shortest path	1.8102411069015962
input utterances	1.5877002039184338
level information	1.9227823643949373
grammar based	1.8720362039624152
need information	1.507007268326853
text types	1.8018809955363717
different analysis	1.530072891325321
language family	1.5296567222481663
descriptive adequacy	1.5556301839047324
k n	1.5637726119349868
candidates per	1.5037367200448983
tagging error	1.5742732827165122
building semantic	1.5288687562201928
whose semantics	1.732852583117074
data annotation	1.5378646275607575
ordering rules	1.5611254282447349
linguistic criteria	1.6162383534717666
senses using	1.548378091852222
text generation	2.084267279036732
c represents	1.5525978349470975
processing technology	1.756912836330746
semantic equivalence	1.6186685762581354
verbal complements	1.5578214127463763
different ones	1.591823957496618
tutoring system	1.773736956575586
like words	1.506173888464562
unannotated corpus	1.6023228650468333
new version	1.9027267996551436
language dependent	1.872543058776883
syntactic patterns	2.059305076005571
symbol p	1.5087853581980926
actual results	1.522760613339579
intended interpretation	1.7762399782386764
search process	1.9459056104485128
processing software	1.6367907690549308
word following	1.5807778092350149
following properties	1.9584680278312976
present work	2.059078574312357
original problem	1.5112051206161443
large dictionary	1.6380064581821157
statistical machine	2.1438648385567913
best paths	1.573487566090456
coreference resolution	1.873753824445217
procedure based	1.7057741658061811
main sentence	1.5805486631742416
online text	1.5482870987025659
simple linear	1.7514011088597066
following condition	1.7854718751434606
formal notation	1.5645241345053709
lexical information	2.31292609785148
define two	1.8586723716687734
one difficulty	1.613624660358136
context models	1.5605902213848624
possible binary	1.529606820290264
automatic clustering	1.587462730782593
gigaword corpus	1.6146364829503415
correct attachment	1.6593175079324927
sequence x	1.637575348261155
string match	1.6445042859713128
becomes less	1.7563379226837867
background knowledge	1.8974456711272334
et les	1.7176255958201356
semantic specification	1.5569772862779974
another project	1.5037367200448983
system decides	1.5929928949821113
ambiguity problems	1.6666455667887732
several machine	1.6431586438232257
null since	1.7935968174739088
course also	1.583051127543801
transition function	1.65592616103261
3 seconds	1.6320354670147565
nodes connected	1.5211629864223775
task domain	1.8581407263963383
given predicate	1.7320416093687379
good predictors	1.6297468697667572
semantic characteristics	1.6936097457534034
previous researchers	1.5514607572627885
automatic induction	1.5771944393968738
features instead	1.522760613339579
terms extracted	1.5997673202306126
line 3	1.650092453800705
processing research	1.7179482056291908
current task	1.713234316857626
dialogue state	1.5756797302392909
spanish sentence	1.5679332442954022
accuracy figures	1.6401003963552472
lexicon model	1.6180568007169798
wide variation	1.5742591873994098
usual case	1.5338735539822874
simpler approach	1.5445134575869865
lisp functions	1.5843044140731486
optimal path	1.5257651597754915
atomic elements	1.5156083168889984
exploring ways	1.530072891325321
english input	1.726737184011391
new evaluation	1.7003453621898497
type x	1.5911379815870004
corresponds roughly	1.6190328102134979
also specifies	1.6315901311540408
particular instance	1.8314503997454978
potential application	1.615262062995654
retrieval conference	1.8542589666005485
6 shows	2.273330534485166
atomic symbols	1.681272431244233
office environment	1.506173888464562
null tence	1.6164339756999313
word occurring	1.6920919321535788
achieve similar	1.6190328102134979
conversational participant	1.50812972873744
anaphoric noun	1.525869450228649
induction process	1.5021213662828548
target verb	1.6495515446110587
semantic argument	1.728834886287908
specific kind	1.6339957705783552
indirect objects	1.803027078222633
15 seconds	1.5371982986173842
information provided	1.888325956096058
parse may	1.5354510398033077
000 utterances	1.6479791814036409
analysis technique	1.5354510398033074
three judges	1.5704284771841746
three tasks	1.8138899329003506
cannot tell	1.6661204761131605
tag assigned	1.6125713976431844
original algorithm	1.7588087618857098
first child	1.5425370803688765
incorporating information	1.5188051370135929
single analysis	1.5722256263320595
possible candidates	1.551017522326816
first interpretation	1.6037378902512214
based trigram	1.6010101084822785
scoring system	1.7569863505182715
interesting issues	1.6056035510036821
complex problems	1.5771944393968738
tokens per	1.5316462561051432
list contains	1.8801142049266344
using sentence	1.5969765857928704
native language	1.8208016379532628
users must	1.5354510398033074
set also	1.5188051370135929
algorithm proceeds	1.8025991388226652
word tagging	1.5172609500992498
using hand	1.7074092050419734
tim system	1.5445134575869865
pentium 4	1.561123157601824
expression generation	1.6399123011720258
empirical basis	1.5869693547698929
psycholinguistic evidence	1.5906723352433731
one module	1.614112765303346
computational techniques	1.5363416292706549
separate word	1.6117266025648125
1 uses	1.5408560126058508
answering systems	1.9770526764110647
similar situations	1.5771944393968738
sentence length	2.1587867946520953
framework presented	1.571949426413436
see fig	2.0522166025900335
used data	1.5707376783218978
software modules	1.5615528847694229
best f	1.7560373930395001
first factor	1.5758494263954068
features found	1.613624660358136
every edge	1.5794376860042765
method requires	1.8575191948600072
p e	1.6794231568650018
state changes	1.570753432646896
random baseline	1.6984842465941261
translation rule	1.5809445259547816
abstract concept	1.5792035811631284
briefly discusses	1.5645241345053709
enables us	2.052095444533319
arbitrary order	1.5749574214167732
experimental comparison	1.5188051370135929
called context	1.5111306094308554
consuming process	1.672219863528485
relative pronouns	1.9107300030203298
longest word	1.5188051370135929
output structures	1.5479449597000374
manual correction	1.6084470588891784
linguistic representations	1.7313066446440133
experimental result	1.7560373930395001
relational database	1.9625755356601517
content analysis	1.7829222611941886
system responses	1.742380344812723
different distribution	1.6661204761131607
particular system	1.735568647358673
grammar framework	1.7561260812931856
syntactic bracketing	1.5226149809040492
feature specification	1.7413392745186258
unseen sentences	1.669489306700537
canadian hansards	1.622994161363779
sentences extracted	1.814109081043752
complex cases	1.7266279442877563
note also	2.111925905553332
within tile	1.522760613339579
significant way	1.5771944393968738
second point	1.7720369317764424
class information	1.8119586719152743
tree whose	1.8923913070003675
two varieties	1.5188051370135929
errors caused	1.7644009529620572
miller et	2.1042088848673837
linguistic string	1.7290075759058003
approach takes	1.653691045130907
salient features	1.6371456055941147
multiple models	1.6286406976823904
general characteristics	1.5156323250636556
segmentation using	1.5908167675916909
tree structure	2.2467120902483644
one label	1.6573127369315372
3 rd	1.5322342087189593
start time	1.5563893320981306
set consisted	1.797275444171776
resulting clusters	1.6747135833174478
local optima	1.5177834651201019
syntactic complexity	1.6839426678748943
modi ed	1.7353359056021564
compact representations	1.5338735539822874
sentences correctly	1.6169986020906966
varying number	1.567922330580758
processing components	1.8324559904080726
restricted form	1.7373468853854628
mapping algorithm	1.5226072530184251
preceding noun	1.6846388563743653
possessive pronoun	1.7165589465140534
variable x	1.9111216688472157
yorktown heights	1.64221184897102
two candidate	1.7006785218730862
database entry	1.521162986422377
special type	1.7610857808394331
english test	1.6970370155340264
higher frequency	1.8521375200319485
augmented transition	1.9020308984279393
information within	1.9594298291507888
object referred	1.5676517392350746
present perfect	1.6432007613388644
systematic study	1.5553381419921846
basic architecture	1.5971626956604803
actual language	1.5036575034310684
ambiguous structures	1.5158072072367401
information sources	2.017416912612486
given context	2.154163420987839
tests using	1.6708590810843598
joint distribution	1.740336768830727
statistical techniques	2.0930779208913854
translation output	1.7209546247976708
second rule	1.9071864111407488
different strategy	1.5739592165010823
functional word	1.5297208513723244
key problem	1.6614869878819327
sur la	1.72550674191652
reference summaries	1.5011525773696106
translation dictionary	1.615072961507462
one wishes	1.648733409908315
head constituents	1.5078797213408794
level structures	1.68759419299977
one described	2.0279593766869937
advanced learner	1.650718044314063
first algorithm	1.7153114942413925
salient entity	1.504439013905301
general approach	1.8218473143927194
future versions	1.7235765222249384
propositional content	1.9387064851315217
verbal expressions	1.558193976095525
similarity relation	1.5421588730604814
lexical level	1.9756858707167888
xerox parc	1.600775206985256
several techniques	1.8357762297688498
high computational	1.6548911840144864
next sections	1.8793860501813107
prague dependency	1.651319872066586
section gives	1.7984810148368515
practical value	1.6031224303496519
remaining words	1.8986806472630613
selection criterion	1.6695311750356157
broader context	1.6320354670147565
linguistic behavior	1.7281094969489679
clause must	1.5777946478302516
relations based	1.579873144038688
compositional semantics	1.813373436983518
aligned corpora	1.6711619300282163
syntactic analyses	1.9441368979004634
recall values	1.788294574281082
ct al	1.7451118789646747
full tree	1.50119175880199
independent manner	1.507007268326853
person pronouns	1.6891421968806704
communication channel	1.512536810936869
june 1990	1.5548464516305516
whose number	1.547986846326364
system used	1.8645466422337802
insightful comments	1.7425729783880297
giving rise	1.8630456488675402
three sections	1.7917261248030714
lexical structures	1.5217886399859557
word occurred	1.583051127543801
next stage	1.9324094014451616
processing system	2.187921274421278
weighting factor	1.6153321225882895
similar examples	1.7436695170447192
texts containing	1.5767760138793054
integrated system	1.8049288066592974
interface must	1.6005750984437832
representation used	1.9186120839985936
domain using	1.6724173909604771
five sentences	1.617837470290549
formal representation	1.8919562669890573
models must	1.6411558246537032
50 words	1.7821541152448945
theoretical work	1.7078674484051781
programming algorithms	1.522438913516297
also accounts	1.5408560126058508
surface information	1.5666685893243946
language must	1.768237377420752
sentences included	1.507007268326853
rules involving	1.6566290674162105
occurrence frequencies	1.846275506407606
large bodies	1.6481174069026685
phase 3	1.5267215368556717
main elements	1.5111306094308554
a0 a1	1.5793826807019762
module also	1.5514607572627885
ie task	1.599134549990069
development set	2.1479365386394846
left side	2.0086509413636784
next utterance	1.7020919425946994
suf cient	1.6936122289572197
single syntactic	1.6666455667887732
another dimension	1.611203449372866
three subjects	1.537405269154868
practical applications	2.0819643785303255
text elements	1.6349068916737026
goal state	1.5027086991362677
representing words	1.5395099438048585
redundant information	1.850090746376718
process repeats	1.6256278638449597
large collection	1.9836529619550842
novel technique	1.5648923336080593
upper case	1.865266371566773
different perspectives	1.665261319104293
word class	2.0031419311502856
terms appearing	1.6284316044082066
resources like	1.6473672030519313
language generator	1.8559980641456941
learning method	2.0847715410374574
systems cannot	1.740227153681778
parsing results	1.874210091997896
long sequences	1.5482870987025659
weighted finite	1.7544991493359499
biomedical domain	1.7548993708100449
improved recall	1.513130125383453
information added	1.526993068994652
news domain	1.6741321684676764
linguistic interest	1.5559715171228279
text according	1.6349068916737024
pattern matches	1.7381258494561371
particular characteristics	1.5511943810330642
subject np	1.9317687773188186
sentence similarity	1.5653620371109196
system therefore	1.507007268326853
alternative solutions	1.5320327917971737
acoustic score	1.5255567620929116
windows nt	1.5156323250636554
relevant items	1.5319875804036818
provide results	1.591823957496618
false positives	1.916584000015169
technique used	1.9022721667475868
structural relation	1.5262713199653124
makes use	2.3691546286837584
situations involving	1.522760613339579
rules r	1.511176588103529
structures obtained	1.5111306094308554
body part	1.6006594272387735
3 let	1.5749574214167732
switchboard corpus	1.729764833678769
penn wall	1.585589015734039
target grammar	1.6299984111869619
molecular biology	1.590940542187214
domain independence	1.6415561730697903
particular argument	1.6339957705783554
current state	2.18701416152215
tagging problem	1.7814943844084845
one technique	1.5547189562170496
highest precision	1.7251961279191768
frequency count	1.7713930702432736
decoding process	1.7710878705686794
specific values	1.750700291476916
ordered sets	1.5360026503169555
independent models	1.6427303794649821
3 depicts	1.5338735539822874
context provided	1.5665826729887178
three types	2.3534772914945634
semantic nature	1.6502761822926497
1000 words	1.7460252717662352
likely interpretation	1.6413192721110739
user interactions	1.6162383534717668
rules cannot	1.7230579229098404
parameter setting	1.7810332760816052
explicit mention	1.532714913365084
summary sentences	1.6357950933203562
improves accuracy	1.5579623782233836
telephone number	1.5526358915484042
spoken input	1.8402352933278647
sample sentences	1.8816063156417406
possible extensions	1.7142241551690973
algorithm must	1.8390002902204527
deep cases	1.5928879209460476
verbmobil corpus	1.5538537124940137
binding theory	1.8027156135328084
also discusses	1.5338735539822874
recall error	1.5777946478302516
directly address	1.6010101084822785
particular approach	1.6084968011212868
questions based	1.548378091852222
based rules	1.707385429743946
minimal set	1.7668619574515796
dependency information	1.6668745120942425
second example	2.0234994096631542
sentence planning	1.7765847404023547
various levels	2.0956165142283796
scale nlp	1.5211629864223775
textual entailment	1.5094348775461894
similar structure	1.7266279442877566
nl systems	1.6007717308465257
similarity metric	1.8699488695092323
approach involves	1.7392306202979373
structure tree	1.9566659290977733
grammar requires	1.586137214651611
information encoded	1.900443678998953
relations could	1.6190328102134979
another event	1.567647148777659
two parallel	1.7504812345368606
several parts	1.7002798641346384
learning framework	1.803251126649147
computational aspects	1.636127944690715
design decisions	1.8327528485360742
e sentence	1.5378646275607575
transfer phase	1.6412041018622059
un ensemble	1.5124062567051255
early days	1.594773126307433
figures 5	1.7201741786343798
one speaker	1.7971269466183348
shows two	1.9609154239304998
grammar produces	1.507007268326853
higher score	1.8670330149631165
linguistic intuitions	1.6658904115324105
first result	1.5482870987025659
user studies	1.5810896556511653
mutual information	2.1845821363824776
another sentence	1.803735150947357
whose argument	1.5260185053825464
major modules	1.507007268326853
model provides	1.9532137885909449
target object	1.525017146734915
one case	2.0647341086589517
last position	1.611203449372866
procedure requires	1.5111306094308554
possible target	1.7385165086934322
surprising result	1.7125697263999058
storage requirements	1.5992286050546423
passive forms	1.608299184330961
input could	1.5306061495832655
test instances	1.6884944462771379
level patterns	1.5496938731393646
immediate children	1.5627352601669195
detailed descriptions	1.8029091296598316
information captured	1.5354510398033074
key ideas	1.6223575629464315
unigram model	1.7307118361807554
underlying model	1.709076712145899
like text	1.594773126307433
choice points	1.572782149127976
input form	1.6040957503720996
techniques using	1.65303397348486
set k	1.5679332442954022
new variables	1.5318465770311396
like john	1.6564643300530968
weighting schemes	1.713031212014427
chain rule	1.6386744258939576
adverbial phrase	1.79382698608172
answer pairs	1.6902499509964994
complex tasks	1.6976654847000268
parallel training	1.6001864201373859
first hypothesis	1.6749532233342206
unification grammar	2.018576361100501
whole sequence	1.62291510937581
partial matching	1.6070145991578793
classes defined	1.651620883115958
shallow analysis	1.6844633832435152
two events	1.881405012353467
additional features	2.0630676304322764
similar sentence	1.6109413779808728
semantic description	1.8371525902881012
feature weights	1.8285073358555115
set used	2.0670343966734093
two runs	1.667232965466539
possible part	1.5784904253929217
sicstus prolog	1.6724457140847901
certain relations	1.60913070763407
essential part	1.8272188571693444
two readings	1.8620030984358156
approach first	1.5408560126058508
tagging algorithm	1.5217605875570066
chinese data	1.5530065163913274
retrieval model	1.6353221625272663
4 seconds	1.5514607572627885
pertinent information	1.5657848105751837
planning process	1.8399097210298803
global variables	1.5288687562201928
last clause	1.5037367200448983
point de	1.5397604718119187
evaluate systems	1.530072891325321
r u	1.6034524551010727
categorial unification	1.5459756252045591
one user	1.6990666498994726
using bayes	1.7867811279202184
lexical patterns	1.7292916278411337
structures rather	1.594773126307433
subject field	1.6655214706578194
context within	1.6661204761131607
label indicating	1.512925200463965
application using	1.5328333954091753
paper concerns	1.5991584292605263
one half	1.7447050080321946
1994 association	1.5111306094308554
one name	1.6023597487734618
average size	1.724849955805923
theoretic interpretation	1.5440207036648976
automatic summarization	1.8273817476122334
acoustic signal	1.7519334864991358
parentheses indicate	1.5511943810330642
interlingual representation	1.5774249263068632
small value	1.6213967902634532
position information	1.6453881278000846
based search	1.8481660395588455
research shows	1.579873144038688
school students	1.509626612173619
whole phrase	1.7098259615519555
surface level	1.8716986976205303
tree path	1.543882014880523
general nature	1.5217605875570066
temporal relationships	1.512026004521657
10th conference	1.672219863528485
practical utility	1.5525978349470975
analysis using	1.8637019770389935
lead one	1.5645241345053709
current prototype	1.5774202899531797
existential quantification	1.6261450449181756
based methods	2.1569896093421193
linguistic levels	1.830767617829535
phrasal structure	1.5578656975467768
context consists	1.5188051370135929
markup language	1.73915437450963
statistical taggers	1.5219007618281095
given label	1.507007268326853
space limitation	1.507007268326853
every combination	1.6392309045877975
discourse constraints	1.513130125383453
world knowledge	2.229206263763678
generate candidate	1.5177700906840073
physical object	1.8823041135558929
surface structures	1.7767080205479677
lower scores	1.6591113583044261
written texts	1.9245519761271388
words appear	1.9123573936584353
syntactic nodes	1.6242476541218667
vocabulary sizes	1.68858638223877
existing tools	1.6036172388145076
first possibility	1.5131301253834533
general constraints	1.6853236803857272
particular grammatical	1.6644972855476707
six words	1.594773126307433
document number	1.5211629864223772
phonetic features	1.538107486973951
acoustic features	1.7196166621345637
produce summaries	1.6063718989139955
news text	1.6981135909969833
whose values	1.8490380140057316
optimization techniques	1.563687738145949
detailed examples	1.5037367200448983
new application	1.5716057488605581
problems occur	1.5511943810330642
various languages	1.8554497088033353
new constituent	1.6545861275831806
method significantly	1.5426832781289812
system outperforms	1.7308621015100836
models p	1.5525978349470977
becomes one	1.5771944393968738
limited success	1.6388370153723533
level linguistic	1.6688496881791428
actions performed	1.5482870987025659
applications using	1.707160349122273
larger corpus	1.9882967470768715
polysemous word	1.7099639836061384
category n	1.6256677123741488
speech transcription	1.58255773593085
null using	1.6084968011212868
irrelevant information	1.6666664790751589
several dictionaries	1.5328333954091753
previous rule	1.5459182428709468
two thresholds	1.521614825883617
method gives	1.6614869878819327
candidate antecedents	1.5286326436698567
use less	1.507007268326853
information system	1.7228906021359034
communicative actions	1.5613065263093455
two respects	1.8119659926140157
following principle	1.5037367200448983
also create	1.636252690439584
two approaches	2.127286457392866
sentences 2	1.5611231576018243
global constraints	1.563543848389163
target phrase	1.7218775024913406
results produced	1.8137066482575523
specific constraints	1.8134041712446565
mechanism also	1.530072891325321
perfect tense	1.5404722092381313
two choices	1.6581363223870689
simple dictionary	1.5991253496404458
tile context	1.659111358304426
document sets	1.64799590093202
good precision	1.6514103959727489
partial derivation	1.5068990275735845
classes c	1.5756869131546112
general description	1.7341395890835507
equal probability	1.6718027974751068
diverse range	1.506173888464562
hypothesis testing	1.5793830661418065
five steps	1.6339957705783554
surface syntax	1.7316363725607624
rules specify	1.6550184848056928
manual inspection	1.7130162238987903
main difference	2.1133157647124587
main results	1.7179482056291908
also cannot	1.5582958710952315
pragmatic information	1.8712705426356815
word may	2.114806540680643
grammar defined	1.5731122769149346
larger class	1.5440207036648976
length 1	1.8076993936199448
left part	1.7774635806513719
condition 2	1.5433071476095503
art performance	1.9167906555155634
corpus showed	1.5445134575869865
6 senses	1.5408560126058508
lemma 2	1.658306764782873
use standard	1.7518332963081182
test material	1.7797110390030197
underlying grammar	1.7213876182010663
two tags	1.8301942052657143
dependency structures	1.9749927136958538
least part	1.5645241345053709
current purposes	1.6666455667887732
inference based	1.5482870987025659
allows easy	1.6190328102134979
many alignments	1.5337472999366386
gram word	1.504934031784749
language problems	1.6976654847000265
c h	1.5882934441894223
one category	1.9657986538876204
head information	1.6604581424365858
shallow syntactic	1.8439309556206311
authors thank	1.6388370153723533
another component	1.646226844257487
two restrictions	1.526993068994652
children nodes	1.5901952289902885
grammatical case	1.530320126517967
preferences among	1.5515205282906606
syntactic operations	1.5548556827926392
long sentences	1.971335019564781
scores computed	1.5211629864223772
look back	1.5432756752761156
possible outcomes	1.6953060531526463
lexical items	2.43362969586743
two translation	1.6930750934816992
case marking	1.761159211813516
40 words	1.7404715928135985
multiple domains	1.6354102761745795
heuristics described	1.586137214651611
different paths	1.7400137177323638
grammar writer	1.9556551976192798
research presented	1.7072113359402379
threshold values	1.7056735054026797
computational semantics	1.710194641617527
n sentences	1.74967355192648
based reasoning	1.749013848551588
number n00014	1.561123157601824
system utterance	1.5471498664789451
particular application	2.0485408033824895
short phrases	1.6732220537262004
application programs	1.5257027020067777
larger sample	1.5973170078864487
second class	1.8204989926953183
lexical resources	2.104337828131401
different sentences	1.9010084761115735
structure directly	1.5338735539822874
help us	1.6862442704000444
8 sentences	1.5132264108652627
inference engine	1.7612464762567923
play important	1.5288687562201928
specific terms	1.8139451811090026
sentences would	1.8273174948480295
three elements	1.7783897922119258
e z	1.51910459028178
avoid redundancy	1.510181823566108
stage process	1.8169609508852207
ellipsis resolution	1.6105297418682811
world application	1.5320698902069678
articles published	1.5707376783218978
meaning representations	1.9204900799324975
another relation	1.5445134575869865
web sites	1.795304446158671
high word	1.504934031784749
answers questions	1.5414180417122596
best score	1.9063861135727265
following word	1.9199300636760146
probable parse	1.629309450260227
single symbol	1.5881174695673654
1 word	1.615262062995654
clustering approach	1.644611186649238
formal theory	1.576892268255695
better models	1.6481174069026685
complex types	1.5176055023927573
9th conference	1.6699400022790052
transitive relation	1.626141669715322
negative polarity	1.612680134834359
efficient parser	1.6375625045004876
time taken	1.7984325968879384
multiple inheritance	1.7973093407028615
every item	1.5592523837539578
model uses	1.9820048873073606
original meaning	1.6288981647434881
kupiec et	1.6901108833115028
six categories	1.5371982986173842
representation structure	1.7480471251456795
reasonable performance	1.7002798641346384
morphological process	1.5777946478302516
require much	1.62412689036662
previous example	2.0605307815738536
tile semantic	1.6606791936831158
via unification	1.5622818522186859
formal terms	1.6164339756999313
common task	1.6656634360012819
associated information	1.5275591303469285
using test	1.5188051370135929
syntactic rules	2.091717719812526
figure 9	2.177248317147143
frequency list	1.621711231343578
information available	1.9336399315695794
phrases within	1.7776011133071408
graph unification	1.5955838901399757
time point	1.6205210241598607
performance drops	1.6500007742396736
preceding np	1.5381766791708387
filtering process	1.761159441869616
x e	1.5614812169146837
given case	1.6342922036019591
entire process	1.7279919232921823
corpus without	1.699626140637926
basic forms	1.5899302670582593
neural nets	1.6000839619842862
similar ways	1.5887468924062063
art system	1.5496092928695908
grammatical descriptions	1.6031224303496519
miller 1990	1.5245348740884508
models used	1.9957772678512007
object types	1.7205093708003458
constraint based	1.5360785028369002
n f	1.6043492558704961
functional descriptions	1.6397949952467243
4 let	1.5582958710952315
second measure	1.579873144038688
uses word	1.6785607164239145
computational efficiency	1.890224500308344
linguistic rule	1.5412463844444775
tagged corpora	1.9468594173856384
central component	1.6339957705783554
top ten	1.7212431033855373
existing machine	1.6580606251764167
including proper	1.586137214651611
search errors	1.594703410426117
vector whose	1.5612815851895034
translation project	1.8183504854315111
discourse interpretation	1.7125848483605228
combination rules	1.5686736500274283
problem may	1.7935968174739088
ambiguous expressions	1.572834038198836
language retrieval	1.5347939011760097
modal verbs	1.68332120187628
thank dr	1.8022217079329124
simple past	1.7289301261509231
single sentences	1.893613401422652
language parser	1.763671545503519
class items	1.5265431024669132
probabilistic model	2.0759997810272055
annotation tasks	1.573678870932111
category pairs	1.5379838338367486
compound nominals	1.5922738530272895
preceding context	1.7624921644184188
method showed	1.5266242261615062
given name	1.569339433103698
selection methods	1.6305199718639063
algorithms proposed	1.526993068994652
score among	1.5306061495832655
information appears	1.5645241345053709
foreign names	1.6156435760956458
sentence would	1.9407735482801876
atn parser	1.6258574628751319
syntactic analyser	1.5106521830350292
appropriate value	1.704895197139951
next module	1.5395099438048587
system computes	1.5422431186751524
two reasons	2.247286301941566
segmentation algorithms	1.6560429830764813
give one	1.5973170078864487
two text	1.8130832225340674
easy access	1.754682578532674
verbal form	1.7475459825393953
additional sources	1.5706734993195162
important facts	1.507007268326853
separating hyperplane	1.576123273231477
syntactic sugar	1.586137214651611
improves significantly	1.5445134575869865
alternative translations	1.5784699981130208
general problem	2.018467941195675
contextual evidence	1.5140548915918028
words among	1.6214578999580551
spanish language	1.518546607060388
kaplan 1982	1.512925200463965
features would	1.8029091296598319
acquisition process	1.8373189930334684
running time	1.8010669843178166
classification results	1.845075991465748
one hand	2.287497888684096
possible alignment	1.6420491785814577
performance metric	1.5697897626673076
complex interaction	1.6500007742396732
extreme example	1.6315901311540408
surface linguistic	1.6301180199407572
publication date	1.5036575034310684
consider tile	1.5511943810330642
english news	1.6743905502399918
special meaning	1.579873144038688
large value	1.640910700935025
inflected languages	1.5107121082831367
users may	1.8679852911962913
grammatical category	1.9204060517170563
phrasal verbs	1.773069622506331
acquisition problem	1.5338735539822874
specific interpretation	1.513006728163953
translation tasks	1.6514574191840672
describe three	1.6822401779866478
attribute values	1.8749592976474292
briefly discussed	1.65303397348486
simple definition	1.6010101084822783
hendrix et	1.5612815851895034
boolean value	1.6117266025648127
scale text	1.5559715171228279
applied linguistics	1.597317007886449
one unique	1.5553381419921846
larger grammars	1.542734882549225
greatly increases	1.62291510937581
tile set	1.7389399216452173
2004 senseval	1.6822401779866478
training purposes	1.5771944393968738
significance tests	1.6793724239483931
order among	1.6655102285632384
shared information	1.6082991843309613
probabilistic models	1.9761470154314285
combinatorial explosion	1.965825502714511
specific problem	1.7720369317764424
rules discussed	1.554089533378209
rewriting rules	1.8215553880648931
higher order	1.9827625517943346
way described	1.6822401779866478
short queries	1.522770931106011
following definition	1.913999752369934
system keeps	1.62412689036662
qualitative evaluation	1.6647965563340175
approaches used	1.643362624369706
acoustic data	1.617618486010906
without difficulty	1.5739592165010823
using language	1.7178162994368975
definition may	1.6056035510036821
whose probability	1.6458143726245438
material used	1.5338735539822874
suggest possible	1.5156323250636556
sentence level	2.2272517340654714
john left	1.5896741966294636
learning rate	1.6191059759814694
experiments described	2.064381621030745
difficult problems	1.8975681815428356
one head	1.655724480844433
general type	1.768858146453948
word lattices	1.6265777058466862
paper provides	1.8877706562088061
real data	1.8903939095900335
data size	1.7356491465864057
article describes	1.6761656524213184
together form	1.6593965399315964
best features	1.6251573559222985
significant improvement	1.750755793482309
combination methods	1.5667945125720018
values must	1.6112034493728657
conceptual framework	1.5839427016111838
characteristic features	1.6036172388145071
relevant rules	1.530072891325321
internal semantic	1.500089141564707
sentence structures	1.9260303773133913
mechanism used	1.693630808034802
hybrid method	1.5618776646932322
general algorithm	1.7609869990584184
distinct senses	1.6458887698068096
art machine	1.5796228681312572
iterative process	1.7886243665567463
verb sense	1.7702469169517865
main motivation	1.6785607164239145
last element	1.7691742845431428
one token	1.7297263851071971
among words	1.9566060933364957
first node	1.6468866812474956
computational linguistics	2.5598177931099815
world data	1.611677089751251
et la	1.7232630844100918
shows examples	1.8997992038231837
bene ts	1.5521184011134357
hypothesis would	1.5425370803688765
words within	2.1097983598478005
de r	1.578379824503081
used simple	1.5338735539822874
first observation	1.5739592165010823
specific type	1.9145659018894734
kamp 1981	1.514469500969361
thank three	1.5445134575869865
new algorithm	1.8904210512522348
grammatical constituents	1.5511943810330642
following fragment	1.5716157403241997
verbs like	2.0123740562653873
system currently	1.9117711273425
difficult cases	1.8154315577564777
tagged version	1.5611135103157157
full information	1.5037367200448983
certain words	1.9376441581401034
new parameter	1.561596045888415
retrieval experiments	1.6037357443466231
system parameters	1.6468307353976082
rather straightforward	1.6246289167870047
three classes	2.0113467526893345
level nodes	1.6184860705285997
successful approaches	1.5445134575869865
following classes	1.646226844257487
specific terminology	1.6075783640375185
given category	1.80597547124652
involves selecting	1.5037367200448983
general design	1.5649365246686138
best n	1.510618969052833
many others	2.017542481482965
entire documents	1.5511943810330642
model combines	1.5111306094308554
skewed distribution	1.5771944393968738
separate sentence	1.5037367200448983
cosine measure	1.8092055428198737
language tasks	1.738379124265367
statistical classifier	1.535714165376438
many concepts	1.5887468924062063
recognition tasks	1.6941347947968268
performance results	1.8671770410820014
performance difference	1.7000083319956576
additional research	1.522760613339579
sources used	1.6785919234312812
existing techniques	1.6500007742396732
syntactic framework	1.5381766791708387
whole system	1.9833185195558296
linguistic processes	1.629958134851444
one referent	1.5558827079957558
evaluation exercise	1.5856160021217733
conversational implicature	1.5175561133617632
translation examples	1.650835098877435
probabilistic grammar	1.640278592862809
discourse cues	1.5021493166259643
interpolation weights	1.5070031267906376
template generator	1.6161340351940847
another difference	1.7671941093962706
best list	1.8592477394970435
lexical nodes	1.5343818686221886
prepositional attachment	1.6034321301038696
rule states	1.8153413658553579
system component	1.619472687377359
case b	1.5107844011873681
explicit word	1.6345681524414148
5 reports	1.6699400022790052
techniques similar	1.6527294781610564
syntactic relationships	1.805160606107626
semantic functions	1.6798162943861712
language may	1.9370374127105772
accuracy measure	1.585589015734039
degraded performance	1.507007268326853
two lists	1.8940786447947549
examples involving	1.6388370153723533
de nes	1.6044489211325175
whose interpretation	1.646226844257487
sense information	1.7209031233744794
distributions p	1.6801137184008006
section deals	1.5408560126058508
examine whether	1.791807943135459
specific attributes	1.5582958710952315
classifier performance	1.5969765857928704
similar results	1.9550846681792635
sufficient amount	1.583051127543801
syntactic categories	2.2331640410521505
free text	2.0291649776345198
connectionist model	1.5245918492409598
task without	1.6002993454155385
software manuals	1.510784401187368
update rule	1.5797539418448143
matrix verb	1.6646801987433255
output probabilities	1.666381064792449
negative logarithm	1.5440207036648979
various problems	1.6903000289233914
language community	1.6902599836287888
parser might	1.6371456055941147
recognition community	1.506173888464562
formed substring	1.5542673033650314
last experiment	1.522760613339579
standard training	1.7247475524027744
formula 1	1.5703170788482572
one person	1.5769108577044846
possible errors	1.6630062643878183
specific part	1.6569666556735587
illustrative example	1.669489306700537
perform semantic	1.5739592165010823
complex word	1.6734657912932496
vice versa	2.2854506723313044
small change	1.5656419863875413
memory requirements	1.8046926212347338
based segmentation	1.6062451581057093
wsd using	1.506173888464562
probable tag	1.6613891836858694
links may	1.530984627359544
mean number	1.670489942166411
9 words	1.5445134575869865
column represents	1.5525978349470975
parser returns	1.6588577629299017
di cult	1.615537897396626
semantic markers	1.6172379918573223
style grammars	1.5378646275607575
syntactic variations	1.6421093653263799
successive sentences	1.5110975570289593
automatic tagging	1.6667422623230679
parsing mechanism	1.8036086125627961
particular algorithm	1.5706734993195162
current framework	1.647367203051931
use one	1.947862472988304
second baseline	1.613624660358136
learning curve	1.8623520128198758
another time	1.5511943810330642
improve parsing	1.6938135148853282
ultimate goal	2.015148427705782
words wi	1.5622818522186857
structure information	1.8437452143040503
algorithm uses	2.0473881165201075
text using	2.0183774214818193
documents whose	1.507007268326853
object relation	1.6950814181449951
attentional state	1.6138838672056834
line 4	1.5825577359308496
connectionist models	1.556111436993317
best sequence	1.6969826936396712
figure also	1.6593965399315964
user model	1.78032253750231
whose type	1.6210934563561632
independent way	1.636252690439584
different styles	1.6303219229622337
pradhan et	1.7016945271384092
current approaches	1.875034325671003
main stages	1.579873144038688
translation applications	1.5408560126058508
2 displays	1.7158821441137806
two tokens	1.785272526956975
algorithm may	1.8735201870806433
recent research	2.0351452956349365
grammar allows	1.773475133795545
squared error	1.5748567577278902
work remains	1.7329292822928468
per line	1.5893265024752143
active verb	1.6375625045004876
n l	1.7886818589107445
every position	1.5706734993195162
type checking	1.5544768442444907
small amount	1.5400133525187458
different frequency	1.5669631493969658
results comparing	1.507007268326853
large vocabularies	1.7087882102908172
systems like	1.8873120634175615
search procedure	1.8724269722185432
corresponding verb	1.6121742428060308
document collections	1.8488716991241334
top n	2.0001606719177767
english would	1.6266256462651403
adequate model	1.5037367200448983
complete model	1.6998973603836447
generate text	1.6961394033344752
pietra et	1.7242330911117034
retrieval research	1.5521452212545803
certain categories	1.6315901311540408
control structures	1.6153545810113643
linguistic problems	1.7223124790096405
one query	1.6010101084822785
current domain	1.660919557194327
chunk parser	1.5340070218577795
two versions	1.984827254219778
3 sentences	1.665622505106641
black box	1.8018665199446955
developing methods	1.5798548254880234
actual use	1.7080353410569553
algorithm outperforms	1.5459182428709468
every clause	1.5637776281819353
parser could	1.789420955354118
third class	1.667913900271795
linguistic phenomenon	1.855165201628581
another tree	1.6948705187579494
texts contain	1.6266256462651403
system checks	1.6037378902512214
new hypothesis	1.5690244007917742
rules arc	1.5188051370135929
state constraints	1.516491127995908
known fact	1.6833475265392548
word according	1.6802940634668404
good translations	1.6185947819602715
tables 3	1.8892608769156347
small difference	1.6164339756999313
statistics based	1.5695222973163587
semantic representation	2.3223677608670927
efficient generation	1.506173888464562
level rules	1.7867121837886752
data must	1.6802940634668402
information gathered	1.6682934633704967
processing algorithm	1.5459182428709468
certain domain	1.6822564918434932
constraint requires	1.5188051370135929
parsing information	1.5524289354793173
abductive inference	1.5058993318450544
separate component	1.626990247505205
5 decision	1.6495359841770274
two tests	1.689777923135894
extraction technology	1.6825185782521233
certain concept	1.5177700906840073
attribute names	1.644002867672538
rules like	1.8757383828641088
another language	2.095843158290668
current paper	1.8147410757803597
resolution process	1.8107428339017297
accuracy may	1.591823957496618
network based	1.6113346817822474
natural human	1.6300610276118175
testing purposes	1.7650139064090153
raw texts	1.613624660358136
second generation	1.502140269292856
n times	1.8137137020839116
provide information	2.0804203583814007
solution may	1.5338735539822874
better understanding	1.76746066193336
sekine et	1.5789113523697564
whose part	1.548378091852222
binary rules	1.6939098347778558
two clusters	1.805196013606386
place holder	1.5716157403242
grammar without	1.6934793309340155
verbal nouns	1.5922738530272893
lexical rules	1.817305421045084
text content	1.5531894390682597
smt systems	1.5806250818430243
semantic annotation	1.7978749664850577
clause types	1.5321810481204974
valued function	1.513130125383453
terminological knowledge	1.5870069538036402
different times	1.7923196053534847
similar language	1.5511943810330642
kernel methods	1.6395322360500697
two speakers	1.7141995714551364
recognition applications	1.5827722622100096
system makes	1.9886115296417728
complete knowledge	1.613624660358136
another aspect	1.7788903461436698
corpus analysis	1.9443567920564369
semantic associations	1.5246980774116032
morphological processing	1.8414004199421645
possible contexts	1.6458143726245438
ne recognition	1.576466896352366
solid lines	1.77043193298085
highest recall	1.625821221429443
ran two	1.5612815851895034
j c	1.6046743940979167
given token	1.5749574214167732
language generation	2.2884622272187274
combined score	1.5754161637940385
different combinations	1.944151493245293
spoken dialogues	1.7822513471371202
also identifies	1.5338735539822874
statistical classification	1.5109871437896232
add features	1.6169986020906966
still contains	1.6388370153723533
gao et	1.588371293810955
require less	1.6462268442574868
small numbers	1.7344099527418115
cell contains	1.5771944393968738
many occurrences	1.5111306094308554
corresponding arguments	1.507007268326853
system creates	1.6411558246537032
discuss related	1.703359686465308
cannot say	1.7425221016807453
topic detection	1.7606293285881662
generation approach	1.567922330580758
discuss future	1.6527294781610564
finer distinctions	1.641155824653703
shared task	1.998818034906618
careful selection	1.5547189562170496
active role	1.5906723352433734
combined model	1.7183042280810827
dative case	1.5667747749920065
node dominating	1.6305425340049553
mohri et	1.5986188132341779
existing dictionaries	1.6981174863848512
recent version	1.5781539692435587
morphological phenomena	1.6212113212773738
uniquely identifies	1.6190328102134979
class probabilities	1.5041134997716235
usually contains	1.5559715171228277
model also	1.9584537519716452
term co	1.521751620236959
semantic classification	1.87988660820761
best feature	1.583769273460954
system described	2.200767147029504
leaf node	2.01661589732311
programming search	1.5443785051334373
different verb	1.770513995010165
characteristic feature	1.513130125383453
entropy modeling	1.706039874831912
structure level	1.6864869779656422
feature representation	1.7713672560190938
french words	1.7728296769370853
frequent words	2.095354398629295
precision drops	1.5354510398033074
base forms	1.8316744455038603
tests whether	1.6794606836045487
two iterations	1.6194726873773588
resulting model	1.8502107609341005
word aligned	1.6960282866869671
per article	1.513130125383453
row represents	1.5511943810330642
information including	1.643362624369706
manual tagging	1.6319601673157313
particular subset	1.613624660358136
model word	1.579873144038688
process involving	1.5547189562170496
l n	1.5626390129985257
correct order	1.568863381218478
recommendations expressed	1.8326833350715812
real number	1.7991879462554783
one term	1.7811291455943445
lexical entry	2.221137293135792
third condition	1.5188051370135929
h c	1.500089141564707
underlying system	1.5681322974096916
algorithm combines	1.5656419863875413
systematic errors	1.5354510398033074
argument relations	1.8558439177237396
broad spectrum	1.5482870987025659
target phrases	1.515006609977775
semantic distinction	1.5716157403242002
galley et	1.5479562189648608
simple examples	1.77852977578713
linear form	1.5558827079957558
automatic parser	1.579552318662148
medline database	1.533947046628388
technical domain	1.5617203866198948
one input	1.7440226570048845
action may	1.567922330580758
previous tag	1.6195225153514352
person may	1.529606820290264
valued features	1.8000418292853073
retrieval system	2.090693173702266
another view	1.5111306094308554
use either	1.757636103678803
errors produced	1.6169986020906966
system results	1.7020903750698642
different heuristics	1.6010101084822785
2000 words	1.6655102285632384
flexible way	1.6169986020906966
discourse phenomena	1.8158434016036058
formal languages	1.7605831068803022
finite form	1.5226071645445156
pages 9	1.522760613339579
new annotation	1.57703572623628
following examples	2.1992065895342128
traditional chinese	1.5528328290313804
likelihood criterion	1.6304481342440433
needed information	1.636252690439584
possible surface	1.6448472409336694
sample sizes	1.5426119998657428
dominance relations	1.684306687195829
alshawi et	1.8002488644630494
language interaction	1.7144207568196006
methods applied	1.5634115707207186
following constraint	1.7044865459645906
active voice	1.8819964169505214
describes one	1.5408560126058508
active features	1.5204779106393116
scoring metrics	1.5414180417122596
different characters	1.5226071645445156
document type	1.691493782850523
high dimensionality	1.5748567577278902
crucial aspect	1.5338735539822874
national language	1.5514607572627885
higher coverage	1.6010101084822785
tagging scheme	1.606132546946497
right place	1.5525978349470972
reference points	1.5601292521985652
manual effort	1.601984878164314
xml elements	1.5019366433566907
features contribute	1.6210934563561632
higher number	1.7342403250027838
corresponding features	1.5771944393968738
good choice	1.6794606836045487
better performance	2.2760820641524786
unseen data	1.9387940876448189
technique described	1.8808726827694708
wrong answers	1.5260185053825464
certain text	1.5887468924062063
translation component	1.6411050677721999
two layers	1.6585946925833874
handle non	1.6415557430847445
latest version	1.6908795366984264
complete list	1.5544459816528735
low accuracy	1.7606007710377156
de cette	1.6134102037040483
path length	1.731187456821857
computational work	1.6963351656268753
conceptual graphs	1.62952832520502
information useful	1.6502761822926497
r rules	1.5106521830350297
secondary stress	1.5361450492096225
clustering algorithm	2.025798742733063
one question	1.7697809063128838
f f	1.545049216974251
constraint solver	1.6262626629883177
sighan workshop	1.656664045947751
longer ones	1.7408279821914405
missing word	1.5327136812744546
hmm tagger	1.5968227728396875
manual construction	1.6857297996413716
markov random	1.5677038452413155
restricted domain	1.8325907361801796
figure la	1.5399822486951469
english task	1.6419870392634195
also performs	1.7360338540431246
word classes	2.1490643337402737
process cannot	1.5188051370135929
base management	1.579028783384322
nonterminal node	1.6211325862114405
way using	1.5536240126687622
distance algorithm	1.5690244007917742
specific phenomena	1.5771944393968738
structure called	1.8624511954756577
two standard	1.6630062643878185
something new	1.553185508137465
attribute type	1.5037367200448983
single verb	1.6905387688911793
dialogue context	1.7552906650226494
significant number	1.9145577813556889
noise ratio	1.5771446266270406
pronunciation dictionary	1.6520002354037389
experiments using	2.20577629167086
many speech	1.5188051370135929
experiments carried	1.7075310135529882
previous sections	2.088290660471843
assigns probabilities	1.5440207036648976
asr systems	1.5000489217893396
axis represents	1.657711736423348
algorithm computes	1.7532757639307686
systems provide	1.6724173909604771
two strategies	1.8798308794532244
different morphological	1.656664045947751
actual linguistic	1.5037367200448986
present implementation	1.7423111959317616
parsed version	1.5288687562201928
incremental approach	1.6895991478845704
extra features	1.6064310100927286
parse using	1.665159100934992
per document	1.7063906559832032
two summaries	1.521404969161276
three constituents	1.6169635240879603
rightmost column	1.558193976095525
declarative form	1.5106521830350292
speaker independent	1.7488449294371666
indefinite noun	1.664802557388035
like subject	1.6445809590877876
individual concepts	1.644120771828903
comparable results	1.842000872799971
better way	1.7145474219008834
main information	1.5177700906840073
sentences already	1.526993068994652
algorithm applies	1.5563893320981304
traditional text	1.583942701611184
translation relation	1.576125616601959
extraction system	2.092932174216302
essential properties	1.507007268326853
good recall	1.643362624369706
level categories	1.6341544738471088
direct object	2.2385881593040327
feature values	2.152380437085859
retrieve information	1.6107689014994975
performance using	1.995735283449639
one test	1.7867811279202186
simple structure	1.6904151818990054
different role	1.5300728913253212
lexical relationships	1.6121742428060306
possible labels	1.6322732044424475
form without	1.5511943810330642
given query	1.7503260804837222
huang et	1.698477940967544
relative contributions	1.512925200463965
different knowledge	1.918193849173476
node labels	1.887976273213671
information plays	1.5408560126058508
large amount	2.0027639255463248
adjacency pairs	1.522131688652254
linguistic tasks	1.5904771784117462
system trained	1.8069881687840836
single characters	1.6332850831009493
complex case	1.62412689036662
without consideration	1.5611231576018243
following structure	1.76382686216374
tagged brown	1.5580764217656649
line 2	1.6706426105940735
lexical categories	1.9979780446673379
classi ers	1.588287178794722
dialogues collected	1.592744274716488
main idea	1.9998064486967144
cutting et	1.7988084959541697
computational theory	1.6769450675287576
passive form	1.7434875701233643
structural information	2.0607320017756545
planning systems	1.6359183163287403
models achieve	1.6056035510036821
many texts	1.543442208482524
hand annotated	1.5656419863875413
phrase translations	1.5589580606973392
without regard	1.8673350402547362
large effect	1.5412463844444777
thus use	1.6056035510036821
frequent categories	1.5117685557139826
recognition component	1.8119430838638462
within documents	1.516491127995908
minimal attachment	1.5578326634447754
distributional analysis	1.6236525902810213
linear combination	2.051728934374621
systems allow	1.583051127543801
linguistic perspective	1.698978605212456
general assumption	1.5188051370135929
two instances	1.9272094970918392
orthographic representation	1.5760526619236255
time spent	1.9081786211414031
cannot serve	1.5739592165010823
electronic dictionary	1.7625496671315009
computational terms	1.522760613339579
multiple derivations	1.5558827079957558
minor differences	1.613624660358136
pang et	1.6577247243287605
sequence must	1.5156323250636556
current training	1.549729132180171
simple sentence	1.9978834919391586
several others	1.6112034493728657
principal component	1.5543659857359193
large difference	1.5973170078864487
several concepts	1.522760613339579
example 4	1.9046377105467123
also denote	1.509778922004791
data problems	1.835467064602909
object case	1.5843383596449399
pereira et	1.8259148538011882
semantic concepts	1.7914805742023294
natural number	1.582219320846301
additional techniques	1.530072891325321
l g	1.5445134575869865
structured language	1.5162742976780446
many levels	1.8162486749821076
system extracts	1.7010406242423506
lexicalized tag	1.5842092955485656
general meaning	1.5818515289928636
individual terms	1.6419403068016445
000 words	2.320979767618531
time axis	1.5009106772831884
false negative	1.5190577100257667
following sentence	2.187225787277745
tags assigned	1.762923160646801
3 introduces	1.781783596261272
linguistic community	1.6073715579162529
john went	1.5930316141353076
therefore require	1.530072891325321
row 4	1.5087853581980923
arise due	1.6164339756999313
representation language	2.0818799953424065
resulting list	1.724249543634281
forms must	1.526993068994652
brown corpus	2.124514584186134
4 sentences	1.513130125383453
based nlp	1.777878318874697
best scoring	1.759811023065809
produce correct	1.586137214651611
syntactic constructs	1.6694356832453723
hovy et	1.6395675443304527
target application	1.6046743940979167
local ambiguities	1.58373187141013
language word	1.8415571953473096
systems development	1.6244843928316635
semantic database	1.5324452505798667
less sparse	1.5037367200448983
corresponding sequence	1.6031224303496519
model since	1.5973170078864487
french translation	1.6941401302623063
probable word	1.5164911279959081
handle complex	1.6157003218917043
preference information	1.585589015734039
actual state	1.5148387121477085
translation module	1.5637316279342839
local word	1.6134030394382908
function g	1.5670800898564918
systems perform	1.7471845324497088
entry consists	1.6411558246537032
bilingual sentences	1.5544768442444907
specific meaning	1.7017362832860266
last decade	1.5937899368358572
real values	1.5445134575869865
expected values	1.6897144982477572
utterances containing	1.577949253332516
four levels	1.7662796901220967
precise information	1.530984627359544
sample data	1.75263967782025
hand side	2.3150976108622534
common words	2.0321154541776663
several criteria	1.664540718454114
input pattern	1.5992816406669932
words except	1.6320354670147565
mutual beliefs	1.577721351717265
likelihood scores	1.5081744982409402
approach uses	1.9992795932004497
complex descriptions	1.5150717731521737
semantic criteria	1.7607282174142989
structure represents	1.6339957705783554
examples discussed	1.65545581040525
temporal modifiers	1.5705757689872493
major importance	1.522760613339579
memory based	1.6901874423804126
proposed model	1.8487890883790619
unbounded number	1.634867760006919
system follows	1.522760613339579
partial translation	1.5432539635505584
every element	1.781767024710009
tense verbs	1.5150717731521737
large numbers	2.0880787179720945
argument position	1.9205587474397114
semantic theory	1.7984640073974925
verbs tend	1.5037367200448983
word belonging	1.507007268326853
approach may	1.9552507024983474
elements must	1.591823957496618
important respects	1.6388370153723533
primary concern	1.7051629942513449
k 1	1.5236819882948263
exist among	1.561123157601824
three letters	1.636127944690715
arrows indicate	1.6573127369315375
annotation tool	1.7868807889080691
syntactic context	1.950847522584042
similar content	1.5112051206161443
maximum distance	1.6303444022729265
model used	2.0855152795699814
tile syntactic	1.5926202003411678
new node	1.9119976610089622
term vector	1.5391384313205587
possible alternative	1.7255694781821613
nearest neighbors	1.8028677921927068
single correct	1.594773126307433
semantic terms	1.5754056990297531
given term	1.6742068992967312
various types	2.249071508643307
several candidates	1.5894574345512695
edge weights	1.5612997438512586
superior results	1.522760613339579
taxonomic information	1.5065859737115481
subsection describes	1.50119175880199
appropriate case	1.567922330580758
positive polarity	1.5105194311609615
second module	1.615262062995654
works quite	1.579873144038688
alignment model	1.765243301910416
probabilistic version	1.5491899194507937
relevant literature	1.526993068994652
query languages	1.5477616296217311
components within	1.5525978349470975
given input	2.1030270852658037
text input	1.8651824052392825
result could	1.5188051370135929
start state	1.8310512778120396
approach outperforms	1.665159100934992
context sensitive	1.8922352065139803
extraction approach	1.5360026503169557
different conditions	1.8095394855795697
linguistic evidence	1.7770100864898124
new concepts	1.7870944638596038
polysemous verbs	1.5592739929322839
human evaluations	1.521362532222667
proposed models	1.6169611769033776
airline guide	1.5150717731521737
original rule	1.5987564835989918
computational process	1.5412463844444777
grammatical forms	1.5111524498016615
data retrieval	1.5500434040293412
major topics	1.5188051370135929
later sections	1.6904151818990054
many languages	2.0859535077774294
word positions	1.7346410467909412
complex category	1.6005251959718443
3 times	1.9014174174377905
using brill	1.5195036419368686
deep case	1.5511132542376938
use features	1.7230579229098404
rule 4	1.7436562948466936
rules defining	1.5893265024752146
four questions	1.5195036419368688
recognition module	1.7543892694269658
word entries	1.7884368897584297
boundary tones	1.6136178353822328
resulting graph	1.618103745190334
representations based	1.594773126307433
phase 2	1.6785777294780129
new database	1.5482870987025659
gives examples	1.683041275722836
r e	1.623069494709867
include part	1.5511943810330642
cognitive load	1.643629766749465
attachment ambiguity	1.7878173302769134
initial trees	1.7977327769400264
query processing	1.5435026937378433
treebank grammar	1.5138199744586274
state automata	2.0026726666530017
including context	1.5111306094308554
square root	1.6027832155234913
probability distribution	2.261399952605494
current context	1.9942178908117996
based paradigm	1.5211629864223772
terminal symbol	1.9491724880843229
given question	1.6531483845478945
distinguish among	1.7788535280262472
component must	1.703338232318599
prepositional complement	1.5777235895798087
verbmobil project	1.7125915194705716
top scoring	1.597801075428692
language makes	1.5547189562170496
translation results	1.8219075843199948
valued feature	1.7628747128039786
four languages	1.7234949533162562
answer type	1.6442445841643094
p c	1.5065661455504373
1 describes	1.7706038030264342
viable alternative	1.709268466970625
tags used	1.771431195733965
john gave	1.755865891491216
common framework	1.5992286050546423
linguistic entities	1.732933409335127
extract words	1.5514607572627885
whose meaning	1.9017750543191323
empirical research	1.6888951120289843
based pattern	1.5187014267751218
two modifications	1.5482870987025659
time applications	1.510181823566108
f l	1.7060406711249585
international conference	1.8740837742211143
task completion	1.5598800603473162
new theory	1.635876749820076
elapsed time	1.5981937492554081
generate summaries	1.6244843928316635
discourse contexts	1.596528355412953
depth first	1.5403034171359224
semantic representations	2.1962287140582157
different formalisms	1.6190328102134979
unification formalisms	1.5571495092265906
use n	1.7105799695961408
results indicate	2.210200705152525
retrieving documents	1.5426832781289812
particular node	1.7044865459645906
concrete example	1.8347737830219164
also plan	2.0001656377480357
oriented programming	1.6819762996803398
method introduced	1.5037367200448983
corresponding feature	1.7238311541316482
entries may	1.636252690439584
plan inference	1.5474043405696283
based approaches	2.279010010832873
different classifiers	1.765791503958411
input chinese	1.5354510398033074
higher error	1.6500007742396734
speech sequences	1.6300610276118177
task model	1.5014705296193138
deep parsing	1.5098050287734852
pragmatic rules	1.5288687562201928
makes predictions	1.5188051370135929
sentences based	1.782842247161332
common noun	2.0346351602337522
automatic generation	1.9473333751646078
lexical definitions	1.5611231576018243
make explicit	1.937470795559661
transformational grammar	1.824971005195629
structure analyses	1.5111306094308554
different aspects	1.8875578827782744
different implementations	1.6213967902634532
relative word	1.552032937886318
possible trees	1.659662148122272
sequence p	1.5081682632911075
bob moore	1.522760613339579
models described	1.8003550683527592
special rule	1.5756869131546112
much help	1.507007268326853
kappa value	1.6125918900346026
corpus containing	1.9466537816956808
language described	1.5371982986173842
classify words	1.6724173909604771
neural net	1.5899445963281074
different order	1.7184093123928232
second difference	1.636252690439584
tree would	1.6934141525857016
parsing complexity	1.6291410447393337
systems tend	1.6658904115324105
use non	1.6413904216258088
two measures	1.9550830626222022
value given	1.5111306094308554
3rd person	1.6980535638300054
first concerns	1.5445134575869865
abstraction hierarchy	1.5193823615803785
correct number	1.5563893320981306
syntactic grounds	1.6445809590877878
specific contexts	1.6411992014239676
basic form	1.8347378814900017
regular verb	1.507007268326853
head rules	1.502594480495715
based concept	1.5037367200448983
descriptions like	1.5381766791708387
general scheme	1.7017362832860266
underlying application	1.5465717448129954
language since	1.5445134575869865
coherent text	1.7931175324715714
system using	2.2374549954830343
e information	1.5275591303469287
introduces two	1.5706734993195162
2 corpus	1.5532825629987506
dependency relationships	1.7427978208319512
role label	1.5263479347564133
approach combines	1.5511943810330642
algorithm makes	1.8585957737782126
unified approach	1.5698846261941362
lose information	1.5037367200448983
available today	1.6266256462651403
nodes must	1.6603900236689957
exponential time	1.7168851559834695
careful examination	1.5445134575869865
best approach	1.6953060531526463
speech features	1.5677785380528464
following ways	1.7777184660433272
case particle	1.5787682268600776
key point	1.6765046864247826
two matrices	1.5058409883043937
data file	1.5402739395672578
order constraints	1.6889468991465737
one vowel	1.516491127995908
appropriate category	1.5839427016111838
also defines	1.641155824653703
consistently outperforms	1.5406210028261875
recognition results	1.9177386701222396
different pairs	1.6785607164239145
based translation	1.9714889833534446
illocutionary act	1.5436048485213774
use maximum	1.62412689036662
main task	1.8386129240515188
shows us	1.530072891325321
dictionary definition	1.726397236338657
e j	1.596528355412953
abstract model	1.510060731399269
b must	1.5300728913253212
moore et	1.522438913516297
approach lies	1.583051127543801
resulting feature	1.5767760138793054
figures 1	2.053140204050166
previous word	1.9297301679474905
input text	2.2410285810075115
right part	1.7626132976152014
interesting property	1.6002993454155385
generation problem	1.7598283215315513
since words	1.7392306202979375
name followed	1.507007268326853
different experiments	1.6770235151848842
natural deduction	1.5445247796866841
semantic web	1.7039962030721854
academia sinica	1.705440728068596
cepstral coefficients	1.652331836908846
often contains	1.667913900271795
ou r	1.6066593791852313
similar information	1.6603900236689957
human interactions	1.5412463844444777
product names	1.5720626281828656
tools like	1.5354510398033074
z e	1.7506986830746194
par une	1.570375204714523
constraint set	1.5491802919930078
2 indicates	1.7229550745276576
baseline results	1.7181848619222264
different data	1.9731955608068499
particular theory	1.5536240126687622
investigate methods	1.583051127543801
improving performance	1.731146052909204
research directions	1.7044865459645906
overall translation	1.6411992014239676
beam width	1.6860113668337697
lexical redundancy	1.5119109400163722
current document	1.5491899194507939
final consonant	1.6240242310228443
3 presents	2.1114953910785
also account	1.583051127543801
larger project	1.5612815851895034
latter task	1.5338735539822874
state transition	1.9002564405302602
tree structures	2.0593609675883595
performance gains	1.8139038709549058
specific lexicon	1.6172806834159354
algorithm described	2.20125455127487
domain would	1.6031224303496519
sparse training	1.5773133211810766
limited amount	1.916619960102538
morphological properties	1.665159100934992
sentence based	1.7692801772988378
non terminal	1.5354510398033074
document indexing	1.5755736939575358
template generation	1.6836488138300183
examples like	1.976927223193147
related work	2.230175832301328
technique using	1.5547189562170496
second run	1.5320698902069676
raw corpus	1.7017069567499061
one pass	1.7740412159111871
corpus sentences	1.6185965759709262
satisfactory solution	1.6190328102134979
individual documents	1.6360584948694443
similar idea	1.6961394033344752
sufficient information	1.7159126280063015
challenging task	1.8572317891436503
word using	1.7542610148908173
features based	1.9349582997182868
approach taken	2.0863289736226953
specific kinds	1.5111306094308554
hypothesis h	1.6000256701286557
class definitions	1.5266242261615062
best values	1.5037367200448983
related concepts	1.8198665485403231
vector similarity	1.5217987622876712
different feature	2.021486352192301
since multiple	1.5547189562170496
focus stack	1.6133528494988036
model derived	1.5466472792181065
inference procedures	1.6217423349150049
final section	1.8879718308961544
categories associated	1.50119175880199
structural patterns	1.5605633452444292
arbitrary depth	1.5408560126058508
corpus studies	1.6778478586578416
approach applies	1.5445134575869865
possible rule	1.5706734993195162
second case	2.1274233237357407
encodes information	1.5037367200448983
great advantage	1.6635555237714856
example rules	1.5061738884645621
grammars used	1.6920919321535786
4 introduces	1.6164339756999313
generation algorithm	1.9102188441213928
results reveal	1.5300728913253212
syntactic considerations	1.541748679610567
trec questions	1.5283106970018867
count noun	1.513246051898229
nlp methods	1.639484236700259
new technologies	1.5440207036648976
f score	1.5982112181458628
following shows	1.6112657873977674
particular choice	1.7017362832860266
different human	1.6845577236692315
way similar	1.8190027986131614
generalized phrase	1.8325943334517094
possible paraphrases	1.5222638249634683
various word	1.5645241345053709
synonym set	1.6158108515570815
ascending order	1.6280743823834243
e would	1.566582672988718
evaluation measures	1.8971347633615672
precedence relation	1.675337072896498
word orders	1.7502977631963852
occurrence matrix	1.6191518787491106
models trained	1.9123825342458072
works better	1.755185558689591
first example	2.0630735338512713
validation test	1.5065859737115481
classes corresponding	1.5188051370135929
fairly straightforward	1.862310359635364
determine appropriate	1.5445134575869865
system architecture	2.056340269887719
material presented	1.5408560126058508
better match	1.5645241345053709
higher probability	1.5568939393930912
best fit	1.6036848953283134
final solution	1.5473635513992763
text databases	1.5536240126687624
clear improvement	1.5338735539822874
indefinite np	1.5911830672899723
also show	1.9775526485605928
word sentence	1.7355784266555054
structures within	1.6550184848056928
way people	1.5707376783218978
direct way	1.7649248176686627
web pages	1.9918740594039144
third word	1.6285930450163058
single data	1.5514607572627885
dictionary used	1.7358032665695413
incremental process	1.5571495092265906
several documents	1.5230483704868893
verb appears	1.6690820382866074
true distribution	1.5981692747321068
semantic composition	1.7141934198325852
example based	1.578081793637426
system works	1.9198083116314066
complete system	1.8723309867539055
feature type	1.5931706269199863
negative instances	1.581144301584498
equation 3	1.9057752524790672
subcat list	1.5275256102508605
3 years	1.5432756752761156
based text	1.8474738441378733
possible output	1.6747135833174478
using log	1.507007268326853
word tagged	1.5081682632911075
central idea	1.703359686465308
reference resolution	2.007931723748406
entities like	1.5482870987025659
write rules	1.711151422700744
method differs	1.583051127543801
accuracy results	1.8481383724578653
related terms	1.7955309891305626
evaluation campaign	1.5184955440524792
sun workstation	1.6458797346140277
acl workshop	1.7229556053533144
iterative procedure	1.6440211519351158
decision theory	1.5399542488609383
collection consists	1.6289349127929256
treebank wall	1.6692314209563577
language user	1.7222663815376675
greatly improves	1.5676517392350746
recall figures	1.7376694199153813
4 illustrates	1.9132982478890297
general interest	1.6190328102134979
particular string	1.507007268326853
capture information	1.6190328102134979
table lookup	1.609277901277801
structure nodes	1.529845541324831
selection step	1.5175269497604567
corpus consists	2.1457553933853015
text segments	1.854220760300201
took advantage	1.6266256462651403
use within	1.6502761822926497
average distance	1.5532531438973205
complete coverage	1.635876749820076
meaning may	1.6811844390857942
3 contains	1.7595929162797401
single feature	1.932516485888968
level context	1.5363485810301154
another level	1.636252690439584
labeled data	1.8655829471692418
agreement rates	1.5399822486951469
best answer	1.6408545325622168
three groups	1.9088279099578895
initial word	1.6356082102387748
possessive pronouns	1.7921805029363227
different corpora	2.0441106935796602
approach extends	1.5111306094308554
system made	1.5486945064545343
final testing	1.5288687562201928
design process	1.5261653439704492
practical use	1.95229440058389
longer sentences	1.9447003457316834
case 3	1.6245247373352116
cannot recognize	1.5445134575869865
bayesian framework	1.5006743580950452
tight coupling	1.5118192695694388
simple discourse	1.5408560126058508
relations across	1.546195991207543
knowledge automatically	1.530072891325321
model whose	1.7125697263999058
experiments use	1.6717533031932033
graphical user	1.9257256214076013
segmentation task	1.6758638565661281
formal properties	1.893966704965271
makes reference	1.6128408409726123
knowledge must	1.761605409140373
language parsing	1.9453063114219113
model structure	1.6086818483232035
next question	1.6757983818147149
log n	1.6396300439715115
optimization process	1.5955573878777636
additional parameter	1.5582958710952315
lexical context	1.6657568663230449
lack thereof	1.526993068994652
several levels	1.9368415398678265
future research	2.3037599429330564
us government	1.6169986020906966
language equivalents	1.5036575034310684
indefinite article	1.7529659651225715
est en	1.5148722773105288
semantic levels	1.6741797752731205
discontinuous constituents	1.7170875696548036
naive user	1.5702560143399067
following discourse	1.6525564249248113
three algorithms	1.6668697692598449
difficulties encountered	1.5037367200448983
individual objects	1.6753114229441863
elements used	1.5459182428709468
three subsets	1.549729132180171
enables users	1.5794243998957866
similar structures	1.626990247505205
standard analysis	1.526993068994652
language descriptions	1.6445809590877876
problem using	1.775588620446508
modification relation	1.534771069588773
conclusions contained	1.8693342036195517
focus shifts	1.5592523837539578
context representation	1.5877046642631583
prior research	1.6785919234312812
grained analysis	1.5556026693790028
analysis includes	1.507007268326853
training document	1.591201877888726
case letters	1.7289637212266924
senses per	1.6729747778028952
grammar learning	1.614346682281305
set included	1.5188051370135929
analysis would	1.8610433777604092
another use	1.5541060999723646
action described	1.509778922004791
also determines	1.6783930166760777
parses produced	1.7044914805734133
two phonemes	1.5128153640906288
adding features	1.5988242883426202
analysis might	1.522760613339579
2 uses	1.6502761822926497
supervised method	1.6071651711679231
asr output	1.6166982614920467
items must	1.522760613339579
alternative parse	1.5156323250636556
null able	1.522760613339579
ordinary text	1.5706734993195162
comparative results	1.7021555513601787
input tokens	1.5920064834596637
learning strategy	1.5311681930612502
contain variables	1.641155824653703
shieber 1986	1.6192113542886106
times real	1.5607331776717805
rules produced	1.5716157403241997
second category	1.6844174045708415
toutes les	1.572888437163301
approach also	1.9446732161683489
combined system	1.7305200082904957
space using	1.6535456510181574
annotation using	1.5378646275607575
analysis methods	1.650178024271336
finite clause	1.6220076061370965
user actions	1.5571495092265906
tree represents	1.8131196934301124
syntactic theories	1.7095724323729775
newspaper corpora	1.5571495092265906
providing information	1.804725963492348
many problems	1.969567618138196
three forms	1.6304481342440433
rule based	1.9168652321349393
based disambiguation	1.61856470920059
special attention	1.952168154865211
rich semantic	1.6488447391751828
noisy data	1.7989545075141335
everyday life	1.5582958710952315
resulting system	1.9888007278599356
computational processing	1.5354510398033074
official policies	1.8516798118160611
different components	1.8747628363032478
companion paper	1.6111096505227929
generate one	1.5578153980282474
techniques employed	1.611203449372866
significant problem	1.7856549343874448
formal grammars	1.6055064337316343
also arise	1.5645241345053709
certain threshold	2.053669552919522
nlp task	1.6959017760440367
recursive application	1.6229108409074118
order unification	1.5093743976126608
indexing system	1.5292387220416233
filling task	1.5636877381459493
structured information	1.698302666461271
inheritance mechanisms	1.5690244007917742
several objects	1.524534874088451
stochastic parsing	1.5396249822688837
phrasal level	1.5887468924062063
acceptable results	1.5611231576018243
n r	1.5835823678261078
particular interest	1.9809923278676589
use another	1.707160349122273
trees based	1.5881174695673654
paper proposes	2.1827134464062294
slight differences	1.522760613339579
values according	1.522760613339579
framework based	1.6336649929396205
section introduces	1.703359686465308
new texts	1.7394384994026204
semantic objects	1.7039028393112727
multilingual applications	1.5648923336080596
parallel sentences	1.647906389886237
inherent difficulty	1.50119175880199
desirable properties	1.710004784518707
wsj corpus	1.913419929977445
theoretical point	1.774296903622264
correct answer	2.072424203041133
first corpus	1.7019709298887997
level semantic	1.8258777805763389
various classes	1.5706734993195162
et us	1.507007268326853
two dependency	1.620995864149759
dialogue manager	1.8724265334220391
algorithm follows	1.5645241345053709
resolution systems	1.6291466617172885
paper reports	2.0661917182314546
output generated	1.5553381419921846
larger structure	1.5622818522186857
tool called	1.62291510937581
category x	1.771453633218962
parser runs	1.50119175880199
special cases	2.0333620259008756
entity names	1.556677730472158
system generates	1.9860204055486288
dans un	1.6390380937556661
compare performance	1.6031224303496519
input layer	1.6017399850228744
third phase	1.5261764002574725
maximum length	1.8754933718852966
right boundary	1.6919704469766665
decision process	1.7372616817641209
gram language	2.02211623872338
good example	1.9597985201157317
existing work	1.7680474203375174
many cases	2.2734568968941256
training set	2.4560342115942806
first parse	1.6620080881943164
query systems	1.6804951004384119
several languages	1.9811905241013268
prepositional phrase	2.3311285776373873
12 shows	1.6593965399315964
two verb	1.6169635240879603
speech understanding	1.9576173700413138
sequence may	1.5482870987025659
exponential growth	1.55955788508706
definite descriptions	1.6777461558569733
human translators	1.8120664099137247
lexical semantic	1.9946878540617548
best understood	1.5739592165010823
mathematical properties	1.6306982954773628
ou de	1.5345873539538384
english words	2.2217846032053523
another task	1.591823957496618
different annotators	1.6433885100347294
preferred antecedent	1.5236669207122855
particular speech	1.5300728913253212
special class	1.543442208482524
key problems	1.5547189562170496
one link	1.7010334352083452
words 1	1.5195036419368686
closed set	1.722259108599251
first class	1.8735159040507114
languages including	1.586137214651611
make errors	1.6320354670147565
microsoft word	1.6000663019311554
current theories	1.5905335586417741
000 tokens	1.7194516503147947
extracted information	1.8410111232264588
many systems	1.9834638011822538
statistical processing	1.5690244007917742
structure rules	2.071765879402858
expository purposes	1.6112034493728657
future version	1.513130125383453
reduction technique	1.506173888464562
terms occur	1.548378091852222
vast number	1.6164339756999313
contains word	1.530072891325321
concepts mentioned	1.5139750619047756
matching techniques	1.8523784796623217
following tree	1.615262062995654
two factors	1.5345492195590824
sentence accuracy	1.6568309252928246
description logic	1.5103141786969165
english parallel	1.687649111712135
simple language	1.7051629942513449
extent possible	1.6458797346140277
forms may	1.6802940634668402
natural interpretation	1.5041134997716232
primary means	1.5511943810330642
type may	1.7125697263999058
four cases	1.7962176395557086
similar task	1.6783930166760777
question would	1.6388370153723533
information theory	1.8884718196716097
two sub	1.9520377992251998
existing database	1.5445134575869865
input stream	1.8011363562987683
smoothing method	1.8397389881412238
implementation issues	1.6246289167870047
dependent information	1.6681744589725425
previous case	1.6783930166760777
low entropy	1.510060731399269
single instance	1.6392309045877975
della pietra	1.677375180101114
dependency graphs	1.64670659850785
speech technology	1.6225301985279315
interpretation function	1.6525294146794527
two clauses	1.9278258292924721
definition 3	1.9021217410675142
ten minutes	1.5433158963528077
different number	1.9127687776077462
semantic processing	2.060815299436769
desired number	1.5486945064545343
different evaluation	1.805160606107626
words x	1.5894574345512698
two characters	1.8636231056497927
4 presents	2.106809852184898
resolution procedure	1.5325928768093444
using relations	1.5156323250636554
ner system	1.629009437254469
process involves	1.7875440830649523
four systems	1.660919557194327
relative positions	1.78792816505069
task domains	1.60189988307561
task consists	1.7734751337955452
working system	1.7701367139271476
meaningful information	1.5081682632911075
subcat feature	1.561113510315716
similar patterns	1.6920919321535788
two alternatives	1.840188641106855
system offers	1.636252690439584
factors may	1.6570039092175395
structure based	1.5160155437099114
models using	1.9901963872234165
personal pronoun	1.8663164763568927
identify relations	1.5111306094308554
average results	1.6525488762732246
language using	1.7984918883977723
model describes	1.549729132180171
research prototype	1.5176055023927577
first parser	1.5556301839047324
relations rather	1.5338735539822874
target representation	1.6012319720994657
new objects	1.6753833762916979
labels assigned	1.6651708868330533
phrase occurs	1.616030265378908
different sets	2.1103245072198877
window sizes	1.732012161445492
extraction algorithm	1.8115873279601098
new questions	1.5436001111037827
sentences describing	1.5839427016111838
cardinal number	1.5290812457000524
entry contains	1.688819916257214
distinct nodes	1.530072891325321
large amounts	1.7736717273850513
world texts	1.6283503116782403
propositional attitudes	1.5176311204298327
probability associated	1.5482870987025659
preceding sentences	1.668950592668055
4 hours	1.506173888464562
model component	1.506688181945904
speech research	1.6440028676725384
parsing rules	1.5310401814047676
conversion rules	1.5472019772793426
level goals	1.5430322980848317
nodes represent	1.9451560565789985
base form	1.972998124814879
driven phrase	1.951746365904165
similarity scores	1.885190270610789
corpus frequency	1.7704218552554878
experienced users	1.5105194311609613
figure shows	1.6378945855304425
sentence john	1.8408653306709835
syntactic dependencies	1.8624442269848667
single finite	1.5288687562201928
system include	1.7400137177323636
two arguments	2.018935113156626
word hypotheses	1.7269108723697393
explicitly model	1.6845577236692315
hard problems	1.5829912568215723
individual modules	1.615262062995654
canadian hansard	1.5908167675916909
system similar	1.583051127543801
perfect agreement	1.5622818522186859
combined approach	1.6185947819602717
scoring program	1.7209969732890458
7 shows	2.1409475789462564
unsupervised algorithms	1.5282170470313718
new languages	1.8794992823759937
variation within	1.5269930689946518
central task	1.5354510398033074
end user	1.699083788671584
systems center	1.64005253972614
two models	2.105402599761893
boolean query	1.50119175880199
systems typically	1.7882649160849862
phrase must	1.7573141382441586
full noun	1.701495518973481
preliminary stage	1.5541060999723646
model seems	1.5739592165010823
based formalism	1.869785542110849
database containing	1.786911455498122
contains two	2.1132679474061793
best interpretation	1.5719513192504324
morphological rule	1.5663147349550492
positive training	1.6613960733677888
tuple g	1.5611231576018243
maximum likelihood	2.245041285244251
related documents	1.6981089970074734
production rules	1.935589642262965
many properties	1.5111306094308554
work needs	1.6977890880751751
two sequences	1.752965965122571
different predicates	1.5445134575869865
earlier approaches	1.635876749820076
representation level	1.6169986020906966
single number	1.6192219158965702
head nouns	1.877739733549975
average numbers	1.5111306094308554
coreference annotation	1.533716585829952
first condition	1.7396979552775518
use mutual	1.507007268326853
component uses	1.6741803270499043
generalized version	1.5611135103157159
language independence	1.650092453800705
karlsson et	1.6907746573665698
different characteristics	1.7637790091991556
many potential	1.6699400022790054
training procedures	1.6413904216258086
experimental evidence	1.8254300690245
algorithm using	1.9281459264095897
grained sense	1.6673459790154517
relative merits	1.6741803270499043
involves computing	1.5188051370135929
retrieval result	1.5156083168889984
whose size	1.6300799349406287
following format	1.6875904224582725
end positions	1.6013202535181605
correct antecedent	1.6861758611308448
aspectual properties	1.5238894580118152
representation structures	1.7757471807349126
oriented system	1.5320698902069676
component may	1.6112034493728662
coded rules	1.644120295137663
result shown	1.5781539692435587
descending order	1.9335108350969297
provide insight	1.6939639871030243
sparck jones	1.8183486403556997
japanese word	1.8164599993824695
determine exactly	1.5037367200448983
independent representation	1.5886561992520427
since training	1.522760613339579
provides information	1.986842940591027
subsequent discourse	1.6090815331517638
research purposes	1.7997802558487326
different argument	1.6288981647434881
decision making	1.5058898918445243
several modules	1.6875904224582725
data according	1.6802580409155776
possible parses	1.90714398465616
significant result	1.5536240126687624
definitions given	1.579873144038688
text mining	1.824546241093745
parser achieves	1.5986001912794885
controlled language	1.5937277002892225
average case	1.5521452212545803
pragmatic issues	1.5408560126058508
different approach	1.847376406540635
parameter values	1.935517997503868
two features	2.0655835461285745
arg max	1.6183566912047842
first requirement	1.526993068994652
general rule	1.9617809245397064
np pp	1.6177307567316914
3 discusses	1.8455066586684679
syntactic analysis	2.3775976114802204
world objects	1.5896353351358337
segmented corpus	1.500790229787345
input alphabet	1.5977445137418091
likely parse	1.6714604389400827
little difficulty	1.5445134575869865
model estimates	1.6375625045004876
simplest method	1.5926202003411678
earlier paper	1.6223575629464313
grammar consists	1.926239628659986
system believes	1.5624154468197915
examples show	1.914020424045213
parsing method	1.8563258761462311
task performance	1.6626867374740901
uses different	1.583051127543801
syntactic ones	1.7088092136999036
templates used	1.6487334099083149
speaker dependent	1.5790934304215294
dialogue strategy	1.5841695519446297
experiment reported	1.688260967450221
two grammar	1.5441583487573467
relation could	1.5378646275607575
bar level	1.5058369357957924
word groups	1.7307297537896829
german verb	1.6747174654307395
word co	1.9477736394982368
entire dictionary	1.50119175880199
function application	1.7170577755038738
accuracy using	1.8874401894708044
query may	1.6169986020906966
sun microsystems	1.6304481342440433
particular languages	1.6304603101475137
first uses	1.6285930450163055
short documents	1.6008079183721948
english translation	2.1498093330909445
complex forms	1.5306061495832655
precision would	1.50119175880199
resources used	1.7633404468657183
best algorithm	1.5982862705928498
evaluation function	1.685946576272113
functions may	1.6103523297384124
many uses	1.5445134575869865
system called	2.0169129402384662
additional advantage	1.742522101680745
better precision	1.8084891485636527
information access	1.76125276629201
complete constituent	1.5261764002574725
alignment system	1.5554763732047365
one form	1.5145590206075557
representation called	1.6615677176372698
procedure consists	1.613364934425665
corresponding answer	1.5275591303469285
specific rule	1.6748230375030981
sections describe	1.842361963116756
discourse segment	1.8337770132634603
relevant text	1.7183850886111571
features defined	1.672884134739408
literal translation	1.707331992422081
set would	1.7389287815275283
formed sentence	1.6764798892239177
starting point	2.0992556726530296
object denoted	1.504934031784749
phrase following	1.567922330580758
appendix b	1.835469047848923
constraint imposed	1.543442208482524
nouns within	1.5300728913253212
high inter	1.504934031784749
information explicitly	1.561123157601824
parser produces	1.8974003767752694
scale grammar	1.6141914849329484
appropriate input	1.507007268326853
section presents	1.9920892374775903
spoken words	1.5515223067572053
immediate constituents	1.7252428883643551
avoid ambiguity	1.507007268326853
foundation grant	1.6946633528721495
subject verb	1.5432756752761156
following equations	1.6661204761131605
normalization constant	1.6498057942521152
fixed set	1.6614499242834975
correct interpretation	2.004437507483967
speech system	1.9041007641644014
nodes may	1.7785412351430132
planning phase	1.5156083168889984
second person	1.8049407118437597
equal importance	1.6458797346140277
confidence intervals	1.7816629963826962
parsing could	1.5482870987025659
corpus available	1.6889191202036424
model shows	1.6514103959727489
average parsing	1.5669629240805358
testing set	1.8898671140023702
short texts	1.6291410447393342
knowledge used	1.831962157445911
defined rules	1.5256428192011384
use machine	1.816649419509691
former type	1.586137214651611
different values	2.098898424800887
precision increases	1.567922330580758
semantic processes	1.5470117266713652
adequate description	1.579873144038688
complete description	1.8885672970632739
morphological analyzers	1.7585629715715474
harabagiu et	1.7925226257570062
standard arabic	1.6186887975856459
b may	1.6500007742396734
multiple translations	1.6727958068683955
thesaurus construction	1.502914950259798
character words	1.6380954450675151
single argument	1.591823957496618
greater variety	1.5582958710952315
module generates	1.5338735539822874
tile l	1.5547189562170496
direct application	1.614343100966803
current analysis	1.615262062995654
language front	1.6266323017646809
tables 2	1.9759550152936771
basic unit	1.9225880928456067
provide answers	1.5806234267011035
typically contain	1.6783930166760777
entity x	1.5189601534336892
maximum possible	1.7355686473586731
words beginning	1.5633179841307894
estimated probability	1.754506916371315
rule requires	1.595491118313467
parsing framework	1.6826213368989764
referring expression	1.9021048717163531
performs slightly	1.6889191202036424
alignments using	1.585589015734039
underlying representation	1.779225426761594
null note	1.7635102323276162
based extraction	1.5658571165804958
two non	1.895289461523002
exact definition	1.591823957496618
remaining sentences	1.7074092050419734
capture linguistic	1.559620194736931
practical solution	1.5627352601669198
research direction	1.6462268442574868
generates one	1.5798548254880234
technique called	1.7706466568240413
related questions	1.5555254798167089
probabilities assigned	1.6432761587737845
specific dictionary	1.5177700906840073
new modules	1.5323410160957247
singular values	1.5990384780798594
resolve ambiguity	1.6717533031932033
word test	1.5552581331940458
corresponding parts	1.5378646275607575
high similarity	1.7692011214362002
automatic parsing	1.6039224906565077
tree nodes	1.9182317011571712
similarity based	1.7386623083843031
information need	1.871889387257723
following section	2.263506978751236
following stages	1.5739592165010823
linguistic descriptions	1.85733165518623
type b	1.6408206786191815
phone number	1.5952261519394824
draw inferences	1.54922150704834
simple voting	1.5098509111163707
word forms	2.1372737473165104
right contexts	1.827012968148258
syntactic differences	1.6637019978300875
ask questions	1.8794955633251966
appropriate use	1.6346135067283294
unification fails	1.6015443630283652
help users	1.7035250436174079
certain degree	1.9170134357636728
turn leads	1.579873144038688
three possibilities	1.7678099140508592
general background	1.5156323250636554
u p	1.5005157983774096
known techniques	1.5365331267030267
two paradigms	1.5559715171228279
proper treatment	1.6819762996803398
corpora consisting	1.507007268326853
verb semantics	1.6469585572624095
many computational	1.5707376783218978
possible situations	1.530984627359544
correlation coefficient	1.8380911070513994
intonation contour	1.5085420518593602
database consists	1.5320698902069676
target node	1.6212320905754414
classifiers based	1.6566290674162105
pairs like	1.742920534919623
1 proc	1.6164339756999313
canonical order	1.5898119479169228
context surrounding	1.765014364489331
oriented approach	1.702457746146822
relevant ones	1.6897328932749935
000 instances	1.524264877919955
another solution	1.6881707359732032
transition probability	1.8040305119244826
major component	1.6822401779866478
translation memories	1.5026881473115052
subordinate clause	1.9548409496466355
different variables	1.5547189562170496
structural rules	1.5834113612570344
named entities	2.205145440291078
common set	1.6977890880751751
experiments designed	1.522760613339579
chinese lexicon	1.5354510398033074
alternative interpretations	1.7582906232749491
order phenomena	1.5360785028369002
part 2	1.5286526850359072
new example	1.5006743580950452
human knowledge	1.700487206593315
important property	1.8688515388228633
5 describes	1.942671156116417
third stage	1.543438402926196
cluster analysis	1.6271028038545232
two vertices	1.5452167257960316
j n	1.5354510398033074
special value	1.5156323250636554
pattern based	1.5337196169937477
document size	1.5894574345512695
section contains	1.6699400022790054
standard maximum	1.5482870987025659
existing speech	1.5156323250636556
interesting features	1.5803694295911777
2 minutes	1.643375534486109
computational systems	1.6988177568127476
relationship among	1.8012230473134536
language query	1.720308152700639
idiomatic expressions	1.9115803564894707
resolving pronouns	1.5497180921575033
expressions must	1.5338735539822874
build models	1.5408560126058508
worth noticing	1.6073715579162529
reduce action	1.5081863832270141
algorithm converges	1.5245348740884508
fixed threshold	1.5947983930134422
extracted patterns	1.5078797213408794
two classifiers	1.6579865320789502
best result	2.0492035119715855
free grammar	2.2827569246592985
two phrase	1.5926202003411678
database interface	1.6238980180175155
data since	1.5645241345053709
concepts within	1.646226844257487
bilingual dictionaries	1.9204648827170359
2 word	1.5706734993195162
typically contains	1.5408560126058508
best solutions	1.5011426563129797
human cognition	1.568863381218478
computational lexicon	1.73121485971763
theoretic measure	1.592111614460519
semantic construction	1.5355437054013588
output text	1.8146115192009702
model developed	1.7158821441137806
project began	1.5111306094308554
sentence context	1.6787752502312567
use english	1.5771944393968738
patterns may	1.7774698676566585
form f	1.5225489364105784
various sorts	1.714925212307726
reference data	1.5302409528327585
model presented	1.8682452155090337
individual systems	1.6863341954731195
different modalities	1.583622115915778
tagging tasks	1.5893097884934126
value depends	1.5078976439458076
results shows	1.7244811761730507
grammatical form	1.617580085414291
multiple candidates	1.5256428192011384
different nature	1.7219655600972597
european language	1.7142684963020163
best hypothesis	1.7649572087165113
tense information	1.5127787034482862
trained model	1.7775964276738212
basic units	1.8238474269737694
tile verb	1.6317923193893897
one derivation	1.641285789456548
feminine singular	1.5157730047675453
three things	1.574957421416773
abductive reasoning	1.5248266889894415
software developed	1.5338735539822874
correct part	1.704491480573413
relations associated	1.6031224303496519
word ends	1.5452167257960316
processing must	1.5706734993195162
extracting relevant	1.5408560126058508
clusters using	1.5378646275607575
development data	1.9569670637471843
fundamental question	1.5306061495832655
implicit relations	1.5363608633412007
sense definitions	1.6690096832364878
different subcategorization	1.561123157601824
different constraints	1.7394743475391299
two paragraphs	1.5912901202317884
3 evaluation	1.6594363061757056
single occurrence	1.6753114229441863
assigning weights	1.5111306094308554
texts written	1.720650346539263
actually use	1.643362624369706
speaker must	1.6608932610754579
run test	1.5261764002574725
limited amounts	1.5354510398033074
parsing accuracy	1.949924858424563
types used	1.6614869878819327
link grammar	1.5777654587561893
bilingual data	1.5794376860042763
value matrices	1.6033675450432086
probabilistic classifier	1.5116492197545166
statistical systems	1.6604827252530852
english counterparts	1.561113510315716
additional work	1.7680599214474209
words immediately	1.6354102761745795
experiments performed	1.7924490323696567
negative score	1.5216788181120133
two pairs	1.8743724750896975
last n	1.5395099438048585
using words	1.8248139332291624
enough data	1.885632474046826
complete interpretation	1.587161501621602
better representation	1.586137214651611
limited information	1.5338735539822874
linguistic constituents	1.5000891415647069
probabilities used	1.586137214651611
unseen texts	1.5525978349470972
run time	1.9256413782316133
full training	1.6140324891645537
statistical parsers	1.8620943338240532
use exactly	1.5739592165010823
entire collection	1.6213967902634534
identify words	1.6904151818990054
head position	1.570817932703902
discriminative training	1.6999320797471773
system often	1.5645241345053709
100 sentences	1.9162588716558486
particular example	1.7669373105292956
naive bayes	1.9887040406023153
corpus contains	2.196643057332306
illustrative examples	1.6904151818990054
systems rely	1.8131196934301124
several systems	1.833104607155818
single letter	1.6212113212773736
present examples	1.5408560126058508
two constructions	1.536533126703027
become clearer	1.522760613339579
arbitrary size	1.5378646275607575
complete problem	1.6244843928316635
simple mechanism	1.6339957705783554
f r	1.5446962575183827
others use	1.5338735539822874
minimal pairs	1.5427684814848859
semantic class	2.0832334340206744
construction algorithm	1.5321142461731552
second term	1.5642592671401674
third sentence	1.7729602264830036
semantic preferences	1.6269188335679583
important reason	1.6388370153723533
identity relation	1.6575643619042633
lexical dependencies	1.6267989877588875
hmm based	1.5508853729212349
expression would	1.5111306094308554
systems must	1.9711637366341581
target side	1.6433803812747936
transitive closure	1.9870697002231827
text could	1.6339957705783554
semantic parsers	1.5416041062057482
verb cannot	1.5378646275607575
head daughter	1.7392347141138897
art speech	1.547363551399276
two sources	1.9625084030527264
distinct categories	1.561123157601824
multiple representations	1.55955788508706
unknown words	2.1736523896419673
polynomial kernels	1.5563893320981306
given form	1.5035193201428414
inactive edges	1.603592980349597
word vector	1.592608335952678
text form	1.5839427016111838
computational properties	1.7802096062349688
running words	1.6964883688870667
technical papers	1.5452167257960316
whole string	1.5425370803688765
work focuses	1.7471845324497086
among alternatives	1.613624660358136
sense defined	1.6031224303496519
briefly sketch	1.5547189562170496
items may	1.7019709298887997
correct result	1.741906694102004
several kinds	1.6362596602597854
using multiple	1.8541485878248753
basic premise	1.5188051370135929
computational lexicons	1.6361023996199326
section 7	2.237958595270582
general phenomenon	1.5445134575869865
present stage	1.60913070763407
n terms	1.5408560126058508
two slots	1.7080705130246854
models include	1.530072891325321
first applies	1.5338735539822874
two feature	1.9495192649222477
new problems	1.6601468074635917
abstract specification	1.507007268326853
additional criteria	1.5408560126058508
naval research	2.0395761687929355
n candidates	1.5078786620077462
provide strong	1.5991584292605263
six sentences	1.5111306094308554
efficient algorithms	1.7854998106931832
formation rules	1.722918576163485
corpus sizes	1.6371439398153491
rules automatically	1.6663716312166414
matched words	1.5420564998392265
main noun	1.533540614646662
tree based	1.7577834516773174
though see	1.5111306094308554
positive integers	1.6525488762732246
probability score	1.5648923336080593
initial sentence	1.6328065464514752
good overview	1.5893265024752146
full stop	1.6819762996803398
language input	1.9928852282938494
larger test	1.6785607164239145
model language	1.6152953234723224
syntax trees	1.7521114080383025
coverage syntactic	1.522760613339579
informational content	1.6624358603626326
section 5	2.6151690140054713
abbreviated form	1.6244843928316637
results presented	2.0812468039021583
word often	1.5547189562170496
est de	1.5519900193858673
confusion matrix	1.8130512949670028
cunningham et	1.686412032849605
document frequencies	1.5139048726835678
constraint relaxation	1.5308884464400971
general cases	1.5378646275607575
different probability	1.5582958710952315
efficient use	1.6978216692284374
one item	1.8030405461053676
new corpus	1.886499439843196
second question	1.8195133117873088
complete algorithm	1.5482870987025659
contains examples	1.5195036419368688
level analysis	1.7478446040037883
determines whether	1.993984855250439
english glosses	1.6010101084822788
possible types	1.801646141161678
classification based	1.7620599677495374
tilburg university	1.5037367200448983
certain sense	1.6594660533960885
pronoun would	1.5177700906840073
recent work	2.3214260811598844
current models	1.6482958236884362
structural relationships	1.6455974214186457
node types	1.5044920645792557
l 1	1.8530048917746342
wu et	1.6081662320033794
parent node	1.9569849712056442
new cases	1.5211544130435135
per se	2.0659533608458727
model instead	1.5338735539822874
target strings	1.5419819962962804
positive instances	1.6238426617087383
statistical properties	1.704158995192193
special treatment	1.894314320743816
phrasal nodes	1.515071773152174
target string	1.6769537539098294
formal definition	1.9760496602751316
tutoring systems	1.6524417213411828
sentence extraction	1.8362808865067388
open source	1.8313424813159926
good evidence	1.6288981647434881
given e	1.530072891325321
traditional model	1.5112595743239683
processes may	1.6210934563561632
present tense	2.01175613883513
syntactic relationship	1.7351954361127877
values using	1.5611231576018243
rule np	1.6012319720994657
underlying concepts	1.55955788508706
full np	1.6149358804791452
pattern matching	2.202865440304288
estimation problem	1.5736418253029636
notable exceptions	1.62412689036662
following information	1.9898167743735793
error messages	1.7536013048250572
feature indicates	1.579984064454253
lexical analysis	1.9145287213837145
computational grammars	1.5482870987025659
joint probability	2.0768805786558
language question	1.7661471221969767
properties may	1.579873144038688
speech production	1.690421513819425
search path	1.5297905781299383
tile correct	1.5156323250636554
esprit project	1.648295823688436
alignment error	1.7194265517437857
several properties	1.6844174045708415
e h	1.5492796887772666
henceforth referred	1.5408560126058508
preceding verb	1.533540614646662
recall measure	1.592111614460519
information processing	1.808509144065478
determining word	1.510181823566108
l x	1.6168481229255143
semantic properties	2.0706989486704965
certain sequences	1.5338735539822874
sensitive rules	1.5809909660275805
document must	1.5408560126058508
one segmentation	1.5645241345053709
simple idea	1.5887468924062063
best alignment	1.716865876952985
subsequent stages	1.5914442382457208
translation directions	1.5646997715483684
graceful degradation	1.5037367200448983
random fields	1.9173968087295477
models use	1.7127967389448444
discourse situation	1.6740987854424878
information requested	1.5647847313011076
component also	1.5645241345053709
pragmatic considerations	1.5177700906840073
one minute	1.5560225527864453
semantic lexicons	1.6625465322025115
linguistic significance	1.50119175880199
parallel english	1.5731122769149348
anaphoric pronoun	1.6005251959718443
earlier versions	1.9505059694056042
additional benefit	1.5645241345053709
prepositional phrases	2.2819281703259002
sentences without	1.8506108235449696
semantic labels	1.7589893314281444
full context	1.5823061650599628
based measures	1.675449195183426
nsf grant	2.0936282917955444
alternative word	1.6197369406879945
spatial relation	1.5053264715851915
particular problems	1.6761656524213184
driven machine	1.6204259949555537
sentence recognition	1.6330340284286617
noun senses	1.63576635409367
classification method	1.7541864386326451
sentence set	1.5672493004697094
system responds	1.595491118313467
idf scores	1.586879687256495
action verbs	1.5819409893846843
description must	1.5195036419368686
left end	1.5525993619477707
major difference	1.9168461441931162
particular phenomenon	1.5378646275607575
grant n66001	1.5547189562170496
syntactic realization	1.7977452495084507
critical role	1.6056035510036821
using domain	1.7472755049001094
planning stage	1.5838923815193273
basic concept	1.553938716092748
improving precision	1.5378646275607575
class given	1.5195036419368686
way would	1.6742047620193485
new insights	1.5547189562170496
underlying theory	1.507007268326853
ones shown	1.5445134575869865
formed input	1.7302218495284098
relational structure	1.578293538012307
find evidence	1.5798548254880234
two steps	2.2875867706760777
new parser	1.5530428448904365
node b	1.6347630951884349
current grammar	1.7577841445947628
seconds per	1.7940656875234058
different interpretation	1.6502761822926497
two variants	1.7536508550451237
estimation algorithm	1.6730322489664582
constraint violation	1.5886033269553643
scoring systems	1.521240524634107
posterior probabilities	1.5881789415768794
similar rules	1.6190328102134979
vogel et	1.7388385917373101
unification process	1.6250542010226754
generate sentences	1.794038126257714
chinese character	1.9216611422326446
repeat step	1.5869693547698929
also point	1.579873144038688
logical formula	1.7293730750456715
experiment 1	1.7757568479545967
parse would	1.55955788508706
value returned	1.5320698902069676
help much	1.5338735539822874
domain ontology	1.6032025572397257
using translation	1.5277988432833451
pronominal references	1.5421252919496695
constituent words	1.670187912810662
role labeling	1.8351174686346663
particular cases	1.757636103678803
simple left	1.5111306094308554
different problem	1.667913900271795
connected graph	1.6034580092758848
driven approach	1.8414044422275277
verb pair	1.5763955456335523
correct alignments	1.5361450492096225
type described	1.6411558246537032
particular parse	1.5706734993195162
word similarity	1.8057892604949184
closer examination	1.7501491085716443
7 proc	1.62412689036662
order logic	1.9628736883761342
builds upon	1.65303397348486
verb argument	1.678704349785014
nigel grammar	1.5332563118031661
semantic grammars	1.7473352007499254
online dictionaries	1.5249668733888961
strategy described	1.689248192241815
words preceding	1.6570039092175395
integrate different	1.567922330580758
resulting algorithm	1.5992286050546423
associated set	1.5798548254880234
limited use	1.6761656524213184
dotted lines	1.883307840026958
special symbols	1.8452657797160954
output layer	1.6010607276032265
added advantage	1.6002993454155385
many ambiguities	1.641155824653703
church et	1.6920866194646296
approaches use	1.7663553975095576
particular class	1.8595424009049804
speci ed	1.656504375279551
transcription system	1.5013915081570173
likelihood estimator	1.651883758794906
interesting feature	1.5739592165010823
three fields	1.5245348740884508
lexical differences	1.5539258376750889
entries whose	1.522760613339579
first word	2.1898851593397755
level syntactic	1.7630919621633117
inter alia	1.878355172747358
cases one	1.5338735539822874
readable form	1.844819599494218
many word	1.863845893077596
practical method	1.542683278128981
multiple documents	1.7560814121157118
greatly simplifies	1.5887468924062063
string concatenation	1.561596045888415
processing problems	1.756257461041527
requires knowledge	1.6565475758379593
paper described	1.6580606251764172
high school	1.750837853748753
phonological features	1.5080184210155185
example set	1.546192678102801
corpus data	2.037592107652305
works well	2.06240076780991
step consists	1.770963117017772
phrase may	1.8563280708628236
sentence without	1.8662893736529487
two components	1.9174872258279776
correct alignment	1.6230934124465777
different type	1.6204118812134443
position paper	1.5613081195452279
three methods	1.9537082318263037
system utterances	1.6027857672282373
order variation	1.6403906950213272
verbs used	1.6210934563561632
features without	1.6706426105940735
scoring procedure	1.5763955456335523
attractive alternative	1.522760613339579
certain points	1.5408560126058508
interpretation system	1.6201491310278733
treebank data	1.7908666335581231
constant value	1.6388370153723533
statistical decision	1.5172802553801839
section discusses	1.911058275487346
speakers use	1.6288981647434881
one individual	1.6303444022729265
semantic tag	1.718177825296683
mass nouns	1.5141389127710934
line text	1.5781539692435587
four word	1.5412463844444777
users often	1.6661204761131607
experience gained	1.6388370153723533
preliminary test	1.5445134575869865
strong association	1.5177700906840073
high level	1.6144787149026294
full semantic	1.700334639536396
key elements	1.6392309045877975
another method	1.8484308050538747
evaluate whether	1.743960294459502
occurs within	1.7907931838134399
rule templates	1.5940165361099639
simple structures	1.5078976439458078
discourse situations	1.5371982986173842
every verb	1.749310794708903
two advantages	1.8131196934301124
words spoken	1.5676517392350746
alphanumeric characters	1.522760613339579
standard way	1.8893182468313676
system scores	1.6342922036019591
made explicit	2.0293799319701624
framework proposed	1.5798548254880234
dans le	1.742970017287004
correct prediction	1.5216788181120133
essential component	1.6699400022790052
ill tile	1.826805702671581
p n	1.5306257931767848
robust syntactic	1.5422431186751524
hundred words	1.6635555237714856
paper presented	1.797413685506606
following structures	1.5547189562170496
existing modules	1.536533126703027
similar method	1.7261608475873556
text preprocessing	1.5217987622876712
tokens within	1.5037367200448983
system performed	1.7916908124409407
word trigram	1.5529268595099772
performance improves	1.7352337189917786
word meanings	1.9320350937506252
require significant	1.579873144038688
different environments	1.5482870987025659
optimization procedure	1.5620473238340082
ir system	1.785952262603625
performance could	1.7105799695961408
c f	1.6076225029788151
speech taggers	1.9231239301035425
resolving anaphora	1.5487453399078306
different target	1.7098259615519555
different attributes	1.6904151818990054
interesting research	1.6761656524213182
prediction accuracy	1.678982628232583
data using	2.1018260353766554
agent must	1.5941380317239633
statistical evidence	1.6491433207766195
target texts	1.6425117834813938
development sets	1.6227790810587186
time information	1.6485570859378273
intonational phrase	1.5134955355622144
english document	1.5309694935737626
insufficient information	1.5321552374951575
prolog program	1.6698694973445618
long documents	1.5892420313344588
performance due	1.5706734993195162
using support	1.6991558271280565
grammatical relations	2.0169786591499514
specific tasks	1.8089416812772368
inheritance hierarchies	1.6071721105629617
given item	1.568863381218478
scale system	1.5611231576018243
different part	1.7989632097491328
city names	1.5780001028501922
functional relations	1.5518616355678834
explicit information	1.750700291476916
possible relation	1.509778922004791
n documents	1.5243712845878497
symbols representing	1.5188051370135929
many projects	1.530072891325321
rhetorical relation	1.7333727270175707
single type	1.7155698049757158
node v	1.557044774738748
previous algorithms	1.6694507637109806
also contributes	1.6164339756999313
trees generated	1.7627626262570735
every model	1.5037367200448983
class c	1.5835515301326213
parsers use	1.5111306094308554
new constraints	1.6520960693732363
bigram statistics	1.524009335643489
also captures	1.5511943810330642
category assignments	1.5810686572743542
approach seems	1.8432267974189886
grammar contains	1.9184031471510579
possible ways	1.8057880139587805
acquire information	1.6037378902512214
curly brackets	1.7175590799366294
among sentences	1.593094242776905
relative time	1.5676517392350746
null based	1.591823957496618
architecture described	1.6276749302936424
search results	1.7553479024797447
occur often	1.6084968011212868
class distribution	1.579115343332791
evaluation tools	1.5381766791708387
minimal number	1.8531498882617803
long texts	1.50812972873744
truth conditions	1.8258294356892601
default settings	1.7002541582984279
semantic roles	2.04547608122146
frequent sense	1.835879948109625
natural choice	1.7392306202979373
syntactic ambiguity	1.9181940769993007
new tag	1.6283489053598212
adequate representation	1.5749574214167732
japanese sentence	1.9258602405298573
rule set	1.9380279593805767
format used	1.641155824653703
works best	1.6303444022729265
classification accuracy	1.9793287320403947
already part	1.5195036419368686
local focus	1.561323572972813
full set	2.0454626131426776
correct meaning	1.7163305096180406
another noun	1.776409225489638
main phases	1.5188051370135929
certain sentences	1.6002993454155385
one unit	1.7478172174640592
text segmentation	1.758578190312213
inheritance network	1.5137835599531777
poisson distribution	1.51614556732755
available training	1.8499065949348492
takezawa et	1.506173888464562
framenet project	1.6276279425990943
straightforward extension	1.561123157601824
one piece	1.6624358603626326
rules learned	1.7230001400758461
partial function	1.7539490042539498
computer interface	1.5854098583082799
functions described	1.5395099438048587
framework allows	1.6862623517634696
hopkins university	1.62291510937581
computational point	1.8099970622572505
probability estimation	1.7812783113727262
one candidate	1.8504809195213372
pcfg parser	1.550030603205738
using svms	1.5518962819518507
threshold used	1.5288687562201928
definition 4	1.8129994453669467
recall value	1.5195036419368686
modified algorithm	1.5156323250636554
grows linearly	1.5188051370135929
x l	1.7214064927584276
single target	1.5676517392350746
tile data	1.507007268326853
28 aoi	1.5156323250636554
slot fillers	1.7900112209723853
modeling approach	1.6954968285914076
alignment problem	1.6915611422434043
dictionary entries	2.090007513093834
best fits	1.5739592165010823
system 2	1.5553381419921846
semantic interpretations	1.9137451374297707
approach outlined	1.6213967902634534
entire utterance	1.6631574457448046
vocabulary size	1.9603904195074502
among linguists	1.5887468924062063
one daughter	1.6040455042669346
simple task	1.7352337189917786
node labelled	1.614659765166461
decision procedure	1.7186544201067346
analyses may	1.507007268326853
one clause	1.7833521513756208
words need	1.530072891325321
either word	1.6527294781610564
since none	1.7280057513570362
many classes	1.5547189562170496
tabular form	1.5482870987025659
direct representation	1.5269930689946518
cannot contain	1.5908167675916909
correct translations	1.8013724063699668
single part	1.5749574214167732
van halteren	1.6553281093402652
various algorithms	1.6358566425614818
output alphabet	1.5422431186751526
two machine	1.6184033873746562
different solutions	1.5819174715567357
search time	1.6746990046895085
various subsets	1.5582958710952315
target audience	1.5156323250636556
il n	1.549822802486843
eric brill	1.6860620732232783
different functions	1.771679799419203
evaluation system	1.5666280910126886
surprising since	1.6881707359732032
two characteristics	1.615262062995654
morphological structure	1.790742341348155
minimal recursion	1.6547580977333693
flight number	1.5698616090409279
class lexical	1.5904674613324936
research issues	1.7202947334696643
natural spoken	1.554089533378209
human computer	1.5596201947369313
agreement features	1.778576240546131
two edges	1.6860346793346626
predefined threshold	1.68710839601938
simple patterns	1.5316566430519976
brute force	1.6580606251764172
achieves good	1.5547189562170496
verb subcategorization	1.740806692836292
rules used	2.1013783232948
top node	1.861953169221852
certain constructions	1.646226844257487
structure generated	1.5771944393968738
different weighting	1.567922330580758
simple declarative	1.7294947296982195
using tf	1.5079211428921846
surface syntactic	1.9321934794851197
labeling task	1.6537921811015623
chinese dictionary	1.5671121232087257
using training	1.6773210626499964
pilot experiments	1.597118703840822
better recall	1.714469744557702
lower one	1.583051127543801
entire structure	1.5395099438048585
second criterion	1.5559715171228279
two actions	1.6635378650610795
finite alphabet	1.6804257391228379
question words	1.5695669737741826
lfg grammar	1.5911063374978154
crucial step	1.6315901311540408
four classes	1.8663171222838264
across documents	1.7167077142673628
strong indicator	1.5111306094308554
word probabilities	1.6366875611456781
first k	1.600091704657666
extraneous information	1.50119175880199
important function	1.530072891325321
state p	1.5080938029506297
parallel data	1.6212092227000516
primary importance	1.5188051370135929
linguistic input	1.5683131564842077
based dependency	1.5153719705776623
target languages	2.0925145947269224
automatic term	1.5890834760230876
information attached	1.60913070763407
pedersen et	1.5066965250642077
unsupervised methods	1.8300293124345473
particular tag	1.5132001828256558
retrieved documents	1.8901491074739079
modular system	1.6864169205234394
six times	1.6502761822926497
use x	1.5111306094308554
time increases	1.5269930689946518
background noise	1.709126960698477
database contains	1.7954908971452508
french grammar	1.6428721706698721
information produced	1.6192219158965697
selected set	1.6774002949116094
second section	1.719365286993631
alignment algorithm	1.8971414647088294
approach shows	1.6056035510036821
require knowledge	1.6502761822926497
equation 2	1.95459817489796
second pass	1.8827950853684
expressions may	1.7179482056291908
various forms	1.6631154426095351
phrasal categories	1.676795371843493
specific value	1.5798548254880234
representation based	1.7088092136999036
discourse entities	1.846438273542385
sections 3	2.045026114719124
differ substantially	1.567922330580758
working hypothesis	1.7409385343750605
variable name	1.5869693547698929
every grammar	1.5794243998957866
grammar formalisms	2.0439089748894625
search tree	1.5749525076754054
likelihood ratio	1.929611612229937
generator must	1.732870708273847
results compare	1.594773126307433
limited resources	1.7099170221540458
system includes	1.9181509317283783
application domain	2.0588415187229008
physical objects	1.846914968836328
important role	2.1179511758391665
hacioglu et	1.5166881074908263
single template	1.5381766791708387
reverse direction	1.7496843400567599
general types	1.6548911840144862
containing information	1.8829589737924446
hmm training	1.5847359287102778
database queries	1.7622550387514722
system might	1.619991428666353
wide use	1.5156323250636556
hierarchical discourse	1.5500434040293412
result obtained	1.7377714803473596
two phrases	1.8765503834851485
language acquisition	1.95210877007309
surface lexical	1.5035193201428414
kipper et	1.539402299280811
connected components	1.5907551918457306
syntactic analyzer	1.836989514770779
participants could	1.5360785028369002
two individuals	1.579873144038688
input signal	1.5656419863875413
speaker uses	1.6563725150211703
second solution	1.583942701611184
contextual information	2.2348231951714697
source information	1.5296567222481663
processing may	1.6339957705783554
loan words	1.6184860705286002
manner described	1.7458878463811769
cannot generate	1.6566290674162105
de nitions	1.5175943584498262
system returns	1.7084259213367536
4 demonstrates	1.5111306094308554
case 2	1.7725725457172743
one copy	1.5459182428709468
specific applications	1.7983846485220092
simple linguistic	1.6056035510036821
textual information	1.8319409256896477
application specific	1.6375625045004873
existing corpus	1.6392309045877975
recognized word	1.6554558104052497
automatic question	1.5525419192763321
training sentences	2.024093358803698
speaker may	1.83938958324203
information status	1.5759934547301855
noisy environments	1.5177700906840073
specific entities	1.5190584552425839
specific types	1.8971638498080303
every object	1.6188354007829795
processing community	1.7139483293868785
human evaluation	1.6860280137600823
novel method	1.8422626646634517
also address	1.62412689036662
average agreement	1.5425370803688765
initial test	1.5739592165010823
express constraints	1.6031224303496519
automatic labeling	1.5461586260253584
linguistic realizations	1.5921441336688504
input features	1.7971189091711852
written discourse	1.6125713976431846
based model	2.0577796781843958
appropriate rules	1.567922330580758
direct correspondence	1.6840631149502636
special case	2.2852282465067684
logical expression	1.6389649086961617
exactly match	1.7151558643929241
important features	1.9199882846489293
distance measure	1.8570418355403273
possible discourse	1.5046383591841703
every non	1.7105799695961408
occurrence statistics	1.8813884242373955
base noun	1.7256154806080066
infinite loop	1.5381766791708387
specific event	1.5620473238340082
essential difference	1.6481174069026685
increasing complexity	1.6458797346140277
summarization evaluation	1.6540013354356922
formal structure	1.6375625045004876
linguistic forms	1.7468138750349271
exact matches	1.801846402467971
limited scope	1.50119175880199
existing systems	1.9177481001988386
word would	1.7663553975095576
subcategorization frame	1.7976669176762679
length two	1.5705757689872493
much knowledge	1.5188051370135929
atomic units	1.533540614646662
empirical observation	1.579873144038688
model information	1.6115694852294897
tagging tool	1.500089141564707
external world	1.5269930689946518
two techniques	1.8271623123146994
separate rules	1.5771944393968738
overall model	1.5693534912382987
represent relations	1.5188051370135929
boundary markers	1.6188076983090254
ir techniques	1.6365810983372249
like data	1.5820198549378146
systems may	1.9130030586457687
machine interaction	1.7554478824032616
existing parsers	1.5037367200448983
empirical work	1.7864780926986454
algorithm correctly	1.6034580092758846
correct choice	1.714890949394713
still others	1.641155824653703
enables one	1.6388370153723533
common sense	1.7964883159379883
existing lexicon	1.5423485783857451
par les	1.566944886416017
see figures	1.6778478586578416
present results	2.106873601700568
japanese verbs	1.6117201772271024
individual word	1.9397953705907547
system improves	1.5188051370135929
different annotation	1.6513805653893878
two extreme	1.583051127543801
magnini et	1.554733948668867
standard english	1.6840790352723043
system designer	1.6278087910291235
uses linguistic	1.5338735539822874
system could	1.9478879727986556
reference translations	1.7928146856630967
ambiguous sentences	1.8307914123652291
human intuition	1.6500000848344394
phrase attachments	1.5448475135077877
work presents	1.5511943810330642
additional processing	1.6553530875798104
first name	1.769931784337186
briefly explain	1.5634115707207186
current performance	1.5926202003411678
information along	1.6002993454155385
arrival time	1.570720417519241
author wishes	1.6213967902634532
recursive process	1.6358566425614818
mixture model	1.6653718234797417
leacock et	1.586966988841553
technical domains	1.708737834943295
likelihood estimate	1.9023062560279613
using context	1.9014086427103216
based models	2.0791300154669283
sentence analysis	1.894891776548424
study described	1.5338735539822874
network representing	1.5904771784117462
overall quality	1.7336402948431133
approach presented	1.9694398748583257
explicitly encode	1.5408560126058508
trec collection	1.588669192462068
grammar assigns	1.6320354670147565
whose value	2.0043561894283557
disambiguation problem	1.732828128611633
subcategorization information	1.8507413333018894
greedy approach	1.506173888464562
provide reliable	1.5887468924062063
level goal	1.63707671970603
different phenomena	1.6429565433589586
bigram probabilities	1.7420865113041721
5 proc	1.6388370153723533
dimensional array	1.57425918739941
two variables	1.8735049119823328
input list	1.5240167120680814
adjacent characters	1.507007268326853
steps involved	1.5794243998957866
tagging results	1.7022567868799465
representation framework	1.608299184330961
standard techniques	1.7309723544238431
different alignment	1.524966873388896
word level	2.1603679484722518
trained system	1.507808027593451
algorithm calculates	1.50119175880199
two columns	1.8159078665198356
different usages	1.5387082898744961
set based	1.6545874861827639
gives rise	2.150895688396689
text shown	1.5288687562201928
management task	1.713101029060975
sense tags	1.735910615434717
statistical word	1.6908080392098936
cosine similarity	1.919826994899775
general treatment	1.636252690439584
given position	1.692742853608829
characters used	1.5269930689946518
syntactic constituency	1.5485015156704642
across languages	2.0652021765182376
hierarchical tree	1.6107689014994975
research field	1.526993068994652
complex reasoning	1.530072891325321
semantic space	1.6937489420923244
following requirements	1.6303444022729265
computer scientists	1.7820132115668372
000 features	1.5216148258836173
morphological component	1.7167225033608038
mean average	1.5858824131299063
given candidate	1.524534874088451
missing information	1.9242471866279391
general principle	1.8302786432793052
every phrase	1.6190328102134979
system developers	1.8442925475023655
french sentences	1.7414212646360907
verb like	1.8570076544238716
matching terms	1.5425370803688765
similar nouns	1.525702702006778
semantic ambiguities	1.777414274128868
large search	1.6315901311540408
restricted domains	1.766134102534036
news data	1.667561409728526
lob corpus	1.6813962685589825
several word	1.7141428077458374
mixture models	1.5518862198820988
method cannot	1.7266279442877566
valuable suggestions	1.6315901311540408
elementary trees	1.958760452945283
practical issues	1.5412463844444777
sole purpose	1.5445134575869865
compilation process	1.7214241886826294
technique could	1.522760613339579
semantic word	1.6169986020906966
structural position	1.5112595743239683
barbara grosz	1.6276749302936424
one pattern	1.7053455296190099
two queries	1.5390877051636749
contemporary english	1.9692535699412093
distinctions among	1.7349545517744833
multilingual information	1.6853985325523189
certain way	1.7504812345368606
undirected graph	1.6301237342253352
based features	1.8086887620761134
new methodology	1.5037367200448986
possible case	1.5881174695673654
word form	2.0423136567877584
attention recently	1.522760613339579
infinitival complements	1.5548975154096096
corresponding node	1.7344099527418115
system intended	1.5739592165010823
art systems	1.771319797872092
conceptual dependency	1.7899437287775173
looking centers	1.6297125352122612
possible source	1.609217242875463
system helps	1.5037367200448983
strict sense	1.7260483072831927
network consists	1.6635555237714856
pairwise comparison	1.5647505415436789
disambiguation algorithm	1.7661094806630033
type 3	1.6329505981836854
model would	2.042046619062214
electronic mail	1.6853874840250063
possible tree	1.511205120616144
guiding principle	1.6289349127929258
translation direction	1.5286729971585589
discourse component	1.5989354457130955
static knowledge	1.6389308261513178
contains various	1.5739592165010823
based grammar	2.0706417543915805
low performance	1.6734445616305185
ungrammatical sentence	1.5634115707207186
term translation	1.5637595923192298
definition 2	1.9443514502342922
grammatical words	1.5635165472794155
specific parameters	1.5622818522186854
work reported	2.202404712506118
present case	1.737796564580819
subject argument	1.5425370803688765
bahl et	1.5884331104408758
written language	1.9220200746595557
two parts	2.1598816968937715
de souza	1.5595834466874932
evaluation techniques	1.6755027241738167
pp attachment	1.7832747469036332
two derivations	1.5423485783857447
temporal sequence	1.5841898575231856
events occur	1.544919098524184
functional words	1.8569179059631928
threshold value	1.9328219109466989
analysis given	1.5111306094308554
lfg f	1.5563443384161162
corresponding concepts	1.5893265024752146
three relations	1.7017362832860266
shed light	1.7527766729997913
similar training	1.5188051370135929
context provides	1.5547189562170496
following relations	1.6741803270499043
third column	1.9942185067882647
bottom right	1.5740300543427277
parameters must	1.5245348740884508
introduce another	1.5634115707207186
one domain	1.9592778576677594
extracting features	1.5459182428709468
defined set	1.7436718516271021
surface realization	1.893051211774665
certain advantages	1.5445134575869865
inflected forms	2.0244680319632575
optimal threshold	1.5153398292144522
appropriate form	1.643362624369706
2 describes	2.1380566277906152
initiative dialogue	1.6169382321747343
processes described	1.5408560126058508
one concept	1.803347822751804
auxiliary trees	1.7733522854476158
decision trees	2.113321691216935
capitalized words	1.6758017973211246
relations among	2.199630064956496
candidate pair	1.585842824175356
based n	1.5604986974461108
verb type	1.6268765913277026
tile number	1.7649639737737153
outside probabilities	1.5882307829655486
problem might	1.6315901311540408
first constraint	1.5378646275607575
limiting factor	1.6838006798046168
many examples	1.919283286772654
lexical ambiguity	2.0603127230238902
driven algorithm	1.5176290836492585
di erence	1.564043960528404
input phrase	1.5539258376750889
helpful information	1.5164911279959081
text generated	1.6012319720994657
simple solution	1.6904151818990054
require considerable	1.522760613339579
efficiency considerations	1.530072891325321
tagger uses	1.65545581040525
independent modules	1.6648444578938926
speech act	2.003333048343351
translation rules	1.7342147885028076
marginal distribution	1.5704284771841746
different means	1.5611231576018243
one attribute	1.6314239324896485
discourse relation	1.7317136024828876
previous section	2.4794789455015622
describe different	1.5188051370135929
nonterminal label	1.5808650697414093
performs quite	1.6169986020906966
one translation	1.8507001531437195
lexicon construction	1.6113346817822476
different meaning	1.8451213361268148
words must	1.8434371967215166
value decomposition	1.8598831043151014
text units	1.5705898016963538
bootstrapping techniques	1.511205120616144
verbal arguments	1.594773126307433
japanese input	1.50812972873744
module based	1.504934031784749
temporal order	1.7314378501429357
learning experiments	1.7663863953410956
first stage	1.8150499845629184
ai research	1.6379377001181807
x n	1.8413387031811441
string project	1.5851197519504596
one element	2.0541751833550577
several projects	1.613624660358136
final translation	1.7381289144710084
project funded	1.567922330580758
documents based	1.7405444483919017
conceptual model	1.6804963160793744
new word	2.0608228138311135
users might	1.6169986020906966
remaining data	1.5771944393968738
every feature	1.825113443250267
pattern set	1.565496474129679
ratio test	1.5284752395235688
whenever one	1.579873144038688
translation based	1.669489306700537
viterbi algorithm	2.070990808410752
phrases occurring	1.5656419863875413
training iteration	1.547363551399276
matching process	1.9462642755786659
computational linguist	1.7201928124143495
times higher	1.5381766791708387
answer keys	1.7379921754844758
specific relation	1.636127944690715
theoretical results	1.5949863093557926
given words	1.5847713660620253
inference techniques	1.6121742428060308
let z	1.6289349127929258
matching problem	1.6249726722509745
semantic definition	1.6288981647434881
partial understanding	1.6093977315174843
constant k	1.5536697401931043
work supported	1.9283067477181333
low frequency	2.0524510407337804
generate english	1.5887468924062063
lexicon provides	1.6244843928316635
query using	1.6202338249864887
language 2	1.5219618966045703
natural languages	2.220623093989417
system actually	1.613624660358136
amongst others	1.585589015734039
system developer	1.613892885330051
three individual	1.5087853581980926
time complexity	2.052291592453635
two taggers	1.5045673983899333
research goals	1.6342922036019591
chapter 6	1.5622818522186857
word combination	1.6012319720994657
much improvement	1.5582958710952315
two agents	1.6740527601268402
new topics	1.6525235650816221
rather ad	1.5111306094308554
translation engine	1.5345098963060382
problem could	1.75172424162674
two dictionaries	1.7147619942696686
performance significantly	1.7396979552775518
full analysis	1.6266256462651403
speech annotation	1.5611135103157159
occurrence probabilities	1.6427542586937922
g e	1.5904622447173336
one interesting	1.8595568985950035
features obtained	1.5731122769149348
recognition rules	1.5146428400538865
correct spelling	1.6157528360591624
filled templates	1.5249942996423669
right parser	1.576912126201492
whose parameters	1.642872170669872
type person	1.5941108460767777
charniak et	1.7329416757513232
precedence relations	1.7063598944163538
intervening words	1.596777818958647
language families	1.6954644009370834
surface sentences	1.5901164719986516
nodes whose	1.6933483923290389
produce translations	1.592620200341168
documents retrieved	1.8087586226215773
contain several	1.7294947296982195
sense selection	1.5664784933720601
algorithm finds	1.804725963492348
open question	1.7512099823518086
another context	1.6785607164239145
discourse module	1.5122897955043961
various possibilities	1.6527294781610564
compound terms	1.5322672086026357
free variable	1.6856095114250396
mandarin chinese	1.7101274412369127
markov modeling	1.5335731739369802
efficient search	1.7583888411615844
em algorithm	1.9656061501516737
future work	2.557560041989532
method relies	1.7482442165567584
human error	1.6039224906565077
translation method	1.7167638954741653
time requirements	1.5690244007917742
third section	1.7455319583030278
whole class	1.6761656524213184
involves finding	1.683041275722836
semantic classes	2.135874314399792
possible senses	1.844870044069656
based phrase	1.55955788508706
figure 12	1.9335391793400276
input feature	1.6324852581312168
results similar	1.5408560126058508
attribute value	1.866622626382894
another action	1.5731122769149346
grammar cannot	1.6548911840144864
different prepositions	1.636252690439584
since muc	1.549772543991908
concepts related	1.5992286050546423
efficient methods	1.6164339756999313
domains like	1.507007268326853
clauses containing	1.5110975570289593
wide scope	1.7404816457015064
american news	1.5558827079957558
modeling problem	1.5150717731521737
words following	1.6830657171447783
space required	1.5425370803688765
important feature	1.8741132404366576
different scales	1.500089141564707
expanded version	1.5440207036648976
functional categories	1.5117569510007092
ranked documents	1.7123432495610307
indexing terms	1.5593208576185018
initial consonant	1.554423275548326
r f	1.529984983809144
similar context	1.6556519153644185
standard dictionaries	1.5996348445739617
dictionary information	1.7071210278351534
yield better	1.8494524475613043
document types	1.5858364978836113
tnt tagger	1.501061266693165
intermediate result	1.549729132180171
identify relevant	1.7292467370219828
first sentence	2.2418238117615648
little agreement	1.5511943810330642
particular word	2.1381263891557643
prior work	1.886881136611633
speaker says	1.5297905781299381
narrow domain	1.6411558246537032
lexical analyzer	1.5282800389612259
corpus study	1.7160755973939523
target corpus	1.6458492479649585
one text	1.8630576883929801
high dimensional	1.7919161161552948
hidden layer	1.6584533024580552
verbs according	1.6288981647434881
checks whether	1.9404642049596106
agent may	1.6270874283099328
also investigate	1.780813241638022
nlp group	1.6445809590877878
gaussian distribution	1.6117266025648125
metrics used	1.7044914805734133
language includes	1.5338735539822874
nearest neighbour	1.6539015303843336
image processing	1.5786032299799224
features mentioned	1.522760613339579
tree generated	1.678220826924736
language would	1.8269494440275762
hard constraint	1.6003082154543702
syntactic role	1.788941810962241
errors found	1.582117970332684
method yields	1.6469586338689493
much data	1.6977890880751751
entropy markov	1.6055431809082763
language might	1.522760613339579
restricted version	1.6286406976823904
location within	1.6002993454155385
translation could	1.586137214651611
final text	1.6517628473615222
syntactic knowledge	1.999963321833839
mental state	1.6471315877459758
2 shows	2.6014450131052564
whose elements	1.8162748770465413
possible constituents	1.5647847313011076
intended referent	1.7032883015603175
additional effort	1.6548911840144862
earlier papers	1.5361450492096225
sentence rather	1.5511943810330642
grammar coverage	1.6923168902351338
collect data	1.6681052948574109
research topic	1.7902853972306823
much wider	1.8213328464053662
uniform distribution	1.9669667190527635
broad sense	1.7531416206280108
prolog implementation	1.7216648869922075
attachment problem	1.5897104636482475
target class	1.5354510398033074
objects within	1.5611231576018243
distinguishing features	1.6629333726074642
given pattern	1.5226149809040495
set accuracy	1.5241763132351238
recognition methods	1.5360026503169557
without reference	1.926530310767702
slot contains	1.58323415821723
capture generalizations	1.6320354670147565
standard data	1.8226480284991085
statistical technique	1.6375625045004876
conceptual hierarchy	1.6591332578718485
words occurring	1.9944126555028678
tree set	1.501695663586041
recall results	1.6978216692284374
automatic acquisition	2.060272985352392
abstract representation	1.7669367398860376
c contains	1.5111306094308554
root node	2.2213488929357608
temporal ordering	1.6980341110324457
various meanings	1.5111306094308554
parameters p	1.562663729908892
system comprises	1.5445134575869865
list may	1.5611231576018243
different contexts	2.079751934251303
structure analysis	1.7661203555668727
entire discourse	1.561113510315716
search mechanism	1.521074033156727
candidate translations	1.6869370927093938
recognition method	1.500089141564707
generative model	1.9063741505210823
radev et	1.6528655488942032
users want	1.561123157601824
later section	1.7517242416267398
general classes	1.6666664790751586
certain phrases	1.5887468924062063
original formulation	1.6943309563521631
slash feature	1.5106521830350295
sentences containing	2.1950092818422915
acquisition tool	1.5461586260253581
training times	1.5525978349470975
following operations	1.672219863528485
maximize p	1.5306061495832655
general statement	1.522760613339579
local level	1.5426832781289812
time expressions	1.8176256653434293
goldstein et	1.5260185053825464
thompson 1988	1.53562061540648
approaches based	1.8235243093691793
recursive transition	1.6091956422084024
successful match	1.5459182428709468
represent words	1.659662148122272
important research	1.7141428077458374
word per	1.5887468924062063
respond appropriately	1.5637776281819353
second form	1.5445134575869865
traditional approach	1.7607672852970255
semantic interpreter	1.825223054526182
new classes	1.5556574818984688
subcategorization patterns	1.5217182833954104
step 7	1.5822149911607093
current english	1.643362624369706
translations per	1.5408560126058508
full syntactic	1.953569207576721
control strategy	1.7293664257507246
actual lexical	1.530072891325321
tasks involved	1.5412463844444777
given domain	1.980898268785291
utterances like	1.6246289167870047
baseline method	1.8209832193662487
features corresponding	1.6190328102134979
algorithm would	1.9914458450370944
resulting string	1.583051127543801
current knowledge	1.6440429413357316
variables x	1.7173986571857272
speech signals	1.5893265024752146
given concept	1.780241029238734
variable z	1.571949426413436
related word	1.7422043205451014
model represents	1.7436706932296322
special issue	1.5370333724795422
equal numbers	1.5408560126058508
algorithm might	1.5908167675916909
answer may	1.6441796864302125
questions related	1.5037367200448983
complex relations	1.5871957071981
process also	1.6570039092175395
figure 2a	1.6525487091262194
parse forests	1.6012107413147452
since non	1.5338735539822874
terms representing	1.506173888464562
mechanism based	1.6320354670147563
models could	1.7158821441137806
constraints used	1.6103523297384124
stop words	2.033812991466111
information taken	1.5037367200448983
successful application	1.7229556053533144
similar semantic	1.650092453800705
10 minutes	1.7319366052224523
find examples	1.6320354670147568
current techniques	1.5547189562170496
prior probability	1.934972160845313
validation method	1.5188051370135929
extract sentences	1.6329374072287361
indexing process	1.5341349682563397
worth considering	1.6289349127929258
experiments showed	1.9674772975416333
3 show	1.9559688341034025
elegant way	1.6711381866791801
generative models	1.8245154484159976
performs better	2.1463796333634333
resolution component	1.6612851113599443
text translation	1.584455396570296
wsj treebank	1.50816320406491
anaphoric expression	1.795450625782522
like representation	1.5930640460654661
300 words	1.59555403645897
stochastic methods	1.5773133211810766
vocabulary used	1.712120909550923
entropy model	2.0051617184424035
examples used	1.6741803270499043
2we use	1.5547189562170496
briefly describe	2.1095427750644884
tree without	1.530072891325321
simple rules	1.9674737782087113
real problems	1.5771944393968735
specific discourse	1.6254116179354607
system handles	1.6303444022729265
feature used	1.6056035510036821
gram based	1.6760013115293027
function composition	1.5237134299158974
pruning strategy	1.5150643889101332
first describes	1.5547189562170496
discuss three	1.6002993454155385
rst relations	1.6108852463963927
original query	1.80742946826981
potential applications	1.7984918883977725
interest group	1.5171904119543882
related topics	1.5764509863537568
practical nlp	1.618758233992408
rules describe	1.6679521469523344
verbal phrase	1.595154937476357
entities referred	1.6371456055941147
state 0	1.5134046647845523
every system	1.567922330580758
word segmentation	2.0453352821354898
one network	1.5306061495832655
full lexical	1.521240524634107
first person	1.8277824723116913
work related	1.5739592165010823
terminal strings	1.6884291631110375
relationship exists	1.5511943810330642
ranking algorithm	1.5868677765514554
spatial information	1.5073375097907054
terminal string	1.7164361943923268
first glance	2.00998529417455
terms within	1.7297052827804613
operating system	1.8813921884070344
english joint	1.5583098370928328
trees obtained	1.5592363370440343
features encoding	1.5188051370135929
two di	1.54088218145704
system asks	1.7006675397225202
average scores	1.6682934633704967
system built	1.6631574457448046
sentence containing	2.055490995979346
everyday language	1.5645241345053709
syntactic conditions	1.5187014267751215
information automatically	1.7164164810084093
cky algorithm	1.6418877979830189
next set	1.6462268442574868
grammar approach	1.740285170263709
unification procedure	1.5272898334338185
types found	1.5525419192763323
processing module	1.7827401611601301
sample size	1.8745562517718932
important components	1.511205120616144
practical considerations	1.5706734993195162
pos tagger	2.071210097774894
candidate terms	1.6541643743112147
entropy models	1.9176641430019017
state transducers	2.0088680461394794
manual editing	1.5260185053825466
2 test	1.639836839822094
coarse grained	1.562086060694525
new technique	1.7744991916117228
words occurred	1.5037367200448983
understanding tasks	1.640471271678234
word relations	1.5405429229938827
truth values	1.8005260723361711
model complexity	1.5054676907462274
particular classes	1.5582958710952313
bold face	1.7329334093351267
next turn	1.6043878265755394
entire tree	1.627796791624738
cartesian product	1.71425633005438
another set	2.0077514939569028
data format	1.658060625176417
rules according	1.6548911840144864
translation accuracy	1.8131862164266355
becomes important	1.6548911840144864
different occurrences	1.5714549873488506
identi ed	1.8177905730325754
approaches could	1.5408560126058508
transformational theory	1.5416041062057484
artificial languages	1.5320698902069678
personal pronouns	1.9182485109470133
pointwise mutual	1.7161184237368163
possible instantiations	1.5706734993195162
limited form	1.6741797752731205
computing power	1.5886561992520425
based research	1.566582672988718
relevant features	1.975073804955671
conceptual graph	1.5320621935181231
highest ranking	1.6991133970569017
occurs twice	1.586137214651611
sentence fragments	1.9036040249681974
language modeling	2.160995670782156
2 outlines	1.5739592165010823
bilingual texts	1.5904229481950105
generative grammar	1.8297136305651909
three words	2.0762703625982857
semantic descriptions	1.7374123085469344
following words	1.7911148668685457
one c	1.5582958710952315
ikehara et	1.6503663844883056
similar set	1.6084968011212868
significant problems	1.5378646275607575
viterbi alignments	1.6037635736755893
across systems	1.7014151720699697
lower bound	2.0393063444372697
general case	2.1314518422556907
many verbs	1.7980956075726104
sequence labeling	1.5486173744108664
1 words	1.808489148563653
heuristic rules	2.037878522584114
alternative hypotheses	1.5437803911693893
parameters used	1.6904151818990054
generation tasks	1.6417595765801332
automated learning	1.5211629864223772
proper use	1.522760613339579
translation requires	1.510181823566108
speech database	1.6831670735030855
similar fashion	1.9232949306904783
pilot study	1.8746279787452476
set defined	1.5395099438048585
la langue	1.6349102213720403
case relations	1.5793309663069084
different researchers	1.6527294781610564
object np	1.8823179648351664
separate training	1.6548911840144862
similarity calculation	1.661468974778595
input system	1.5109871437896232
finite clauses	1.5028077182269155
2006 conference	1.8191665297540134
recent systems	1.62412689036662
test documents	1.7810718519053559
packed representation	1.5069623425757093
maximum matching	1.6190511454618752
stored information	1.5637776281819353
translation table	1.5958690962704707
frame slots	1.5023344388492128
linear model	1.9283386616977438
subsequent utterances	1.6309593672149651
simple method	1.9919482797098287
lexical verb	1.549729132180171
system gives	1.6785607164239145
smadja et	1.6161849521165763
specific assumptions	1.507007268326853
accuracy improves	1.549729132180171
precision rate	1.8079086413035002
similar considerations	1.5511943810330642
given text	2.099546522634263
capture semantic	1.5887468924062063
specific case	1.7958337914877562
best example	1.5111306094308554
additional condition	1.526993068994652
semantic frames	1.6933770535752977
trees rather	1.5731122769149348
kernel function	1.8214906676327596
heuristic based	1.5739592165010823
syntactic aspects	1.5881174695673657
following method	1.636252690439584
collection process	1.569522297316359
major task	1.5511943810330642
r l	1.732978339061759
new user	1.5767760138793054
linear function	1.7480213364468253
greedy algorithm	1.7273918241863648
previous methods	1.7949220450301444
approach enables	1.5645241345053709
zelenko et	1.6216030700185482
discourse parsing	1.5162389874866542
overall performance	2.13104584183137
speaker wishes	1.525702702006778
three runs	1.5260185053825466
document relevance	1.5041134997716235
test texts	1.6552822491682655
generated text	1.8811316949550783
argument slots	1.5942089669097723
processing since	1.5338735539822874
scheme used	1.7315826662352172
answer types	1.5773095578017111
words represent	1.5177700906840073
semantic category	1.9662231654610933
general parsing	1.672795806868395
sister nodes	1.6806643253699995
time proportional	1.5716157403241997
several examples	1.9461467822380163
edge labels	1.651319872066586
surrounding context	1.9909672288830296
tag assignment	1.5083310619902166
one layer	1.5041134997716235
paper use	1.522760613339579
category labels	1.8375581075882506
absolute time	1.5482870987025659
based tools	1.6082991843309609
tag formalism	1.6150427553458493
answer generation	1.5583002735647704
text given	1.6083510676201516
since f	1.5440207036648976
rule corresponding	1.5771944393968738
entity tagging	1.5944768731966894
original input	1.861645363432148
semantic constraint	1.666138386684005
computational applications	1.644710550221581
horse raced	1.536685793735597
free parameter	1.5211629864223772
information also	1.5338735539822874
hierarchical information	1.543442208482524
phrases may	1.7990408723360427
common way	1.6711381866791801
contain three	1.5188051370135929
linear models	1.7799574222567316
passive verb	1.6499712298804288
parsing strategy	1.9793222512464412
trees corresponding	1.618546065161532
features representing	1.6934793309340155
additional requirement	1.5338735539822874
parsed corpus	1.885642517150062
resolving ambiguities	1.6227790810587186
data extracted	1.7569336274605503
baseline accuracy	1.7303548552033614
resulting transducer	1.5260608698177955
improve translation	1.6840631149502632
following approximation	1.5445134575869865
one root	1.5320698902069676
based scoring	1.5290812457000524
tree kernel	1.5724130798753142
clarke et	1.562493587115223
also describe	1.9383007015490001
training portion	1.553185508137465
dialogue corpora	1.584471600525857
uses multiple	1.611203449372866
sometimes use	1.507007268326853
right frontier	1.5566608578491092
articles used	1.567922330580758
great importance	1.7329564081810807
systems described	1.821040970205683
classifier uses	1.5908167675916909
reduce ambiguity	1.6487873734413878
paragraph structure	1.5450498006663638
present day	1.5806234267011035
formal language	2.00604669742807
one reading	1.8218438810778144
abstract concepts	1.6742529672801947
mechanisms described	1.5338735539822874
conditional random	1.9343287734351902
highest confidence	1.6591671830061065
heuristic method	1.6157528360591624
woods 1970	1.5087853581980926
priori reason	1.5338735539822874
errors made	2.0459554165080513
pos information	1.8750818611348852
based version	1.5078976439458076
english grammar	2.019375517840052
following step	1.5482870987025659
current implementations	1.500089141564707
show results	1.7088092136999036
original question	1.7041911154470026
two novel	1.5332563118031657
syntactic data	1.5874817977813946
wider variety	1.77478134539852
arbitrary word	1.5839427016111838
sentence f	1.6592985818290584
grosz et	1.8820010040157416
example sentences	2.0655438214858877
simple noun	1.8359116739891255
direct objects	1.872669910224493
distinguishing feature	1.6881707359732032
essential information	1.7934582091806202
existing word	1.6456640619152285
input specification	1.5501943492812618
briefly described	1.8637019770389933
constraints derived	1.5637776281819353
h 1	1.5381766791708387
various methods	1.9403247475566163
ralph weischedel	1.595086062877347
functions include	1.5111306094308554
verb meaning	1.6599278955475714
mean value	1.6336649929396208
level knowledge	1.7317946021014856
structure rather	1.583051127543801
seed examples	1.5016202899982725
three approaches	1.79651033201312
based system	2.2230774138029323
features f	1.6949045224196924
certain aspects	1.5467800013744955
x r	1.632977144581962
object position	1.8987010319009145
individual sentence	1.6213967902634536
different authors	1.6468866812474954
following noun	1.6661204761131605
process used	1.7352337189917788
rules described	1.8015467393708282
learning approach	2.0753059465923123
algorithm stops	1.609217242875463
normal forms	1.5274708904736203
sentence consists	1.7087378349432947
following categories	1.8380078102489095
particular user	1.6985905455568948
phase one	1.5015563038202964
important characteristics	1.530072891325321
bc used	1.5547189562170496
graduate student	1.5509191069304942
node j	1.5430322980848317
category may	1.686125560340702
step 1	2.1768004734882958
present information	1.6223575629464313
natural class	1.5627352601669195
features associated	1.892378793007408
general category	1.59555403645897
synonymous terms	1.5408560126058508
syntactic feature	1.8266522778176575
several applications	1.7637790091991556
multiple interpretations	1.7370644233219208
lexical knowledge	2.1023375029756397
possible states	1.6116770897512507
understanding applications	1.596528355412953
specify constraints	1.5111306094308554
overall system	2.086249806850033
analysis without	1.5739592165010823
first input	1.595491118313467
theory provides	1.6661204761131607
semantic structure	2.1125842425697723
test questions	1.6599148034454858
data included	1.5177700906840073
new category	1.6428721706698721
words wl	1.6188354007829795
methods work	1.5445134575869865
two metrics	1.7525648008925516
knowledge represented	1.7230159273201577
demonstration system	1.5771054949602568
monolingual corpus	1.6377999218598585
trees containing	1.5547189562170496
linear programming	1.5330086065550708
par la	1.6260222113622609
new examples	1.700041328226896
new constraint	1.6641034862491684
three tags	1.5893265024752146
assisted language	1.6378377786030573
many groups	1.504934031784749
term extraction	1.7805637797445055
structure corresponding	1.807647704461981
word unit	1.5564242395353634
third level	1.772664615729094
provide adequate	1.5395099438048585
user must	1.9420561573446444
grammar constraints	1.5219618966045698
250 words	1.5260185053825464
string matches	1.5177700906840073
set union	1.6494883641301301
set perplexity	1.6237629210453108
one phoneme	1.5977445137418091
method performs	1.8730363400398784
5 note	1.5547189562170496
different size	1.613624660358136
contain many	1.987284992263131
several orders	1.6481174069026685
central topic	1.530072891325321
difficult task	1.9190783860892078
alternative forms	1.5887468924062063
accurate word	1.5422431186751526
word candidate	1.5297707679499353
output list	1.5404446292249803
situations described	1.5669631493969658
whose name	1.6465270800577967
telephone numbers	1.5473635513992758
meaningful unit	1.5111306094308554
utterance length	1.5166422018536703
training conditions	1.520353997580974
working memory	1.6780292806234738
identical words	1.629220782646339
2 senses	1.506173888464562
system generated	1.6638116287958908
use speech	1.550611280307232
never occurs	1.6010101084822783
binary features	1.915402686259722
summaries using	1.5288687562201928
figure h	1.583051127543801
many algorithms	1.6053932208928947
computational treatment	1.6710205119960557
chinese treebank	1.7674048675371345
linguistic methods	1.554089533378209
words may	2.1287108513286337
standard metrics	1.5241968977937237
four groups	1.7455828664829705
components must	1.522760613339579
judged relevant	1.5219444744517117
topic structure	1.5531984759362856
uses domain	1.5269930689946518
correct tagging	1.5611231576018243
entity class	1.5473190543531457
main differences	1.7888752560939947
july 2004	1.7651107526366039
relational information	1.724079300321595
systems command	1.591823957496618
example consists	1.5536240126687622
spelling correction	1.9003551189630845
tile problem	1.5445134575869865
contains information	2.0748005614886598
readable text	1.506173888464562
interesting result	1.7843055584917944
notice also	1.709268466970625
general conclusions	1.5749574214167732
design philosophy	1.5482870987025659
categorial information	1.5300492202283453
best parse	1.840749941872799
term recognition	1.5761818313197278
ordering constraints	1.774046119534139
example consider	1.6998973603836447
automatic methods	1.90963386142512
described method	1.5011917588019899
presents two	1.6903000289233914
techniques applied	1.5887468924062063
grammatical formalism	1.7298960431796446
linguistic level	1.767901538688276
table indicates	1.6770235151848842
whose entries	1.509778922004791
given c	1.507007268326853
signi cantly	1.8561573223832926
simple yet	1.5656419863875413
lexical hierarchy	1.567787754632867
additional evidence	1.7318457172857804
relative salience	1.5665766566590529
order may	1.5547189562170496
detecting errors	1.5053790653551418
atis domain	1.8635899926847421
shorter sentences	1.634648582284131
statistical significance	1.9775818908522262
journal reference	1.6527294781610564
sentences selected	1.775546960912705
term goals	1.542026188334053
wsj test	1.5684202189689347
general principles	1.673770329041365
system employs	1.8119659926140157
complex words	1.6003995217573053
full sentences	1.7989146589849443
algorithms like	1.5582958710952315
learned rules	1.705877538015927
hierarchical structures	1.7095194417032662
function mapping	1.5885693966639942
based machine	2.0449219367728606
feature whose	1.561707184647436
approach would	1.956159789373717
feature structure	2.125463997252334
particular category	1.6857927648164617
6 summarizes	1.6658904115324105
structure containing	1.7691761099408332
multimodal interaction	1.5083162252189979
recent advances	1.8771823780814803
values would	1.5156323250636554
tag set	2.019214671758882
traditional ir	1.6163274371635212
map directly	1.5459182428709468
tile type	1.5514607572627885
lower performance	1.8384307003489633
retrieval community	1.579873144038688
polysemous nouns	1.5178558907301445
practical reasons	1.8083721100016197
features necessary	1.5111306094308554
outside world	1.5582958710952315
relevant feature	1.592111614460519
context also	1.5188051370135929
coordinate constructions	1.5423419575395276
greatly enhance	1.507007268326853
separate semantic	1.5656419863875413
english sentences	2.1966567671709907
white space	1.8079693569017519
linguistic form	1.85667877951011
data mining	1.6773630767500418
genetic algorithm	1.6691063770282235
greater flexibility	1.683041275722836
next experiment	1.579873144038688
random selection	1.8417453817842577
character sequences	1.7047043103853503
tile algorithm	1.5514607572627885
four test	1.6352803771654205
choice point	1.6552359193833492
evaluation metrics	2.0539135092718146
greatly reduces	1.79791571192699
bateman et	1.6045468213354237
time required	1.7126052550533737
distance matrix	1.5250120459348997
word units	1.756942527405391
partial match	1.6734188617295795
another experiment	1.8575191948600072
possible structures	1.7509248047539316
probabilistic parsers	1.5283078246733592
affect performance	1.6002993454155385
protein interactions	1.5154984062237111
rhetorical structures	1.5659131311717311
complex events	1.5463791065316865
direct mapping	1.6770235151848842
one relevant	1.5665826729887178
sphinx system	1.502803459077262
rules apply	1.8309879144355892
c r	1.5737005355762872
clustering technique	1.6448472409336705
numerical data	1.5129946150496698
7 words	1.7279772726970641
every application	1.513130125383453
measure using	1.6010101084822783
capital letter	1.8522971165038762
new sentence	1.949249392254287
task involves	1.7106802946855555
system runs	1.7125369004275144
every constituent	1.6602505116127793
three text	1.5408560126058508
possible subsets	1.5288687562201928
similar experiment	1.6339957705783554
standard classification	1.5656419863875413
information used	2.013948494034635
line model	1.5767760138793054
logic grammar	1.5572842112180287
models built	1.6539594340296568
contextual rules	1.5921454644920847
rule applied	1.659111358304426
tile target	1.6184033873746562
reader must	1.5731122769149346
2 proc	1.7329564081810807
disk space	1.6226449082271768
since l	1.5408560126058508
weights associated	1.6530753578877089
grained classification	1.6031519553661342
query language	1.9293959965098082
variable length	1.7096420811651962
hpsg grammar	1.7268775786803654
logic programming	1.9534561167565594
possible use	1.5739592165010823
equivalence relation	1.8534986426578193
consuming task	1.8055148355029196
correct parse	2.0196190270255836
similar reasons	1.6164339756999313
syntactic components	1.5778973208841207
also results	1.6002993454155385
proof tree	1.5147903974381864
unsupervised approaches	1.5810686572743542
semantic inference	1.5423419575395276
coherence relation	1.5641211888068205
noun modifier	1.693136222545863
rule consists	1.792162570502364
work done	2.0849714588927655
inner product	1.7367906240381972
individual verbs	1.660386774722844
science foundation	2.2339616760899874
class contains	1.7147048368522455
trees associated	1.6367551419864548
first language	1.6760010434924044
features selected	1.6662036326177267
text stream	1.6569504373202975
many language	1.867388930127078
intermediate representations	1.6443751076989268
output pairs	1.5512297189099384
lexicon also	1.5582958710952315
correct class	1.5976467986375371
training data	2.569611756453779
encode semantic	1.5706734993195162
name recognition	1.766857022402802
recursive procedure	1.530072891325321
dependency model	1.6031462493484034
cooperative response	1.5194055206936903
general architecture	1.835094698177906
understanding component	1.7562472181374773
verb lexicon	1.6414457706957157
brevity penalty	1.573471460981951
means something	1.5547189562170496
appropriate words	1.619472687377359
following facts	1.6614869878819327
specific content	1.55955788508706
500 documents	1.5194321743502117
leibler divergence	1.7168453981642386
relevance score	1.5676638981076447
case words	1.5582958710952315
grammar like	1.5445134575869865
common english	1.6831049849869426
separate set	1.7517242416267398
access information	1.6943309563521631
tree correspond	1.5459182428709468
languages may	1.7794431571581761
different rule	1.6531124445181864
world applications	1.8688518273029164
common knowledge	1.817502966233862
adaptation techniques	1.5002001131767213
basic model	1.6954757220193382
corpora available	1.7125369004275144
sense ambiguity	1.8456366532102881
syntactical information	1.579028783384322
beth sundheim	1.532341016095725
6 compares	1.5338735539822874
similar vein	1.6527294781610564
character n	1.5968028678887571
subordinate clauses	1.860515561504305
names may	1.597317007886449
section show	1.5111306094308554
bayes rule	1.6707749282439646
whole word	1.6799380192266815
causal relations	1.576804693153826
typed feature	1.9259140021525536
r n	1.6667400037527433
complex categories	1.6608297637128002
grammar written	1.6445809590877873
important application	1.5908167675916909
true probability	1.566582672988718
clearly indicates	1.6084968011212868
generation component	1.9800827716866636
based techniques	2.027344426720032
one property	1.6164339756999313
general issue	1.5511943810330642
expressions using	1.6217423349150049
multiple word	1.7936820305897023
representing concepts	1.5338735539822874
expressions whose	1.6190328102134979
temporal constraints	1.5414487642721164
used language	1.5111306094308554
distribution using	1.5156323250636556
rst step	1.6115694852294897
weight w	1.6562905552119107
plural nouns	1.80188205189092
large parallel	1.613624660358136
human beings	1.957137008134815
important parts	1.6317923193893897
particular sentence	1.9043480713379857
distinction made	1.5973170078864487
news story	1.7730948087009022
martha palmer	1.5523893701784712
batch mode	1.7138299339685799
complex domain	1.594773126307433
scores across	1.5210740331567272
human annotation	1.7896012017560723
possible directions	1.5111306094308554
many adjectives	1.510181823566108
elementary structure	1.525170114883428
entity may	1.6229151093758099
two translations	1.5935786033631953
syntactic coverage	1.7273007597651548
several ways	2.1041530985091117
model based	2.1415759495180997
tile second	1.7539102218410891
brief outline	1.5707376783218978
misspelled words	1.636320879877015
intelligent information	1.6023158871024248
inflected form	1.8332837275773635
comprehensive set	1.619472687377359
units within	1.5425370803688765
whose leaves	1.6838006798046168
attribute name	1.510618969052833
speech synthesis	2.0275664421267043
translated sentences	1.675575560530313
similar analysis	1.7088092136999036
structure f	1.5633918315296067
desired output	1.7436292604420185
tile result	1.5637776281819353
small experiment	1.586137214651611
using models	1.5461959912075431
vertical bars	1.5188051370135929
processing using	1.6548911840144862
referential information	1.522760613339579
single point	1.667913900271795
good test	1.522760613339579
p 1	1.554089533378209
similar ones	1.561123157601824
remaining terms	1.522760613339579
corpus annotation	1.898735775557846
see sections	1.6319916984844927
words taken	1.5656419863875413
fewer words	1.6822511556851647
parser performance	1.6877106211040185
large percentage	1.7758448309690167
syntax tree	1.7109935009691775
word stems	1.7129667874216166
testing conditions	1.5258093094263683
first extract	1.579873144038688
structure associated	1.7441108250858939
remaining candidates	1.6656225051066411
chronological order	1.731507008148888
small text	1.5676517392350746
primary difference	1.5111306094308554
second position	1.7870651693875945
structure consists	1.7057741658061811
target word	2.146060245453838
empty list	1.7373495006721285
possible patterns	1.6904151818990054
parallel texts	1.8926761941425256
first column	2.081330386272784
three lists	1.549729132180171
two concepts	1.9299128990566679
mean reciprocal	1.6791996785010739
related applications	1.5511943810330642
therefore define	1.507007268326853
morphological information	2.0820609810880755
order predicate	1.8289713779240588
lexical data	1.9056566295388742
scientific papers	1.6556402997689075
automatic derivation	1.507007268326853
1 lists	1.9894031102219707
single rule	1.8992804785182247
data would	1.8926520821336161
ordinary english	1.579873144038688
boundary marker	1.6371590467451758
occurring data	1.627796791624738
trigram probabilities	1.575602896960514
tree obtained	1.6116770897512507
template matching	1.672457449662768
corpora contain	1.6084968011212868
pairs used	1.513130125383453
sparse data	2.1677071683969382
sections 2	2.0650106054486868
pragmatic factors	1.7675077064695917
medical terms	1.5077093148038294
null let	1.7105799695961408
contextual effects	1.5245697118479868
discourse history	1.6651870716497226
sgml tags	1.6401055406898184
word categories	1.7368744024316207
corpus show	1.586137214651611
second clause	1.902169509663054
dependency features	1.5379084734603223
head verb	1.7640568113981392
sentence boundaries	2.042431374284603
relevance feedback	1.7552686011515657
also extracts	1.5445134575869865
using sgml	1.5269930689946518
forward pass	1.5078415773245863
best tag	1.5519900193858671
also avoids	1.5482870987025659
toutanova et	1.6892123551766485
reference corpus	1.6397029590187921
produces better	1.6458797346140277
parsed trees	1.5781539692435587
performance across	1.740285170263709
many senses	1.6817139837574677
emotional states	1.5083553424132767
rewriting system	1.709905017816388
whose content	1.6805174920443955
whose head	1.809932553531522
systems using	2.027904774521793
set threshold	1.548378091852222
relative prominence	1.5473635513992758
rules operating	1.5511943810330642
basic features	1.7955694316374287
vocabulary v	1.6221843551054547
lexical feature	1.7646969164021125
subjects would	1.5266242261615064
theoretical level	1.507007268326853
grammar design	1.526993068994652
make assumptions	1.5771944393968738
synonym sets	1.7071494929764028
system achieves	1.914338466814563
spoken sentences	1.6303444022729263
two criteria	1.8965466751584503
larger units	1.7727308472331926
rows represent	1.5300728913253212
popular approach	1.5645241345053709
worth mentioning	1.8716021653641668
based application	1.5771944393968738
discourse information	1.8241600114852754
missing words	1.6105726559815163
rules directly	1.591823957496618
potential source	1.5739592165010823
one reason	1.7009633508460043
theorem 1	1.8498286674632043
passive constructions	1.7044914805734135
user preferences	1.573332763586901
single segment	1.6535456510181574
asked questions	1.5790358641614346
generated summaries	1.6633316105516651
feature engineering	1.6347818663290041
certain kind	1.7912633783955425
previous literature	1.5547189562170496
vp node	1.7628599146036261
complete parse	1.9515866326151414
user wants	1.8805679862956093
sentences contain	1.8624620477686027
qa system	1.8356096386104968
scores obtained	1.7885756102303294
syntactic characteristics	1.6115694852294897
july 2002	2.024023537667593
sun workstations	1.5338735539822874
many mistakes	1.522760613339579
context used	1.5563893320981304
bilingual corpus	1.9523492958737676
four senses	1.6012319720994657
friendly interface	1.6031224303496519
1 c	1.6844919157561309
john hit	1.5554791681843847
text files	1.7657022855661257
word translation	1.957536007024896
one subject	1.7280858226657188
matching procedure	1.679144015957024
three tokens	1.5036575034310684
knowledge management	1.5668914575951527
two terms	2.0153764208813225
large extent	1.5168195473888266
either language	1.5107844011873681
hidden markov	2.2697710336918684
string language	1.5198893458484917
fixed order	1.7630115553268326
standard notion	1.5189601534336892
texts used	1.738928781527528
appropriate domain	1.582117970332684
database access	1.7315649304783145
features available	1.5559715171228277
retrieve relevant	1.6765209127702825
additional word	1.5756510084230786
two candidates	1.7276631917172427
dimensional vector	1.7494811155728345
generation module	1.8991508933200072
achieve performance	1.561123157601824
4 indicates	1.561123157601824
syntactic argument	1.504934031784749
princeton university	1.5771944393968738
likelihood function	1.6708616888869878
approximate p	1.5288687562201928
computational lexicography	1.6349068916737024
event representation	1.5584368837489075
minimum error	1.5696088451528314
information derived	1.9018089881030755
speech processing	1.92667665237871
specific information	2.1703254495125184
sentence final	1.5502548653412629
similar distribution	1.6339957705783554
concepts like	1.6744785539419769
river bank	1.511205120616144
semantics must	1.592620200341168
rank correlation	1.531424133773238
positive feedback	1.543045653325533
three versions	1.6113785935009042
syntactic approach	1.571949426413436
first criterion	1.5656419863875413
last paragraph	1.592620200341168
temporal adverbials	1.6968373779282375
quantitative results	1.6501599070396704
particular person	1.5558827079957558
similar syntactic	1.7240216208258077
form like	1.6169986020906966
spelling corrector	1.540438321966469
appropriate translation	1.7701444543587799
various relations	1.583051127543801
english vocabulary	1.7059312630314079
simple context	1.6811844390857942
another strategy	1.65027618229265
planning task	1.5641946940632916
annotator agreement	1.9745840902707419
patterns found	1.650937328526823
models generated	1.510181823566108
remaining problem	1.5445134575869865
structured text	1.5525927262531272
training instance	1.7217042287849247
learning problem	1.7977930569546214
processing programs	1.5354510398033074
bilingual lexicon	1.7872599365213708
corresponding parse	1.5459182428709468
n p	1.8454900495591218
hybrid approach	1.8612795852022916
including machine	1.6084968011212868
using non	1.7367485809285093
way around	1.8258474532000915
test corpus	2.26807839467808
different genres	1.8248064102996242
surface order	1.7727767194386104
low number	1.6273943412233316
experiments aimed	1.6002993454155385
following context	1.5771944393968735
first pass	1.9679013770900584
performance obtained	1.7355686473586727
detection task	1.6409370192528343
two quite	1.5408560126058508
first compute	1.60913070763407
psychological reality	1.55955788508706
immediate context	1.7991519369458444
also worth	1.7294947296982195
entity types	1.895918878050452
documents without	1.5011917588019899
initial state	2.077847161756437
similar strategy	1.6527294781610564
earlier version	2.015294792587687
correct tag	1.7740401198934228
corresponding class	1.5707376783218978
single example	1.6289349127929258
identify non	1.5908167675916909
document using	1.6961394033344752
irrelevant features	1.623873755644789
domain models	1.6304670059826227
test cases	1.7446538172526111
developed techniques	1.5440207036648976
utterances produced	1.5514607572627885
interesting observation	1.731146052909204
tree corresponding	1.7262567032039862
prepositional objects	1.6257326646409984
tree learning	1.7515766078732793
certain predicates	1.513130125383453
abstract objects	1.620264557299379
testing data	1.983590653941552
varying amounts	1.569522297316359
news documents	1.5280901644790268
lexicon used	1.7991879462554778
initial segmentation	1.5257230336885035
many possibilities	1.7158821441137806
intermediate levels	1.6227790810587188
english version	1.8351100706151413
words also	1.7125369004275144
word identity	1.5951120373654004
u u	1.5177834651201016
perform better	2.139744388783247
substitution grammar	1.5311271242082034
rare events	1.5469471428549095
based taggers	1.5425370803688765
surrounding text	1.5914442382457208
visual inspection	1.5306061495832655
natural interaction	1.5257027020067777
particular verbs	1.5706734993195162
score according	1.5111306094308554
strong impact	1.5445134575869865
across word	1.521897139079
main goal	2.039112486337416
line 5	1.5164911279959081
two representations	1.7810503014635082
highest similarity	1.723275227996101
selection techniques	1.6304481342440433
success rate	1.8927329822354522
important factor	1.5026176109476748
column contains	1.6882949662489444
string length	1.6166641453691732
newswire text	1.7971086843631412
four stages	1.548378091852222
detailed analysis	2.0833693256346755
pragmatic features	1.6614080435903023
evaluation criteria	1.876407248791632
previous experience	1.7294947296982195
ted briscoe	1.5088952226197785
given threshold	1.9135258268744113
typical word	1.530072891325321
right sides	1.5148722773105283
hierarchy used	1.5037367200448983
language words	1.8195452670335632
symbolic approaches	1.5036575034310684
construct new	1.5739592165010823
two strings	1.9451921818418034
target classes	1.5521452212545803
uses context	1.6031224303496519
automatic segmentation	1.6979467211934778
parallel processing	1.7923750613635383
main drawback	1.6570039092175395
biomedical texts	1.5274095711263573
possible analyses	1.931202570411663
news texts	1.6334831561595657
poesio et	1.5805791160437694
problem domain	1.6508381569223591
statistical test	1.6221843551054547
item may	1.6894837465944073
substitution operation	1.5292030587584846
also causes	1.5378646275607575
mellon university	1.8994955305064065
resulting parse	1.7935968174739088
underlying word	1.5585752399605224
conditional independence	1.7112644600234215
learning algorithm	2.2828790842873308
multiple parses	1.7004618021108335
language users	1.7501157617215726
text spans	1.7098636944991754
great value	1.636252690439584
news articles	2.09759337553155
linguistic relations	1.6380669096866296
general term	1.737213208027503
different problems	1.6690280524444292
tools described	1.50119175880199
string edit	1.5769542411311108
dependency graph	1.7352809405635385
coreference chain	1.5348468125485641
category assigned	1.510618969052833
research goal	1.5971626956604803
acoustic observations	1.5041134997716235
direct implementation	1.5514514275224367
knowledge contained	1.6573127369315375
value 1	1.5179684141393763
tagger using	1.549729132180171
head features	1.659797139208264
differ greatly	1.579873144038688
bonnie webber	1.6864415156037387
clear example	1.5188051370135929
building large	1.6246289167870047
phonological representation	1.5778538879776773
information structure	1.8445893889743756
order according	1.5421588730604814
good generalization	1.5338162779728006
validation set	1.8014129327626693
type 2	1.6966931658234068
identi cation	1.6062879254301226
grammars based	1.6190328102134979
work quite	1.5408560126058508
questions like	1.8664920737970483
en effet	1.5895949513154928
usual sense	1.613624660358136
de deux	1.5460120652194373
regular languages	1.7378119190433696
sciences research	1.5547189562170496
test sample	1.686211623177178
two occurrences	1.8402271783017643
linguistic literature	1.8153413658553579
default strategy	1.532234208718959
construction may	1.5378646275607575
information present	1.8910412283297653
sample tasks	1.5705267462670767
work addresses	1.6002993454155385
english morphology	1.5156323250636556
formedness conditions	1.7425975155031377
speech recognizer	2.1686898327576456
common data	1.5410230457446352
new input	1.840656903719398
xml tags	1.616031378434591
lexical transfer	1.6932563553097282
using sentences	1.62412689036662
specific term	1.7152215891075944
generic summaries	1.566582672988718
text material	1.5533493466045314
grammatical subject	1.5704284771841746
frequency word	1.584518534249992
11 shows	1.7204874537317836
correct value	1.567922330580758
document content	1.7308525584163377
simple semantic	1.703338232318599
tree containing	1.656074792582219
test procedure	1.6005843102359503
appropriate text	1.5794243998957866
naive users	1.6959667372480363
linguistic devices	1.5528318200607132
increase performance	1.5622818522186857
capture syntactic	1.5037367200448983
different cases	1.8247510471803425
uniform manner	1.5408560126058508
space restrictions	1.5482870987025659
identify sentences	1.522760613339579
window size	1.9949123879619877
existing structure	1.5156323250636556
kappa statistic	1.7982376531062076
measures may	1.5511943810330642
ideal case	1.6553752285909602
individual language	1.5731122769149346
training sets	2.111800654087717
different parse	1.5582958710952313
remaining part	1.8322561761980525
optimal performance	1.793520620126473
score assigned	1.7173074988048835
uses wordnet	1.5111306094308554
grammar defines	1.5808501342373025
syllable boundaries	1.5238520895237218
5 words	1.8881601006436664
similarity measures	1.9885034015099994
words belong	1.6635555237714856
words remain	1.5547189562170496
make mistakes	1.669444390143633
large text	2.017649637671772
larger domain	1.5408560126058508
sentence contains	2.0364244295668223
unstructured text	1.6522417818837116
user may	2.118944607040582
related verbs	1.6046743940979167
general question	1.7111514227007443
generalization performance	1.629855024542529
various purposes	1.5611231576018243
resulting parser	1.5637776281819353
words assigned	1.516491127995908
different class	1.7017362832860266
resolution mechanism	1.6369558071118375
tree fragments	1.7469984784993793
recognizer output	1.7427154811154082
computational load	1.5671012235059067
model requires	1.8162724854967824
previous clause	1.5461317120818734
new algorithms	1.6855050078480978
programming system	1.6035281590022317
bell labs	1.6259121276584785
three layers	1.6392495009069876
possible solution	1.6406947443527446
words aligned	1.50119175880199
correct interpretations	1.5582958710952315
success rates	1.7140043660016135
indefinite number	1.524534874088451
form x	1.7815689354071509
user community	1.5784904253929217
small clusters	1.5086912108599555
linguistic specifications	1.5626637299088917
function may	1.6031224303496519
one page	1.5306061495832655
flat list	1.5656419863875413
statistical language	2.0865798301522727
rules determine	1.5037367200448983
sets contain	1.543442208482524
future systems	1.5526611030689654
extract key	1.5194321743502117
tile meaning	1.5412463844444777
components may	1.7247475524027744
specific question	1.6375625045004876
optimal way	1.6084968011212868
semantic tree	1.5273379389516317
utterance level	1.5741287921042546
6 times	1.6565475758379593
syntactic nature	1.507007268326853
uses semantic	1.703359686465308
symbol c	1.530072891325321
thank michael	1.6315901311540408
theoretical linguistic	1.5904771784117462
performs well	1.9785255781862399
ner systems	1.527469367733353
also compare	1.83619879598517
knowledge extracted	1.50119175880199
refers back	1.554089533378209
different parses	1.530072891325321
detect errors	1.6312740102560648
working systems	1.530072891325321
several language	1.6661204761131607
input x	1.6344086105050284
three strategies	1.595557387877764
nous avons	1.6132859225633598
lexical coverage	1.7159676738991143
syntactic subcategorization	1.590816767591691
dialog system	1.7519568578233309
automated knowledge	1.5058409883043937
word disambiguation	1.603709812460194
lexieal entries	1.5941108460767777
new test	1.8529984766234873
word training	1.5318465770311394
system implemented	1.6635555237714856
syntactic constituent	1.9025415344222132
linguistic annotations	1.5690842690631324
syntactic constraints	2.112464073528244
knowledge acquisition	2.1267513338894997
robust system	1.6541948100726152
avoid confusion	1.7678099140508592
shared knowledge	1.8106530187549539
evaluation process	1.8340115049315715
brief summary	1.7635102323276162
particular input	1.6953060531526463
principled manner	1.5511943810330642
simple set	1.7960004146943098
low values	1.6714604389400827
single parse	1.7728741084861208
treebank corpus	1.8500534142196066
medical texts	1.5295712062430367
see equation	1.6050500968708334
long tradition	1.591823957496618
kevin knight	1.5559715171228279
based semantics	1.6656225051066413
semantic categorization	1.585589015734039
use corpus	1.5445134575869865
reported speech	1.580248689445477
past participles	1.7873482246958596
current study	1.668283661305752
second paper	1.5195036419368688
five types	1.7847464314599064
speech tagging	2.294004510242431
work involves	1.707160349122273
tree representing	1.6725539769122406
following set	1.9790728764681065
training approach	1.5111765881035293
compare systems	1.5338735539822874
sibling nodes	1.554089533378209
pairs whose	1.7325198671385467
word overlap	1.6227350273305647
hand column	1.5836260411907608
empirical evidence	1.9932975582664803
dictionary could	1.5445134575869865
two documents	1.796746352271193
syntactic description	1.7658629253600282
second element	1.8134465768738113
words since	1.5547189562170496
parsing times	1.6589026948517511
possible utterances	1.5176055023927575
numerical values	1.706186072312876
scheme described	1.6904151818990054
subsequent reference	1.5515335323443233
less straightforward	1.5037367200448983
par le	1.6769417806534634
embedded structures	1.6010101084822783
whose input	1.6164339756999313
constraints apply	1.5908167675916909
sls system	1.516491127995908
one dependency	1.579873144038688
user interacts	1.5511943810330642
list using	1.5771944393968738
english phrases	1.7215575224829014
bootstrapping process	1.6339979907412272
space reasons	1.558193976095525
journal text	1.7023607714333093
language translation	1.9051209738022576
plan recognition	1.7846123445256032
learning program	1.7022065767453105
est pas	1.6103702391044705
way without	1.5111306094308554
important result	1.6056035510036821
many details	1.6939639871030243
system starts	1.667913900271795
following considerations	1.507007268326853
lexicon derived	1.543442208482524
current work	2.1679307733303728
10 shows	1.8820960088398875
position feature	1.5035193201428414
table also	1.8595491469034577
new source	1.570430085810516
verb must	1.7074092050419734
classic example	1.5582958710952315
step process	1.8844382178407295
sentences including	1.6182248546741609
clear cases	1.581413256199572
f must	1.5354510398033074
attributes used	1.5559715171228279
search based	1.510181823566108
bayesian classifiers	1.541443704835237
two texts	1.8524452750928784
previous words	1.7828207286074438
small improvements	1.6303444022729265
construction types	1.5563893320981306
limited extent	1.6699400022790052
stochastic approaches	1.5941108460767777
analysis grammar	1.5131283330141707
terminal labels	1.6043878265755396
valid parse	1.5657848105751837
groups based	1.504934031784749
paying attention	1.6213967902634532
tous les	1.5390877051636753
word c	1.6406234082599247
linguistic unit	1.7306708941666042
strings generated	1.5580764217656649
main conference	1.9481644940498741
psychological experiments	1.6679498141181661
declarative specification	1.5466472792181065
inference process	1.755929023575577
speci cally	1.7801641754145012
depends directly	1.5329785555225088
reader may	1.9204674447915706
imperative sentences	1.6261524724056347
system obtained	1.506173888464562
extract useful	1.579873144038688
word tokens	2.032508137824555
michael collins	1.6727958068683955
total time	1.8228209859251472
data consist	1.6190328102134979
tree associated	1.561596045888415
illustrative purposes	1.653691045130907
brief discussion	1.7856549343874448
particular test	1.6031224303496519
definite noun	1.9317901706422465
typical situation	1.5408560126058508
system selects	1.7647203904955413
component would	1.5869693547698929
new language	2.0190810954041414
certain concepts	1.6002993454155385
sentence order	1.5787598537974747
single measure	1.5758494263954068
developing techniques	1.5771944393968738
model defined	1.6375625045004873
two levels	2.187697205850945
level approach	1.6259962529124787
significant effect	1.503748068286553
elementary discourse	1.5585963416364663
basic research	1.794000039754325
low error	1.5212396502385472
inverse document	1.987333701973187
detailed models	1.5156083168889984
one sub	1.6035104581026864
show similar	1.5611231576018243
search engine	2.096813527687056
performs best	1.8250463566354274
specific patterns	1.7746406958638883
real numbers	1.749502338846713
model training	1.8132121389735083
user types	1.6822776816725193
sense knowledge	1.7933961083347953
whose use	1.511205120616144
used information	1.6002993454155385
thank prof	1.6570039092175395
enough evidence	1.687791832007258
particular time	1.6845577236692315
varies greatly	1.5408560126058508
sentential structure	1.553185508137465
phrase followed	1.6053932208928945
agent architecture	1.5403034171359224
significant results	1.7412091257207436
significant difference	2.098640669431351
case c	1.5170741079059482
different situations	1.7820977010245433
definition given	1.703359686465308
partial representation	1.5111306094308554
different tasks	1.9684744465992328
make possible	1.7280666768298492
overall precision	1.791428617383501
systems include	1.7164164810084093
several senses	1.6213967902634534
alternative methods	1.648095340184636
second experiment	2.068481085670313
process described	1.902012181032301
available tools	1.6053932208928947
next goal	1.5403034171359227
cannot account	1.7691761099408334
processing algorithms	1.707775983781434
relevant parts	1.7169512780278169
speech corpus	1.797996546126254
lexical approach	1.533256311803166
est un	1.5912882381157827
dialogue participant	1.558090208384841
6 presents	1.8565854777663446
fixed number	2.0164024642851457
one model	1.834915536615163
step involves	1.6411558246537032
example 7	1.5057127850400087
search method	1.7480477489428377
value pair	1.7996335030959045
rule contains	1.6817139837574677
another goal	1.5338735539822874
html tags	1.6677529490670933
system knows	1.8127341839465567
ranked output	1.50119175880199
different perspective	1.6853874840250063
method takes	1.6939639871030243
strung together	1.5338735539822874
base grammar	1.5060194132000424
another property	1.507007268326853
query would	1.6190328102134979
bias toward	1.6934793309340155
normal form	2.0258938517930285
ways ambiguous	1.5634490562307113
generative description	1.5910160474943131
morphological complexity	1.509778922004791
pour une	1.5332563118031661
local contextual	1.5134857210445507
per second	1.9746003043442861
highly domain	1.6164339756999313
subsumption relation	1.7115657305824419
since tile	1.5965283554129528
also express	1.6164339756999313
algorithm achieved	1.5156323250636554
internal node	1.8538203063749656
final model	1.7452554481303733
among terms	1.5841544365077214
one function	1.7706038030264344
mt evaluation	1.739156773414244
system consisting	1.6961394033344752
business news	1.6329266367902653
problem remains	1.6783930166760777
one sense	2.133400028997267
early experiments	1.6481174069026685
example 5	1.7675180695805044
process results	1.5338735539822874
rules include	1.5819174715567357
general picture	1.522760613339579
parsing model	1.9546214069299026
rule corresponds	1.5525978349470975
speech systems	1.8309350873565866
6 hours	1.5354510398033074
discourse segments	1.8163246955774919
following procedures	1.5445134575869865
testing sets	1.7167225033608036
speaker believes	1.7087941768912183
head feature	1.7576901703088386
algorithm achieves	1.7063153898332737
null ing	2.055348489093406
two objects	1.9301023613278872
art supervised	1.512536810936869
voted perceptron	1.55793539997679
language text	2.125377971610922
full list	1.6125713976431841
line version	1.524534874088451
natural classes	1.508366082952001
original form	1.795538398773003
penn treebank	2.295702293802233
different tag	1.6706426105940735
based classifier	1.754596116946106
two questions	1.975564139807389
entry must	1.5111306094308554
know something	1.543442208482524
input representation	1.7588422616508803
training error	1.5270622026314862
striking difference	1.507007268326853
open issues	1.5338735539822874
figures 3	1.9032774308372633
words belonging	1.8831641518034918
2 note	1.5739592165010823
general tendency	1.5547189562170496
allow users	1.5497937736341867
previous step	1.9461538540148384
processing time	2.114679280595655
b would	1.567922330580758
particular language	2.022086848196534
basic vocabulary	1.5566444907537573
certain probability	1.5378646275607575
line system	1.698978605212456
linguistic units	1.8981503607449106
dashed line	1.7573529725944697
user interaction	1.8453736623488963
group together	1.8436994661709332
morphological analyses	1.7906216248377969
e f	1.842620128089613
english queries	1.6266177402210222
correct word	1.9636743313964495
syntactic interpretation	1.5150717731521737
hundred sentences	1.5245348740884508
tile lexicon	1.5511943810330642
systems use	2.101127060372117
repeat steps	1.5690244007917742
textual context	1.5555254798167089
larger training	1.8849219186764876
two languages	2.234101179560519
parallel corpus	2.0522403134105476
previous approaches	2.1489315485600096
complicated cases	1.530072891325321
phonological theory	1.5066881819459041
state information	1.6137947517368634
also generates	1.711151422700744
accuracy achieved	1.7841811153993103
selected features	1.7196774974554123
state automaton	1.918019056426251
set membership	1.6595315975547575
first level	1.972731002512959
programming techniques	1.74933525125252
high weight	1.584483801902334
null symbol	1.5237795247209103
documents returned	1.7216763746225299
possible rules	1.6993812786179885
structures may	1.8108881035691438
precision obtained	1.5676517392350746
tile original	1.5338735539822874
research grant	1.750700291476916
initial words	1.526770016884718
third method	1.6030475663869699
several stages	1.750700291476916
systems require	1.796147981711569
base containing	1.591823957496618
particular aspect	1.617761370492699
allow multiple	1.7244811761730507
tagged sentence	1.7190416879327548
word group	1.6076912185760295
next consider	1.6031224303496519
algorithm produces	1.8078194710997098
section 8	1.950870196667685
elementary tree	1.901776094315835
cognitive models	1.512925200463965
semantic similarity	1.9930886320622707
basic version	1.515071773152174
whole sentence	2.1564386544677223
extraction technique	1.5929928949821113
generation using	1.613693223189432
specific items	1.5412463844444777
example data	1.506173888464562
semantic notions	1.507007268326853
cross language	1.5520305945742492
fellbaum 1998	1.5819174715567357
reflexive pronoun	1.6851024467559848
dry run	1.7690228700308908
unique set	1.6246289167870047
space limitations	1.9817759437036424
using frequency	1.6056035510036821
additional examples	1.5403091007130407
entity classes	1.6154842909469829
indefinite nps	1.5926422779534501
segmentation performance	1.5865646203284989
additional constraint	1.8248139332291626
structure contains	1.8019976951701842
6 contains	1.5338735539822874
analysis provided	1.5973170078864487
using l	1.5645241345053709
extracts information	1.6010101084822783
rules use	1.5941108460767777
completed parse	1.5226149809040495
core language	1.819387848669631
ontological concepts	1.5325734635766792
corresponding predicate	1.5378646275607575
one leaf	1.5354510398033074
features related	1.6869953502761277
target structure	1.5493104822287047
model significantly	1.6525488762732246
ibm model	1.931218089204394
associated feature	1.5302382484774217
vectors using	1.566582672988718
sentences involving	1.7098259615519553
basic system	1.5852013521892068
nouns occurring	1.5482870987025659
sentence w	1.6985501109872037
specific resources	1.5360026503169555
incorrect parse	1.5395099438048587
model gives	1.7577705970423196
performance measures	1.8810369070148132
tag grammar	1.6583267369818961
total length	1.652025005898412
rule 7	1.5571495092265906
describe events	1.5414180417122596
indirect request	1.5369492623682013
best described	1.5511943810330642
referential expressions	1.6071000717451769
special form	1.6082991843309609
recursive definition	1.6185965759709262
user says	1.6813763339848702
condition 1	1.5705933996691273
low probability	1.872347939444094
shallow parse	1.601320253518161
identification task	1.7812195365718113
different subjects	1.6482958236884362
syntactic type	1.765523042661122
interpretation must	1.549729132180171
files containing	1.6266256462651403
lemma 1	1.7120204882600372
way one	1.5611231576018243
one grammar	1.6190886481498603
step towards	1.5941873493480543
evaluation program	1.522438913516297
structure corresponds	1.522760613339579
message passing	1.6213455966427133
previous paper	1.6411558246537032
three measures	1.7831726392707996
natural numbers	1.7446646302075182
different value	1.6658904115324105
connected speech	1.5511410205936218
keeping track	1.9709509392742521
arda aquaint	1.5378646275607575
interrogative sentence	1.5195036419368688
based metrics	1.5867941830474335
stanford university	1.7633213752432315
redundancy rules	1.5515527304427232
parseval measures	1.5332563118031661
godfrey et	1.5275591303469285
specific requirements	1.667913900271795
phrases whose	1.6627269937559803
system running	1.5547189562170496
isolated word	1.6742873213876714
auxiliary tree	1.8021589286402753
decision tree	2.1455279235210947
current problem	1.5425370803688765
various approaches	1.8821230942040525
mckeown et	1.783779290850383
rule 5	1.7170920188041296
similarity score	1.9608808556837067
physical location	1.5111306094308554
algorithm given	1.7543385354298753
related language	1.5412463844444775
applying bayes	1.5188051370135929
approach based	2.0676242705962675
transformational grammars	1.6528138647598327
processing speed	1.74036817626622
lexical lookup	1.7931542591987222
chunk tags	1.528196699553037
clustering algorithms	1.8187017596983213
nlg community	1.505422627645193
third step	1.9082373631156735
process natural	1.5926202003411678
target sentence	1.9878298222486817
examples using	1.549729132180171
various categories	1.6289349127929256
main verb	2.2381706938383115
nominal head	1.5955573878777636
word counts	1.7497244590191428
term containing	1.5188051370135929
sentenc e	1.511205120616144
based representation	1.9221966521912555
ibm models	1.8171439035667325
word extraction	1.5574477063720038
million tokens	1.6121742428060308
singular value	1.8374324913521785
common representation	1.6811844390857942
patterns would	1.5506112803072318
important distinction	1.707160349122273
karttunen et	1.6214207366819529
grammar induction	1.739461115465931
approaches differ	1.5328333954091753
standard parseval	1.509778922004791
dictionary definitions	1.916129269540143
generate texts	1.5269930689946518
computational mechanism	1.586137214651611
place predicates	1.6189360171179383
assigned tags	1.5036575034310684
spoken english	1.8190392666989452
narrow sense	1.586137214651611
grammatical function	2.0152964573330423
sparseness problem	1.9655902081265326
words derived	1.610941377980873
find information	1.5473635513992758
phonological representations	1.540321146546792
level constituent	1.542683278128981
upper bound	2.264663814960181
one object	1.8625391005273682
syntactic environments	1.6010101084822783
different entities	1.766140926343247
analysis within	1.5408560126058508
different types	2.3791416158778445
sentences cannot	1.5645241345053709
research interests	1.5266242261615062
information pertaining	1.6635555237714856
application domains	1.8741925487841165
corpus size	1.9944031491686394
grammar checker	1.5158832111433553
scientific research	1.909366746435378
deeper understanding	1.6846388563743653
bilingual dictionary	2.0298973131287417
resolution task	1.5620860606945246
declarative nature	1.521162986422377
simple experiment	1.5445134575869865
substantial reduction	1.594773126307433
typical examples	1.84387559419494
long sentence	1.6743380288450973
additional structure	1.5887468924062063
proper nouns	2.1732659971702066
shallow parser	1.8116670620299378
parameter b	1.5563893320981306
speaking rate	1.684657967903977
grammar provides	1.6978216692284374
great amount	1.5887468924062063
analysis consists	1.7139483293868785
correct label	1.6310229992752068
initial step	1.7714711599478519
lower precision	1.8413542318846414
every node	1.9962266556422084
lexical constraints	1.746775888191622
distributional properties	1.6822776816725191
logical operators	1.7087673282023343
anonymous reviewer	1.7961446965684997
syntactic part	1.652336014047674
quadratic time	1.5156159685357804
common approach	1.8514970476582602
corresponding output	1.526993068994652
level description	1.6386937903432783
chinese information	1.6320121285521267
us something	1.507007268326853
generation architecture	1.5422431186751526
parent category	1.5038095001588148
method first	1.6289349127929256
word recognition	1.9819982012993504
general solution	1.7796583190082031
remaining errors	1.8463589810990293
texts according	1.5771944393968738
dialogue act	1.7603210183360474
adjectival modifier	1.5582958710952315
larger context	1.8657647018608843
next iteration	1.8030200462909929
knowledge includes	1.5440207036648979
single value	1.7597912336202828
hierarchical relationships	1.5425370803688767
large portions	1.6056035510036821
first task	1.9278216457928241
semantic networks	1.948859533353715
full data	1.5461959912075431
tagging methods	1.5977445137418091
information presented	1.7268726578291138
variables occurring	1.5164911279959081
nlp tools	1.7817655884610129
tile first	1.8181466204578616
knowledge might	1.6056035510036821
reference word	1.5365331267030269
60 words	1.513130125383453
linguistic approaches	1.6169635240879603
utterances must	1.5511943810330642
speech may	1.6229919786001088
possible answers	1.8672985382549567
given model	1.6246289167870045
one symbol	1.7048341608116455
straightforward manner	1.7300440241321193
inflected words	1.6780370053614417
test samples	1.6147303756632543
model proposed	1.898226525847408
linguistic categories	1.7031291649231113
two modules	1.8883751710860877
function word	1.8735118983313581
existing discourse	1.511068454944984
huge number	1.8794232755433686
lexical description	1.6459606485985105
parameters using	1.6862623517634696
complete word	1.5035193201428414
1 let	1.7329334093351272
presents results	1.7680599214474206
slavic languages	1.5074121615021698
measure similarity	1.5739592165010823
first member	1.5880900834779994
correctly understood	1.5760874077004157
automatic approaches	1.5217605875570064
based classification	1.6149253215472865
current design	1.5269930689946518
5 lists	1.5739592165010823
n b	1.5613424139354941
previous text	1.6256278638449597
news corpus	1.8621295326818665
two datasets	1.5177700906840073
english data	1.9156523281610733
given sense	1.703614613571324
collocation extraction	1.6094869691790854
full feature	1.5482236808838805
language constructs	1.5656419863875413
score using	1.6527294781610564
speech tag	2.0480831164766116
using heuristics	1.7178162994368975
design principles	1.678973586324036
without recourse	1.7527766729997913
second conjunct	1.645908690496716
grammatical features	1.8765433283134536
also use	2.2793155520140704
interpretation processes	1.5361450492096225
possible relationships	1.5926202003411678
ten words	1.7098773566635042
base consists	1.6315901311540408
best case	1.7497679558703203
second layer	1.544378505133437
including part	1.698978605212456
optimization criterion	1.550129323569366
words using	2.0155801949738574
sentence cannot	1.7192050380109327
quantitative information	1.5381766791708387
3 reports	1.6339957705783554
certain attributes	1.55955788508706
words whose	2.0361789222028435
trees t1	1.5338162779728006
main role	1.5547189562170496
recall scores	1.7466818845742909
present framework	1.5299605543466968
involves determining	1.5973170078864487
input node	1.502594480495715
document management	1.5281852037607777
lines show	1.615262062995654
one phrase	1.805547905936639
dictionaries contain	1.5992286050546423
dowding et	1.5877046642631587
example involving	1.5547189562170496
related senses	1.7065096286168218
experiment also	1.5739592165010823
relation among	1.7648603130277967
system compares	1.5887468924062063
contains multiple	1.583051127543801
features cannot	1.5111306094308554
single event	1.6775848317234212
functional grammar	2.059388781366396
early version	1.6724173909604771
preference heuristics	1.5122639093520438
recursion semantics	1.6608064596982492
important observation	1.6548911840144862
driven parser	1.5093464527050402
including zero	1.5217605875570066
word without	1.7294947296982195
length increases	1.5473635513992758
frequent terms	1.5434176300418245
compares favorably	1.8042104622639004
simple analysis	1.5645241345053709
words present	1.6289349127929256
syntactic functions	1.898601878148313
top k	1.636500977788303
fine grained	1.8255271745043522
underlying structure	1.7711412033038474
model showed	1.50119175880199
corresponding word	1.895723952724911
performs two	1.6117266025648125
worth noting	2.1479135049043476
simple ones	1.5260185053825466
accurate results	1.7414848008364259
different treatment	1.6112034493728657
extracted sentences	1.764495364772672
lexical probability	1.6014715506305457
systems often	1.7486023401067095
file system	1.5526354282726207
automatic translation	1.924523376376863
retrieval using	1.6343543529378617
bigram counts	1.5002436683101648
two processes	1.876769393438867
tagset used	1.5266242261615062
usual way	1.9091098815471617
set containing	1.9462538372170024
second reading	1.622184355105455
following analysis	1.7734751337955452
person plural	1.6082695379999925
sophisticated approach	1.5338735539822874
knowledge could	1.6548911840144862
important concepts	1.7029351383643831
single predicate	1.5269930689946518
possible variations	1.583051127543801
rule ordering	1.6491709296660884
knowledge required	1.8506108235449696
across sentence	1.5806234267011037
make use	2.44877808538176
e l	1.915312735291571
powerful tool	1.7744272249450754
idea underlying	1.620425994955554
information source	1.7251585372986904
following ones	1.579873144038688
multiple ways	1.75176197502999
7 seconds	1.50119175880199
select relevant	1.5349991839448194
f measure	1.7106787580740002
sentence number	1.513807122361449
former case	1.971267765435709
high degree	1.9722160165177811
user chooses	1.6073715579162529
argument representation	1.5037367200448986
first note	1.6084968011212868
atn formalism	1.532972969170748
vector representing	1.587646356112346
useful way	1.7279919232921823
next page	1.7726446947589505
questions arise	1.583051127543801
factual information	1.6747079389488897
pas de	1.5678160139956427
user makes	1.5395099438048585
errors due	1.775449425304777
model allows	1.8451213361268148
issues involved	1.5509539006409865
inflectional endings	1.640146919921098
harmonic mean	1.8961854382911474
algorithm constructs	1.530072891325321
corpus frequencies	1.6035281590022317
human judge	1.7479479622919856
results clearly	1.6229919786001088
augmented context	1.6564925049889878
words 3	1.5111306094308554
syntactic markers	1.5190584552425839
pattern matcher	1.7789088798206716
whether x	1.5408560126058508
basic english	1.510181823566108
text features	1.5450137680585834
dot product	1.6636499002417842
typed unification	1.5354594818690515
compound word	1.751618767371858
second group	1.8158121142039954
using knowledge	1.8162433384989423
data based	1.716800345112432
true iff	1.637162156262956
subsequent references	1.5568336133190415
target verbs	1.5485227522345542
definite np	1.694519608590157
resulting word	1.6892481922418148
algorithm starts	1.8484308050538747
given grammar	1.8114885608996452
useful clues	1.5329785555225088
document vector	1.5488216646501045
common error	1.5381766791708387
definite reference	1.6394554785828772
progressive aspect	1.5327836472479632
japanese words	1.8196637361324715
three papers	1.563687738145949
regular grammars	1.6386283535999033
section three	1.5459182428709468
structural features	1.80447195397079
hard task	1.6927904370768552
using techniques	1.8121238105123403
disambiguation tasks	1.7315826662352172
probabilities based	1.65027618229265
analysis tools	1.7847486878965964
user wishes	1.6164339756999313
parsing experiments	1.6210934563561632
serious problem	1.6914189276858587
multiple hypotheses	1.5737074106908622
given entity	1.6111337654434248
extraction using	1.6724173909604771
existing language	1.5190584552425839
corpus must	1.6392309045877975
system depends	1.611203449372866
baker et	1.840042454556946
concern us	1.6658904115324105
specific permission	1.925120091448104
separate sentences	1.6756673695466664
words share	1.5806234267011035
given n	1.732266191908249
000 concepts	1.526993068994652
high reliability	1.6456640619152285
temporal logic	1.5303601396711208
grammar checking	1.6004856788802266
classification model	1.7310583238935537
acts like	1.6514103959727489
subject must	1.59555403645897
disambiguation may	1.506173888464562
combine different	1.678973586324036
n v	1.685732473711113
model without	1.7993664567026375
nl interfaces	1.5231242788593953
different elements	1.6943309563521631
verbal stem	1.5376694107437232
every string	1.6676437375475484
mouse click	1.5318465770311396
based work	1.6123145120618658
sample results	1.507007268326853
parameter estimates	1.6680360490376362
given object	1.686062073223278
noun modifiers	1.7068274097866478
run without	1.586137214651611
input side	1.5148387121477085
morphological rules	1.9019921203733618
user satisfaction	1.666457247628545
scoring mechanism	1.6561888989465818
template element	1.614823912501751
elaboration relation	1.564195287561538
phonotactic constraints	1.5699272240490858
aravind joshi	1.8305472758233803
possible links	1.526993068994652
supporting evidence	1.5985995282484877
prototype implementation	1.667258270811144
darpa tides	1.5749574214167732
minimal effort	1.5707376783218978
accuracy rates	1.7203742456163185
larger set	2.0046891082403535
6 discusses	1.7204874537317836
contingency table	1.7493432805859679
weak generative	1.6555169666918408
results showed	1.6074286017338575
application system	1.689062480335181
simple syntactic	1.7785412351430132
aligning sentences	1.5453587447792687
information found	1.86018786174233
automatic text	1.9793004110433117
highly context	1.530072891325321
compression rate	1.5975953356743169
top half	1.5611231576018243
feature information	1.7621691936114705
paragraph level	1.6788816102630104
problem without	1.613624660358136
phenomena discussed	1.5078976439458074
positive values	1.50119175880199
actual system	1.656664045947751
definite article	1.9223498692436614
also influence	1.5445134575869865
languages whose	1.5963645436754335
documents containing	1.9339498675292912
parsed sentence	1.7089948509241317
training part	1.5012088632724712
becomes evident	1.5445134575869865
lexical access	1.6701480067668069
test materials	1.536533126703027
penn tree	1.9649816440641887
another verb	1.7111514227007443
syntactic annotation	1.7652857782248303
interesting example	1.6458797346140277
neural network	1.8798079816455233
400 words	1.6079132085425911
thank john	1.5445134575869865
phrase would	1.6614869878819327
interpretation systems	1.5188051370135929
bayesian approach	1.5520575080036068
tagged input	1.5378646275607575
paper gives	1.7329783390617592
simple strategy	1.7153006833611064
training documents	1.7747056201780531
function k	1.5986374288963399
results comparable	1.7279919232921823
rule r	1.8375727841497131
improved accuracy	1.7266279442877566
approach produces	1.5547189562170496
whole input	1.6411938887350184
mutual belief	1.631870404064625
transitive verb	2.0738725936974096
public domain	1.7098199218430143
entropy classifier	1.7150370655931464
theoretical framework	1.9381086369219565
lexical type	1.500274111536787
atomic feature	1.5354441621383106
morphological analyser	1.811363988912646
arbitrary length	1.7424501966659687
actual number	1.77530945408058
perplexity 60	1.5786067012560172
parsing technology	1.7150521188748642
set f	1.7596960229818661
syntactic phenomena	1.8909015470336261
rule cannot	1.5511943810330642
following strategy	1.6458797346140277
based clustering	1.6510428126029009
similarity among	1.6533744882355494
whose scores	1.5406210028261875
principled way	1.9412835631658136
let h	1.5955573878777638
nouns may	1.646226844257487
two tables	1.7449661385739974
signi cance	1.5215525803293102
three sub	1.6103523297384124
attachment preferences	1.5188051370135929
manual translation	1.5556026693790028
label c	1.511205120616144
event occurs	1.6339957705783554
based theory	1.6782219405894565
scale corpus	1.6589224276852044
aquaint program	1.547363551399276
input string	2.2258478538773194
25 words	1.6738170056537425
save space	1.754682578532674
two humans	1.5226149809040495
test phase	1.5734214393930075
previous versions	1.6822401779866478
many relations	1.6125713976431841
semantic level	2.0050232215700294
uses syntactic	1.6857927648164617
related works	1.8514970476582602
tipster phase	1.6286442727729618
set consists	2.023964874968307
two systems	2.0077933968838577
algorithm proposed	1.866176562293589
hurt performance	1.597317007886449
ir systems	1.8175777359813408
data consortium	1.9773423518339766
standard corpus	1.6876979704810908
significantly increases	1.583051127543801
phrase level	1.9417911545615767
common terms	1.5000891415647069
selectional restriction	1.7442540056297875
specific rules	1.981101816227887
gram lm	1.504156401708101
occurrence counts	1.761090550319635
two parsers	1.682913407313605
modal verb	1.6527982963767172
semantic structures	2.044933584995777
process works	1.65027618229265
grammatical constructions	1.8034186773056557
grammar size	1.6851829335448325
given phrase	1.6814864655388848
remains constant	1.6614869878819327
domain knowledge	2.125587449406114
common names	1.5230483704868893
supervised machine	1.8648657622128153
third line	1.6731227187422073
different random	1.6661204761131605
initial training	1.6712816184019108
language allows	1.611203449372866
tree contains	1.6849207890717186
3 types	1.506173888464562
scale systems	1.524534874088451
place names	1.8825956184053925
full coverage	1.6793833626587444
features except	1.5620473238340082
8 shows	2.0695572598289447
method must	1.6458797346140277
values given	1.6339957705783554
difference may	1.583051127543801
tile rule	1.5425370803688765
correct category	1.5563596026723359
information directly	1.720326606282666
experiment used	1.5806234267011037
use word	1.8573407523913184
standard statistical	1.7414848008364259
root word	1.6213559560468538
novel way	1.6977890880751751
expression containing	1.5511943810330642
high quality	1.6292226832083438
perform well	1.785761876576021
following sense	1.636252690439584
first prototype	1.6536910451309068
satisfactory way	1.5887468924062063
work provides	1.586137214651611
easiest way	1.62412689036662
company names	1.8531734456983715
case one	1.6785607164239145
conclusions drawn	1.5177700906840073
two sample	1.640044071538913
three arguments	1.83619879598517
final performance	1.5645241345053709
constraint propagation	1.6226530431430781
automatic disambiguation	1.5460120652194373
tree corresponds	1.6527294781610564
computational task	1.5408560126058508
candidate word	1.7222470081313501
whole test	1.55955788508706
susanne corpus	1.5221650785228467
nlp techniques	1.8961483649144069
speech output	1.7746893571278057
uses lexical	1.62412689036662
examples include	1.9822324162958738
general methods	1.55955788508706
languages share	1.5111306094308554
current dialogue	1.7194341301851725
rule sets	1.7730566170507907
three models	1.818778781510296
demonstration sessions	1.522760613339579
supervised approach	1.6545073185241201
2 words	1.7712784651614681
set inclusion	1.544158348757346
optimal parameter	1.6754000717273434
recursive structure	1.7692002331769703
japanese translation	1.698451176058163
greek letters	1.6005750984437832
vowel harmony	1.5754629177621884
model assumes	1.8315192032655923
solution proposed	1.5706734993195162
corpus tagging	1.513130125383453
inferences based	1.542348578385745
various versions	1.5706734993195162
vast amount	1.7329292822928468
support verb	1.5366834984867999
indefinite articles	1.558193976095525
one way	2.0076306168452986
whose probabilities	1.5349991839448194
4 reports	1.6844174045708415
subsequent analysis	1.6448472409336696
formedness constraints	1.6134861735793382
commonsense knowledge	1.6626051379122302
term frequency	1.9619172024761524
sense number	1.6012376643689819
corpora annotated	1.6870531608563337
structure cannot	1.6169986020906966
evaluation corpus	1.7508198304028715
orthographic information	1.5177700906840073
select one	1.9679840642190958
utterances used	1.5378646275607575
based representations	1.6841005995560927
explicitly represent	1.7021555513601787
systems performed	1.5859118906985712
words respectively	1.6266551395241793
variables may	1.5887468924062063
available data	1.895229052127106
language without	1.7158821441137806
similar role	1.5408560126058508
interesting problems	1.7117895855245222
state machines	1.935353954023683
cky parser	1.5010662334333806
statistical modeling	1.7629794614887127
acquisition method	1.538719277039856
correct constituents	1.5356235311682054
algorithm allows	1.7145331795844636
lattice structure	1.5748567577278902
mean time	1.571949426413436
compute p	1.5810686572743542
serious difficulties	1.5111306094308554
different classes	2.0501502244591494
van deemter	1.708249268097449
compare two	1.9797247742034472
ibm research	1.512980120370072
u e	1.6506141714618447
maximization algorithm	1.630705616344183
p denotes	1.5536240126687624
john bought	1.595868801362832
feature structures	2.1414651496894996
coreference chains	1.5912455545566457
text within	1.5188051370135929
filtering step	1.5226071645445156
estimation technique	1.5925561662226717
source text	2.0696350719791825
words encountered	1.5771944393968738
finite subset	1.6169986020906966
original position	1.5645241345053709
decisions must	1.5408560126058508
convey information	1.7906687288569545
desired effect	1.6498434360228813
two lexicons	1.511205120616144
text sources	1.6498333906762652
identify key	1.5156323250636556
small sub	1.5645241345053709
sensitive grammar	1.5614608356518007
germanic languages	1.6296746535090856
sentence error	1.6999133590617053
evaluation indicates	1.5188051370135929
one version	1.6996261406379258
increase precision	1.5887468924062063
two labels	1.632318029101388
driven generation	1.5416576927430514
overall recall	1.712082896834022
several directions	1.7051629942513449
main feature	1.6565475758379593
generating appropriate	1.5037367200448983
preference order	1.5905335586417744
sentences annotated	1.628898164743488
three features	1.9402203698782736
many nodes	1.586137214651611
different results	1.8969865832403359
illocutionary force	1.6807144643859568
example demonstrates	1.672884134739408
related information	1.6388289572157362
general form	2.0411019972099975
verb types	1.6394491797169057
formalisms used	1.5188051370135929
useful knowledge	1.507007268326853
linguistic constructs	1.6358566425614818
6 evaluation	1.5310785803838152
interesting cases	1.5515223067572055
information included	1.6246289167870047
one system	1.928084074429
extended version	1.910200444025301
final version	1.8711450172624189
common problem	1.731146052909204
many purposes	1.6761656524213182
lexical functions	1.6185367942256224
input speech	1.7116058037794364
text understanding	2.0058307873248387
binary classifier	1.75299369249059
summary length	1.6167743137248345
class 1	1.591096280690849
single label	1.5194321743502117
tree 7	1.5335731739369804
corresponding pos	1.5111306094308554
two rows	1.733477157864191
useful feedback	1.5611231576018243
step 6	1.7473270269621857
information content	2.011830467820622
head constituent	1.6400947763949572
branching factor	1.7152738495926139
oflazer et	1.5408560126058508
overall process	1.618103745190334
similar features	1.6946633528721495
mentioned entities	1.530072891325321
morphological variations	1.6244843928316635
informative features	1.688641176416493
using data	1.9237154873417415
final system	1.8037885370469815
interpretation may	1.6548911840144862
multiple language	1.502140269292856
computational linguistic	1.9140100909014162
tipster project	1.5386199925350132
question arises	1.929115331913271
edges e	1.5177700906840073
vector quantization	1.5798279917567954
nlp application	1.777690593851437
short vowels	1.6305000502342903
iterative method	1.5177700906840073
tree classifiers	1.535693840548428
specific form	1.6447632521349471
phenomena like	1.7722692119182184
perform best	1.6741797752731205
process generates	1.507007268326853
keyword search	1.5206711799175472
inherent limitations	1.5886561992520427
parsing system	2.012681431145376
algorithm without	1.5482870987025659
central role	1.7902286652986104
thus cannot	1.7344099527418115
hidden variables	1.6185954776150528
ordered rules	1.512120645939121
correct sentence	1.7805731024715368
system integration	1.5109871437896232
likelihood ratios	1.5266127603343378
class x	1.5367637113989594
feature agreement	1.510181823566108
parameter tuning	1.789739263094394
occur independently	1.5656419863875413
verb predicate	1.5650484087585572
two grammars	1.78295202450654
hypotheses produced	1.5156323250636556
evaluation procedure	1.814012812403687
corresponding positions	1.5111306094308554
two uses	1.5713778350675331
later versions	1.586137214651611
common type	1.7303616248288165
unsupervised manner	1.6037357443466231
temporal intervals	1.516491127995908
hotel reservation	1.6178836620206607
linguistic formalism	1.6161932921112092
particular level	1.5973170078864487
act theory	1.6360584948694448
darpa speech	1.678220826924736
transformation rule	1.5926451412720581
constituent parts	1.6836836094425731
different numbers	1.8938967463584737
additional test	1.5037367200448983
using tile	1.6862623517634696
input texts	1.8342395328513648
linear representation	1.5473635513992763
unordered set	1.7065229638148365
magnitude larger	1.6073715579162529
following points	1.75176197502999
frequently co	1.6149795964904234
grammar rule	2.0797468234899696
wordnet taxonomy	1.560169636756968
difference lies	1.7017362832860266
computational complexity	2.0927751959877376
analysis techniques	1.837869846979613
scale grammars	1.5408327377227247
interesting insights	1.511201220738707
target language	2.3433735955341426
psycholinguistic studies	1.6320354670147565
three non	1.5459182428709468
example discussed	1.5408560126058508
potential solution	1.5378646275607575
better ways	1.5188051370135929
formal framework	1.732266191908249
processing would	1.6213967902634534
grammar must	1.8405633778995192
computational resources	1.7982285980279782
open text	1.5181855732584753
possible context	1.507007268326853
corpora like	1.5338735539822874
english expression	1.6261307844552735
procedural knowledge	1.5927740069700231
turn may	1.6481174069026685
cardinal numbers	1.5432756752761154
speaker would	1.7384345768444645
rules 2	1.5706734993195162
bootstrapping procedure	1.5707237269685685
verbs could	1.5408560126058508
formalism based	1.6010101084822783
set l	1.8065815830643248
different assumptions	1.6073715579162529
speaker might	1.6387978386658182
let e	1.7330996727906176
different case	1.5905396773117442
procedure must	1.594773126307433
attachment decision	1.5404153166140095
new task	1.78663051161727
constraint logic	1.5563408731789325
particular form	1.7680599214474206
criteria based	1.511205120616144
often cause	1.5445134575869865
lynette hirschman	1.5697576149933161
processing methods	1.6686149068743488
000 documents	1.7345265568300638
obvious reason	1.583051127543801
computational requirements	1.6058952092043035
words w1	1.5440207036648976
step procedure	1.6320354670147565
two copies	1.604998684026954
present context	1.6974749152736235
two pieces	1.8037318445317454
incorrect word	1.6406052752274713
article presents	1.5893265024752146
james allen	1.5514607572627885
level alignments	1.5730113211053913
classification experiments	1.6444336812943763
section reports	1.6002993454155385
better estimates	1.5188051370135929
multilingual text	1.6883585682921605
based speech	1.7596138825877101
original words	1.636127944690715
several sites	1.5977445137418091
quite straightforward	1.7344099527418115
complex model	1.6693803523650859
line 1	1.606339064235895
level annotation	1.5570089052947091
experimental framework	1.5164911279959081
adapted version	1.5645241345053709
elementary structures	1.6274957428357184
additional source	1.6630783357424366
strong evidence	1.8831804098028342
distinct components	1.522760613339579
texts using	1.8483396492887714
term appears	1.606182584773287
different user	1.6300799349406287
declarative representation	1.5988242883426205
treebank parses	1.5528316696194875
syntactic contexts	1.8425915590611501
sentences like	2.1013524364573772
various strategies	1.6919937194855956
phonological processes	1.5262809036837006
component words	1.7717656185708348
expected agreement	1.552192240824893
middle ground	1.5926202003411678
rationale behind	1.783245667489377
information gain	1.8791889640120607
preference ordering	1.5202826496498665
correct referent	1.5358858658794248
systems might	1.579873144038688
complement structure	1.542683278128981
anaphoric definite	1.5071716658516903
tree rooted	1.7591426148939062
bottom left	1.554089533378209
two character	1.533540614646662
matrix whose	1.5690244007917742
writing style	1.744966138573997
relevant work	1.6315901311540408
introduce three	1.5156323250636556
following theorem	1.6994631953974593
complement types	1.50119175880199
grammatical system	1.5211544130435133
resolution based	1.5087853581980926
multiple senses	1.85399402792753
conceptual representations	1.6631881630993925
used ill	1.650718044314063
performance improvement	1.9549040998395182
last condition	1.530072891325321
phenomena observed	1.5037367200448983
level alignment	1.684592532668004
preference semantics	1.5881482659815926
detailed knowledge	1.7615962632264215
test examples	1.7870560811073617
performance among	1.6289349127929256
different parsing	1.7861276361125964
function whose	1.5525978349470977
punctuation symbols	1.5016130704164397
relative entropy	1.643616563726165
primitive actions	1.5457643600698971
relevant concepts	1.714900048640947
main components	2.013526973079352
feature represents	1.5657848105751835
different positions	1.8455857116900105
first blush	1.522760613339579
syntactic variation	1.6449156310370956
based summarization	1.6344976581023163
important questions	1.594773126307433
semantic resources	1.7328473799645296
boolean combinations	1.5237356070158423
single speaker	1.6956182228602206
following sentences	2.0333433427734358
theoretical issues	1.6070437084218803
says something	1.533540614646662
complex models	1.7680474203375174
processing required	1.66156771763727
wilson et	1.5270498256194385
probability p	2.1946387670750473
resources needed	1.5634115707207188
different entries	1.6023597487734618
domain data	1.5960117026070866
act like	1.6411558246537032
give examples	1.835517533194863
documents selected	1.5036589189128708
rules might	1.583051127543801
wordnet 1	1.8301820023927975
different descriptions	1.5487461920851402
system models	1.530072891325321
single case	1.5622818522186857
false alarm	1.6935731081262189
query words	1.6262683798388764
phrases found	1.5406210028261875
directly correspond	1.5338735539822874
two roles	1.7232829055338486
simplest approach	1.6289349127929256
e g	1.7590441567601354
associated probability	1.542348578385745
parsing approach	1.8479983875956139
third set	1.6791278880803084
task definition	1.7432423871421163
work well	2.1405333366566524
algorithm depends	1.5412463844444777
free grammars	2.226170811154158
left context	1.9482134475972674
structured knowledge	1.5323410160957247
dragon systems	1.5611265469383597
providing feedback	1.5547189562170496
matrix sentence	1.5683054507240135
particular task	1.955951463418374
lower recall	1.8056266150218963
chance agreement	1.6261881202533706
reference sentence	1.5490207795581834
special words	1.5222638249634683
categories may	1.6976636234158158
dependent model	1.5156083168889984
type restrictions	1.535727155868841
selecting words	1.5148722773105288
describes two	1.75176197502999
unsupervised learning	2.0095927335787573
first words	1.5511943810330642
subsequent iterations	1.586137214651611
head child	1.527861603038757
selection process	1.9896734851697218
many words	2.167694763240309
binary classification	1.9420664872282067
extraction algorithms	1.5906723352433734
computer implementation	1.6782208269247358
top ranking	1.530320126517967
wsd system	1.747318484290781
consider figure	1.7177247972056076
current corpus	1.6482958236884362
example 10	1.5942408057154909
knowledge needed	1.85671442895934
complete sentence	1.984057102097533
automatic extraction	2.0083075231307124
john loves	1.6005366924583784
object role	1.530238248477422
two trees	1.9650794800475597
resolution method	1.562099207014657
treebank project	1.6528845242312589
directed edges	1.5749574214167732
words together	1.777414274128868
table shows	2.124425229865015
certain contexts	1.837405021489463
cannot know	1.6056035510036821
many users	1.725393852123075
information states	1.5063527688487115
existing information	1.613624660358136
negative class	1.5056777563238999
sentence like	2.057117493711016
particular combination	1.6661204761131607
verb v	1.8287801445521739
observed distribution	1.5156083168889984
also argue	1.5445134575869865
comparative studies	1.5395099438048585
human judges	2.0031506892440483
approach proposed	1.8288230671874963
word sense	2.355170226911954
one english	1.7776327665121143
systems could	1.8433145374338125
produces two	1.7051629942513449
two interpretations	1.8151277728250559
position 1	1.5608527151436362
given system	1.6961394033344752
sentence interpretation	1.6258574628751319
text system	1.5538017058661813
relative weight	1.699911021301493
use pos	1.5418612528934028
precision score	1.7597204739600085
confidence level	1.904601374939091
features like	1.9055265581482663
review previous	1.5111306094308554
appropriate model	1.643362624369706
natural question	1.5973170078864487
four words	1.8505152155432596
earlier work	2.1906688277966326
search procedures	1.5622755420968637
distinct sets	1.5188051370135929
01 iv	1.6392309045877975
current systems	1.9380869313012727
similar experiments	1.6614869878819327
current status	1.8375763449853415
quantifier scoping	1.6415154480404053
act type	1.534363554462397
universal grammar	1.685185998036805
literal translations	1.5621227545901766
consonant cluster	1.5443807925117121
dempster et	1.8349907738187798
three sources	1.7286956190781877
word models	1.8224866731879212
know anything	1.507007268326853
accuracy measures	1.5269930689946518
task since	1.6783930166760777
complex grammar	1.5445134575869865
speaker turns	1.5335872277066736
become increasingly	1.8838667647361274
unigram language	1.5148546983121989
underlying language	1.507007268326853
automatically identifies	1.5188051370135929
name appears	1.5156323250636556
relationships among	2.0998512911339633
semantic relations	2.2316688557343687
words instead	1.6658904115324105
one grammatical	1.5378646275607575
key words	1.8606324784158275
form must	1.530072891325321
better results	2.019705555146717
clustering methods	1.7672970964232508
three domains	1.5630361863305906
appropriate sense	1.8339079546678931
disambiguation module	1.5654766728996532
term weights	1.7018960227231137
word sequences	2.145646383221661
rules could	1.5796853575155074
relations must	1.6339957705783554
three slots	1.517094492681228
corresponding sentence	1.7083443654723212
arabic text	1.5517982852003214
full description	1.801274333792275
two children	1.7040280573849351
processing task	1.6930755887020918
alternative approaches	1.8288230671874963
table look	1.512536810936869
corpus may	1.8195136393974707
another feature	1.9172525188683882
known examples	1.579873144038688
focus articulation	1.580589614195869
tile parser	1.6008079183721948
representation contains	1.5887468924062063
directly model	1.5156323250636554
screen shot	1.5156323250636554
phase 1	1.6187508998330613
using finite	1.7982285980279786
words plus	1.5177700906840073
related task	1.641155824653703
ways depending	1.6056035510036821
useful data	1.5511943810330642
subject may	1.559620194736931
phone numbers	1.500669693444868
possible set	1.6481174069026685
science research	1.5211629864223775
oriented parsing	1.696918689477927
human judgment	1.7428229365679606
first approximation	1.9791896903880488
sentence type	1.7251932847286442
nominal expressions	1.563902840263915
test sentence	1.8275472575528902
tagger would	1.530072891325321
similar feature	1.5739592165010823
disambiguate words	1.507007268326853
small improvement	1.8189037239643289
extracted features	1.6300792061475067
specified context	1.5378646275607575
task used	1.5378646275607575
example like	1.60913070763407
optimal values	1.672245431695917
form larger	1.524534874088451
lexical meaning	1.8321749783170327
left daughter	1.5129453488866833
improve performance	2.1673381906470413
linguistics literature	1.7903014439406069
certain kinds	1.7175674170390836
error detection	1.7476380272039096
used method	1.6084968011212868
discourse segmentation	1.7371597383658144
model obtained	1.5408560126058508
structures derived	1.5210740331567267
roughly correspond	1.7720369317764424
algorithms used	1.8617440494186417
data driven	1.7135781995394148
netherlands organization	1.6164339756999313
original paper	1.5230483704868893
abstracts away	1.6920919321535788
different constituents	1.5977445137418091
current implementation	2.198826245891472
one crucial	1.5445134575869865
work best	1.6961394033344752
dependent features	1.6491433207766195
soon et	1.5778306884429045
ary predicate	1.5086023602732959
new state	1.6167034602344441
first step	2.120734803997002
computational purposes	1.583051127543801
compound sentence	1.547363551399276
systems capable	1.6548911840144862
e w	1.5338229730039599
patterns based	1.6618077693135969
performance gap	1.6064310100927288
classes used	1.6785607164239145
word corresponding	1.524534874088451
approach allows	2.003834070963369
rr 1992	1.6976636234158158
individual nodes	1.6817139837574677
markov models	2.1176895155425064
3 results	1.5781539692435587
examples extracted	1.5230483704868893
id number	1.5234217453831536
results across	1.7471845324497086
resulting sentence	1.6481174069026685
structure constraints	1.5642725698574718
word grammar	1.6201899134230133
annotated text	1.886080668375267
sequence given	1.7060406711249585
analyses per	1.5371982986173842
weighting method	1.5159316299042302
many approaches	1.7501491085716443
main characteristics	1.7458878463811769
first noun	1.7900116515163775
mckeown 1985	1.554901635920694
weighted combination	1.683041275722836
many noun	1.5378646275607575
english terms	1.638191985297384
based processing	1.7730743483512408
many categories	1.5412463844444777
general relations	1.5269930689946518
semantic specifications	1.5525978349470975
generative process	1.705151829522187
bigram model	1.9063176946675053
lambda abstraction	1.5748567577278902
perform two	1.6502761822926497
appropriate features	1.6941875360751872
root category	1.5171492770579162
one variable	1.7249229868280846
information structures	1.6225998747572694
system took	1.507007268326853
model pr	1.5690355522643935
ne tagging	1.5822154891425846
use examples	1.5536240126687622
one application	1.8426924091583354
new relations	1.6350313088824242
among researchers	1.6164339756999313
7 points	1.5378646275607575
future applications	1.62412689036662
preliminary experimental	1.6137947517368634
possible readings	1.8187130091957417
human subjects	2.0090767315884914
structure description	1.561707184647436
many domain	1.5211629864223772
future tense	1.7044524735471671
closer inspection	1.6458797346140277
paper deals	1.8701517965257952
predicate names	1.5738708991826258
certain properties	1.8213252074629112
overall processing	1.579873144038688
special tags	1.5731122769149346
space model	1.9623222421710156
sixth message	1.579873144038688
case grammar	1.778529850422441
two samples	1.5695222973163587
english side	1.837596322345619
default feature	1.5525638623447429
clearly shows	1.7999124414051257
structural characteristics	1.5908167675916909
query translation	1.5310916338826117
specific issues	1.653075357887709
two output	1.504934031784749
either words	1.5482870987025659
applications including	1.6388370153723533
four categories	1.9198238878340121
dependency representation	1.703666617296777
context information	2.017600500223544
avoid problems	1.6002993454155385
user said	1.505155750429114
several nodes	1.5156323250636556
protein names	1.609994623076196
three steps	2.127785614087691
large grammar	1.7647785456547198
multilingual lexical	1.6305794768868784
general world	1.680699922040811
simple regular	1.5706734993195162
readable version	1.6569504373202975
problematic case	1.530072891325321
data consisting	1.636127944690715
first translation	1.5112051206161443
classification system	1.7992051438159762
first paper	1.658060625176417
three variants	1.5519690017139713
high performance	1.7559437726216702
method uses	2.044850878208248
preliminary analysis	1.774777000835246
many translation	1.5156323250636554
several purposes	1.5511943810330642
training session	1.5257027020067777
normal text	1.507007268326853
notational variant	1.5111306094308554
x occurs	1.6075544899430227
based dialogue	1.6501184476566637
derivation steps	1.5107844011873683
improve system	1.6744785539419769
representation formalisms	1.55955788508706
better language	1.5767760138793052
pronoun resolution	1.8639532165725972
user query	1.8184205426765834
recent successes	1.5188051370135929
x denotes	1.6031224303496519
distinct values	1.513130125383453
iterative training	1.5041134997716235
efficient manner	1.6724173909604771
structures whose	1.5156323250636556
latter approach	1.847453304617525
words inside	1.5985995282484877
translation model	2.061925957278484
word containing	1.5037367200448986
particular state	1.6028999098112309
tag sequence	1.9919211115204734
representing word	1.62412689036662
method may	1.8481803798836942
learner may	1.5378646275607575
first element	2.0564314486595157
set contains	2.057644719980217
generative probabilistic	1.5947983930134422
complex sentences	1.998912933621239
applied rules	1.507007268326853
supervised algorithm	1.5689783184335884
also improves	1.648787373441388
thematic structure	1.654752725308517
large n	1.5037367200448983
rule like	1.7536775400520195
type hierarchies	1.588641312122693
operations used	1.5547189562170496
used e	1.5645241345053709
appropriate points	1.526993068994652
component produces	1.5412463844444777
different head	1.5041134997716235
methods used	1.8566249989387078
range 0	1.6816988370728096
original text	2.0873617524155064
grammar used	2.0399543060808814
sri language	1.513130125383453
also form	1.5445134575869865
system provided	1.5378646275607575
standard dictionary	1.5707378216701455
clustering task	1.5403034171359227
sun 4	1.5177715764513198
results obtained	2.3187204392775715
key concept	1.5172519159413136
linked words	1.521162986422377
linguistic theories	2.0629305266455398
arc labels	1.6074842784073688
see figure	2.42352217955289
including speech	1.6631574457448046
attachment ambiguities	1.811698503620892
phonetic similarity	1.5319286547610256
x must	1.5525978349470977
words extracted	1.7562780519615577
method outperforms	1.7672765267660713
documents contain	1.6770235151848842
performance varies	1.5612815851895034
another instance	1.5636877381459493
following way	2.25205893730824
features provide	1.6361279446907149
aone et	1.5036575034310684
possible choices	1.6701008999321498
american journal	1.6989004071215583
et l	1.697033836591878
grammar parser	1.7906889577889733
performance degrades	1.712733419282903
occurrence count	1.5563893320981304
direct relation	1.62412689036662
list includes	1.5511943810330642
two minutes	1.6160313784345908
becomes apparent	1.583051127543801
first round	1.7152876918955582
words correctly	1.7037457231259912
list would	1.6655102285632384
form p	1.8025200387192477
conditional log	1.5441561418865781
basic building	1.7051629942513449
second model	1.7548248288904362
e r	1.684266813219379
speech category	1.6589667385758489
practical system	1.7943022735270753
bigram features	1.5236684449442244
tense verb	1.7186122217342656
two word	1.9936883144183606
kim et	1.7459126974440875
set sizes	1.7016431049622436
data extraction	1.805080169369907
e q	1.703236210259739
n possible	1.5156323250636554
corresponding verbs	1.5408560126058508
including pronouns	1.522760613339579
specific reference	1.5929928949821113
right edge	1.5848288443610183
research contract	1.6213967902634536
potential answers	1.5428110407314217
output string	1.774413773004466
infinitive verb	1.5081682632911075
pos tags	2.1626658326440564
free rule	1.8364808840231002
great detail	1.6084968011212868
complex constructions	1.659662148122272
data sparsity	1.7737221578415046
semantic operations	1.5987853107912973
highest accuracy	1.8943143207438158
uniform prior	1.5689783184335884
second function	1.5408560126058508
individual rules	1.7695402601932653
old information	1.5345245923169326
possible tags	1.7936710533635452
second sense	1.6553167927357264
recognized words	1.5642106254237662
shallow parsing	2.0025225117505037
high frequency	2.025853188204727
object pairs	1.5350303091333972
common assumption	1.5547189562170496
names found	1.5290812457000524
require two	1.5571495092265906
language model	2.3524724530745558
new nodes	1.757865397794149
hmm approach	1.5573537001177145
evaluation methodology	1.922434329700506
recognition model	1.5494218654279388
lexical choice	1.8943979395399808
word sentences	1.5876019946008202
use sentence	1.5188051370135929
travel domain	1.5884159036814398
people tend	1.6229151093758099
parser first	1.5926202003411678
specified set	1.5973170078864487
lexical variation	1.5406755700533967
system within	1.6939639871030243
describe one	1.6785607164239145
inherent ambiguity	1.522760613339579
learning problems	1.672795806868395
extends beyond	1.591823957496618
two topics	1.561123157601824
maximum score	1.7062395598419626
brill et	1.588513119950588
complex domains	1.6276749302936424
various degrees	1.7595297220275463
probabilistic language	1.7452934699936842
every parse	1.5156323250636554
candidate sentence	1.6231163786630538
data flow	1.593003447759218
frequency statistics	1.560800925855315
scenario template	1.6051546246136799
baseline model	1.926565465151963
model containing	1.6550184848056928
4 shows	2.4661451781070793
words included	1.6984019290755374
compound verbs	1.5147674253307652
text production	1.597384443400315
structure like	1.7639249949365914
constituents within	1.522760613339579
work also	1.7857836815978843
1 illustrates	2.124820052021077
type c	1.6311019295144318
permits us	1.7753084176520224
inference rule	1.8344038699751941
similar system	1.6315901311540408
current character	1.5102048384948001
alignment using	1.5806234267011035
pruning algorithm	1.526347934756413
every class	1.5371982986173842
data files	1.7856562922407349
reverse order	1.8707617104136087
sample input	1.513130125383453
french lexicon	1.506173888464562
backward algorithm	1.874664403812382
tree approach	1.5069356006558667
negative evidence	1.5792080612908714
discourse unit	1.6864516358737887
single token	1.8597419358292
theoretical considerations	1.507007268326853
value indicates	1.5839427016111838
topic identification	1.5889007066609084
performance degradation	1.7597088129876912
find answers	1.6515771498200786
database used	1.5839427016111838
extracted data	1.5567607724222934
past perfect	1.5474027376614612
relevant texts	1.5473883632515912
user input	1.9569958088061752
common errors	1.6152953234723224
eurotra project	1.5036589189128706
language models	2.2648021805416008
additional mechanism	1.6002993454155385
string e	1.6037357443466234
large improvement	1.5582958710952315
important problems	1.710004784518707
model differs	1.5839427016111838
brief review	1.6213967902634532
speech using	1.672219863528485
user groups	1.507007268326853
classification approach	1.618594781960272
approach assumes	1.65303397348486
distinct parts	1.5645241345053709
applications involving	1.5459182428709468
one child	1.657705854632255
two random	1.5784904253929217
anonymous acl	1.5111306094308554
lexicon would	1.62412689036662
baayen et	1.5657848105751835
symmetric relation	1.5230483704868893
sentential level	1.690112227936557
distortion model	1.5488689013749322
previous papers	1.6190328102134979
method combines	1.5306061495832655
metrics based	1.503285808680889
experiment shows	1.8556571080420678
computational method	1.567922330580758
word f	1.6093823223544597
word within	1.804067810738927
input corpus	1.537033177033388
lexical gaps	1.6040455042669342
nominal modifier	1.587161501621602
stochastic model	1.7357822165220393
select sentences	1.5338162779728006
semantic units	1.8192396988392743
good translation	1.7617170852878963
small vocabulary	1.6741797752731205
practical machine	1.5033530761956282
structure unification	1.5093397457543452
atis system	1.6484716611697317
terminal elements	1.56497555876579
likely translation	1.5620473238340082
user profile	1.612328142862135
generation rules	1.5497574868312136
dynamic programming	2.2495278774210603
text editor	1.6603917469516178
using simple	2.0160688283033146
includes word	1.5338735539822874
rule x	1.641593353048265
word consists	1.7016656224425795
ratnaparkhi et	1.5571126996125668
strategies used	1.7301741018323267
answer could	1.5037367200448983
given application	1.7586107048444115
test shows	1.507007268326853
main parts	1.8396434092696248
data without	1.78166034049388
randomly drawn	1.530072891325321
based decoder	1.5562031510100192
descriptive texts	1.5465098262657855
agglutinative language	1.7179520738135299
effort involved	1.7179482056291908
pour les	1.5846233162854135
english part	1.737760329410712
linguistic expertise	1.5459182428709468
necessary step	1.60913070763407
source material	1.563687738145949
cluster size	1.5271920990130223
desired information	1.7861276361125966
concepts used	1.7074092050419734
increase recall	1.5973170078864487
wall street	1.6918114252782588
clause grammar	1.803426109945851
categories according	1.55955788508706
smallest set	1.7199340743580924
different query	1.507007268326853
specified threshold	1.5887468924062063
important class	1.5771944393968738
performs poorly	1.6876319470917474
examples 2	1.5459182428709468
sentences tend	1.7321204056549946
language learning	2.1406755784847773
say something	1.743330208909181
english documents	1.6014407763165555
verb entries	1.6899809028316808
words involved	1.7897594399579615
probability value	1.6555099422832618
starting position	1.520353997580974
procedure may	1.7141428077458374
dif cult	1.8013199419980666
third row	1.719041687932755
05 level	1.6178589689425367
single person	1.5578214127463763
information contained	2.191655604723052
possible reasons	1.743960294459502
stochastic models	1.75873646280147
practical approach	1.5578656975467773
error message	1.6820847341778178
much attention	1.6884496795750303
separate data	1.6010101084822783
perform tasks	1.591823957496618
additional information	2.0732427681710015
task structure	1.5001511914224923
handle various	1.6210934563561632
language grammar	1.7591858570052392
support system	1.557699393996081
straightforward application	1.5188051370135929
corresponding input	1.5482870987025659
standard definition	1.5760693093552205
statistical method	1.9421245873520283
independent information	1.5459756252045591
computational framework	1.7437754374725918
constraint must	1.5408560126058508
case slots	1.5413323463709347
simple heuristics	1.8722402029590433
vector w	1.5652188378639968
nirenburg et	1.680953447337163
sentence boundary	1.8590923144367903
predictive power	1.8820174978590305
many training	1.5645241345053709
atn grammar	1.5941783241429945
x p	1.6812731569662351
second object	1.5475277297445245
treebank ii	1.6743374918961345
entire set	2.005545986231595
social context	1.5177700906840073
parallel structure	1.5225304125061803
parsing becomes	1.5111306094308554
sentence alignment	1.805807629338616
example showing	1.522760613339579
last word	1.8359067943777883
terms occurring	1.6544208897318426
parser tries	1.6303444022729265
systems built	1.6411558246537032
syntactic rule	1.8210784607659367
single frame	1.5381766791708387
category information	1.7008271520832876
unsupervised way	1.5265421490264863
similar characteristics	1.5111306094308554
dans ce	1.5688928744775175
english texts	1.9852587350027373
following heuristics	1.6285930450163055
tables 7	1.530072891325321
another argument	1.643362624369706
information resources	1.602359748773462
connect two	1.586137214651611
initial analysis	1.6010101084822783
formal semantic	1.6000582427363232
one parameter	1.755185558689591
large feature	1.7023607714333093
using constraints	1.5211629864223775
system performance	2.2367388824040306
unlabeled examples	1.5239718708139227
separate feature	1.611203449372866
using svm	1.618103745190334
german word	1.7810022790103979
structural description	1.871911227316431
basic concepts	1.7869277998142328
constraints based	1.5416598359716174
project number	1.5511943810330642
two modes	1.8554435614074336
thus becomes	1.5547189562170496
reference time	1.6449289174230564
another approach	2.1003773917014135
topic shifts	1.6798009358048587
thousand sentences	1.5945134301392114
seed words	1.5950067538654529
experimental setup	1.9073768774975373
lafferty et	1.9094991820058893
put forward	1.514095573914823
deep understanding	1.6768381249411437
preceding sections	1.710004784518707
first line	1.622800721765011
im walde	1.5914624841423683
parsing algorithm	2.2141891535255103
smaller corpus	1.6113346817822478
character strings	1.8862590444223801
5 concludes	1.8191665297540134
direct connection	1.6031224303496519
phrases would	1.510181823566108
next subsection	1.8434383677004231
corpus according	1.6862623517634696
abstract structure	1.5756869131546112
character string	1.6642016002701587
search strategy	1.9720589514094675
means algorithm	1.668934655813926
current point	1.5432756752761154
feature based	1.7247462184201
similar approach	2.032775158126581
wordnet database	1.5657848105751835
existing resources	1.8205873617478974
substring table	1.5191487533489458
made use	1.901215750558498
statistical framework	1.7308451372786298
transformational component	1.5773363985654312
single query	1.619472687377359
one variant	1.507007268326853
general words	1.5736712336159786
word similarities	1.5276926419628722
infinitival complement	1.5689717897953792
basic notion	1.5482870987025659
information state	1.5665272019136158
three phrases	1.506173888464562
instance x	1.6092156388257597
model consists	1.891235000769177
results indicated	1.6527294781610564
dependency theory	1.5150601109177377
embedded clause	1.850839809065501
cant improvement	1.5571495092265906
hold across	1.5425370803688765
certain point	1.8187710772010472
system components	1.6036397658099333
unsupervised system	1.5536697401931043
new words	2.08780797041817
make predictions	1.7679712432108752
per class	1.535727155868841
target structures	1.5270912097144373
current rule	1.5877046642631585
null however	1.7882649160849862
questions used	1.517770090684007
retrieved information	1.522086685140527
give results	1.7158821441137806
significantly improves	1.882367585477925
statistical approaches	2.077938133401966
possible ones	1.594773126307433
random variables	1.855515096581466
semantic framework	1.5856160021217736
please refer	1.7291664368104873
experience suggests	1.594773126307433
integrating speech	1.5195036419368688
disambiguation using	1.7680827580894374
comparable performance	1.7903014439406069
based systems	2.234847951112581
called discourse	1.5839427016111838
new users	1.5514607572627885
maintain consistency	1.579873144038688
watson research	1.7605101651189745
set w	1.5881174695673654
substitution node	1.6289676568979266
precise description	1.655018484805693
naval warfare	1.5547189562170496
empty string	2.0490930604213733
constitutes one	1.5547189562170496
processing unit	1.518745282578566
also calculate	1.583051127543801
reliable information	1.6112034493728657
local maximum	1.6183941633562267
made precise	1.5547189562170496
using bigram	1.5784904253929217
e de	1.6193977708174407
higher probabilities	1.5712230900628588
based parsers	1.832020256257443
basic terms	1.510181823566108
using terms	1.5245348740884508
following subsections	1.9445620190879063
text coherence	1.5863740273717388
idiomatic phrases	1.6327406427880256
requesting information	1.5177700906840073
b c	1.924113651469707
ensure consistency	1.5690244007917742
second phrase	1.504934031784749
robust speech	1.5140838598892352
measure based	1.7783897922119258
daelemans et	1.9231873705098288
learning tasks	1.7686143996532129
synonym dictionary	1.5100283957099991
several categories	1.7305508816393718
translation pairs	1.7829643480097686
conflicting information	1.5571495092265906
see http	1.7875532997987096
sentence means	1.5156323250636554
different input	1.6741803270499043
brief explanation	1.507007268326853
muc task	1.5571495092265906
like structure	1.8291599140008665
semantic categories	2.0678081102817627
category would	1.522760613339579
like tense	1.5482870987025659
perl script	1.5354510398033074
previous models	1.7303318132678416
sentence meaning	1.6212261508646861
translation experiments	1.5631903793254223
different people	1.8520943581984037
current segment	1.5454933677594005
different groups	1.777466451484343
test time	1.5051557504291142
underlying idea	1.6031224303496519
help identify	1.7660159833858404
three techniques	1.5078976439458078
anonymous ftp	1.5414180417122596
foot node	1.765680700544587
input file	1.5673998042253925
semantic domain	1.7792105773916014
tags associated	1.609217242875463
word vocabulary	1.8713756493515945
simple features	1.716869108797698
discourse topic	1.6097140705755992
representations produced	1.6164339756999313
statistical translation	1.8965640124870902
n u	1.6665893162717775
additional level	1.613364934425665
semantic concept	1.6949078976153151
languages involved	1.754934152540829
speaker intends	1.7711208856047302
one conceptual	1.530072891325321
total words	1.5716157403242
et ai	1.925581217228099
form may	1.7436706932296322
head node	1.5801915838759841
language sentences	2.033991397768834
relations may	1.8771290980432245
uses knowledge	1.6190328102134979
fold crossvalidation	1.567922330580758
figure l	1.7691724745346729
index terms	1.5714987698626155
prague school	1.5484545589550256
xue et	1.521798762287671
different algorithms	1.8716021653641666
different question	1.5798548254880234
feature could	1.5156323250636556
et al	2.8392753145207257
generate two	1.6635555237714856
different subsets	1.7557648698659123
learning community	1.5611231576018243
rules needed	1.591823957496618
hash table	1.6829478678885816
clausal complements	1.593648130926087
effort would	1.5445134575869865
one classifier	1.7369229275416953
contiguous sequence	1.5078976439458078
action sequences	1.5277406914997418
score would	1.5482870987025659
nouns using	1.6329374072287361
label sequences	1.5719457650756206
main problem	2.0125208862625295
dependency representations	1.5740300543427277
complex morphology	1.5749574214167732
semantic translation	1.6812614877039236
obtaining information	1.5177700906840073
two pos	1.5216148258836173
c p	1.5703715244044354
particular words	1.8469626804893908
also handles	1.6862623517634696
lexicalized pcfg	1.5285526259576043
foreign words	1.6820881803339733
sections 02	1.722332235516736
atomic formulas	1.5183187372176896
input may	1.6431586438232255
strategy allows	1.522760613339579
makes explicit	1.8061958069999318
three columns	1.6192219158965702
evaluation used	1.5378646275607575
average score	1.7906517373030924
words would	1.9751851142888157
price et	1.5011917588019899
phrase translation	1.7353443979780068
pages containing	1.587404912495944
reflexive pronouns	1.745210518498927
general pattern	1.798876943144634
morphological tags	1.6048244870028905
single term	1.66721404272091
scoring method	1.8265395991148292
language skills	1.5360785028369
quintus prolog	1.754682578532674
parsing result	1.610799000344806
specific topic	1.6963149953958456
word frequency	1.9926976272776826
good estimate	1.579873144038688
differs considerably	1.5111306094308554
studies suggest	1.5111306094308554
new framework	1.6672233936075371
node c	1.6284879140154502
practical application	1.864739321992286
one kind	1.636889636791354
another study	1.5706734993195162
sentences produced	1.6487873734413878
rules shown	1.5425370803688765
group words	1.5414180417122596
general approaches	1.561123157601824
various models	1.7303616248288163
latter problem	1.719365286993631
large vocabulary	2.0129421964404055
single input	1.653691045130907
explicit indication	1.5211629864223775
information already	1.7663553975095576
2 seconds	1.6661204761131607
common properties	1.5652297135534465
complete picture	1.530072891325321
research problem	1.659662148122272
default values	1.8931821672604872
major goal	1.715155864392924
clause constructions	1.5078797213408792
theoretical reasons	1.5338735539822874
compile time	1.685779092491382
occur within	1.6635559094612693
develop algorithms	1.6339957705783554
prior discourse	1.5944509306191263
important area	1.6392309045877975
bound variables	1.6176091897330616
syntactic tagging	1.5705757689872493
simple list	1.6635555237714856
separate components	1.594773126307433
early work	1.926579958380938
one obtains	1.527798843283345
corpus described	1.7341395890835507
recognition experiments	1.7013319387680468
two positions	1.7848562803694743
new approaches	1.6785607164239145
medline abstracts	1.7728355686089998
significant correlation	1.5766585145254073
different forms	1.855150217703474
two principal	1.6877698035461837
probabilistic chart	1.5338735539822874
shows another	1.5408560126058508
specific pattern	1.5414180417122596
analysis reveals	1.6761656524213182
organization name	1.7458791772026419
challenging problem	1.6785607164239145
word association	1.7416365634360504
paper summarizes	1.5037367200448983
major drawback	1.658060625176417
translation technology	1.646226844257487
tag sets	1.824220580296467
language constructions	1.5300728913253212
new categories	1.5769568439112072
manual analysis	1.7189056453255587
following list	1.8085266261493145
remaining sections	1.6388370153723533
shen et	1.5059553434740955
first group	1.8114840858385595
two descriptions	1.6782493549364816
fixed window	1.5582958710952313
general domain	1.7130804508438076
linguistic model	1.7957296126258662
strong bias	1.530072891325321
one possibility	1.759131664415351
optimization method	1.5676517392350746
general concept	1.748027173239186
set r	1.7121443372936684
dialogue situation	1.6066593791852315
data concerning	1.6190328102134979
cognitive linguistics	1.5187014267751215
intuition behind	1.9278086857170953
compare results	1.7458878463811769
case system	1.5149643858508846
grouping words	1.5459182428709468
stress patterns	1.5111076347572878
set q	1.6184033873746564
new domain	2.117047257453244
retrieval methods	1.6679521469523344
without modification	1.7637790091991556
double quotes	1.5317115329863435
core engine	1.5453687309821331
single question	1.5037367200448983
machine translation	2.5488081388126718
new concept	1.8437724157774869
probability space	1.5260185053825464
john ate	1.6430682280793687
phonetic form	1.506173888464562
three parts	2.1129764078934836
following conditions	2.063272913153911
language consists	1.6288981647434881
programming languages	2.0122856050638878
current efforts	1.6192113542886108
roles associated	1.586137214651611
strong constraint	1.5226149809040495
also examine	1.7088092136999036
formed sentences	1.7837855549883075
linguistic sense	1.5839427016111838
vocal tract	1.62886116945921
large system	1.567922330580758
objects may	1.7522485220301844
semantic links	1.6560135133127376
higher level	1.9446282548157048
validation experiments	1.6418946342326874
deutsche forschungsgemeinschaft	1.6289349127929256
software tools	1.834811885495787
translation research	1.636252690439584
language understanding	2.3552364246479067
used wordnet	1.6637019978300875
formalism allows	1.7341469152558133
two frames	1.509276906027357
process based	1.6593965399315964
decision based	1.524534874088451
data stored	1.5612815851895034
underlying structures	1.5245212235290597
bound variable	1.5599821699643834
partial trees	1.5636683456546905
psycholinguistic experiments	1.645625606458848
using unification	1.6468866812474954
new translation	1.6136283502300106
validation data	1.500533151699699
grant gr	1.7294947296982195
first document	1.5156323250636556
le syst	1.5121970837262828
based grammars	1.9842684213958368
automatic sense	1.616031378434591
dcg rules	1.5439078859965492
anaphoric elements	1.5156159685357808
probability according	1.5482870987025659
parses generated	1.5908167675916909
connects two	1.5869693547698929
linguistic models	1.7683469669374234
structure annotation	1.5442598746891636
et en	1.5995504718848752
xu et	1.5942034172791657
occurrence within	1.5230483704868893
traditional word	1.5839427016111838
grammar currently	1.507007268326853
quotation marks	1.8612147820056604
tables 1	2.020431324491007
large coverage	1.6327406427880258
one representation	1.651501706949397
partial interpretations	1.539624982268884
anaphoric relation	1.5108782700175043
novel approach	1.9931006703377678
generation requires	1.5188051370135929
new element	1.6594660533960885
typical use	1.526993068994652
syntactic phenomenon	1.507007268326853
theoretical analysis	1.6930821463176002
10 words	1.9207893720908107
rules generated	1.642138733295409
main part	1.771286489264682
fixed size	1.6946633528721495
internal organization	1.5037367200448983
separate evaluation	1.5156323250636556
end points	1.7010807553955514
possible reading	1.510181823566108
language interface	2.027820914749652
logic program	1.5525978349470975
make reference	1.795991922364895
contexts may	1.567922330580758
two examples	2.181235664911301
use various	1.7360338540431246
following sequence	1.8361987959851698
tag pair	1.547278971787174
object argument	1.544378505133437
method makes	1.7504812345368606
synthesized speech	1.6745681078560701
contains syntactic	1.507007268326853
final representation	1.6142546735403962
recognition technology	1.8018852633103501
sentence 6	1.5378646275607575
test different	1.5408560126058508
system appears	1.5338735539822874
data analysis	1.86350085460044
annotation tools	1.683213076422712
please note	1.7014507751270238
important roles	1.678973586324036
one pair	1.8420145388633884
particular noun	1.667913900271795
parser processes	1.5156323250636554
complex phrases	1.6058952092043035
koskenniemi 1983	1.5678009097741024
van dijk	1.6295179664261885
extraction methods	1.5045228098560695
6 show	1.7829330146960636
architecture allows	1.6413904216258084
small scale	1.792162570502364
relationship holds	1.567922330580758
explicit knowledge	1.60913070763407
performance slightly	1.5111306094308554
segmentation process	1.7521079362233114
cases may	1.5706734993195162
graphical interface	1.8339423564804156
level rule	1.6731580036926332
root forms	1.81427104457038
separate sets	1.5338735539822874
valuable comments	2.0228533533567825
algorithm consists	1.875813126570177
syntactic parse	1.9791019201188007
main ideas	1.6840631149502636
performs comparably	1.513130125383453
syntactical analysis	1.6354788372480167
x may	1.5466472792181063
noisy input	1.664540718454114
various points	1.7455319583030278
acoustic modeling	1.7331752660894237
segmentation problem	1.6569668113669356
speech input	1.9670363890994491
paper compares	1.530072891325321
following aspects	1.672219863528485
one advantage	1.6252915599767626
often co	1.5676517392350746
particular domain	2.0822604031756127
entire text	1.8303019099109294
smart system	1.5032721164250884
acquire knowledge	1.6325661083056722
interpretation using	1.5195036419368686
computational issues	1.524534874088451
case arises	1.5893265024752146
modern standard	1.5807806095357817
particular concept	1.6428413718077
spoken dialog	1.7098852942122225
languages would	1.5645241345053709
comprehensive evaluation	1.530072891325321
syntactic class	1.7300522568331105
determine whether	2.06448420535173
language research	1.593605786401664
lexicalized grammar	1.759071139792347
recognition research	1.6169986020906966
elliptical constructions	1.5188051370135929
lexical rule	1.6043340210492127
knowledge obtained	1.507007268326853
learned model	1.5727128100169887
base nps	1.505127222573353
des langues	1.538912113255405
functional linguistics	1.5395099438048585
probabilistic information	1.6564643300530966
whose top	1.587700203918434
alignment algorithms	1.6911486409436547
part 1	1.5064508452069114
time stamps	1.638898782932206
multiple analyses	1.6692314209563577
using part	1.7551451061153254
processing step	1.992270976948819
content determination	1.6809235074043167
particular translation	1.5338735539822874
previous discussion	1.6580606251764172
previous sentences	1.8789395510580067
grammar symbols	1.5581620113888106
automatic learning	1.8272509071141254
annotation system	1.5763955456335526
process could	1.707160349122273
words task	1.5822157411862747
sparseness problems	1.6515771498200786
verb frame	1.5642349074192772
main result	1.7600573103280819
selection procedure	1.784282998900181
word appears	1.9828051548057002
argument labels	1.632224608175143
general text	1.750471191761399
possible improvements	1.6888951120289843
analyse de	1.538719277039856
zero otherwise	1.5749574214167732
statistical disambiguation	1.584483801902334
correlate well	1.6144455608275863
aligned words	1.6202338249864887
statistical training	1.549729132180171
paper investigates	1.8733554652861342
start node	1.5913762424923776
7 presents	1.591823957496618
two sorts	1.667913900271795
chapter 4	1.6266551395241795
collaborative effort	1.551149422647893
main difficulties	1.5887468924062063
current address	1.5803694295911777
model learns	1.6169986020906966
recognition performance	1.9796447694634516
syntactic parses	1.6927428536088291
final classification	1.5354510398033074
less effort	1.7538341131741777
tree algorithm	1.684333780550066
word phrases	1.7082998694230107
constant c	1.5794243998957866
following experiments	1.862004303283314
different segments	1.5906852501842725
additional linguistic	1.7617026276762806
test whether	2.0504979031360433
human parsing	1.5183970414243269
second way	1.6862623517634696
aligned parallel	1.7751666863585458
text classifier	1.5731943075943373
syntactic parser	2.051194848716911
use manually	1.5111306094308554
tile results	1.591823957496618
written grammar	1.5425370803688767
intuitive sense	1.574957421416773
per utterance	1.6731225542715984
typical patterns	1.5037367200448983
detailed error	1.6056035510036821
memory size	1.60913070763407
latter result	1.5111306094308554
linguistic object	1.6658523615605025
air travel	1.9195648375165288
following data	1.7060406711249585
symbols used	1.7229556053533144
semantic disambiguation	1.763439898880302
errors introduced	1.767276526766071
whose translation	1.5156323250636554
retrieval task	1.7834451698564695
string position	1.5691829726693056
translation system	2.3377036126852797
clustering words	1.5852397984856759
use language	1.7088092136999036
specific text	1.717445586512029
detailed study	1.6315901311540408
problems encountered	1.846559007233761
linguistic realization	1.757384895920954
nlp technologies	1.5756510084230786
classification algorithm	1.8385299857355688
algorithms described	1.7780946637328394
french dictionary	1.5996979081158844
28 ao	1.9266460226010487
two orders	1.6862623517634696
optimal alignment	1.6067793309785081
mistakes made	1.6802940634668402
2 let	1.6445809590877876
examples per	1.511808504950452
source model	1.5296231101897346
simple feature	1.6806432720347568
comparative experiments	1.5748567577278902
sub j	1.803452155107821
length normalization	1.5478546294028148
language tools	1.5243210366355704
system reported	1.5511943810330642
2 depicts	1.6527294781610564
extensive training	1.5445134575869865
best search	1.530865174829735
use instead	1.5188051370135929
obvious way	1.99719180048792
two sentence	1.6959667372480363
principles governing	1.526993068994652
indirect way	1.5781539692435587
language vocabulary	1.510618969052833
recognition vocabulary	1.516806288742341
show later	1.6213967902634536
problem arises	1.9962269660095813
appropriate places	1.5188051370135929
existing rules	1.65303397348486
vocabulary item	1.507007268326853
speech time	1.5600807108076276
second version	1.7088092136999036
another interpretation	1.6056035510036821
translation evaluation	1.6480429229621574
source code	1.8023483070433408
functional representation	1.50249733128034
nodes without	1.5408560126058508
informal description	1.6036848953283132
additional complexity	1.5511943810330642
information based	1.7744985774486133
resource management	1.9393719060512522
matching mechanism	1.5371982986173842
tagger based	1.6573127369315375
forms one	1.5111306094308554
one explanation	1.6354788372480171
cannot infer	1.5408560126058508
structures found	1.611677089751251
exponential number	1.7231843316253117
model score	1.647996588145832
fixed point	1.5844421472272137
another problem	1.8439941946017293
methodology used	1.7210275256038292
first define	1.8847769891890014
second system	1.5431447863229173
dialogue acts	1.7496071300437617
possible attachment	1.6298906651521838
complex queries	1.5781539692435587
transfer approach	1.6350816105136645
maximal length	1.5266242261615064
derived forms	1.6333642698925133
parser provides	1.6190328102134979
generates two	1.6411558246537032
qa systems	1.850511427179508
spoken dialogue	2.002303159138344
appropriate english	1.5371982986173842
combine multiple	1.5210740331567272
always lead	1.5111306094308554
one simple	1.8390002902204527
il ne	1.5248266889894415
constraints defined	1.521240524634107
basili et	1.5360703051027298
raw text	1.9985596099365721
ef cient	1.6038325077240128
error occurs	1.6910568858239943
morpheme boundaries	1.714243608845098
provided training	1.5011917588019899
similar lines	1.5338735539822874
notational conventions	1.584618990870829
similar problem	1.8516798118160611
rule induction	1.6595559896120569
u c	1.582839560846368
system treats	1.5269930689946518
see e	2.1085506964958673
extraction task	2.006833381260293
kyoto university	1.6983045396571637
precision values	1.764678679463087
grammar makes	1.567922330580758
two baselines	1.5695222973163587
contextual knowledge	1.6374133877096615
translation equivalence	1.5124902244873273
correct response	1.6035281590022317
improve precision	1.732418304911139
semantic primitives	1.8044565901322143
recognition system	2.219839499011914
valuable resource	1.6192219158965702
learning component	1.6198211566182126
r must	1.5514514275224363
string generated	1.6320354670147565
state set	1.5379460474606592
pitch accent	1.6304909693532503
coreference relations	1.6920901417156688
minimum value	1.6782219405894565
six types	1.6202338249864887
specific ones	1.7873685540637707
every word	2.25810834239896
rule takes	1.5440207036648979
10 ms	1.5041134997716235
null finally	1.7857836815978843
use different	2.0838915251460195
common characteristic	1.5445134575869865
limited vocabulary	1.7341395890835507
tile case	1.7229556053533144
new training	1.811401229360166
animate subject	1.568863381218478
chen et	1.838335836282922
last stage	1.7329783390617592
arc used	1.5408560126058508
problem appears	1.5111306094308554
de son	1.5187014267751215
baseline system	2.0681374203616842
text representation	1.7467644414788253
partial word	1.567922330580758
two programs	1.5354510398033074
systems achieved	1.5111306094308554
categories using	1.5300728913253212
framework provides	1.6502761822926497
cannot always	1.8453265714865426
identify important	1.5107844011873683
method extracts	1.5731122769149346
remaining elements	1.6289349127929256
following formula	2.0647095992203175
operations described	1.522760613339579
occurrence frequency	1.851657720888248
called head	1.5338735539822874
recursive structures	1.6822918105762317
di erent	1.7568104747691744
accuracy compared	1.6481174069026685
formal analysis	1.592111614460519
english corpus	2.023171924573533
grammar described	1.5188051370135929
system description	1.566706833990371
estimated probabilities	1.6431586438232257
increased performance	1.5511943810330642
weakly supervised	1.5008742129322192
various domains	1.774777000835246
exactly matches	1.5306061495832655
given pair	1.8474216032106785
structural representation	1.6064605913450087
many theories	1.5582958710952315
leftmost child	1.5148387121477085
unannotated data	1.5396444551238493
information improves	1.579873144038688
similar algorithm	1.586137214651611
disambiguation algorithms	1.5970637587789573
features alone	1.6651967012160775
assign probabilities	1.7155698049757155
different possibilities	1.6289349127929256
actual implementation	1.8629322215110862
analysis process	1.901857769953813
additional relations	1.567922330580758
semantic context	1.7470885415092032
node labeled	1.8270035515652028
central problem	1.7843055584917944
developing systems	1.5582958710952315
candidates whose	1.5781539692435587
possible paths	1.7869472891728853
crucial problem	1.62412689036662
possible interactions	1.5547189562170496
previous research	2.157284126692513
earlier systems	1.583051127543801
procedure described	1.9858199293382197
equivalent words	1.5314169148210515
different expressions	1.672884134739408
input space	1.61569000140401
commercial product	1.5729286010575074
process requires	1.7378109296453574
phonetic transcriptions	1.6779335178017818
produce good	1.7125697263999058
coverage parser	1.7080446043382929
add information	1.7329334093351267
semantic formalism	1.5855468428052397
noun hierarchy	1.5566454309306599
primitive concepts	1.6102963001672241
one binary	1.516491127995908
containing n	1.50119175880199
translation approach	1.7555666227415416
relevant properties	1.613364934425665
surface generator	1.5583184730098076
words consist	1.5188051370135929
large scale	1.8744232497050524
discriminating among	1.5245348740884508
applications like	1.813446576873811
precision error	1.526993068994652
syntactic tag	1.6062320033170863
highest weight	1.6031224303496519
event descriptions	1.6340181504998839
5 illustrates	1.8201300659766098
recall rates	1.762799476533874
entity task	1.7960043736016709
l j	1.6329374072287361
section 3	2.7118458549052864
argument roles	1.5256428192011384
input word	2.0613388899171765
trec qa	1.570526213855082
best matching	1.6285965502783153
tells us	2.0934488801401594
common usage	1.60913070763407
noun n	1.7372688344652816
verbs occur	1.5547189562170496
basic processing	1.5288687562201928
english lexicon	1.9037040916367656
linguistic constructions	1.8425754027477843
methods developed	1.6889191202036424
important word	1.507007268326853
previous steps	1.6756673695466664
top level	1.8662980975223382
pairs based	1.579873144038688
methods presented	1.6785607164239145
e x	1.7155402482281323
limited coverage	1.760655880476912
comparison among	1.5869693547698929
corpus using	2.103030330805997
closed class	1.834019683222592
value assigned	1.6761656524213182
figure 7	2.330968712767217
f u	1.510618969052833
target nodes	1.5280271794602216
differences across	1.5683356946968088
text classification	1.9014276079596844
agglomerative clustering	1.7369838619050373
certain situations	1.6904151818990054
theoretical basis	1.7923196053534847
referring expressions	2.0534785497859636
constructive comments	1.5611231576018243
serious limitation	1.5511943810330642
basic travel	1.5511943810330642
see table	2.2902985517107206
trigram model	1.972380785321034
r example	1.6002993454155385
last sentence	1.8459317368485926
analysis involves	1.6315901311540408
random subset	1.50119175880199
single pass	1.769780906312884
ambiguous cases	1.8311399561490025
design issues	1.510181823566108
model contains	1.7934582091806202
chart parser	2.0620406298984273
entries containing	1.5466472792181065
correct form	1.6702449774262638
different things	1.811747719264186
different meanings	2.0498017897213208
diagonal covariance	1.5021402692928563
scientific texts	1.642258136665147
new versions	1.5771944393968735
enough information	1.83217418878472
subcategorization features	1.56819296354213
german sentence	1.7023978607577286
means clustering	1.6529513452325522
japanese grammar	1.6909216405111342
comprehensive study	1.524534874088451
figure 17	1.613192179261201
two verbs	1.8714603864144117
best combination	1.828301838057456
bar theory	1.630554559183625
generative grammars	1.5272843767273039
chinese nlp	1.542348578385745
lower right	1.6075783640375185
context based	1.5514514275224365
relative frequency	2.108728978512711
using dialogue	1.585589015734039
performance comparison	1.6392309045877975
exhaustive list	1.6266256462651403
phrase consisting	1.5354510398033077
results even	1.586137214651611
model corresponds	1.5547189562170496
one side	1.7319095063177272
different distributions	1.7072113359402379
question like	1.6300792061475065
two ways	2.3687894621144983
stopping criterion	1.6005843102359503
complex data	1.7231944509785055
unique feature	1.539138431320559
new context	1.717244254027547
subsequent experiments	1.6289349127929258
traditional parsing	1.595491118313467
20 words	1.9063542174286923
verb forms	1.9918119353407044
verb agreement	1.8862095066227655
passive construction	1.6695121045905643
simple system	1.611203449372866
trigram models	1.7367451678059327
negative examples	1.99517192184752
contains sentences	1.5706734993195162
arises whether	1.5482870987025659
real world	2.0710309997800365
000 word	2.0252414989944203
different news	1.615262062995654
perform poorly	1.760655880476912
new formalism	1.5648606112684615
corpus tagged	1.6339957705783554
collocational information	1.6670741052479974
systems differ	1.5547189562170496
user would	1.8552630948203255
numeric values	1.594773126307433
domain objects	1.689570213556308
situation arises	1.567922330580758
sentences taken	1.7373468853854626
unannotated corpora	1.5117685557139826
technical reports	1.5894574345512698
regular language	1.712176334096271
wilks et	1.723896238484237
different scores	1.5118192695694388
new root	1.5398159895780754
baseline approach	1.5950178406214337
values within	1.5408560126058508
initial attempt	1.5412463844444777
lexical class	1.5591469435962066
ran experiments	1.720326606282666
corpora automatically	1.507007268326853
parser uses	2.011369006484668
crucial part	1.6761656524213184
local decisions	1.5257176783128212
cross validation	2.0628348962047776
additional nodes	1.5354510398033077
similar result	1.5547189562170496
previous version	1.7151558643929241
data points	1.951729913261707
language phrases	1.5856160021217733
example 8	1.6466901508912573
computer manuals	1.7058254759903893
r p	1.5778973208841207
subcategorization list	1.5262228780956786
approach consists	1.6920919321535786
make choices	1.5408560126058508
interesting issue	1.6213967902634532
rules given	1.802041140006571
function would	1.5611231576018243
based word	1.830170736799549
section describes	2.257260545802467
give information	1.7106430744882983
salton et	1.685166869397891
free parameters	1.7615746626494477
collected corpus	1.504934031784749
paper briefly	1.613624660358136
helpful feedback	1.5111306094308554
high number	1.9315432094221796
component analysis	1.6322286214202775
alternative analysis	1.6056035510036821
null tences	1.5338735539822874
source phrase	1.6528600807905856
following kind	1.5926202003411678
noun phrase	2.4608039838116262
zhang et	1.779322060815506
two hypotheses	1.7253227786661793
response time	1.8654136490675228
like language	1.567922330580758
one arc	1.6329374072287361
ordered lists	1.5482870987025659
function calls	1.58255773593085
dialog systems	1.786491780331433
pragmatic processing	1.5999010403398195
linguistic research	1.9699728805765477
data containing	1.5973170078864487
different views	1.7932322850313147
tile current	1.7057741658061807
syntactic types	1.6237629210453108
various tools	1.5731122769149348
two subtasks	1.6539594340296566
phrase extraction	1.7455199620789499
natural extension	1.7836550413109447
lexicon based	1.6734445616305185
one string	1.8272891384792251
question type	1.6697851058159752
document text	1.6165326130378854
overall success	1.5391384313205592
examples shown	1.5188051370135929
larger scale	1.7352337189917786
form described	1.5037367200448983
overall word	1.5282691109878925
left corner	1.765527437991882
main objectives	1.5973170078864487
within text	1.5226149809040495
words already	1.5798548254880234
instructional texts	1.5193557442764303
continuous speech	2.0937603375167253
lisp function	1.5637726119349868
sciences institute	1.602359748773462
template slots	1.7048218540828795
average performance	1.861397855997523
initial steps	1.5547189562170496
method using	2.015657529182352
terminology extraction	1.643980963551956
clue words	1.6183081387534428
current utterance	1.8910979455831436
classifying words	1.5164911279959081
null ment	1.7378109296453574
commercial advantage	1.585038460341591
roughly corresponds	1.7471845324497086
one process	1.5195036419368688
following sets	1.5690244007917742
event structure	1.7573128671968177
poorly understood	1.60913070763407
comparative evaluation	1.8142106679219576
complement structures	1.5459182428709468
initial performance	1.507007268326853
channel model	1.747109013480333
end system	1.6385894516964086
c b	1.5261646229482937
given node	1.8610301212011986
darpa resource	1.7106116384189844
learning results	1.5899302670582593
sign test	1.6027583578110984
context could	1.5547189562170496
ie tasks	1.6314736016751001
particular context	1.974375497503868
current location	1.5078786620077462
recognition techniques	1.675602286196862
sequence model	1.5813204243594625
tagged words	1.7752347819581022
linguistic performance	1.6294439702545598
lexicon entry	1.878309805760762
grammatical theory	1.6868317362255512
whole words	1.5631633330774495
model estimation	1.6229496984434086
nlp processing	1.5037367200448983
particular action	1.5432756752761154
noun pairs	1.7250322790997634
computer interaction	1.8568511530531095
edit distances	1.5752380537256625
output file	1.5369135289434825
feature templates	1.5849148548175185
also capture	1.526993068994652
phrase recognition	1.5853857911920444
small proportion	1.6580606251764167
01 level	1.5699730727825831
three metrics	1.62144031588269
first analysis	1.6045962492509205
robust parser	1.6562462379563088
interactive system	1.8329735721402398
discriminant analysis	1.6088522181598408
short introduction	1.5547189562170496
paris et	1.5131301253834533
reasonable approximation	1.6285930450163058
intuitive understanding	1.5749574214167732
adjacent word	1.6253349867805624
improve classification	1.5265421490264868
novel algorithm	1.5497180921575033
new document	1.7609892520892207
test would	1.6246289167870047
description given	1.613624660358136
inference component	1.5046383591841703
language production	1.721882616614201
among entities	1.5928235606322894
seven words	1.526993068994652
identifying words	1.513130125383453
gram approach	1.5432138445379904
expansion techniques	1.5296095598069797
semantic considerations	1.6073715579162529
textual data	1.8722120302447018
application program	1.5920220194866368
certain applications	1.6213967902634532
several basic	1.5408560126058508
linguistic tools	1.6635922718209233
japanese sentences	1.978496875782775
one type	2.1515286809497196
also experiment	1.5378646275607575
training dataset	1.5318299550667902
error occurred	1.570430085810516
5 hours	1.7546184934668503
text database	1.6234281355528346
joint probabilities	1.6603794485733023
two groups	1.5903596431898062
analysis program	1.6350638888307887
john carroll	1.530240952832758
work shows	1.6614869878819327
np np	1.5333653632047948
dans l	1.549748366412734
main criteria	1.522760613339579
retrieval purposes	1.608299184330961
jensen et	1.5716157403242
target words	2.06053592075987
text annotated	1.567922330580758
automatic way	1.7678099140508592
measure proposed	1.526993068994652
major features	1.5288687562201928
say anything	1.77530945408058
simple approach	1.900917255412424
following pair	1.7606738632426728
probability using	1.592620200341168
contextual feature	1.544948381866022
english equivalent	1.636969735352857
compact way	1.6570039092175395
seed set	1.5475377768335534
sentence must	1.8129542976985498
estimation methods	1.66180745069878
second application	1.5482870987025659
word w	2.2315096013895452
graduate students	1.8522377724118968
small subset	1.9702531158167493
idf weighting	1.6029051454724903
planner must	1.513153217011753
theoretical linguists	1.6392495009069876
sur des	1.520516913385963
dimensionality reduction	1.6887433067748425
symbol x	1.646171744841988
initial version	1.7251449120621891
verb complements	1.5494622740022495
german verbs	1.5860449683290572
language queries	1.717303285292958
linguistic clues	1.6499593285564695
way classification	1.6371688467182763
run experiments	1.530072891325321
phrases based	1.6160313784345912
features must	1.734954551774483
two discourse	1.7713410051824834
tile head	1.506173888464562
text theory	1.6325831274270783
quality translation	1.749397385453837
following approach	1.5706734993195162
possible segmentations	1.7884357120176229
better solution	1.7360338540431246
slight difference	1.5645241345053709
parameter settings	2.0120884268017623
lower bounds	1.7457253008795766
acl 2006	2.005970130211569
speech tagger	2.211344607813405
one need	1.6388370153723533
pitch accents	1.65642034673973
take advantage	1.7810342550315588
active form	1.6356518875390391
matching patterns	1.5602367449199286
phrase without	1.5440207036648979
relative pronoun	1.8655219642262277
art accuracy	1.5497725439919081
microsoft research	1.780588481384279
every user	1.524534874088451
two passes	1.6367907690549308
representation schemes	1.5426876474064137
iff x	1.6053932208928947
sentence generator	1.739625319446306
boundary tone	1.572502896524521
feature set	2.2122097950320194
numerical value	1.5980461907093884
evaluation shows	1.8464027237075766
information request	1.530984627359544
much difference	1.594773126307433
level may	1.5611231576018243
verb followed	1.6329374072287361
different configurations	1.685387484025006
language application	1.599125349640446
cannot apply	1.613624660358136
paper examines	1.7788903461436698
new set	1.5995529819632717
theoretic measures	1.5037367200448983
evaluation metric	2.0010569051754743
way distinction	1.571377835067533
single link	1.6202338249864887
model assigns	1.8719467385565365
particular semantic	1.909254871972173
also differ	1.6502761822926497
available corpus	1.6741803270499043
incorporates two	1.522760613339579
coordinating conjunction	1.7271602911957809
experience shows	1.636252690439584
answering system	2.007909951436243
similar phenomena	1.586137214651611
following expression	1.6785607164239145
one speech	1.5381766791708387
database entries	1.5537803403144643
effective way	1.6369534545436824
source software	1.5189601534336892
time system	1.6476641556212046
triphone models	1.6188028635487293
substitution errors	1.5838923815193273
particular document	1.7517178731420846
segmentation errors	1.6958092873072488
classifier based	1.6581363223870689
oriented dialogues	1.7739385668368322
different fields	1.6217423349150049
consecutive words	1.7266186053590928
automatic system	1.8512874449665608
syntax analysis	1.6434440540342152
use information	1.9278086857170957
original model	1.7219273625048686
two test	1.8825312348319352
collegiate dictionary	1.5929928949821113
object x	1.5976182618398191
nlp modules	1.5419819962962804
words ill	1.5177700906840073
term memory	1.703533174368921
example rule	1.5459182428709468
experiments demonstrate	1.6960474084795663
linguistics volume	2.1422358734806988
behaves like	1.6921138523491286
interactive machine	1.5117685557139826
much simpler	1.7875117383761359
nodes correspond	1.762179856320777
arithmetic mean	1.5201190767490376
hereafter referred	1.5459182428709468
equation 1	2.029420869015257
different search	1.6966657909604443
data consists	1.926566852640651
word 1	1.5859118906985712
types associated	1.5445134575869865
dialogue module	1.5104086541694208
child node	1.7584161351512648
model human	1.7182011479768873
c v	1.6618342697139283
high values	1.66667186338803
nominal modifiers	1.6165326130378856
two vectors	1.8849202653351838
heuristic approach	1.6270880267949739
depth one	1.580090580759919
speaker knows	1.5217605875570064
results among	1.522760613339579
following values	1.6053932208928947
igure 2	1.5445134575869865
different methods	1.9339792966003186
crafted rules	1.7592391351266856
primary aim	1.5338735539822874
line indicates	1.6227790810587184
leaves open	1.6458797346140277
american english	1.9477074846293687
specific instances	1.6103523297384124
probabilities associated	1.7223913317859245
section 24	1.650249425112447
25 sentences	1.5445134575869865
mit press	1.5925561662226717
sentence comprehension	1.5432138445379904
partial descriptions	1.6587178348115796
distinguishes two	1.5378646275607575
grammar using	1.7844902445128512
one conclusion	1.5111306094308554
phonetic units	1.5071419420006529
intended sense	1.5557458328156017
experimental setting	1.7370644233219208
rank order	1.6089594446224194
charles university	1.5869693547698929
textual corpora	1.648651915999573
medical language	1.6818613783068397
singleton set	1.6160511154605368
various heuristics	1.636252690439584
dependency parse	1.6444846658272487
interannotator agreement	1.5871653297042405
accuracy reported	1.5188051370135929
prevent us	1.6881707359732032
person pronoun	1.6438766862662455
provides access	1.6210934563561632
number information	1.5390022563617327
total set	1.602042226800564
represent concepts	1.6614869878819327
first strategy	1.7546529227297882
substantial work	1.5037367200448983
work presented	2.1004708260683667
technical manuals	1.6775792181013172
reasonable accuracy	1.6235405672398668
case 1	1.7928920789348093
british english	1.8188325869943174
rules obtained	1.5037367200448986
logical forms	2.0337359408826727
syntactic elements	1.7060155988149832
larger data	1.829200940701061
lambda calculus	1.5906723352433734
utterance containing	1.5571495092265906
following lemma	1.5936506658968466
type n	1.5230482200667932
procedure using	1.591823957496618
reversed order	1.5188051370135929
mcdonald et	1.6160677568007102
intuitive way	1.5706734993195162
structures would	1.5634115707207186
den bosch	1.6737715500124963
develop methods	1.7846437503087036
correct segmentation	1.7431975981712027
semantic theories	1.6555587634974358
text strings	1.6234281355528344
alternative models	1.5037367200448983
translate english	1.522760613339579
given string	1.8726481436725337
also lists	1.5582958710952315
module performs	1.6112034493728657
x k	1.5414105911591163
independent method	1.5111306094308554
text string	1.7037954076691264
clear idea	1.5354510398033074
complete derivation	1.531929325947856
dependency parsers	1.5930730262621933
concept nodes	1.5634335222172868
quality text	1.5288687562201928
best individual	1.5632272049575842
evaluation results	2.179179988116789
certain parameters	1.507007268326853
logic grammars	1.546195106059554
two tree	1.6121742428060308
structural representations	1.535727155868841
next paragraph	1.613624660358136
text summarisation	1.519886284952312
based technique	1.6619550982161897
alignment probabilities	1.636478828553809
mapping function	1.6363643746952432
data base	2.014848418016016
stage consists	1.530072891325321
many alternatives	1.5269930689946518
design goals	1.5275591303469285
words based	1.9557484520916761
relative weights	1.5973170078864487
generic concepts	1.5087993241956938
using ibm	1.5908167675916909
using em	1.6682901261112977
moldovan et	1.6885118865668618
hpsg grammars	1.6015856165419797
large majority	1.6977890880751751
since p	1.6785919234312812
chinese text	1.8957019353815423
every student	1.542137632662564
state network	1.539516012866898
system still	1.7006344357165344
different form	1.7139483293868785
evaluation presented	1.5078976439458076
header information	1.5395099438048585
possible combinations	2.1418002501277362
following simple	1.8729129357779122
documents written	1.6094040109095726
two language	1.8449293577330508
morphological ambiguity	1.53861431083629
word cannot	1.6481174069026685
target domain	1.6791958788419632
various places	1.5188051370135929
natural science	1.6342922036019591
communicative intentions	1.683426484441799
lingual information	1.731755946864966
per speaker	1.5404107512156902
target concept	1.6237066422615143
2 sentences	1.630071009401472
human input	1.5156159685357806
high error	1.6780129579824068
class may	1.7105799695961408
similar meaning	1.7538341131741775
concrete object	1.530984627359544
specific goal	1.5496092928695906
tree induction	1.6048582987213842
one tries	1.6400440715389129
possible dependency	1.57425918739941
automatic machine	1.5893265024752146
little research	1.6920919321535788
first sense	1.7965503465584822
original features	1.525702702006778
yield good	1.6817139837574675
error recovery	1.6510617448019325
dependency parser	1.9728768689748954
text structuring	1.5062598739902455
processing strategies	1.7634432952479044
4 figure	1.5288687562201928
meaningful comparison	1.5445134575869865
parsing proceeds	1.6742529672801947
human annotator	1.8031259265293937
document clusters	1.504723529945745
typically use	1.6939639871030243
retrieval based	1.5824798263750006
computational reasons	1.5707376783218978
system development	2.083474339191353
terms used	1.8878336165737863
detailed account	1.7734751337955452
singular form	1.7708668433965404
using surface	1.5781539692435584
selectional constraints	1.7379936094924426
core set	1.5494864747794046
data necessary	1.5887468924062063
among elements	1.5511943810330642
adjunction operation	1.5970659157922116
next node	1.592111614460519
mental representation	1.5613081195452279
original grammar	1.838262688652896
parser output	1.876479609195065
additional step	1.5739592165010823
r may	1.5464237918963106
including word	1.7882649160849862
rule must	1.8909800780188086
answer given	1.611203449372866
many models	1.5408560126058508
open research	1.6773210626499961
pp headed	1.5321552374951573
another node	1.769214475049279
rules rather	1.5408560126058508
experiments conducted	1.7999124414051257
language data	1.9730777103232335
uses simple	1.5445134575869865
representations used	1.6956577128326704
trees must	1.5511943810330642
across multiple	1.901095126603935
word number	1.5407012775844184
take place	1.6417635533107142
analysis modules	1.615262062995654
binary trees	1.6593872628705117
n e	1.7085364596442592
models make	1.586137214651611
based pos	1.5605435464625717
helpful suggestions	1.74724601164107
corpora used	1.907963868431275
certain relation	1.5378646275607575
always correspond	1.6822401779866478
error may	1.517770090684007
strong tendency	1.808489148563653
section outlines	1.65027618229265
various techniques	1.8169609508852202
based user	1.5354510398033074
semantics would	1.522760613339579
recall measures	1.7619988521883343
certain distance	1.579873144038688
frequency words	1.858521162216756
second factor	1.640910700935025
main interest	1.6031224303496519
feature weight	1.515050704596927
following constraints	1.8325101490329694
named entity	2.268371311335696
questions posed	1.7047689458394855
expression like	1.6459404763673078
immediate constituent	1.6165326130378854
lexical types	1.5536810786166217
one defines	1.5111306094308554
human use	1.5445134575869865
standard prolog	1.586137214651611
methods like	1.611203449372866
optional elements	1.622998078987595
default assumption	1.51910459028178
cannot directly	1.6502761822926497
certain ways	1.5706734993195162
conjugate gradient	1.5299472051836376
natural sciences	1.77530945408058
distributional information	1.703563717569127
multilingual corpora	1.5738708991826256
nl system	1.7032066297501958
el al	1.9359525065363596
problems involved	1.790285397230682
single form	1.530072891325321
translation algorithm	1.6201712527551195
segmentation systems	1.5630133766687315
result may	1.7528478693701954
statistical mt	1.7366935123286629
events occurring	1.522760613339579
2 f	1.5587255214251448
let f	1.8490221102922046
arbitrary context	1.5957304947643147
using hidden	1.5547189562170496
verb meanings	1.5045293046371402
relevant word	1.60913070763407
basic properties	1.6213967902634536
supporting information	1.5338735539822874
derived rules	1.5087430717568184
provide important	1.6289349127929258
occurrence data	1.7311417938418483
language learners	1.7361734999559268
procedure works	1.6289349127929258
shows part	1.649843436022881
special features	1.732266191908249
partial structure	1.5903030603047636
associated press	1.5149341120016542
information needed	1.9544938265918463
automatic morphological	1.5361450492096225
research interest	1.5445134575869865
major sources	1.6002993454155385
wahlster et	1.632664136446598
large subset	1.6496419421411992
ten times	1.6250027111259429
sentences used	1.850672848414353
best candidate	1.8156735020239874
future directions	2.0187389545956878
root form	1.8951340436300301
human agreement	1.6194767963976466
parser also	1.7761936996438532
research center	1.6959746269296907
bilingual resources	1.5171904119543882
domain corpus	1.6935890309583947
breiman et	1.6685665271708663
model cannot	1.7702567190575853
devices used	1.5338735539822874
using cosine	1.507007268326853
english output	1.5596201947369313
computational analysis	1.5839972688383934
speech would	1.5408560126058508
significant percentage	1.5611231576018243
modified form	1.583051127543801
alignment methods	1.6075416216169949
frequency events	1.5299138869986475
gram probability	1.5487461920851402
language related	1.5412463844444777
communication science	1.5338735539822874
different combination	1.6056035510036821
still room	1.5408560126058508
definition 1	1.9859961448595633
english dictionary	1.9867059067832178
ungrammatical sentences	1.7872310539932055
given speech	1.533540614646662
take precedence	1.6544208897318429
major feature	1.5156323250636556
present model	1.5893265024752146
training parameters	1.5266242261615062
based learner	1.7810673457840953
value 0	1.742868720984371
nodes corresponding	1.7803299708091225
form using	1.6699400022790052
every sentence	2.0878189712095736
bias towards	1.8086294002240377
classifier achieved	1.5408560126058508
third columns	1.5482870987025659
approach appears	1.5706734993195162
3 explains	1.6527294781610564
structure rule	1.850642818569461
possible arguments	1.5318465770311396
relations using	1.699626140637926
nearest neighbours	1.5998186112372657
two senses	2.019732096150345
preliminary experiments	2.1364035815045157
complex expression	1.5992286050546423
morphological analyzer	2.137895752566724
linguistic techniques	1.6774002949116094
improve recall	1.7044865459645908
system retrieves	1.6256278638449597
quantitative analysis	1.7130162238987903
communicative goals	1.856048901159358
across categories	1.512925200463965
structure must	1.8882294614882305
paper focuses	2.0719474875021655
partial sentence	1.5582958710952315
many distinct	1.5354510398033074
system evaluates	1.5354510398033074
mildly context	1.702515130726585
text documents	1.9193804392644427
sentence parse	1.559137125070758
also combine	1.591823957496618
total weight	1.5172316974283468
relative order	1.8735035233962278
top nodes	1.532341016095725
specific subject	1.5275591303469285
dependency grammars	1.7556696560373746
three terms	1.5869693547698929
free language	1.7683626006559285
null tive	1.5706734993195162
syntactic classes	1.7574412316477082
unique index	1.5637776281819353
adjoining grammar	2.0317572361413636
senses may	1.586137214651611
classi ed	1.6869595308608418
naval ocean	1.5078786620077462
input using	1.611203449372866
language semantics	1.687991698517088
essential role	1.6409107009350248
fixed value	1.5156323250636556
tagging model	1.7010730865783388
distinct levels	1.5656419863875413
conflict resolution	1.601062548329091
automatic techniques	1.6053932208928947
implementation uses	1.743960294459502
nlp systems	2.133767556077223
input structure	1.6350650494196617
marcus et	2.194577923522046
log 2	1.5633179841307894
relations expressed	1.5893265024752146
key features	1.7237747585396455
word pair	2.012937728548475
per category	1.5611135103157157
provides sufficient	1.526993068994652
high probability	2.0517967183502916
first object	1.560319828223895
various sizes	1.6392309045877975
theory based	1.5482870987025659
following probability	1.5408560126058508
relation must	1.664540718454114
statistical analysis	2.0734505249543957
one concerns	1.5269930689946518
many points	1.5511943810330642
features indicate	1.533540614646662
text summarization	2.030505095720015
various conditions	1.573112276914935
one dialogue	1.533540614646662
appropriate representation	1.7125369004275144
four distinct	1.6939639871030243
tile input	1.7274979301322098
precedence constraints	1.5390522972309097
word associations	1.6783529399808708
document summaries	1.5464311985334573
two equations	1.5156083168889984
first non	1.5418395969957306
underlying domain	1.5992286050546423
baseline algorithm	1.5760019251405901
differs significantly	1.6570039092175395
interesting aspect	1.74724601164107
input japanese	1.504934031784749
reduce parser	1.7722262814704959
number agreement	1.91853581335298
possible correct	1.50119175880199
underlying concept	1.5771944393968738
lexical anchor	1.579227589207835
infinite set	1.8317094330554375
large variety	1.8929039686137612
research grants	1.5622818522186857
user asks	1.7528934574373198
semantic analysis	2.357421901747836
generated texts	1.6584699325631185
broad classes	1.6429565433589586
like information	1.6718027974751068
pos features	1.625084757511151
input sentence	2.332115059006708
tagged training	1.8204363625844233
xia et	1.591087581572047
models 1	1.635868428837512
low confidence	1.6753114229441863
semantic ones	1.5798548254880234
features used	2.245087108280566
u l	1.54793093127061
1997 association	1.6315901311540408
phrase analysis	1.5422214142294637
whole training	1.775052914184583
possible features	1.7638268621637399
essential features	1.615262062995654
modal logic	1.6817276268321741
correct candidate	1.5313886555102305
elements within	1.7626447090992958
generate natural	1.6529674243345007
flexible framework	1.533540614646662
special symbol	1.839390564632378
10 seconds	1.7098259615519553
better method	1.522760613339579
multilingual system	1.509778922004791
genetic algorithms	1.5659205276957853
example parse	1.530072891325321
news stories	1.9922971560620888
first symbol	1.666529689989241
extensive analysis	1.5973170078864487
correlation coefficients	1.5764980843342338
one mapping	2.000757221908648
cross references	1.5025792150970103
word classification	1.6256053440867353
grammar generates	1.6770235151848842
word stem	1.6538344261933344
many incorrect	1.5406210028261875
output word	1.5947983930134422
capital letters	1.9564848266013777
recursive rules	1.6792084070822382
learning architecture	1.506173888464562
extraction step	1.5440207036648976
certain amount	1.5755795243337898
two stages	1.7319205815570011
situation semantics	1.5729686026087983
extraction pattern	1.5021997361368005
two similarity	1.510711371954136
individual differences	1.5401090502987478
sentence null	1.522760613339579
performance decreases	1.583051127543801
two notions	1.6802576002626086
objects whose	1.5739592165010823
results cannot	1.613364934425665
examples given	1.9607547954691378
translation hypothesis	1.5341207207545393
current discourse	1.8479575395824706
9 shows	1.991753172296084
tile use	1.6635555237714856
inflectional forms	1.6201411209671561
text plan	1.6657778299403883
initial conditions	1.5500571206028209
functional generative	1.5847316634463144
complex nps	1.6097022588707386
mccallum et	1.6781515578523085
extraction approaches	1.512925200463965
grant n00014	1.79791571192699
section 23	1.924804453846552
several papers	1.591823957496618
shallow methods	1.5107844011873683
surface generation	1.595297149981688
larger number	2.101804634095276
0 indicates	1.6658904115324105
argument positions	1.855672862909879
various aspects	1.740298310895326
node n	1.9260851360959665
greater accuracy	1.561123157601824
feature selection	2.0515439938177815
certain discourse	1.6539594340296566
proposition p	1.7352687946928051
parallel structures	1.5651148413477767
whose subject	1.636508732851384
slot name	1.5191487533489458
following interpretation	1.5217605875570064
minimum edit	1.532653153352447
martin kay	1.6229853717503135
using character	1.5403034171359224
source phrases	1.5025037565896633
node corresponds	1.6802940634668402
sense tagging	1.6996535286941326
based applications	1.7309723544238431
accurate semantic	1.516491127995908
branching trees	1.6078424239387175
following lines	1.579873144038688
analysis provides	1.5929928949821113
second classifier	1.5657848105751837
work carried	1.636252690439584
selecting sentences	1.5269930689946518
lexical organization	1.572018110177801
following problem	1.5511943810330642
last column	1.9804323604293455
derivation process	1.7017332652748798
user experience	1.5082141307457628
entries contain	1.568863381218478
first sentences	1.5669629240805358
sentential context	1.66606790055387
module receives	1.5037367200448983
predicate p	1.7383130987045725
syntactic case	1.549729132180171
ill figure	1.6289349127929256
common types	1.6711381866791801
many sentences	1.9981192356246231
data available	1.866030333311563
abstract level	1.8717259896699219
subcategorisation frames	1.5167634688004923
000 sentence	1.6622275274832838
model relies	1.5511943810330642
second hypothesis	1.5122266712907972
larger sets	1.636252690439584
model approach	1.5395099438048585
paper discusses	2.10921829937927
concepts involved	1.6137947517368634
relevant passages	1.6019573477956586
lexical elements	1.8271615385114075
application systems	1.6827835193912475
2 introduces	1.8516798118160611
arbitrary set	1.5482870987025659
algorithms use	1.5831219419480949
several differences	1.5188051370135929
improve speech	1.5977445137418091
hold among	1.6129023444267458
labeling problem	1.59555403645897
represent documents	1.5131301253834533
gold standards	1.6428822166793657
new analysis	1.5195036419368688
existing set	1.60913070763407
relative ordering	1.7178162994368975
overall structure	1.9458842500156608
constituent labels	1.5217205480859748
real text	1.7838959092727333
semantic differences	1.6794606836045487
major components	1.639081282553483
simplifying assumption	1.8969250153334711
cannot determine	1.7889182799754448
logical expressions	1.650937328526823
recognition result	1.6036043037608811
use linguistic	1.6770235151848842
singular nouns	1.6031224303496519
restrictions may	1.5408560126058508
list consisting	1.5771944393968735
every path	1.6650008022438314
conditional entropy	1.5828809208932033
xml representation	1.6008696634890398
analysis indicates	1.510181823566108
methods would	1.6084968011212868
word occurrence	1.7507295783696022
analysis systems	1.7526660750199679
disambiguation methods	1.658372257319995
component consists	1.6266256462651403
intrinsic evaluation	1.5768549909399516
english resource	1.5867854287817869
plausible explanation	1.5440207036648976
space representation	1.5658571165804958
algorithms require	1.507007268326853
practical interest	1.6920919321535786
close proximity	1.6458143726245438
new york	1.517979854360979
optimality theory	1.5360714679843377
dependency trees	1.9963311821749528
corpus contained	1.737796564580819
phrasal lexicon	1.5591422731530395
based sentence	1.5688239460897873
different segmentation	1.5970086311644893
complex concepts	1.5382275852283713
four sentences	1.6845577236692315
formal way	1.6487334099083149
nodes representing	1.7042859255023926
individual lexical	1.8342096736369407
head grammars	1.5211847964185519
special thanks	1.9219694224455586
sets used	1.8473247825721257
world text	1.5378646275607575
spelling rules	1.5620668319665505
similar behavior	1.5887468924062063
recognition hypothesis	1.5650015790049234
approach leads	1.7158821441137806
contain words	1.8220942614263078
database query	1.9598690458197043
value v	1.5883180446335292
baseline result	1.530072891325321
classified based	1.5188051370135929
following type	1.6964136633312572
spoken output	1.6258212214294425
ron kaplan	1.5164911279959081
constraints imposed	2.014149463399405
multiple reference	1.5254754246507192
simple algorithm	1.922067524880551
various data	1.6229919786001088
discuss later	1.5511943810330642
returns true	1.6389074724266854
many linguists	1.5929928949821113
rules applied	1.780282513498202
minutes per	1.6811844390857942
human translator	1.7845949019682479
2 l	1.6244843928316635
architecture consists	1.524534874088451
word error	2.1338418006678372
relative contribution	1.6463388111355446
current speaker	1.5652297135534465
let l	1.8202194794057995
linguistic aspects	1.7396979552775518
size n	1.7746042237196777
pos sequence	1.5823807206676685
semantic filtering	1.502065644379364
place predicate	1.69894305729261
combinatory rules	1.5838899963006803
second possibility	1.6339957705783554
full parse	1.9095059459482955
syntactic parsing	2.1294177705455897
partial matches	1.7484596872074878
rewrite rules	1.9266664015974413
positive examples	1.9620025530788519
plans include	1.5445134575869865
rules associated	1.7120828968340218
following algorithm	1.9166261918405962
correct words	1.7209237183394115
driven methods	1.5637776281819353
possible sequence	1.5111306094308554
parser used	1.861521432125642
terms whose	1.6349068916737024
current case	1.5177700906840073
different relations	1.8681912389481576
additional training	1.8201114388356359
trec evaluations	1.5705757689872493
different mechanisms	1.5217605875570064
text processing	2.1892098946295926
naming conventions	1.6227790810587186
single item	1.6694443901436329
also differs	1.6920919321535788
efficient computation	1.6058952092043035
ranked based	1.5806234267011035
procedure similar	1.5645241345053709
current tree	1.5027582619516857
simpler model	1.5177700906840073
intermediate level	1.8402359042767642
ted pedersen	1.507007268326853
e first	1.5328333954091753
syntactic descriptions	1.54109970635607
certain assumptions	1.636252690439584
synthesis system	1.6625794031824728
per token	1.570503449504594
data contain	1.543442208482524
like structures	1.7201741786343798
analysis procedure	1.6909554741534256
jelinek et	1.593664149772333
shares many	1.6164339756999313
described herein	1.580090580759919
lambek calculus	1.60364943508818
possible phrase	1.6010101084822783
2005 association	1.9907111575649767
base systems	1.607968377164029
efficient representation	1.5602367449199286
order given	1.6477541750656948
many domains	1.8231399049705783
r 1992	1.949574903088584
computational overhead	1.6144455608275865
following attributes	1.570430085810516
possible improvement	1.579873144038688
discriminative power	1.5767085787503843
highest f	1.6949656132519992
probabilistic framework	1.8027307653945475
experiments suggest	1.7680599214474206
matching rules	1.7365684583636973
particular analysis	1.5111306094308554
significance level	1.7224292290801726
server architecture	1.568427794038901
dictionary system	1.5146825507823987
closed classes	1.6097015195559554
appropriate translations	1.5037367200448983
initial data	1.6706160994105437
model fails	1.5622818522186859
3 e	1.5870597156530897
query interface	1.530984627359544
framework used	1.561123157601824
following part	1.5078976439458074
certain language	1.6756673695466664
frame structure	1.5701713815169815
voting scheme	1.5967579411573758
sgml markup	1.550611280307232
last section	2.0970130297694416
absolute frequency	1.585836497883611
condition holds	1.669777280074386
training results	1.5177700906840073
correctly predicts	1.6791278880803084
linguistic data	2.190436997333761
class word	1.6734656889828754
trivial task	1.9257129088324205
medical information	1.5214529152145553
full parser	1.6716873179455751
representing information	1.5482870987025659
wilcoxon signed	1.512536810936869
better accuracy	1.864731956298055
arabic morphology	1.5478934661523545
answer extraction	1.6501384115409448
second pair	1.6365087328513845
multinomial distribution	1.515972312283922
system translates	1.6998973603836447
4 explains	1.6002993454155385
syntactic category	2.2397566400404387
system takes	1.9569418364509312
verb could	1.5338735539822874
corpus statistics	1.8702779522362547
best translation	1.8411191903959776
type system	1.7863278661962323
possible pairs	1.8183911305926084
interested reader	1.8272188571693444
character word	1.6276664806217234
function h	1.6511962288566582
first thing	1.767056580432808
requirements imposed	1.522760613339579
one encounters	1.5378646275607575
argument structure	2.2179768805994513
example may	1.6783930166760777
verb form	1.95854003251831
efficiently compute	1.567922330580758
brief overview	2.0159634567688207
see appendix	1.9073710298392128
events may	1.6631574457448046
normal way	1.6289349127929258
many irrelevant	1.6112034493728657
contextual constraints	1.6955280323087392
negra corpus	1.5630036514653405
semantic models	1.629507789064117
preliminary evaluation	1.8319095069935993
second advantage	1.5338735539822874
first attempt	1.7605592442588043
see text	1.511201220738707
family name	1.5381583865940875
constructions used	1.5445134575869865
possible exception	1.5582958710952315
paper argues	1.6655102285632384
specific verb	1.596743419722522
larger values	1.594773126307433
using rule	1.6501262640161984
work involved	1.5482870987025659
thanks also	1.946872935695514
language pair	2.0377609378338226
representation scheme	1.7990856625552474
figure 10	2.0838853256563583
natural text	1.6256278638449597
context set	1.5077494248092886
one corpus	1.6879738241933557
auxiliary inversion	1.5440207036648976
modeling toolkit	1.725393852123075
parsing problems	1.6650008022438314
human assessors	1.5610753689034713
grammatical errors	1.7368564202636736
le module	1.5138416108890473
performance levels	1.7314807712355123
null ture	1.6164339756999313
techniques may	1.7426543199393398
utterance boundaries	1.5389174027895098
various attempts	1.522760613339579
present experiments	1.7436706932296322
discourse model	1.952645567933496
two information	1.5245348740884508
possible sources	1.611203449372866
word terms	1.7196323165314635
linguistic sophistication	1.510181823566108
write grammars	1.5378646275607575
knowledge provided	1.6002993454155385
short text	1.6570039092175395
system capabilities	1.6012319720994657
shared tasks	1.6140406995754497
three levels	2.0608645526479528
empty word	1.5838948177390686
que les	1.673538821677424
word candidates	1.7250157988339256
large database	1.7278119884488832
general model	1.8332675088004757
per frame	1.522438913516297
category np	1.656464330053097
user clicks	1.6666455667887732
lexical classes	1.6645537625804747
obvious advantage	1.5408560126058508
following result	1.6320354670147565
many similarities	1.5482870987025659
satisfy certain	1.6213967902634534
existing theories	1.5156083168889984
specific sub	1.586137214651611
number 4	1.8389967602674446
000 verbs	1.5087853581980926
theory underlying	1.5111306094308554
unrestricted text	1.9820133967656342
alternative way	1.7158821441137806
morphological knowledge	1.6526053663591038
statistical learning	1.8266065919288048
terminal nodes	2.0565142462071644
adjectival phrase	1.6554558104052497
tedious task	1.530072891325321
poster sessions	1.944780873277881
possible feature	1.7179482056291908
word probability	1.7064360529939244
essential feature	1.572834038198836
tasks involving	1.6904151818990054
discourse function	1.5477210767662073
welch algorithm	1.634780791112636
additional context	1.7315826662352172
syntactic constituents	2.003517662163519
alphabetical order	1.748878905321836
matching method	1.7031066760239324
dimensional feature	1.7864792883436682
agreement results	1.504934031784749
function based	1.6169986020906966
first steps	1.7266279442877563
important consequence	1.6002993454155385
collect information	1.5037367200448983
language expression	1.789019772177938
two states	1.8136314220728535
common elements	1.5514514275224365
patterns associated	1.66156771763727
similar size	1.5037367200448983
coverage parsing	1.6258212214294425
first iteration	1.8147840406433224
complex process	1.7244811761730507
original version	1.7562780519615577
standard information	1.6285930450163055
author thanks	1.5445134575869865
genia corpus	1.6105525634165567
specific model	1.6614869878819327
application area	1.7502038476867035
adds information	1.5511943810330642
maximum depth	1.6581363223870689
linguistic semantics	1.630542534004955
function words	2.21841198757658
discourse models	1.7149313144935094
data sizes	1.516058707239509
single individual	1.567922330580758
grained information	1.583051127543801
new sense	1.5624795423981932
relations include	1.6213967902634536
wit h	1.7318977260025548
task complexity	1.510987143789623
primary goal	1.9098903108807286
personal communication	1.959759939286593
another agent	1.6739611532350311
memory space	1.6402946932301046
semantic grammar	1.8371542993616312
using transformation	1.5037367200448983
category assignment	1.5745206634989883
semantic relationship	1.8603417184100002
sentence types	1.7886510715368789
per query	1.5961459312567108
relations hold	1.7262659090417687
3 table	1.5637776281819353
general models	1.5771944393968738
original system	1.735989297796044
interpretation would	1.6479791814036409
researchers use	1.513130125383453
sentence generation	1.8955448411510956
phrase grammar	1.5148722773105288
morphological features	1.914546449276828
target document	1.5049591214491018
search system	1.51713904847304
correct translation	1.9582900314265637
several distinct	1.797413685506606
time algorithm	1.7014008471656008
embedded verb	1.540109050298748
2 presents	2.110743671065646
google search	1.5836260411907606
potential ambiguity	1.5506112803072318
satisfaction problem	1.6569419851479923
good compromise	1.522760613339579
concept definitions	1.5088517809286333
u n	1.6221553332381422
class label	1.852389710033
relational data	1.734095923987765
knowledge within	1.507007268326853
common subsequence	1.5896755631157196
tree must	1.7345949146852133
knowledge sources	2.2413183503759724
special way	1.6125713976431844
relevant examples	1.5111306094308554
x z	1.5683356946968088
bottom row	1.5627352601669195
annotation work	1.5623707818060826
handle multiple	1.678973586324036
sensitive languages	1.6723088587449553
variable bindings	1.6517365380108489
different position	1.5611231576018243
system behavior	1.653691045130907
three questions	1.6679479902095415
phonetic transcription	1.7517273533579163
rule may	1.9218639733727219
programming technique	1.646226844257487
alone cannot	1.6845577236692315
grover et	1.654101387421287
chart parsing	1.999627568345834
among constituents	1.561123157601824
human scores	1.5762828210223647
english tasks	1.5306061495832655
learning algorithms	2.200151940064484
nonterminal symbol	1.8092520814664914
word strings	1.8656091100163374
mexico state	1.7525648008925518
section reviews	1.5037367200448983
trees may	1.750187272843137
several points	1.7158821441137806
complex type	1.510017014423469
short ones	1.5771944393968735
semantic object	1.6560269348513226
cannot use	1.6035443768923663
reciprocal rank	1.647475728495623
uppercase letters	1.592620200341168
manually sense	1.549307420869995
grammatical construction	1.515820533860577
rich source	1.7788903461436698
takes advantage	2.0742518387134687
clear cut	1.6031224303496519
average precision	1.9976022077467708
without explicit	1.8224278217877456
generate possible	1.5338735539822874
partial credit	1.6930079127626927
problem caused	1.6002993454155385
particular needs	1.5338735539822874
second paragraph	1.5256428192011384
corresponding words	1.7871717676014156
word like	1.8842194650363984
comparing results	1.583051127543801
coherent discourse	1.7759919743665777
cognitive psychology	1.6649693445712368
discourse features	1.6252686703545647
embedded clauses	1.8172769320031987
system involves	1.5445134575869865
three languages	1.8491290526208461
basic word	1.6982525270878095
system got	1.5037367200448983
inference mechanism	1.7234028666563055
adjective phrase	1.7750616521067617
research efforts	1.951275052501272
results discussed	1.5582958710952315
items within	1.579873144038688
like figure	1.6566290674162105
template fills	1.5078797213408792
every time	1.7542429402804494
machine learning	2.4266525745575045
examine two	1.5645241345053709
general features	1.639484236700259
plausible interpretation	1.6212994670841538
promising results	2.055858451559209
j j	1.7389869865698824
induction system	1.521614825883617
model probability	1.7479852333503536
discourse grammar	1.504833669347171
hoc analysis	1.5131301253834533
word problem	1.658402486102255
whose score	1.5821179703326838
test words	1.6940355468871653
present research	1.6894837465944075
dialogue history	1.7127912683046307
process begins	1.797413685506606
result suggests	1.7408279821914405
similar languages	1.5381766791708387
syntactic head	1.8414933466271772
additional semantic	1.7528356299631067
sense may	1.5706734993195162
briefly outline	1.5547189562170496
complication arises	1.5111306094308554
one condition	1.5798548254880234
identifying relevant	1.6285930450163058
general grammar	1.684893702680196
good thing	1.5156323250636554
phrase containing	1.7595946766966697
parameter set	1.7002604588036023
particular sense	1.82504458065598
verb requires	1.510181823566108
discourse entity	1.7974720703532556
section 2	2.6942319061898012
computer programs	1.5625587867906856
entire list	1.5547189562170496
individual sentences	1.9144614652618612
words around	1.79079318381344
approach attempts	1.5511943810330642
general setting	1.5482870987025659
actual value	1.5514607572627885
two level	1.6197369406879945
minimum cost	1.6046514812515742
global features	1.6519370667865956
new grammar	1.8448640296219685
specific features	1.9789091959616143
system need	1.5188051370135929
stemming algorithm	1.5870950865396134
check whether	2.1222140932330475
compound words	1.914541160501536
takes values	1.5459182428709468
system first	1.8912350007691765
single unit	1.8393441360306588
human dialogues	1.682719267615911
brief sketch	1.5637776281819353
second observation	1.5378646275607575
highest frequency	1.7311413532228244
including sentence	1.5482870987025659
sense tag	1.5758848349277779
strategy adopted	1.5771944393968738
two structures	1.9193889839409972
right corner	1.6569951345867324
multiple readings	1.5037367200448983
copy without	1.925120091448104
transfer lexicon	1.5144165808991479
testing results	1.522760613339579
unseen text	1.6370816583498602
word type	1.7325804002982288
every tree	1.6410226696990478
penn chinese	1.6643979505205362
third kind	1.5645241345053709
main problems	1.8785595436329492
jing et	1.6059888577271781
noun taxonomy	1.521798762287671
whose function	1.6645407184541139
bresnan 1982	1.6811764941013194
2 contains	1.8723262728637915
1 denotes	1.5926202003411678
recorded speech	1.683905764192819
using p	1.6255631748207642
structure may	1.9222746933284982
english examples	1.5839427016111838
specific entity	1.6169635240879605
verbs appear	1.6084968011212868
partial tree	1.5919153125441847
per input	1.567922330580758
single clause	1.6501780242713362
whose arguments	1.6832100598491304
input vectors	1.5757613876241834
markov process	1.6728198403419337
several paths	1.530072891325321
every language	1.76647029757056
many times	1.8981631269383263
new object	1.671227059795756
another rule	1.8147963404354717
e must	1.561123157601824
conference registration	1.5015752148233952
sproat et	1.6140830255800644
argument relation	1.6262025659080148
tagging performance	1.5896049380052686
program uses	1.699626140637926
different nlp	1.564750541543679
sentences according	1.8075789987419428
earlier stage	1.6246289167870047
algorithm looks	1.6289349127929256
impose constraints	1.6570039092175395
several sentences	1.9079723855824557
read speech	1.7465708734993957
tables 6	1.584483801902334
following dialogue	1.6719189153325114
coordinate structure	1.6325593702767922
meaning postulates	1.672719655273481
research problems	1.5245348740884508
best lists	1.8023175333112091
data including	1.5111306094308554
module takes	1.6953705367010592
relation may	1.8089456211558894
model performs	1.8972441009765768
given corpus	1.9531877092882433
good performance	1.7387678128612039
knowledge encoded	1.6875904224582725
ith word	1.7052344734492981
text corpus	2.1102589602162496
lexical entries	2.3008598883612974
compute probabilities	1.530072891325321
much use	1.6315901311540408
entire sequence	1.6266551395241793
case involves	1.526993068994652
pcfg model	1.5528698076567191
dominance relation	1.6633791594961387
different words	2.145288399444892
additional set	1.6862623517634696
every aspect	1.50119175880199
corpus 2	1.5865191587719019
current token	1.5839149627506484
system displays	1.6285809665614899
wsd systems	1.74544304533168
several studies	1.8796021849732587
independent set	1.582117970332684
four times	1.8217464629439601
coordinating conjunctions	1.7000280191396475
per question	1.532780034969733
ensemble des	1.545648148496062
anaphoric expressions	1.8669186861532618
categories would	1.530072891325321
graph g	1.722237427666678
robust parsing	1.8638264133045526
complete translation	1.5257027020067777
relative strengths	1.513130125383453
dictionary lookup	1.8788782117807206
logical form	2.131789727986776
based discourse	1.5823790519757677
upper right	1.7170147407965204
synthetic speech	1.682671445235844
particular grammar	1.8161281669476859
errors involving	1.5731122769149348
greedy search	1.769409857064543
entire phrase	1.6448472409336696
major constituent	1.513130125383453
category set	1.5230862154213907
existing ones	1.8673187016577948
important source	1.7178002527269736
rule types	1.6581877218837529
parsing may	1.5977445137418091
new symbol	1.5035193201428414
viterbi decoding	1.6896262977693461
phrases without	1.6137947517368634
major parts	1.646226844257487
approach cannot	1.636252690439584
draw upon	1.6023597487734618
modal operators	1.6067731970983814
preliminary study	1.750842451665186
good way	1.5150522556668098
broadcast news	1.9045048232524517
useful discussions	1.7897207708399192
global maximum	1.5829912568215723
outside probability	1.5187873255017093
similarity measure	2.1372258132099096
head word	2.090054383929547
exceptional case	1.5078976439458078
overall discourse	1.5906723352433731
overall results	1.933407966605018
input language	1.7468029224714094
somewhat simpler	1.5111306094308554
language theory	1.6988762411798246
corresponding concept	1.6078422675944997
three modules	1.832510149032969
dashed lines	1.7836561414858965
structures based	1.530072891325321
much detail	1.6213967902634534
different events	1.592616376335582
newswire texts	1.5561865963877775
sentence elements	1.5707237269685683
domain expert	1.7648590653914222
different runs	1.6190328102134979
grammatical properties	1.6830657171447783
preceding section	1.8732648617684167
process terminates	1.6631574457448046
c j	1.5617071846474357
two directions	1.5299900915786304
reading comprehension	1.55092714775953
original english	1.7967941368608327
multiple tags	1.5399822486951469
quantitative measures	1.513130125383453
frequency less	1.5266242261615062
class name	1.6550962102827325
significant improvements	2.0191819069699948
dictionaries used	1.5973170078864487
lexical category	1.9240100318125053
distinction among	1.6266551395241795
annotation process	1.8538194966016066
language terms	1.524429465603113
saw mary	1.519857682604814
stress pattern	1.5608938716604346
slot filler	1.6267135847298793
horizontal axis	1.7540428827033379
particular object	1.7174451438973322
compound noun	1.7657445552322455
ir task	1.5315256267744481
model considers	1.6235405672398666
semantic ambiguity	1.8330371969586094
third argument	1.6741875611759776
projective dependency	1.5925333455641386
sentence according	1.7950989193179039
wide variety	1.9251907374557122
corresponding translation	1.6047332098418523
multiple references	1.6558508196205624
information across	1.688641176416493
sun sparcstation	1.648787373441388
slots may	1.522760613339579
lexical insertion	1.544656228352609
phrase within	1.522760613339579
output using	1.5320698902069676
default rule	1.7421611184188976
text retrieval	2.0051932101862215
correct disambiguation	1.5009595257430632
system tries	1.8639456979323028
several domains	1.6950524701636023
given sample	1.5288687562201928
one character	1.9084971415803513
accurate analysis	1.5582958710952315
expression corpus	1.526993068994652
corpus includes	1.7151558643929241
probability distributions	2.1094544271181532
important aspect	2.041097141847228
single words	2.1372982405618615
third approach	1.7460252717662352
model thus	1.530072891325321
improving accuracy	1.524534874088451
tagging system	1.8387454446885036
semantic domains	1.6084623147796178
precision figures	1.5476987392238875
features according	1.6164339756999313
approach aims	1.5300728913253212
svm classifiers	1.5833106029796844
next example	1.8125176997277848
frequency counts	2.0305193759520552
possible sets	1.507007268326853
new sentences	1.7937464706827815
corresponding element	1.5321810481204974
funding agencies	1.522760613339579
relative error	1.5898853948139446
highest score	2.090918067852099
problem involves	1.5806234267011035
language side	1.544816178928851
task performed	1.5445134575869865
phrase consists	1.6320354670147565
sample text	1.8293993892993479
infrequent words	1.689342431937988
surface case	1.544928982081788
linguistic representation	1.8848083551245254
complex objects	1.5521452212545803
notational convention	1.5622818522186857
another example	2.1012975106845992
many decisions	1.5188051370135929
network using	1.5188051370135929
level constituents	1.5996348445739617
acyclic graphs	1.7538341131741773
use local	1.567922330580758
experimental conditions	1.778189867125665
also briefly	1.6164339756999313
rule b	1.5503658814390868
demonstrative pronoun	1.6252686703545647
root words	1.636724045699399
overall time	1.510181823566108
summarization methods	1.5165319983042167
asr system	1.5801456007640695
current frame	1.5321810481204972
different pattern	1.5445134575869865
governing verb	1.6468865036157796
morphological feature	1.5880288594247598
select documents	1.5078976439458076
three processes	1.524534874088451
similar example	1.6990441916139194
many places	1.524534874088451
differences among	1.9709929357091696
4 table	1.5188051370135929
different thresholds	1.6645330200623705
conventional dictionary	1.5011414167674753
various values	1.6445809590877876
output sentence	1.8038601618902572
level using	1.6861938694548688
semantic interface	1.506018211025399
classification problems	1.8123446319821452
preferred reading	1.7452467917940717
takes precedence	1.5839427016111838
hold true	1.5482870987025659
common case	1.672884134739408
different approaches	2.181611124387362
complete text	1.6084968011212868
length l	1.6566456221656103
large texts	1.5261764002574725
input length	1.5250665147854683
ambiguity occurs	1.579873144038688
resulting translation	1.5771944393968738
different choices	1.7255890150855713
let g	1.9012497716114265
formal aspects	1.5645241345053709
automatic detection	1.8109990228043924
expressions containing	1.5676517392350746
method 2	1.619895865537991
input format	1.645531244915114
representation languages	1.7737763419601573
constraints associated	1.7331815484820083
nodes contain	1.543442208482524
first discuss	1.8463589810990293
current information	1.703359686465308
probabilistic context	1.9636333087849382
position k	1.5321552374951573
many knowledge	1.567922330580758
provide evidence	1.9106079280460433
feature f	1.8897291263097913
use multiple	1.7074092050419734
possibility would	1.6570039092175395
morphological processor	1.5360026503169557
structural properties	1.7796854321435898
trace back	1.5037367200448983
words without	1.8942208025525173
language strings	1.5269930689946518
based language	1.9811618422952235
different domain	1.8982478935642846
property p	1.630837428117134
language string	1.6415855085304663
possible actions	1.6778478586578416
information needs	1.8667815316244003
issues associated	1.526993068994652
different concept	1.543442208482524
supervised word	1.530320126517967
dashed arrows	1.5395099438048587
discourse understanding	1.649252627111982
creation process	1.5217605875570064
e p	1.7538042058894365
key phrases	1.5396324758642346
specific constructions	1.5381766791708387
linguistic expression	1.8328082331734292
information obtained	1.899995865797707
qualitative analysis	1.6647965563340175
possible argument	1.5547189562170496
make significant	1.526993068994652
preceding utterances	1.5296068202902637
language like	1.9214818083851102
complex processing	1.507007268326853
different tags	1.7308451372786295
2 data	1.6207783658263315
users need	1.6169986020906966
forward application	1.530504624629937
corpus could	1.7051629942513449
place name	1.6385400598448192
context analysis	1.5466472792181065
least number	1.6229811029106591
absolute improvement	1.6034321301038696
set size	1.9564885083482981
score threshold	1.5036575034310684
particular sentences	1.5582958710952313
scope ambiguity	1.6245038604979847
new domains	2.0800982224082154
evaluation task	1.7640791934505464
recognition process	1.9257236996439273
possible approach	1.7486023401067097
previous work	2.46248969524292
domain ontologies	1.5224308685587151
edr corpus	1.6018111767218126
semantic similarities	1.586879687256495
linguistics research	1.6981914981631419
central importance	1.5581939760955252
main tasks	1.729614000806837
different translation	1.7819011289140605
two participants	1.802520038719248
crucial importance	1.65027618229265
text file	1.7683184290471212
initial values	1.7122003863637203
level 3	1.6243420381472053
practical problems	1.7504644607315647
analysis also	1.7261608475873556
commercial systems	1.68948640867548
fair comparison	1.883180409802834
language technologies	1.7949565795730698
cannot resolve	1.507007268326853
automatic classification	1.785526439684209
acceptable translations	1.5894574345512695
times larger	1.7425964505644491
proposed method	2.107864151575564
actual word	1.7551383241328506
specific feature	1.6707729740772388
effective approach	1.6881707359732032
present approach	1.7787927179774492
work represents	1.5408560126058508
parser always	1.522760613339579
intransitive verb	1.9046465176347676
declarative sentence	1.8017051414996827
user defined	1.5171904119543882
imposes constraints	1.6112034493728657
network architecture	1.5177700906840073
candidate answers	1.652800128755512
standard linguistic	1.6481174069026685
two items	1.8916371845129123
precise characterization	1.5188051370135929
operations performed	1.5908167675916909
also model	1.507007268326853
web using	1.6320354670147565
english treebank	1.5613297824665775
language access	1.5980728991023367
c u	1.5382060927774193
upper bounds	1.77066761130575
reasoning system	1.7044914805734135
higher threshold	1.5511943810330642
obtained results	1.7377965645808189
first generate	1.5269930689946518
small example	1.6002993454155385
text may	1.9212365025670606
spoken utterance	1.6828225421438074
correct transcription	1.5140548915918028
relations extracted	1.6112214892443586
pattern may	1.6439164934982253
cost function	1.7767820772876575
state model	1.6684371938870253
two resources	1.5699768329764012
major problem	1.7712365572899047
empty category	1.6141192554679487
practical cases	1.5078976439458076
entity extraction	1.8170369849746637
path p	1.640310304432606
sections 1	1.636252690439584
studies using	1.597317007886449
function defined	1.6921138523491286
shows several	1.611203449372866
also function	1.6056035510036821
lexical head	1.931676012218274
grammars written	1.672884134739408
representation must	1.7812020673584557
features described	1.9381853483218838
single domain	1.63365241090954
latter may	1.643362624369706
journal texts	1.5408560126058508
null tional	1.5111306094308554
automatically construct	1.672245431695917
word processing	1.8061041816362697
single concept	1.7060406711249585
con dence	1.6325605368463072
used features	1.567922330580758
dialogue management	1.8218920333977369
ordinary language	1.510181823566108
frequency threshold	1.746329232299106
significantly reduces	1.7392306202979373
passive sentence	1.6350313088824244
final point	1.6802940634668402
manual review	1.5110975570289593
important criterion	1.5037367200448983
significant challenge	1.526993068994652
recall performance	1.573967658566826
information rather	1.5645241345053709
using shallow	1.530072891325321
experimental data	1.8296302062942507
practical systems	1.703338232318599
two requirements	1.5571495092265906
similar problems	1.841200695411781
viable approach	1.5482870987025659
task based	1.5371982986173842
stochastic process	1.6953058648112405
example suggests	1.5408560126058508
improvement due	1.5739592165010823
oriented approaches	1.5354510398033074
specified number	1.6002993454155385
single attribute	1.592620200341168
proposed framework	1.6523222612561272
words marked	1.5483353340489396
insertion errors	1.5827722622100096
define three	1.6565475758379593
various constraints	1.6822401779866478
access system	1.539982248695147
random sample	1.982959077170092
two analyses	1.754506916371315
let w	1.8539867171807975
proper interpretation	1.5188051370135929
repeated application	1.5582958710952315
2 discusses	1.8191665297540134
research community	2.0537615296660094
closer look	1.9327274965064511
grammatical roles	1.7651638894999986
transition network	1.931424422849108
algorithms based	1.8162486749821076
single translation	1.5706734993195162
different contextual	1.5177700906840073
related sentences	1.645106579923286
various concepts	1.507007268326853
possible transitions	1.530072891325321
particular domains	1.6961394033344752
functional roles	1.6678072131354984
achieves better	1.734954551774483
enough training	1.7962019045157949
two conjuncts	1.5906776650800025
entity recognition	2.151780382921464
7 show	1.6783930166760777
automatic thesaurus	1.5422419908714167
test bed	1.7136112519789526
classes based	1.7747770008352457
performs worse	1.7961446965684997
event structures	1.633834155706024
clark et	1.5117352980956709
word boundary	1.922037639765384
single element	1.766134102534036
always contain	1.522760613339579
similarity value	1.8096997142740474
sections 15	1.5172389229238532
following scheme	1.5582958710952315
word alignment	2.034342995743704
used n	1.5511943810330642
state sequences	1.5431734562852524
information corresponding	1.5839427016111838
10 sentences	1.6760269815500823
similar cases	1.6256278638449597
van den	1.7775332095819754
specific corpora	1.5872653572425959
considerable variation	1.5328333954091753
another class	1.8292009407010608
sense ambiguities	1.5559715171228279
three systems	1.9075395077809174
speaker adaptation	1.6420406848266702
system modules	1.634964699301337
case occurs	1.5943267639908942
information loss	1.5738708991826256
one segment	1.7595874308748354
1 indicates	1.8072825657183405
different text	1.8756910650024319
model defines	1.5611231576018243
given character	1.5118192695694388
ordered pairs	1.7411930170691616
one output	1.6985905455568948
cue words	1.7785210305974861
international workshop	1.9231406149890145
lowest node	1.5656419863875413
explicitly state	1.5611231576018243
person names	1.9278194274942195
valuable information	1.8305472758233803
formal semantics	1.9097721356930641
terms found	1.5521452212545803
algorithm involves	1.5547189562170496
one dictionary	1.5647847313011076
vertical bar	1.6190975549190842
incorrect words	1.506173888464562
contain less	1.5511943810330642
languages without	1.6853874840250063
approach fails	1.55955788508706
sequence w	1.8148370026547094
results provide	1.7141428077458374
body parts	1.6283489053598212
two relations	1.8336899110428218
syntactic forms	1.8164805425654504
requested information	1.6140406995754497
conditional maximum	1.525702702006778
document summary	1.5472090326638352
initial work	1.7531599391786739
deictic expressions	1.5517755001545184
result using	1.632350904386585
final output	1.7608101011174562
good knowledge	1.5111306094308554
adjectival phrases	1.6117266025648125
processing within	1.5440207036648976
alternative formulations	1.5111306094308554
language documents	1.6234672043956777
communicative goal	1.826820698805251
three kinds	2.132355129743829
following kinds	1.6570039092175395
atis corpus	1.7816626577182235
linguistic work	1.6699377361934178
possible answer	1.7201741786343798
current question	1.5082012992856186
varies widely	1.659662148122272
degrades performance	1.5188051370135929
gaussian mixture	1.505953137363071
certain structures	1.5378646275607575
distinct types	1.771679799419203
role assignments	1.6416838320933163
fifth sighan	1.65027618229265
performs significantly	1.8124237782167478
primary function	1.5798548254880234
traditional methods	1.6977890880751751
closely resembles	1.7471845324497086
general way	1.5114357332105497
paragraph boundaries	1.6697252789930348
general vocabulary	1.5253423342469956
grammar rules	2.325914474176199
morphological system	1.57425918739941
simple implementation	1.579873144038688
finite verbs	1.7723534279145714
quantified noun	1.5890815344405405
linguistic concepts	1.633108977455518
official evaluation	1.570430085810516
speech categories	1.757521608454697
chinese characters	2.015152144676855
separate rule	1.5771944393968735
dice coefficient	1.6064888643966242
improvements could	1.5511943810330642
selected word	1.6231636014612831
mean either	1.60913070763407
new vocabulary	1.6630062643878185
kingsbury et	1.5211629864223772
right manner	1.609217242875463
measure performance	1.6799182793359324
total sum	1.504934031784749
words one	1.602359748773462
several times	1.530487961188286
language texts	2.030181167666746
five iterations	1.5149639400339463
spanning tree	1.5971843003845083
structure according	1.5269930689946518
data representation	1.7353383545920011
tree may	1.8048819793127076
rich information	1.6010101084822783
later work	1.7650139064090151
analysis phase	1.8261373767911946
smallest number	1.6631574457448046
word recognizer	1.5620113386492556
difficult problem	1.8635214951605477
system processes	1.7014225922673947
set must	1.608191142133971
many aspects	1.9276870456518873
phonological level	1.510060731399269
rule makes	1.522760613339579
branching structure	1.7484868478424942
algorithms could	1.522760613339579
main types	1.920471327589695
unlimited number	1.626990247505205
f1 score	1.6385344585934378
specific vocabulary	1.6802580409155776
specific components	1.5637776281819353
linguistic notion	1.5612815851895034
geometric mean	1.7999632069111708
methods based	1.9747534932294495
lookup table	1.5214705084632532
optimal parameters	1.5054226276451934
based similarity	1.7521669441945047
single text	1.7309209164744939
small corpus	1.99521919139974
small constant	1.636252690439584
greatest number	1.6012319720994657
theoretical approach	1.6071185545190714
sentences found	1.5582958710952313
transfer rule	1.5804734081275431
one word	2.3798318760340296
paper considers	1.65303397348486
binary relation	1.921934213672465
possible parts	1.6810948227334366
typographical errors	1.585280499601101
state corresponds	1.510243608460478
include features	1.7125697263999058
problem occurs	1.7088092136999036
grammar includes	1.5798548254880234
study reported	1.592620200341168
speech translation	1.9329322352440872
general system	1.613364934425665
let r	1.8743953780786402
direct reference	1.5219007618281093
agreement information	1.5307872641429805
individual characters	1.6828225421438074
incorrect answers	1.64614664139343
que la	1.6424085836824824
linguistic theory	2.1724905364605482
null tions	1.776193699643853
grammar might	1.6320354670147568
humans use	1.5338735539822874
technical term	1.5732570275527316
np subject	1.5303201265179671
let q	1.6530753578877089
additional analysis	1.530072891325321
final word	1.7153048211379445
situational context	1.566582672988718
search problem	1.8003035537676262
dependency structure	2.02654449880652
implementation using	1.507007268326853
adverbial clauses	1.5371982986173842
correct recognition	1.633034028428662
type 1	1.6914489282294998
adding words	1.5445134575869865
linguistic terms	1.6734620396688837
nlp tasks	2.134638811278883
procedure would	1.7455319583030278
small class	1.5514514275224365
user knows	1.6787092555328271
adjective phrases	1.6097881142640582
contain relevant	1.522760613339579
rules within	1.6375625045004876
acquiring information	1.5482870987025659
main predicate	1.7229939326802977
aligned pairs	1.5507886996118012
grammars provide	1.522760613339579
sentence parsing	1.7615708919779496
necessary knowledge	1.5269930689946518
parser developed	1.7201741786343798
analysis used	1.6031224303496519
tasks like	1.7796583190082031
framework described	1.6770235151848842
problem concerns	1.586137214651611
native speaker	1.9994288776754896
parser presented	1.5378646275607575
language program	1.5894574345512698
specific system	1.6433885100347294
wsj data	1.615557466585002
nouns based	1.516491127995908
word errors	1.6257489036975992
classification decision	1.5519690017139713
limited memory	1.500329875064992
briefly reviewed	1.5547189562170496
important implications	1.6339957705783554
inflectional suffixes	1.5948693379005436
three rows	1.5698733514826038
motion verbs	1.579766275535406
corresponding context	1.6010101084822783
context rather	1.5111306094308554
simple enough	1.7344099527418115
whose performance	1.586137214651611
transfer dictionary	1.5813786432833834
knowledge acquired	1.5738708991826256
growing body	1.5445134575869865
conjoined noun	1.5936481309260868
document pairs	1.5612350999965177
particular way	1.8083721100016197
results showing	1.5781539692435584
two nodes	2.1006296145848498
several heuristics	1.6939639871030243
typed input	1.5936506658968468
yarowsky 1992	1.5769434287295148
also incorporates	1.6960474084795663
two purposes	1.7455319583030278
markov assumption	1.6946710621309762
gain ratio	1.5993823808600023
scaling algorithm	1.5565549888082182
length n	2.0088804604207477
definite articles	1.5892444095206755
irregular verbs	1.608852218159841
less time	1.8565999308243935
passage retrieval	1.5818013974225582
morphological characteristics	1.5676517392350746
language questions	1.6750143133292412
limited size	1.7292467370219826
discriminate among	1.6883139709498902
processing steps	1.9420461217527214
morpheme boundary	1.6049770903179505
complement clause	1.6373215606459914
negative log	1.7310995500600885
meaning representation	2.024473772140774
small sample	1.9358462369202443
structure makes	1.583051127543801
total frequency	1.749225968725275
low agreement	1.5825577359308496
corpus comprises	1.613624660358136
systems attempt	1.522760613339579
basic sentence	1.541748679610567
methods employed	1.530072891325321
based tagger	1.7632110843994984
content selection	1.7400581324842352
state sequence	1.805455422252594
search algorithm	2.0151214644303
structures defined	1.6053932208928947
feature combinations	1.8101868510021784
efficient processing	1.8715335627228111
step 4	2.0305920853753037
performance may	1.762033687546193
dotted line	1.8509634171455036
understanding model	1.5538017058661813
estimate based	1.5482870987025659
e may	1.5156323250636554
certain patterns	1.594773126307433
n c	1.6454898170583583
operations required	1.627796791624738
module must	1.6003082154543704
system encounters	1.5707376783218978
sur un	1.5322342087189593
similar tasks	1.6056035510036821
head category	1.5154654281287878
difficult part	1.5645241345053709
consistent way	1.6881707359732032
expected frequency	1.5609976850705434
grammar could	1.8056266150218963
average recall	1.7501400012658674
queries based	1.530072891325321
third problem	1.6761656524213184
possible antecedent	1.7206285652891589
whose components	1.579873144038688
objects involved	1.6581363223870689
worse performance	1.7461343450093207
various lengths	1.5926202003411678
5000 words	1.585589015734039
one chooses	1.5440207036648976
formal evaluation	1.786047789158116
algorithm tries	1.6761656524213182
deerwester et	1.7533909808430497
translation must	1.513130125383453
several places	1.5412463844444775
categorization task	1.6008486386649374
source node	1.5478448355443346
document understanding	1.7078185423375856
parsing module	1.6340273500061624
procedure could	1.6246289167870047
human judgments	1.8003215067240073
remaining rules	1.526993068994652
ambiguous input	1.6229108409074118
generation model	1.6383720469609018
proper subset	1.7840870297476317
processing tools	1.7577567031029648
system maintains	1.6137947517368632
structure described	1.6339957705783554
intermediate step	1.6961394033344752
position 0	1.618103745190334
statistical knowledge	1.6484851239377274
manual work	1.6655102285632384
nominal arguments	1.5225304125061803
system analyzes	1.5839427016111838
better capture	1.5511943810330642
used feature	1.50119175880199
intuitive notion	1.6128408409726123
see sec	1.559620194736931
database search	1.5258093094263683
cluster together	1.5749574214167732
japanese thesaurus	1.517094492681228
results show	2.3145039340448488
given segment	1.5669629240805358
smoothing technique	1.743651523063082
text samples	1.6784089454399802
users tend	1.530072891325321
k nearest	1.6720665303659392
like noun	1.5771944393968738
time spoken	1.504934031784749
language processing	2.674247687105666
test suites	1.552298147660753
discuss one	1.6593965399315964
columns represent	1.6615677176372698
automatically extracts	1.6210934563561632
different versions	1.9178902536832736
shows three	1.6724173909604771
text output	1.5226149809040495
reduced set	1.6411558246537032
induction algorithm	1.7020591600066624
higher degree	1.8626248918258574
absolute number	1.615262062995654
additional syntactic	1.6802580409155776
complex systems	1.6933483923290389
new edges	1.662464914162065
fewer errors	1.6525698527725523
third type	1.8929728383208502
several knowledge	1.6469586338689493
based mechanism	1.5188051370135929
list containing	1.7517242416267398
factual knowledge	1.5010363518452905
plural pronouns	1.537389468626839
strategies based	1.570430085810516
task requires	1.8151814695189707
driven approaches	1.6104618331127913
resolving ambiguity	1.6266551395241795
semantic knowledge	2.1112498702619646
action part	1.517539519417869
information associated	2.029556687015729
figure 6	2.440633492886535
one limitation	1.5887468924062063
precision p	1.6246289167870047
specific characteristics	1.5996348445739619
one percent	1.5346168194755097
defense advanced	2.07543120221222
information consists	1.6056035510036821
generated output	1.5461586260253581
speci c	1.9384102587846896
automatic process	1.713540519833073
speaking style	1.559479142283194
closed test	1.6109519029251624
different tests	1.5131301253834533
simulated annealing	1.634172678741571
like e	1.6246289167870047
wide web	2.0236098144752304
set p	1.7250059325289542
recent papers	1.6661204761131605
cannot produce	1.6881707359732032
necessary conditions	1.6804257391228377
actual parsing	1.5547189562170496
lexical co	1.6802508256771125
early stage	1.5645066618629546
people involved	1.5887468924062063
significant degree	1.5111306094308554
location information	1.5807883816225794
sense numbers	1.526001002341818
free backbone	1.646555068006574
word contexts	1.505939753278791
yarowsky et	1.5514607572627888
muc evaluations	1.5456364232853632
exact nature	1.674180327049904
extensive knowledge	1.5408560126058508
first attempts	1.6084968011212868
addresses two	1.5482870987025659
various attributes	1.5893265024752146
many attempts	1.5582958710952315
reference set	1.5660699831993166
sentence constituents	1.6676915250545765
contains 10	1.5547189562170496
comparable corpus	1.50733176736105
evaluation purposes	1.8631694515662238
different clusters	1.595307764398636
possible interpretation	1.7150595002164728
equation 6	1.5828009925736395
word corpus	2.008908968915578
relationships may	1.513130125383453
freer word	1.5479868463263637
evaluation scores	1.579636353040832
shows example	1.5188051370135929
europarl corpus	1.620246492690542
uses several	1.613624660358136
every level	1.5765551203290775
english speaker	1.5874189863574613
probable alignment	1.5173104105953445
column 3	1.6612361635833772
language dialogue	1.7558531284199328
length 5	1.551650782189995
also assigns	1.506173888464562
different speakers	1.8069517880796067
single tokens	1.5309731874553978
whole grammar	1.5500434040293412
test document	1.6560993274375209
words correspond	1.5188051370135929
linear time	1.959158230111203
additional lexical	1.6161849521165763
domain entities	1.5745133558211544
learning methods	2.121710918893746
anaphoric links	1.6082169556169195
rule name	1.561430576533386
english source	1.5310785803838152
approach performs	1.6875904224582725
experiments comparing	1.6339957705783554
sentence using	1.975811596370657
small grammars	1.540109050298748
four corpora	1.5350217077405839
nlp applications	2.1899477383463197
